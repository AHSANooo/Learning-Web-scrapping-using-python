title,authors,abstract
VIPeR: Visual Incremental Place Recognition with Adaptive Mining and Lifelong Learning,"Authors:Yuhang Ming,Minyang Xu,Xingrui Yang,Weicai Ye,Weihan Wang,Yong Peng,Weichen Dai,Wanzeng Kong","Abstract:…we propose a probabilistic knowledge distillation to explicitly safeguard the previously learned knowledge. We evaluate our proposed VIPeR on three large-scale datasets, namelyOxfordRobotcar, Nordland, and TartanAir. For comparison, we first set a baseline performance with naive finetuning. Then, several more recent lifelong learning methods are compared.…▽ MoreVisual place recognition (VPR) is an essential component of many autonomous and augmented/virtual reality systems. It enables the systems to robustly localize themselves in large-scale environments. Existing VPR methods demonstrate attractive performance at the cost of heavy pre-training and limited generalizability. When deployed in unseen environments, these methods exhibit significant performance drops. Targeting this issue, we present VIPeR, a novel approach for visual incremental place recognition with the ability to adapt to new environments while retaining the performance of previous environments. We first introduce an adaptive mining strategy that balances the performance within a single environment and the generalizability across multiple environments. Then, to prevent catastrophic forgetting in lifelong learning, we draw inspiration from human memory systems and design a novel memory bank for our VIPeR. Our memory bank contains a sensory memory, a working memory and a long-term memory, with the first two focusing on the current environment and the last one for all previously visited environments. Additionally, we propose a probabilistic knowledge distillation to explicitly safeguard the previously learned knowledge. We evaluate our proposed VIPeR on three large-scale datasets, namelyOxfordRobotcar, Nordland, and TartanAir. For comparison, we first set a baseline performance with naive finetuning. Then, several more recent lifelong learning methods are compared. Our VIPeR achieves better performance in almost all aspects with the biggest improvement of 13.65% in average performance.△ Less"
MSSPlace: Multi-Sensor Place Recognition with Visual and Text Semantics,"Authors:Alexander Melekhin,Dmitry Yudin,Ilia Petryashin,Vitaly Bezuglyj","Abstract:…comprehensive place descriptors. We employ a late fusion approach to integrate these modalities, providing a unified representation. Through extensive experiments on theOxfordRobotCar and NCLT datasets, we systematically analyze the impact of each data source on the overall quality of place descriptors. Our experiments demonstrate that combining data from…▽ MorePlace recognition is a challenging task in computer vision, crucial for enabling autonomous vehicles and robots to navigate previously visited environments. While significant progress has been made in learnable multimodal methods that combine onboard camera images and LiDAR point clouds, the full potential of these methods remains largely unexplored in localization applications. In this paper, we study the impact of leveraging a multi-camera setup and integrating diverse data sources for multimodal place recognition, incorporating explicit visual semantics and text descriptions. Our proposed method named MSSPlace utilizes images from multiple cameras, LiDAR point clouds, semantic segmentation masks, and text annotations to generate comprehensive place descriptors. We employ a late fusion approach to integrate these modalities, providing a unified representation. Through extensive experiments on theOxfordRobotCar and NCLT datasets, we systematically analyze the impact of each data source on the overall quality of place descriptors. Our experiments demonstrate that combining data from multiple sensors significantly improves place recognition model performance compared to single modality approaches and leads to state-of-the-art quality. We also show that separate usage of visual or textual semantics (which are more compact representations of sensory data) can achieve promising results in place recognition. The code for our method is publicly available: https://github.com/alexmelekhin/MSSPlace△ Less"
"The SHERLOCK pipeline: new exoplanet candidates in the WASP-16, HAT-P-27, HAT-P-26, and TOI-2411 systems","Authors:Martín Dévora-Pajares,Francisco J. Pozuelos,Antoine Thuillier,Mathilde Timmermans,Valérie Van Grootel,Victoria Bonidie,Luis Cerdeño Mota,Juan C. Suárez","Abstract:The launches of NASA Kepler and TESS missions have significantly enhanced the interest in the exoplanet field during the last 15 years, providing a vast amount of public data that is being exploited by the community thanks to the continuous development of new analysis tools. However, using these tools is not straightforward, and users must dive into different codes, input-output formats, and metho…▽ MoreThe launches of NASA Kepler and TESS missions have significantly enhanced the interest in the exoplanet field during the last 15 years, providing a vast amount of public data that is being exploited by the community thanks to the continuous development of new analysis tools. However, using these tools is not straightforward, and users must dive into different codes, input-output formats, and methodologies, hindering an efficient and robust exploration of the available data. We present the SHERLOCK pipeline, an end-to-end public software that allows the users to easily explore observations from space-based missions such as TESS or Kepler to recover known planets and candidates issued by the official pipelines and search for new planetary candidates that remained unnoticed. The pipeline incorporates all the steps to search for transit-like features, vet potential candidates, provide statistical validation, conduct a Bayesian fitting, and compute observational windows from ground-based observatories. Its performance is tested against a catalog of known and confirmed planets from the TESS mission, trying to recover the official TESS Objects of Interest (TOIs), explore the existence of companions that have been missed, and release them as new planetary candidates. SHERLOCK demonstrated an excellent performance, recovering 98% of the TOIs and confirmed planets in our test sample and finding new candidates. Specifically, we release four new planetary candidates around the systems WASP-16 (with P∼10.46 d and R∼2.20R⊕), HAT-P-27 (with P∼1.20 d and R∼4.33R⊕), HAT-P-26 (with P∼6.59 d and R∼1.97R⊕), and TOI-2411 (with P∼18.75 d and R∼2.88R⊕).△ Less"
Generative AI and the problem of existential risk,"Authors:Lynette Webb,Daniel Schönberger","Abstract:Ever since the launch of ChatGPT, Generative AI has been a focal point for concerns about AI's perceived existential risk. Once a niche topic in AI research and philosophy, AI safety and existential risk has now entered mainstream debate among policy makers and leading foundation models developers, much to the chagrin of those who see it as a distraction from addressing more pressing nearer-term h…▽ MoreEver since the launch of ChatGPT, Generative AI has been a focal point for concerns about AI's perceived existential risk. Once a niche topic in AI research and philosophy, AI safety and existential risk has now entered mainstream debate among policy makers and leading foundation models developers, much to the chagrin of those who see it as a distraction from addressing more pressing nearer-term harms. This chapter aims to demystify the debate by highlighting the key worries that underpin existential risk fears in relation to generative AI, and spotlighting the key actions that governments and industry are taking thus far to helping address them.△ Less"
"Generative Discrimination: What Happens When Generative AI Exhibits Bias, and What Can Be Done About It","Authors:Philipp Hacker,Brent Mittelstadt,Frederik Zuiderveen Borgesius,Sandra Wachter","Abstract:As generative Artificial Intelligence (genAI) technologies proliferate across sectors, they offer significant benefits but also risk exacerbating discrimination. This chapter explores how genAI intersects with non-discrimination laws, identifying shortcomings and suggesting improvements. It highlights two main types of discriminatory outputs: (i) demeaning and abusive content and (ii) subtler bias…▽ MoreAs generative Artificial Intelligence (genAI) technologies proliferate across sectors, they offer significant benefits but also risk exacerbating discrimination. This chapter explores how genAI intersects with non-discrimination laws, identifying shortcomings and suggesting improvements. It highlights two main types of discriminatory outputs: (i) demeaning and abusive content and (ii) subtler biases due to inadequate representation of protected groups, which may not be overtly discriminatory in individual cases but have cumulative discriminatory effects. For example, genAI systems may predominantly depict white men when asked for images of people in important jobs.
  This chapter examines these issues, categorizing problematic outputs into three legal categories: discriminatory content; harassment; and legally hard cases like unbalanced content, harmful stereotypes or misclassification. It argues for holding genAI providers and deployers liable for discriminatory outputs and highlights the inadequacy of traditional legal frameworks to address genAI-specific issues. The chapter suggests updating EU laws, including the AI Act, to mitigate biases in training and input data, mandating testing and auditing, and evolving legislation to enforce standards for bias mitigation and inclusivity as technology advances.△ Less"
From Principles to Rules: A Regulatory Approach for Frontier AI,"Authors:Jonas Schuett,Markus Anderljung,Alexis Carlier,Leonie Koessler,Ben Garfinkel","Abstract:Several jurisdictions are starting to regulate frontier artificial intelligence (AI) systems, i.e. general-purpose AI systems that match or exceed the capabilities present in the most advanced systems. To reduce risks from these systems, regulators may require frontier AI developers to adopt safety measures. The requirements could be formulated as high-level principles (e.g. 'AI systems should be…▽ MoreSeveral jurisdictions are starting to regulate frontier artificial intelligence (AI) systems, i.e. general-purpose AI systems that match or exceed the capabilities present in the most advanced systems. To reduce risks from these systems, regulators may require frontier AI developers to adopt safety measures. The requirements could be formulated as high-level principles (e.g. 'AI systems should be safe and secure') or specific rules (e.g. 'AI systems must be evaluated for dangerous model capabilities following the protocol set forth in...'). These regulatory approaches, known as 'principle-based' and 'rule-based' regulation, have complementary strengths and weaknesses. While specific rules provide more certainty and are easier to enforce, they can quickly become outdated and lead to box-ticking. Conversely, while high-level principles provide less certainty and are more costly to enforce, they are more adaptable and more appropriate in situations where the regulator is unsure exactly what behavior would best advance a given regulatory objective. However, rule-based and principle-based regulation are not binary options. Policymakers must choose a point on the spectrum between them, recognizing that the right level of specificity may vary between requirements and change over time. We recommend that policymakers should initially (1) mandate adherence to high-level principles for safe frontier AI development and deployment, (2) ensure that regulators closely oversee how developers comply with these principles, and (3) urgently build up regulatory capacity. Over time, the approach should likely become more rule-based. Our recommendations are based on a number of assumptions, including (A) risks from frontier AI systems are poorly understood and rapidly evolving, (B) many safety practices are still nascent, and (C) frontier AI developers are best placed to innovate on safety practices.△ Less"
CTRL-F: Pairing Convolution with Transformer for Image Classification via Multi-Level Feature Cross-Attention and Representation Learning Fusion,"Authors:Hosam S. EL-Assiouti,Hadeer El-Saadawy,Maryam N. Al-Berry,Mohamed F. Tolba","Abstract:…whether trained from scratch on large data or even with low-data regime. For Instance, CTRL-F achieves top-1 accuracy of 82.24% and 99.91% when trained from scratch onOxford-102 Flowers and PlantVillage datasets respectively, surpassing state-of-the-art models which showcase the robustness of our model on image classification tasks. Code at: https://github…▽ MoreTransformers have captured growing attention in computer vision, thanks to its large capacity and global processing capabilities. However, transformers are data hungry, and their ability to generalize is constrained compared to Convolutional Neural Networks (ConvNets), especially when trained with limited data due to the absence of the built-in spatial inductive biases present in ConvNets. In this paper, we strive to optimally combine the strengths of both convolution and transformers for image classification tasks. Towards this end, we present a novel lightweight hybrid network that pairs Convolution with Transformers via Representation Learning Fusion and Multi-Level Feature Cross-Attention named CTRL-F. Our network comprises a convolution branch and a novel transformer module named multi-level feature cross-attention (MFCA). The MFCA module operates on multi-level feature representations obtained at different convolution stages. It processes small patch tokens and large patch tokens extracted from these multi-level feature representations via two separate transformer branches, where both branches communicate and exchange knowledge through cross-attention mechanism. We fuse the local responses acquired from the convolution path with the global responses acquired from the MFCA module using novel representation fusion techniques dubbed adaptive knowledge fusion (AKF) and collaborative knowledge fusion (CKF). Experiments demonstrate that our CTRL-F variants achieve state-of-the-art performance, whether trained from scratch on large data or even with low-data regime. For Instance, CTRL-F achieves top-1 accuracy of 82.24% and 99.91% when trained from scratch onOxford-102 Flowers and PlantVillage datasets respectively, surpassing state-of-the-art models which showcase the robustness of our model on image classification tasks. Code at: https://github.com/hosamsherif/CTRL-F△ Less"
PDiscoFormer: Relaxing Part Discovery Constraints with Vision Transformers,"Authors:Ananthu Aniraj,Cassio F. Dantas,Dino Ienco,Diego Marcos","Abstract:…multiple connected components of any size, substantially outperforms previous work. We test our approach on three fine-grained classification benchmarks: CUB, PartImageNet andOxfordFlowers, and compare our results to previously published methods as well as a re-implementation of the state-of-the-art method PDiscoNet with a transformer-based backbone. We co…▽ MoreComputer vision methods that explicitly detect object parts and reason on them are a step towards inherently interpretable models. Existing approaches that perform part discovery driven by a fine-grained classification task make very restrictive assumptions on the geometric properties of the discovered parts; they should be small and compact. Although this prior is useful in some cases, in this paper we show that pre-trained transformer-based vision models, such as self-supervised DINOv2 ViT, enable the relaxation of these constraints. In particular, we find that a total variation (TV) prior, which allows for multiple connected components of any size, substantially outperforms previous work. We test our approach on three fine-grained classification benchmarks: CUB, PartImageNet andOxfordFlowers, and compare our results to previously published methods as well as a re-implementation of the state-of-the-art method PDiscoNet with a transformer-based backbone. We consistently obtain substantial improvements across the board, both on part discovery metrics and the downstream classification task, showing that the strong inductive biases in self-supervised ViT models require to rethink the geometric priors that can be used for unsupervised part discovery.△ Less"
Monopole Excitation and Nuclear Compressibility: Present and Future Perspectives,"Authors:J. C. Zamora,S. Giraud","Abstract:Isoscalar giant resonances are nuclear collective excitations associated with the oscillation in phase of protons and neutrons according to a certain multipolarityL. In particular, the isoscalar giant monopole resonance (L=0) is the strongest nuclear compression mode, and its excitation energy is directly related to the compression modulus for finite nuclei. Typically, microscopic calculation…▽ MoreIsoscalar giant resonances are nuclear collective excitations associated with the oscillation in phase of protons and neutrons according to a certain multipolarityL. In particular, the isoscalar giant monopole resonance (L=0) is the strongest nuclear compression mode, and its excitation energy is directly related to the compression modulus for finite nuclei. Typically, microscopic calculations are utilized to establish a relationship between the experimental compression modulus and the nuclear incompressibility that is a crucial parameter of the equation of state for nuclear matter. The incompressibility of nuclear matter has been determined with an accuracy of 10 to 20\% using relativistic and non-relativistic microscopic models for describing the monopole distributions in208Pb and90Zr isotopes. However, the same theoretical models are not able to describe data for open-shell nuclei, such as those of tin and cadmium isotopes. In fact, only effective interactions with a softer nuclear-matter incompressibility are able to predict the centroid energy of monopole distributions for open-shell nuclei. An unified description of the monopole resonance in208Pb and other open-shell nuclei remains unsolved from the theory side. Most of this uncertainty is due to our poor knowledge of the symmetry energy, which is another essential component of the equation of state of nuclear matter. Therefore, new experimental data along isotopic chains covering a wide range inN/Zratios, including neutron-deficient and neutron-rich nuclei, are of paramount importance for determining both the nuclear-matter incompressibility and the symmetry energy more precisely.△ Less"
How Does Culture Evolve?,Authors:Liane Gabora,"Abstract:This chapter synthesizes evidence from cognitive science, evolutionary theory, anthropology, psychological studies, and computational models for a complex systems inspired theory of creativity, and its role in cultural evolution. Creativity is guided by the global shape of one's integrated network of memories, concepts, and beliefs: one's worldview. This integrated structure and its dynamical chan…▽ MoreThis chapter synthesizes evidence from cognitive science, evolutionary theory, anthropology, psychological studies, and computational models for a complex systems inspired theory of creativity, and its role in cultural evolution. Creativity is guided by the global shape of one's integrated network of memories, concepts, and beliefs: one's worldview. This integrated structure and its dynamical change over time are described using autocatalytic networks. Autocatalytic networks can interact with each other, and they can grow and evolve; through interactions between their components, they generate novel components. Thus, they are used to describe cultural change both within and between individuals, as well as across cultural lineages. The chapter outlines autocatalytic network models of the origin of culture, the cognitive developmental process by which each child becomes a participant in cultural evolution, and the role of imitation, leadership, and social media on cultural evolution, as well as the trade-off between creativity and continuity.△ Less"
A mapping-free NLP-based technique for sequence search in Nanopore long-reads,"Authors:Tomasz Strzoda,Lourdes Cruz-Garcia,Mustafa Najim,Christophe Badie,Joanna Polanska","Abstract:…salmon-estimated read counts differed from the classical approach on average by 3.48% for the complete dictionary and by 5.82% for the reduced one. We conclude that for longOxfordNanopore reads, an NLP-based approach can successfully replace classical mapping in case of emergency. The developed NLP model can be easily retrained to identify selected transcr…▽ MoreIn unforeseen situations, such as nuclear power plant's or civilian radiation accidents, there is a need for effective and computationally inexpensive methods to determine the expression level of a selected gene panel, allowing for rough dose estimates in thousands of donors. The new generation in-situ mapper, fast and of low energy consumption, working at the level of single nanopore output, is in demand. We aim to create a sequence identification tool that utilizes Natural Language Processing (NLP) techniques and ensures a high level of negative predictive value (NPV) compared to the classical approach. The training dataset consisted of RNASeq data from 6 samples. Having tested multiple NLP models, the best configuration analyses the entire sequence and uses a word length of 3 base pairs with one-word neighbor on each side. For the considered FDXR gene, the achieved mean balanced accuracy (BACC) was 98.29% and NPV 99.25%, compared to minimap2's performance in a cross-validation scenario. Reducing the dictionary from 1024 to 145 changed BACC to 96.49% and the NPV to 98.15%. Obtained NLP model, validated on an external independent genome sequencing dataset, gave NPV of 99.64% for complete and 95.87% for reduced dictionary. The salmon-estimated read counts differed from the classical approach on average by 3.48% for the complete dictionary and by 5.82% for the reduced one. We conclude that for longOxfordNanopore reads, an NLP-based approach can successfully replace classical mapping in case of emergency. The developed NLP model can be easily retrained to identify selected transcripts and/or work with various long-read sequencing techniques. Our results of the study clearly demonstrate the potential of applying techniques known from classical text processing to nucleotide sequences and represent a significant advancement in this field of science.△ Less"
Galaxy-group-associated distances to Very High Energy gamma-ray emitting BL Lacs KUV 00311-1938 and S2 0109+22,"Authors:Karri I. I. Koljonen,Elina Lindfors,Kari Nilsson,Pekka Heinämäki,Jari Kotilainen","Abstract:Blazars constitute the most numerous source class in the known extragalactic population of very high energy (VHE) gamma-ray sources. However, determining their redshifts is often challenging due to weak or non-existent emission lines in their spectra. This study focuses on two BL Lacs, KUV 00311-1938 and S2 0109+22, where previous attempts at redshift determination have faced difficulties. By comb…▽ MoreBlazars constitute the most numerous source class in the known extragalactic population of very high energy (VHE) gamma-ray sources. However, determining their redshifts is often challenging due to weak or non-existent emission lines in their spectra. This study focuses on two BL Lacs, KUV 00311-1938 and S2 0109+22, where previous attempts at redshift determination have faced difficulties. By combining spectroscopic observations with photometric redshift estimates, we tentatively assign a redshift of z = 0.634 to KUV 00311-1938 and a likely redshift of z = 0.49 to S2 0109+22. Establishing redshift estimates for high-redshift blazars is crucial for understanding extragalactic VHE gamma-ray sources and their interactions with the surrounding universe.△ Less"
Making AI Intelligible: Philosophical Foundations,"Authors:Herman Cappelen,Josh Dever","Abstract:Can humans and artificial intelligences share concepts and communicate? 'Making AI Intelligible' shows that philosophical work on the metaphysics of meaning can help answer these questions. Herman Cappelen and Josh Dever use the externalist tradition in philosophy to create models of how AIs and humans can understand each other. In doing so, they illustrate ways in which that philosophical traditi…▽ MoreCan humans and artificial intelligences share concepts and communicate? 'Making AI Intelligible' shows that philosophical work on the metaphysics of meaning can help answer these questions. Herman Cappelen and Josh Dever use the externalist tradition in philosophy to create models of how AIs and humans can understand each other. In doing so, they illustrate ways in which that philosophical tradition can be improved.
  The questions addressed in the book are not only theoretically interesting, but the answers have pressing practical implications. Many important decisions about human life are now influenced by AI. In giving that power to AI, we presuppose that AIs can track features of the world that we care about (for example, creditworthiness, recidivism, cancer, and combatants). If AIs can share our concepts, that will go some way towards justifying this reliance on AI. This ground-breaking study offers insight into how to take some first steps towards achieving Interpretable AI.△ Less"
Fundamental effective temperature measurements for eclipsing binary stars -- V. The circumbinary planet system EBLM J0608-59,"Authors:P. F. L. Maxted,N. J. Miller,D. Sebastian,A. H. M. J. Triaud,D. V. Martin,A. Duck","Abstract:EBLM J0608-59 / TOI-1338 / BEBOP-1 is a 12th-magnitude, F9V star in an eclipsing binary with a much fainter M-dwarf companion on a wide, eccentric orbit (P=14.6 d). The binary is orbited by two circumbinary planets: one transiting on a 95-day orbit and one non-transiting on a 215-day orbit. We have used high-precision photometry from the TESS mission combined with direct mass measurements for the…▽ MoreEBLM J0608-59 / TOI-1338 / BEBOP-1 is a 12th-magnitude, F9V star in an eclipsing binary with a much fainter M-dwarf companion on a wide, eccentric orbit (P=14.6 d). The binary is orbited by two circumbinary planets: one transiting on a 95-day orbit and one non-transiting on a 215-day orbit. We have used high-precision photometry from the TESS mission combined with direct mass measurements for the two stars published recently to measure the following model-independent radii:R1=1.32±0.02R⊙,R2=0.309±0.004R⊙. UsingR1and the parallax from Gaia EDR3 we find that this star's angular diameter isθ=0.0309±0.0005mas. The apparent bolometric flux of the primary star corrected for both extinction and the contribution from the M-dwarf (<0.4%) isF⊕,0=(0.417±0.005)×10−9ergcm−2s−1. Hence, this F9V star has an effective temperatureTeff,1=6031K±46K(rnd.)±10K(sys.). EBLM J0608-59 is an ideal benchmark star that can be added to the sample of such systems we are establishing for ""end-to-end"" tests of the stellar parameters measured by large-scale spectroscopic surveys.△ Less"
TheOxfordOlympics Study 2024: Are Cost and Cost Overrun at the Games Coming Down?,"Authors:Alexander Budzier,Bent Flyvbjerg","Abstract:The present paper is an update of the ""OxfordOlympics Study 2016"" (Flyvbjerg et al. 2016). We document that the Games remain costly and continue to have large cost overruns, to a degree that threatens their viability. The IOC is aware of the problem and has initiated reform. We assess the reforms and find: (a) Olympic costs are statistically signifi…▽ MoreThe present paper is an update of the ""OxfordOlympics Study 2016"" (Flyvbjerg et al. 2016). We document that the Games remain costly and continue to have large cost overruns, to a degree that threatens their viability. The IOC is aware of the problem and has initiated reform. We assess the reforms and find: (a) Olympic costs are statistically significantly increasing; prior analysis did not show this trend; it is a step in the wrong direction. (b) Cost overruns were decreasing until 2008, but have increased since then; again a step in the wrong direction. (c) At present, the cost of Paris 2024 is USD 8.7 billion (2022 level) and cost overruns is 115% in real terms; this is not the cheap Games that were promised. (d) Cost overruns are the norm for the Games, past, present, and future; they are the only project type that never delivered on budget. We assess a new IOC policy of reducing cost by reusing existing venues instead of building new ones. We find that reuse did not have the desired effect for Tokyo 2020 and also look ineffective for Paris 2024. Finally, we recommend that the Games look to other types of megaprojects for better data, better forecasting, and how to generate the positive learning curves that are necessary for bringing costs and overrun down. Only if this happens are Los Angeles 2028 and Brisbane 2032 likely to live up to the IOC's intentions of a more affordable Games that more cities will want to host.△ Less"
Extending Structural Causal Models for Use in Autonomous Embodied Systems,"Authors:Rhys Howard,Lars Kunze","Abstract:Much work has been done to develop causal reasoning techniques across a number of domains, however the utilisation of causality within autonomous systems is still in its infancy. Autonomous systems would greatly benefit from the integration of causality through the use of representations such as structural causal models (SCMs). The system would be afforded a higher level of transparency, it would…▽ MoreMuch work has been done to develop causal reasoning techniques across a number of domains, however the utilisation of causality within autonomous systems is still in its infancy. Autonomous systems would greatly benefit from the integration of causality through the use of representations such as structural causal models (SCMs). The system would be afforded a higher level of transparency, it would enable post-hoc explanations of outcomes, and assist in the online inference of exogenous variables. These qualities are either directly beneficial to the autonomous system or a valuable step in building public trust and informing regulation. To such an end we present a case study in which we describe a module-based autonomous driving system comprised of SCMs. Approaching this task requires considerations of a number of challenges when dealing with a system of great complexity and size, that must operate for extended periods of time by itself. Here we describe these challenges, and present solutions. The first of these is SCM contexts, with the remainder being three new variable categories -- two of which are based upon functional programming monads. Finally, we conclude by presenting an example application of the causal capabilities of the autonomous driving system. In this example, we aim to attribute culpability between vehicular agents in a hypothetical road collision incident.△ Less"
AI with Alien Content and Alien Metasemantics,"Authors:Herman Cappelen,Josh Dever","Abstract:AlphaGo plays chess and Go in a creative and novel way. It is natural for us to attribute contents to it, such as that it doesn't view being several pawns behind, if it has more board space, as bad. The framework introduced in Cappelen and Dever (2021) provides a way of thinking about the semantics and the metasemantics of AI content: does AlphaGo entertain contents like this, and if so, in virtue…▽ MoreAlphaGo plays chess and Go in a creative and novel way. It is natural for us to attribute contents to it, such as that it doesn't view being several pawns behind, if it has more board space, as bad. The framework introduced in Cappelen and Dever (2021) provides a way of thinking about the semantics and the metasemantics of AI content: does AlphaGo entertain contents like this, and if so, in virtue of what does a given state of the program mean that particular content? One salient question Cappelen and Dever didn't consider was the possibility of alien content. Alien content is content that is not or cannot be expressed by human beings. It's highly plausible that AlphaGo, or any other sophisticated AI system, expresses alien contents. That this is so, moreover, is plausibly a metasemantic fact: a fact that has to do with how AI comes to entertain content in the first place, one that will heed the vastly different etiology of AI and human content. This chapter explores the question of alien content in AI from a semantic and metasemantic perspective. It lays out the logical space of possible responses to the semantic and metasemantic questions alien content poses, considers whether and how we humans could communicate with entities who express alien content, and points out that getting clear about such questions might be important for more 'applied' issues in the philosophy of AI, such as existential risk and XAI.△ Less"
AdapNet: Adaptive Noise-Based Network for Low-Quality Image Retrieval,"Authors:Sihe Zhang,Qingdong He,Jinlong Peng,Yuxi Li,Zhengkai Jiang,Jiafu Wu,Mingmin Chi,Yabiao Wang,Chengjie Wang","Abstract:…To assess the performance, we construct two datasets with low-quality queries, which is built by applying various types of noise on clean query images on the standard RevisitedOxfordand Revisited Paris datasets. Comprehensive experimental results illustrate that AdapNet surpasses state-of-the-art methods on the Noise Revisited…▽ MoreImage retrieval aims to identify visually similar images within a database using a given query image. Traditional methods typically employ both global and local features extracted from images for matching, and may also apply re-ranking techniques to enhance accuracy. However, these methods often fail to account for the noise present in query images, which can stem from natural or human-induced factors, thereby negatively impacting retrieval performance. To mitigate this issue, we introduce a novel setting for low-quality image retrieval, and propose an Adaptive Noise-Based Network (AdapNet) to learn robust abstract representations. Specifically, we devise a quality compensation block trained to compensate for various low-quality factors in input images. Besides, we introduce an innovative adaptive noise-based loss function, which dynamically adjusts its focus on the gradient in accordance with image quality, thereby augmenting the learning of unknown noisy samples during training and enhancing intra-class compactness. To assess the performance, we construct two datasets with low-quality queries, which is built by applying various types of noise on clean query images on the standard RevisitedOxfordand Revisited Paris datasets. Comprehensive experimental results illustrate that AdapNet surpasses state-of-the-art methods on the Noise RevisitedOxfordand Noise Revisited Paris benchmarks, while maintaining competitive performance on high-quality datasets. The code and constructed datasets will be made available.△ Less"
Scalable Bayesian Inference for Bradley--Terry Models with Ties: An Application to Honour Based Abuse,"Authors:Rowland G Seymour,Fabian Hernandez","Abstract:…efficient Markov Chain Monte Carlo algorithm to fit the model, allowing for a wide range of prior distributions on the model parameters. Working with South Yorkshire Police andOxfordAgainst Cutting, we mapped the risk of honour based abuse at community level in two counties in the UK.▽ MoreHonour based abuse covers a wide range of family abuse including female genital mutilation and forced marriage. Safeguarding professionals need to identify where abuses are happening in their local community to best support those at risk of these crimes and take preventative action. However, there is little local data about these kinds of crime. To tackle this problem, we ran comparative judgement surveys to map abuses at local level. In previous comparative judgement studies, participants reported fatigue associated with comparisons between areas with similar levels of abuse. Allowing for ties reduces fatigue, but increase the computational complexity when fitting the model. We designed an efficient Markov Chain Monte Carlo algorithm to fit the model, allowing for a wide range of prior distributions on the model parameters. Working with South Yorkshire Police andOxfordAgainst Cutting, we mapped the risk of honour based abuse at community level in two counties in the UK.△ Less"
Dusk Till Dawn: Self-supervised Nighttime Stereo Depth Estimation using Visual Foundation Models,"Authors:Madhu Vankadari,Samuel Hodgson,Sangyun Shin,Kaichen Zhou Andrew Markham,Niki Trigoni","Abstract:…weaknesses in the evaluation of current depth estimation algorithms, we present novel evaluation metrics. Our experiments, conducted on challenging datasets includingOxfordRobotCar and Multi-Spectral Stereo, demonstrate the robust improvements realized by our approach. Code is available at: https://github.com/madhubabuv/dtd▽ MoreSelf-supervised depth estimation algorithms rely heavily on frame-warping relationships, exhibiting substantial performance degradation when applied in challenging circumstances, such as low-visibility and nighttime scenarios with varying illumination conditions. Addressing this challenge, we introduce an algorithm designed to achieve accurate self-supervised stereo depth estimation focusing on nighttime conditions. Specifically, we use pretrained visual foundation models to extract generalised features across challenging scenes and present an efficient method for matching and integrating these features from stereo frames. Moreover, to prevent pixels violating photometric consistency assumption from negatively affecting the depth predictions, we propose a novel masking approach designed to filter out such pixels. Lastly, addressing weaknesses in the evaluation of current depth estimation algorithms, we present novel evaluation metrics. Our experiments, conducted on challenging datasets includingOxfordRobotCar and Multi-Spectral Stereo, demonstrate the robust improvements realized by our approach. Code is available at: https://github.com/madhubabuv/dtd△ Less"
Compared influence of the Atlantic Multidecadal Variability and of spring soil moisture on summer heat waves in Europe,"Authors:Valeria Mascolo,Clément Le Priol,Fabio D'Andrea,Freddy Bouchet","Abstract:In this work, we study and compare the influence of the Atlantic Multidecadal Variability (AMV) and of spring soil moisture in Southern Europe on the duration and intensity of European summer heat waves. We study common heat waves with return times of a few years like in previous studies, but we also propose a new methodological approach, return time maps, that allows us to study rare heat waves w…▽ MoreIn this work, we study and compare the influence of the Atlantic Multidecadal Variability (AMV) and of spring soil moisture in Southern Europe on the duration and intensity of European summer heat waves. We study common heat waves with return times of a few years like in previous studies, but we also propose a new methodological approach, return time maps, that allows us to study rare heat waves with return times from 10 to 50 years. We use the outputs from three climate models, namely IPSL-CM6A-LR, EC-Earth3, and CNRM-CM6-1, in which North Atlantic sea surface temperatures are restored towards the observed AMV anomalies. The three models give consistent results, with the exception of EC-Earth simulating a much greater influence of soil moisture. Typical AMV or spring soil moisture anomalies induce changes in the temperature and duration of heat waves that are of comparable amplitude, but follow different regional patterns. As might be expected, a positive AMV phase or low soil moisture induces hotter and longer typical heat waves over most of Europe. However, counter-intuitively, they also induce less heat wave days and cooler heat waves over part of Northeast Europe. For more extreme events, the influence of the AMV and soil moisture increase, according to rather similar regional patterns as for typical heat waves. However, while the amplitude of the influence is greater, the regions with decreased heat wave temperature and less heat wave days extend in size.△ Less"
RATLIP: Generative Adversarial CLIP Text-to-Image Synthesis Based on Recurrent Affine Transformations,"Authors:Chengde Lin,Xijun Lu,Guangxi Chen","Abstract:…utilizes CLIP's ability to comprehend complex scenes to accurately assess the quality of the generated images. Extensive experiments have been conducted on the CUB,Oxford, and CelebA-tiny datasets to demonstrate the superiority of the proposed model over current state-of-the-art models. The code is https://github.com/OxygenLu/RATLIP.▽ MoreSynthesizing high-quality photorealistic images with textual descriptions as a condition is very challenging. Generative Adversarial Networks (GANs), the classical model for this task, frequently suffer from low consistency between image and text descriptions and insufficient richness in synthesized images. Recently, conditional affine transformations (CAT), such as conditional batch normalization and instance normalization, have been applied to different layers of GAN to control content synthesis in images. CAT is a multi-layer perceptron that independently predicts data based on batch statistics between neighboring layers, with global textual information unavailable to other layers. To address this issue, we first model CAT and a recurrent neural network (RAT) to ensure that different layers can access global information. We then introduce shuffle attention between RAT to mitigate the characteristic of information forgetting in recurrent neural networks. Moreover, both our generator and discriminator utilize the powerful pre-trained model, Clip, which has been extensively employed for establishing associations between text and images through the learning of multimodal representations in latent space. The discriminator utilizes CLIP's ability to comprehend complex scenes to accurately assess the quality of the generated images. Extensive experiments have been conducted on the CUB,Oxford, and CelebA-tiny datasets to demonstrate the superiority of the proposed model over current state-of-the-art models. The code is https://github.com/OxygenLu/RATLIP.△ Less"
A quantitative and typological study of Early Slavic participle clauses and their competition,Authors:Nilo Pedrazzini,"Abstract:This thesis is a corpus-based, quantitative, and typological analysis of the functions of Early Slavic participle constructions and their finite competitors (jegda-'when'-clauses). The first part leverages detailed linguistic annotation on Early Slavic corpora at the morphosyntactic, dependency, information-structural, and lexical levels to obtain indirect evidence for different potential functi…▽ MoreThis thesis is a corpus-based, quantitative, and typological analysis of the functions of Early Slavic participle constructions and their finite competitors (jegda-'when'-clauses). The first part leverages detailed linguistic annotation on Early Slavic corpora at the morphosyntactic, dependency, information-structural, and lexical levels to obtain indirect evidence for different potential functions of participle clauses and their main finite competitor and understand the roles of compositionality and default discourse reasoning as explanations for the distribution of participle constructions andjegda-clauses in the corpus. The second part uses massively parallel data to analyze typological variation in how languages express the semantic space of Englishwhen, whose scope encompasses that of Early Slavic participle constructions andjegda-clauses. Probabilistic semantic maps are generated and statistical methods (including Kriging, Gaussian Mixture Modelling, precision and recall analysis) are used to induce cross-linguistically salient dimensions from the parallel corpus and to study conceptual variation within the semantic space of the hypothetical concept WHEN.△ Less"
Modelling the nanopore sequencing process with Helicase HMMs,"Authors:Xuechun Xu,Joakim Jaldén","Abstract:Recent advancements in nanopore sequencing technology, particularly the R10 nanopore fromOxfordNanopore Technology, have necessitated the development of improved data processing methods to utilize their potential for more than 9-mer resolution fully. The processing of the ion currents predominantly utilizes neural network-based methods known for their high…▽ MoreRecent advancements in nanopore sequencing technology, particularly the R10 nanopore fromOxfordNanopore Technology, have necessitated the development of improved data processing methods to utilize their potential for more than 9-mer resolution fully. The processing of the ion currents predominantly utilizes neural network-based methods known for their high basecalling accuracy but face developmental bottlenecks at higher resolutions. In light of this, we introduce the Helicase Hidden Markov Model (HHMM), a novel framework designed to incorporate the dynamics of the helicase motor protein alongside the nucleotide sequence during nanopore sequencing. This model supports the analysis of millions of distinct states, enhancing our understanding of raw ion currents and their alignment with nucleotide sequences. Our findings demonstrate the utility of HHMM not only as a potent visualization tool but also as an effective base for developing advanced basecalling algorithms. This approach offers a promising avenue for leveraging the full capabilities of emerging high-resolution nanopore sequencing technologies.△ Less"
Spin-aware movement of electrons and time-of-flight momentum spectroscopy,Authors:Siddhant Das,"Abstract:In the framework of the de Broglie-Bohm pilot-wave theory, or Bohmian mechanics, we examine two pedagogical problems that illustrate the bound and unbound motion of spin-1/2 particles: First, a single spin-1/2 particle trapped in the ground state of a spherical box is studied in both the relativistic and nonrelativistic versions of the theory; second, the free time evolution of this particle once…▽ MoreIn the framework of the de Broglie-Bohm pilot-wave theory, or Bohmian mechanics, we examine two pedagogical problems that illustrate the bound and unbound motion of spin-1/2 particles: First, a single spin-1/2 particle trapped in the ground state of a spherical box is studied in both the relativistic and nonrelativistic versions of the theory; second, the free time evolution of this particle once the confinement is released is examined, demonstrating how the Fourier transform of the prepared wave function yields the statistics of the particle's far-field (asymptotic) velocity, thereby providing a deeper understanding of time-of-flight momentum spectroscopy techniques.△ Less"
Automated Quantum Circuit Generation for Computing Inverse Hash Functions,"Authors:Elena R. Henderson,Jessie M. Henderson,William V. Oxford,Mitchell A. Thornton","Abstract:Several cryptographic systems depend upon the computational difficulty of reversing cryptographic hash functions. Robust hash functions transform inputs to outputs in such a way that the inputs cannot be later retrieved in a reasonable amount of time even if the outputs and the function that created them are known. Consequently, hash functions can be cryptographically secure, and they are employed…▽ MoreSeveral cryptographic systems depend upon the computational difficulty of reversing cryptographic hash functions. Robust hash functions transform inputs to outputs in such a way that the inputs cannot be later retrieved in a reasonable amount of time even if the outputs and the function that created them are known. Consequently, hash functions can be cryptographically secure, and they are employed in encryption, authentication, and other security methods. It has been suggested that such cryptographically-secure hash functions will play a critical role in the era of post-quantum cryptography (PQC), as they do in conventional systems. In this work, we introduce a procedure that leverages the principle of reversibility to generate circuits that invert hash functions. We provide a proof-of-concept implementation and describe methods that allow for scaling the hash function inversion approach. Specifically, we implement one manifestation of the algorithm as part of a more general automated quantum circuit synthesis, compilation, and optimization toolkit. We illustrate production of reversible circuits for crypto-hash functions that inherently provide the inverse of the function, and we describe data structures that increase the scalability of the hash function inversion approach.△ Less"
Transfer Learning for Molecular Property Predictions from Small Data Sets,"Authors:Thorren Kirschbaum,Annika Bande","Abstract:…models and corrected by simple linear regression on the target data set to obtain labels that are close to those of the original data. This strategy is tested on the HarvardOxfordPhotovoltaics data set (HOPV, HOMO-LUMO-gaps), for which excellent results are obtained, and on the Freesolv data set (solvation energies), where this method is unsuccessful due t…▽ MoreMachine learning has emerged as a new tool in chemistry to bypass expensive experiments or quantum-chemical calculations, for example, in high-throughput screening applications. However, many machine learning studies rely on small data sets, making it difficult to efficiently implement powerful deep learning architectures such as message passing neural networks. In this study, we benchmark common machine learning models for the prediction of molecular properties on small data sets, for which the best results are obtained with the message passing neural network PaiNN, as well as SOAP molecular descriptors concatenated to a set of simple molecular descriptors tailored to gradient boosting with regression trees. To further improve the predictive capabilities of PaiNN, we present a transfer learning strategy that uses large data sets to pre-train the respective models and allows to obtain more accurate models after fine-tuning on the original data sets. The pre-training labels are obtained from computationally cheap ab initio or semi-empirical models and corrected by simple linear regression on the target data set to obtain labels that are close to those of the original data. This strategy is tested on the HarvardOxfordPhotovoltaics data set (HOPV, HOMO-LUMO-gaps), for which excellent results are obtained, and on the Freesolv data set (solvation energies), where this method is unsuccessful due to a complex underlying learning task and the dissimilar methods used to obtain pre-training and fine-tuning labels. Finally, we find that the final training results do not improve monotonically with the size of the pre-training data set, but pre-training with fewer data points can lead to more biased pre-trained models and higher accuracy after fine-tuning.△ Less"
Spinor-valued Higgs fields,Authors:Nigel Hitchin,"Abstract:We investigate the geometry of holomorphic vector bundlesEover a Riemann surfaceCtogether with a section of the endomorphism bundle tensored withK1/2-- a square root of the canonical bundleK. These parallel to some extent the various features of usual Higgs bundles, such as spectral curve constructions, but some features are radically different. We make essential use of the mod 2…▽ MoreWe investigate the geometry of holomorphic vector bundlesEover a Riemann surfaceCtogether with a section of the endomorphism bundle tensored withK1/2-- a square root of the canonical bundleK. These parallel to some extent the various features of usual Higgs bundles, such as spectral curve constructions, but some features are radically different. We make essential use of the mod 2 index to distinguish two families of moduli spaces, and provide examples in low genus.△ Less"
The Effect of Pulsar Geometry on the Observed Gamma-ray Spectrum of Millisecond Pulsars,"Authors:Sheridan J. Lloyd,Paula M. Chadwick,Anthony M. Brown","Abstract:We analyse 13 yrs ofFermi-LAT PASS 8 events from 127 gamma-ray emitting millisecond pulsars (MSPs) in the energy range 0.1−100 GeV and significantly detect 118 MSPs. We fit the stacked emission with a log parabola (LP) spectral model which we show is preferred to two previously published models. We consider the influence of pulsar properties and observer geometric effects on spectral…▽ MoreWe analyse 13 yrs ofFermi-LAT PASS 8 events from 127 gamma-ray emitting millisecond pulsars (MSPs) in the energy range 0.1−100 GeV and significantly detect 118 MSPs. We fit the stacked emission with a log parabola (LP) spectral model which we show is preferred to two previously published models. We consider the influence of pulsar properties and observer geometric effects on spectral features by defining energy flux colours for both the individual MSPs, and our stacked model as a baseline. There is no correlation of colours with pulsar luminosity,E˙, surface magnetic field or magnetic impact angle. We also find that pulsar geometry has little effect on the observed gamma-ray spectrum which is in tension with previous modelling of gamma-ray emission with respect to pulsar geometry. Our LP MSP model is applicable to problems where an ensemble of gamma-ray MSPs is considered, such as that of the Galactic centre excess or in the case of emission from globular clusters.△ Less"
Logic in Mathematics and Computer Science,Authors:Richard Zach,"Abstract:Logic has pride of place in mathematics and its 20th century offshoot, computer science. Modern symbolic logic was developed, in part, as a way to provide a formal framework for mathematics: Frege, Peano, Whitehead and Russell, as well as Hilbert developed systems of logic to formalize mathematics. These systems were meant to serve either as themselves foundational, or at least as formal analogs o…▽ MoreLogic has pride of place in mathematics and its 20th century offshoot, computer science. Modern symbolic logic was developed, in part, as a way to provide a formal framework for mathematics: Frege, Peano, Whitehead and Russell, as well as Hilbert developed systems of logic to formalize mathematics. These systems were meant to serve either as themselves foundational, or at least as formal analogs of mathematical reasoning amenable to mathematical study, e.g., in Hilbert's consistency program. Similar efforts continue, but have been expanded by the development of sophisticated methods to study the properties of such systems using proof and model theory. In parallel with this evolution of logical formalisms as tools for articulating mathematical theories (broadly speaking), much progress has been made in the quest for a mechanization of logical inference and the investigation of its theoretical limits, culminating recently in the development of new foundational frameworks for mathematics with sophisticated computer-assisted proof systems. In addition, logical formalisms developed by logicians in mathematical and philosophical contexts have proved immensely useful in describing theories and systems of interest to computer scientists, and to some degree, vice versa. Three examples of the influence of logic in computer science are automated reasoning, computer verification, and type systems for programming languages.△ Less"
The OxMat dataset: a multimodal resource for the development of AI-driven technologies in maternal and newborn child health,"Authors:M. Jaleed Khan,Ioana Duta,Beth Albert,William Cooke,Manu Vatish,Gabriel Davis Jones","Abstract:…However, the effectiveness of such technologies depends upon the availability of large, high-quality datasets that are suitable for machine learning. This paper introduces theOxfordMaternity (OxMat) dataset, the world's largest curated dataset of CTGs, featuring raw time series CTG data and extensive clinical data for both mothers and babies, which is…▽ MoreThe rapid advancement of Artificial Intelligence (AI) in healthcare presents a unique opportunity for advancements in obstetric care, particularly through the analysis of cardiotocography (CTG) for fetal monitoring. However, the effectiveness of such technologies depends upon the availability of large, high-quality datasets that are suitable for machine learning. This paper introduces theOxfordMaternity (OxMat) dataset, the world's largest curated dataset of CTGs, featuring raw time series CTG data and extensive clinical data for both mothers and babies, which is ideally placed for machine learning. The OxMat dataset addresses the critical gap in women's health data by providing over 177,211 unique CTG recordings from 51,036 pregnancies, carefully curated and reviewed since 1991. The dataset also comprises over 200 antepartum, intrapartum and postpartum clinical variables, ensuring near-complete data for crucial outcomes such as stillbirth and acidaemia. While this dataset also covers the intrapartum stage, around 94% of the constituent CTGS are antepartum. This allows for a unique focus on the underserved antepartum period, in which early detection of at-risk fetuses can significantly improve health outcomes. Our comprehensive review of existing datasets reveals the limitations of current datasets: primarily, their lack of sufficient volume, detailed clinical data and antepartum data. The OxMat dataset lays a foundation for future AI-driven prenatal care, offering a robust resource for developing and testing algorithms aimed at improving maternal and fetal health outcomes.△ Less"
Evolving Loss Functions for Specific Image Augmentation Techniques,"Authors:Brandon Morgan,Dean Hougen","Abstract:…CIFAR-10 and CIFAR-100 across each of the five image augmentation techniques. The best from that were then taken and evaluated by fine-tuning EfficientNetV2Small on the CARS,Oxford-Flowers, and Caltech datasets across each of the five image augmentation techniques. Multiple loss functions were found that outperformed cross-entropy across multiple experiment…▽ MorePrevious work in Neural Loss Function Search (NLFS) has shown a lack of correlation between smaller surrogate functions and large convolutional neural networks with massive regularization. We expand upon this research by revealing another disparity that exists, correlation between different types of image augmentation techniques. We show that different loss functions can perform well on certain image augmentation techniques, while performing poorly on others. We exploit this disparity by performing an evolutionary search on five types of image augmentation techniques in the hopes of finding image augmentation specific loss functions. The best loss functions from each evolution were then taken and transferred to WideResNet-28-10 on CIFAR-10 and CIFAR-100 across each of the five image augmentation techniques. The best from that were then taken and evaluated by fine-tuning EfficientNetV2Small on the CARS,Oxford-Flowers, and Caltech datasets across each of the five image augmentation techniques. Multiple loss functions were found that outperformed cross-entropy across multiple experiments. In the end, we found a single loss function, which we called the inverse bessel logarithm loss, that was able to outperform cross-entropy across the majority of experiments.△ Less"
Low-Cost Generation and Evaluation of Dictionary Example Sentences,"Authors:Bill Cai,Clarence Boon Liang Ng,Daniel Tan,Shelvia Hotama","Abstract:…evaluation of dictionary example sentences. We introduce a new automatic evaluation metric called OxfordEval that measures the win-rate of generated sentences against existingOxfordDictionary sentences. OxfordEval shows high alignment with human judgments, enabling large-scale automated quality evaluation. We experiment with various LLMs and configurations…▽ MoreDictionary example sentences play an important role in illustrating word definitions and usage, but manually creating quality sentences is challenging. Prior works have demonstrated that language models can be trained to generate example sentences. However, they relied on costly customized models and word sense datasets for generation and evaluation of their work. Rapid advancements in foundational models present the opportunity to create low-cost, zero-shot methods for the generation and evaluation of dictionary example sentences. We introduce a new automatic evaluation metric called OxfordEval that measures the win-rate of generated sentences against existingOxfordDictionary sentences. OxfordEval shows high alignment with human judgments, enabling large-scale automated quality evaluation. We experiment with various LLMs and configurations to generate dictionary sentences across word classes. We complement this with a novel approach of using masked language models to identify and select sentences that best exemplify word meaning. The eventual model, FM-MLM, achieves over 85.1% win rate againstOxfordbaseline sentences according to OxfordEval, compared to 39.8% win rate for prior model-generated sentences.△ Less"
Exploring Probabilistic Models for Semi-supervised Learning,Authors:Jianfeng Wang,"Abstract:This thesis studies advanced probabilistic models, including both their theoretical foundations and practical applications, for different semi-supervised learning (SSL) tasks. The proposed probabilistic methods are able to improve the safety of AI systems in real applications by providing reliable uncertainty estimates quickly, and at the same time, achieve competitive performance compared to thei…▽ MoreThis thesis studies advanced probabilistic models, including both their theoretical foundations and practical applications, for different semi-supervised learning (SSL) tasks. The proposed probabilistic methods are able to improve the safety of AI systems in real applications by providing reliable uncertainty estimates quickly, and at the same time, achieve competitive performance compared to their deterministic counterparts. The experimental results indicate that the methods proposed in the thesis have great value in safety-critical areas, such as the autonomous driving or medical imaging analysis domain, and pave the way for the future discovery of highly effective and efficient probabilistic approaches in the SSL sector.△ Less"
Designing a Photonic Physically Unclonable Function Having Resilience to Machine Learning Attacks,"Authors:Elena R. Henderson,Jessie M. Henderson,Hiva Shahoei,William V. Oxford,Eric C. Larson,Duncan L. MacFarlane,Mitchell A. Thornton","Abstract:Physically unclonable functions (PUFs) are designed to act as device 'fingerprints.' Given an input challenge, the PUF circuit should produce an unpredictable response for use in situations such as root-of-trust applications and other hardware-level cybersecurity applications. PUFs are typically subcircuits present within integrated circuits (ICs), and while conventional IC PUFs are well-understoo…▽ MorePhysically unclonable functions (PUFs) are designed to act as device 'fingerprints.' Given an input challenge, the PUF circuit should produce an unpredictable response for use in situations such as root-of-trust applications and other hardware-level cybersecurity applications. PUFs are typically subcircuits present within integrated circuits (ICs), and while conventional IC PUFs are well-understood, several implementations have proven vulnerable to malicious exploits, including those perpetrated by machine learning (ML)-based attacks. Such attacks can be difficult to prevent because they are often designed to work even when relatively few challenge-response pairs are known in advance. Hence the need for both more resilient PUF designs and analysis of ML-attack susceptibility. Previous work has developed a PUF for photonic integrated circuits (PICs). A PIC PUF not only produces unpredictable responses given manufacturing-introduced tolerances, but is also less prone to electromagnetic radiation eavesdropping attacks than a purely electronic IC PUF. In this work, we analyze the resilience of the proposed photonic PUF when subjected to ML-based attacks. Specifically, we describe a computational PUF model for producing the large datasets required for training ML attacks; we analyze the quality of the model; and we discuss the modeled PUF's susceptibility to ML-based attacks. We find that the modeled PUF generates distributions that resemble uniform white noise, explaining the exhibited resilience to neural-network-based attacks designed to exploit latent relationships between challenges and responses. Preliminary analysis suggests that the PUF exhibits similar resilience to generative adversarial networks, and continued development will show whether more-sophisticated ML approaches better compromise the PUF and -- if so -- how design modifications might improve resilience.△ Less"
On Train-Test Class Overlap and Detection for Image Retrieval,"Authors:Chull Hwan Song,Jooyoung Yoon,Taebaek Hwang,Shunghyun Choi,Yeong Hyeon Gu,Yannis Avrithis","Abstract:…sets to not have class overlap in image retrieval? We revisit Google Landmarks v2 clean, the most popular training set, by identifying and removing class overlap with RevisitedOxfordand Paris [34], the most popular evaluation set. By comparing the original and the new RGLDv2-clean on a benchmark of reproduced state-of-the-art methods, our findings are stri…▽ MoreHow important is it for training and evaluation sets to not have class overlap in image retrieval? We revisit Google Landmarks v2 clean, the most popular training set, by identifying and removing class overlap with RevisitedOxfordand Paris [34], the most popular evaluation set. By comparing the original and the new RGLDv2-clean on a benchmark of reproduced state-of-the-art methods, our findings are striking. Not only is there a dramatic drop in performance, but it is inconsistent across methods, changing the ranking.What does it take to focus on objects or interest and ignore background clutter when indexing? Do we need to train an object detector and the representation separately? Do we need location supervision? We introduce Single-stage Detect-to-Retrieve (CiDeR), an end-to-end, single-stage pipeline to detect objects of interest and extract a global image representation. We outperform previous state-of-the-art on both existing training sets and the new RGLDv2-clean. Our dataset is available at https://github.com/dealicious-inc/RGLDv2-clean.△ Less"
Heracles: A Hybrid SSM-Transformer Model for High-Resolution Image and Time-Series Analysis,"Authors:Badri N. Patro,Suhas Ranganath,Vinay P. Namboodiri,Vijay S. Agneeswaran","Abstract:…Heracles-C-Huge further improve accuracy to 85.9\% and 86.4\%, respectively. Additionally, Heracles excels in transfer learning tasks on datasets such as CIFAR-10, CIFAR-100,OxfordFlowers, and Stanford Cars, and in instance segmentation on the MSCOCO dataset. Heracles also proves its versatility by achieving state-of-the-art results on seven time-series da…▽ MoreTransformers have revolutionized image modeling tasks with adaptations like DeIT, Swin, SVT, Biformer, STVit, and FDVIT. However, these models often face challenges with inductive bias and high quadratic complexity, making them less efficient for high-resolution images. State space models (SSMs) such as Mamba, V-Mamba, ViM, and SiMBA offer an alternative to handle high resolution images in computer vision tasks. These SSMs encounter two major issues. First, they become unstable when scaled to large network sizes. Second, although they efficiently capture global information in images, they inherently struggle with handling local information. To address these challenges, we introduce Heracles, a novel SSM that integrates a local SSM, a global SSM, and an attention-based token interaction module. Heracles leverages a Hartely kernel-based state space model for global image information, a localized convolutional network for local details, and attention mechanisms in deeper layers for token interactions. Our extensive experiments demonstrate that Heracles-C-small achieves state-of-the-art performance on the ImageNet dataset with 84.5\% top-1 accuracy. Heracles-C-Large and Heracles-C-Huge further improve accuracy to 85.9\% and 86.4\%, respectively. Additionally, Heracles excels in transfer learning tasks on datasets such as CIFAR-10, CIFAR-100,OxfordFlowers, and Stanford Cars, and in instance segmentation on the MSCOCO dataset. Heracles also proves its versatility by achieving state-of-the-art results on seven time-series datasets, showcasing its ability to generalize across domains with spectral data, capturing both local and global information. The project page is available at this link.\url{https://github.com/badripatro/heracles}△ Less"
VXP: Voxel-Cross-Pixel Large-scale Image-LiDAR Place Recognition,"Authors:Yun-Jin Li,Mariia Gladkova,Yan Xia,Rui Wang,Daniel Cremers","Abstract:…in a two-stage manner that first explicitly exploits local feature correspondences and enforces similarity of global descriptors. Extensive experiments on the three benchmarks (OxfordRobotCar, ViViD++ and KITTI) demonstrate our method surpasses the state-of-the-art cross-modal retrieval by a large margin.▽ MoreRecent works on the global place recognition treat the task as a retrieval problem, where an off-the-shelf global descriptor is commonly designed in image-based and LiDAR-based modalities. However, it is non-trivial to perform accurate image-LiDAR global place recognition since extracting consistent and robust global descriptors from different domains (2D images and 3D point clouds) is challenging. To address this issue, we propose a novel Voxel-Cross-Pixel (VXP) approach, which establishes voxel and pixel correspondences in a self-supervised manner and brings them into a shared feature space. Specifically, VXP is trained in a two-stage manner that first explicitly exploits local feature correspondences and enforces similarity of global descriptors. Extensive experiments on the three benchmarks (OxfordRobotCar, ViViD++ and KITTI) demonstrate our method surpasses the state-of-the-art cross-modal retrieval by a large margin.△ Less"
ReFeree: Radar-based efficient global descriptor using a Feature and Free space for Place Recognition,"Authors:Byunghee Choi,Hogyun Kim,Younggun Cho","Abstract:…also outstanding from the perspective of place recognition performance. For concrete validation, we test a single session from the MulRan dataset and a multi-session from theOxfordOffroad Radar,OxfordRadar RobotCar, and the Boreas dataset.▽ MoreRadar is highlighted for robust sensing capabilities in adverse weather conditions (e.g. dense fog, heavy rain, or snowfall). In addition, Radar can cover wide areas and penetrate small particles. Despite these advantages, Radar-based place recognition remains in the early stages compared to other sensors due to its unique characteristics such as low resolution, and significant noise. In this paper, we propose a Radarbased place recognition utilizing a descriptor called ReFeree using a feature and free space. Unlike traditional methods, we overwhelmingly summarize the Radar image. Despite being lightweight, it contains semi-metric information and is also outstanding from the perspective of place recognition performance. For concrete validation, we test a single session from the MulRan dataset and a multi-session from theOxfordOffroad Radar,OxfordRadar RobotCar, and the Boreas dataset.△ Less"
Language Evolution with Deep Learning,"Authors:Mathieu Rita,Paul Michel,Rahma Chaabouni,Olivier Pietquin,Emmanuel Dupoux,Florian Strub","Abstract:Computational modeling plays an essential role in the study of language emergence. It aims to simulate the conditions and learning processes that could trigger the emergence of a structured language within a simulated controlled environment. Several methods have been used to investigate the origin of our language, including agent-based systems, Bayesian agents, genetic algorithms, and rule-based s…▽ MoreComputational modeling plays an essential role in the study of language emergence. It aims to simulate the conditions and learning processes that could trigger the emergence of a structured language within a simulated controlled environment. Several methods have been used to investigate the origin of our language, including agent-based systems, Bayesian agents, genetic algorithms, and rule-based systems. This chapter explores another class of computational models that have recently revolutionized the field of machine learning: deep learning models. The chapter introduces the basic concepts of deep and reinforcement learning methods and summarizes their helpfulness for simulating language emergence. It also discusses the key findings, limitations, and recent attempts to build realistic simulations. This chapter targets linguists and cognitive scientists seeking an introduction to deep learning as a tool to investigate language evolution.△ Less"
CCC++: Optimized Color Classified Colorization with Segment Anything Model (SAM) Empowered Object Selective Color Harmonization,"Authors:Mrityunjoy Gain,Avi Deb Raha,Rameswar Debnath","Abstract:…Ratio (TAR), to quantify the richness of color components. We compare our proposed model with state-of-the-art models using six different dataset: Place, ADE, Celeba, COCO,Oxford102 Flower, and ImageNet, in qualitative and quantitative approaches. The experimental results show that our proposed model outstrips other models in visualization, CNR and in our…▽ MoreIn this paper, we formulate the colorization problem into a multinomial classification problem and then apply a weighted function to classes. We propose a set of formulas to transform color values into color classes and vice versa. To optimize the classes, we experiment with different bin sizes for color class transformation. Observing class appearance, standard deviation, and model parameters on various extremely large-scale real-time images in practice we propose 532 color classes for our classification task. During training, we propose a class-weighted function based on true class appearance in each batch to ensure proper saturation of individual objects. We adjust the weights of the major classes, which are more frequently observed, by lowering them, while escalating the weights of the minor classes, which are less commonly observed. In our class re-weight formula, we propose a hyper-parameter for finding the optimal trade-off between the major and minor appeared classes. As we apply regularization to enhance the stability of the minor class, occasional minor noise may appear at the object's edges. We propose a novel object-selective color harmonization method empowered by the Segment Anything Model (SAM) to refine and enhance these edges. We propose two new color image evaluation metrics, the Color Class Activation Ratio (CCAR), and the True Activation Ratio (TAR), to quantify the richness of color components. We compare our proposed model with state-of-the-art models using six different dataset: Place, ADE, Celeba, COCO,Oxford102 Flower, and ImageNet, in qualitative and quantitative approaches. The experimental results show that our proposed model outstrips other models in visualization, CNR and in our proposed CCAR and TAR measurement criteria while maintaining satisfactory performance in regression (MSE, PSNR), similarity (SSIM, LPIPS, UIUI), and generative criteria (FID).△ Less"
The Negative Energy Sea,Authors:Simon Saunders,"Abstract:The Dirac negative energy sea introduced the concept of antimatter, and explained it, not least in its relationship to negative-energy solutions to the wave equation. Post-war, it was largely displaced by what I shall call the 'standard formalism', dependent, among other things, on normal-ordering. A much better explanation is provided by the 'two complex structures' viewpoint, as first introduced…▽ MoreThe Dirac negative energy sea introduced the concept of antimatter, and explained it, not least in its relationship to negative-energy solutions to the wave equation. Post-war, it was largely displaced by what I shall call the 'standard formalism', dependent, among other things, on normal-ordering. A much better explanation is provided by the 'two complex structures' viewpoint, as first introduced by Irving Segal: the one ('natural') kind of complex numbers at the level of covariant, local fields; and the other ('particle') complex numbers at the level of the one-particle Hilbert space and Fock space. The former is local, the latter non-local: therein lies the fundamental difference between relativistic and non-relativistic quantum theory.△ Less"
SIDE-real: Supernova Ia Dust Extinction with truncated marginal neural ratio estimation applied to real data,"Authors:Konstantin Karchev,Matthew Grayling,Benjamin M. Boyd,Roberto Trotta,Kaisey S. Mandel,Christoph Weniger","Abstract:We present the first fully simulation-based hierarchical analysis of the light curves of a population of low-redshift type Ia supernovae (SNae Ia). Our hardware-accelerated forward model, released in the Python package slicsim, includes stochastic variations of each SN's spectral flux distribution (based on the pre-trained BayeSN model), extinction from dust in the host and in the Milky Way, redsh…▽ MoreWe present the first fully simulation-based hierarchical analysis of the light curves of a population of low-redshift type Ia supernovae (SNae Ia). Our hardware-accelerated forward model, released in the Python package slicsim, includes stochastic variations of each SN's spectral flux distribution (based on the pre-trained BayeSN model), extinction from dust in the host and in the Milky Way, redshift, and realistic instrumental noise. By utilising truncated marginal neural ratio estimation (TMNRE), a neural network-enabled simulation-based inference technique, we implicitly marginalise over 4000 latent variables (for a set of≈100SNae Ia) to efficiently infer SN Ia absolute magnitudes and host-galaxy dust properties at the population level while also constraining the parameters of individual objects. Amortisation of the inference procedure allows us to obtain coverage guarantees for our results through Bayesian validation and frequentist calibration. Furthermore, we show a detailed comparison to full likelihood-based inference, implemented through Hamiltonian Monte Carlo, on simulated data and then apply TMNRE to the light curves of 86 SNae Ia from the Carnegie Supernova Project, deriving marginal posteriors in excellent agreement with previous work. Given its ability to accommodate arbitrarily complex extensions to the forward model -- e.g. different populations based on host properties, redshift evolution, complicated photometric redshift estimates, selection effects, and non-Ia contamination -- without significant modifications to the inference procedure, TMNRE has the potential to become the tool of choice for cosmological parameter inference from future, large SN Ia samples.△ Less"
MinkUNeXt: Point Cloud-based Large-scale Place Recognition using 3D Sparse Convolutions,"Authors:J. J. Cabrera,A. Santo,A. Gil,C. Viegas,L. Payá","Abstract:…more complex and sophisticated proposals such as Transformers, Attention-Layers or Deformable Convolutions. A thorough assessment of the proposal has been carried out using theOxfordRobotCar and the In-house datasets. As a result, MinkUNeXt proves to outperform other methods in the state-of-the-art.▽ MoreThis paper presents MinkUNeXt, an effective and efficient architecture for place-recognition from point clouds entirely based on the new 3D MinkNeXt Block, a residual block composed of 3D sparse convolutions that follows the philosophy established by recent Transformers but purely using simple 3D convolutions. Feature extraction is performed at different scales by a U-Net encoder-decoder network and the feature aggregation of those features into a single descriptor is carried out by a Generalized Mean Pooling (GeM). The proposed architecture demonstrates that it is possible to surpass the current state-of-the-art by only relying on conventional 3D sparse convolutions without making use of more complex and sophisticated proposals such as Transformers, Attention-Layers or Deformable Convolutions. A thorough assessment of the proposal has been carried out using theOxfordRobotCar and the In-house datasets. As a result, MinkUNeXt proves to outperform other methods in the state-of-the-art.△ Less"
Stealing Stable Diffusion Prior for Robust Monocular Depth Estimation,"Authors:Yifan Mao,Jian Liu,Xianming Liu","Abstract:…models in acquiring meaningful knowledge independently, thus reducing their dependency on the teacher models. The effectiveness of the approach is evaluated on nuScenes andOxfordRobotCar, two challenging public datasets, with the results showing the efficacy of the method. Source code and weights are available at: https://github.com/hitcslj/SSD.▽ MoreMonocular depth estimation is a crucial task in computer vision. While existing methods have shown impressive results under standard conditions, they often face challenges in reliably performing in scenarios such as low-light or rainy conditions due to the absence of diverse training data. This paper introduces a novel approach named Stealing Stable Diffusion (SSD) prior for robust monocular depth estimation. The approach addresses this limitation by utilizing stable diffusion to generate synthetic images that mimic challenging conditions. Additionally, a self-training mechanism is introduced to enhance the model's depth estimation capability in such challenging environments. To enhance the utilization of the stable diffusion prior further, the DINOv2 encoder is integrated into the depth model architecture, enabling the model to leverage rich semantic priors and improve its scene understanding. Furthermore, a teacher loss is introduced to guide the student models in acquiring meaningful knowledge independently, thus reducing their dependency on the teacher models. The effectiveness of the approach is evaluated on nuScenes andOxfordRobotCar, two challenging public datasets, with the results showing the efficacy of the method. Source code and weights are available at: https://github.com/hitcslj/SSD.△ Less"
The extreme coronal line emitter AT 2022fpx: Varying optical polarization properties and late-time X-ray flare,"Authors:Karri I. I. Koljonen,Ioannis Liodakis,Elina Lindfors,Kari Nilsson,Thomas M. Reynolds,Panos Charalampopoulos,Konstantinos Kouroumpatzakis,Callum McCall,Helen E. Jermak,Iain A. Steele,Juan Carbajo-Hijarrubia","Abstract:Supermassive black holes disrupt passing stars, producing outbursts called tidal disruption events (TDEs). TDEs have recently gained attention due to their unique dynamics and emission processes, which are still not fully understood. Especially, the so-called optical TDEs, are of interest as they often exhibit delayed or obscured X-ray emission from the accretion disk, making the origin of the pro…▽ MoreSupermassive black holes disrupt passing stars, producing outbursts called tidal disruption events (TDEs). TDEs have recently gained attention due to their unique dynamics and emission processes, which are still not fully understood. Especially, the so-called optical TDEs, are of interest as they often exhibit delayed or obscured X-ray emission from the accretion disk, making the origin of the prompt emission unclear. In this paper, we present multiband optical polarization observations and optical spectrometry of a recent TDE candidate AT 2022fpx, alongside monitoring observations in optical, ultraviolet and X-rays. The optical spectra of AT 2022fpx show Bowen fluorescence as well as highly-ionized iron emission lines, which are characteristic of extreme coronal line emitters. Additionally, the source exhibits variable but low-polarized continuum emission at the outburst peak, with a clear rotation of the polarization angle. X-ray emission observed approximately 250 days after the outburst peak in the decay appear flare-like but is consistent with constant temperature black-body emission. The overall outburst decay is slower than for typical TDEs, and resembles more the ones seen from Bowen fluorescence flares. These observations suggest that AT 2022fpx could be a key source in linking different long-lived TDE scenarios. Its unique characteristics, such as extreme coronal line emission, variable polarization, and delayed X-ray flare, can be attributed to the outer shock scenario or a clumpy torus surrounding the supermassive black hole. Further studies, especially in the context of multi-wavelength observations, are crucial to fully understand the dynamics and emission mechanisms of these intriguing astrophysical events.△ Less"
OORD: TheOxfordOffroad Radar Dataset,"Authors:Matthew Gadd,Daniele De Martini,Oliver Bartlett,Paul Murcutt,Matt Towlson,Matthew Widojo,Valentina Muşat,Luke Robinson,Efimia Panagiotaki,Georgi Pramatarov,Marc Alexander Kühn,Letizia Marchegiani,Paul Newman,Lars Kunze","Abstract:…Nevertheless, rugged offroad deployments are important application areas which also present unique challenges and opportunities for this sensor technology. Therefore, theOxfordOffroad Radar Dataset (OORD) presents data collected in the rugged Scottish highlands in extreme weather. The radar data we offer to the community are accompanied by GPS/INS referen…▽ MoreThere is a growing academic interest as well as commercial exploitation of millimetre-wave scanning radar for autonomous vehicle localisation and scene understanding. Although several datasets to support this research area have been released, they are primarily focused on urban or semi-urban environments. Nevertheless, rugged offroad deployments are important application areas which also present unique challenges and opportunities for this sensor technology. Therefore, theOxfordOffroad Radar Dataset (OORD) presents data collected in the rugged Scottish highlands in extreme weather. The radar data we offer to the community are accompanied by GPS/INS reference - to further stimulate research in radar place recognition. In total we release over 90GiB of radar scans as well as GPS and IMU readings by driving a diverse set of four routes over 11 forays, totalling approximately 154km of rugged driving. This is an area increasingly explored in literature, and we therefore present and release examples of recent open-sourced radar place recognition systems and their performance on our dataset. This includes a learned neural network, the weights of which we also release. The data and tools are made freely available to the community at https://oxford-robotics-institute.github.io/oord-dataset.△ Less"
CCC: Color Classified Colorization,"Authors:Mrityunjoy Gain,Avi Deb Raha,Rameswar Debnath","Abstract:…Number Ratio (CNR), to quantify the richness of color components. We compare our proposed model with state-of-the-art models using five different datasets: ADE, Celeba, COCO,Oxford102 Flower, and ImageNet, in both qualitative and quantitative approaches. The experimental results show that our proposed model outstrips other models in visualization and CNR m…▽ MoreAutomatic colorization of gray images with objects of different colors and sizes is challenging due to inter- and intra-object color variation and the small area of the main objects due to extensive backgrounds. The learning process often favors dominant features, resulting in a biased model. In this paper, we formulate the colorization problem into a multinomial classification problem and then apply a weighted function to classes. We propose a set of formulas to transform color values into color classes and vice versa. Class optimization and balancing feature distribution are the keys for good performance. Observing class appearance on various extremely large-scale real-time images in practice, we propose 215 color classes for our colorization task. During training, we propose a class-weighted function based on true class appearance in each batch to ensure proper color saturation of individual objects. We establish a trade-off between major and minor classes to provide orthodox class prediction by eliminating major classes' dominance over minor classes. As we apply regularization to enhance the stability of the minor class, occasional minor noise may appear at the object's edges. We propose a novel object-selective color harmonization method empowered by the SAM to refine and enhance these edges. We propose a new color image evaluation metric, the Chromatic Number Ratio (CNR), to quantify the richness of color components. We compare our proposed model with state-of-the-art models using five different datasets: ADE, Celeba, COCO,Oxford102 Flower, and ImageNet, in both qualitative and quantitative approaches. The experimental results show that our proposed model outstrips other models in visualization and CNR measurement criteria while maintaining satisfactory performance in regression (MSE, PSNR), similarity (SSIM, LPIPS, UIQI), and generative criteria (FID).△ Less"
A Photonic Physically Unclonable Function's Resilience to Multiple-Valued Machine Learning Attacks,"Authors:Jessie M. Henderson,Elena R. Henderson,Clayton A. Harper,Hiva Shahoei,William V. Oxford,Eric C. Larson,Duncan L. MacFarlane,Mitchell A. Thornton","Abstract:Physically unclonable functions (PUFs) identify integrated circuits using nonlinearly-related challenge-response pairs (CRPs). Ideally, the relationship between challenges and corresponding responses is unpredictable, even if a subset of CRPs is known. Previous work developed a photonic PUF offering improved security compared to non-optical counterparts. Here, we investigate this PUF's susceptibil…▽ MorePhysically unclonable functions (PUFs) identify integrated circuits using nonlinearly-related challenge-response pairs (CRPs). Ideally, the relationship between challenges and corresponding responses is unpredictable, even if a subset of CRPs is known. Previous work developed a photonic PUF offering improved security compared to non-optical counterparts. Here, we investigate this PUF's susceptibility to Multiple-Valued-Logic-based machine learning attacks. We find that approximately 1,000 CRPs are necessary to train models that predict response bits better than random chance. Given the significant challenge of acquiring a vast number of CRPs from a photonic PUF, our results demonstrate photonic PUF resilience against such attacks.△ Less"
DeepEraser: Deep Iterative Context Mining for Generic Text Eraser,"Authors:Hao Feng,Wendi Wang,Shaokai Liu,Jiajun Deng,Wengang Zhou,Houqiang Li","Abstract:…and trained in an end-to-end manner. To verify its effectiveness, extensive experiments are conducted on several prevalent benchmarks, including SCUT-Syn, SCUT-EnsText, andOxfordSynthetic text dataset. The quantitative and qualitative results demonstrate the effectiveness of our DeepEraser over the state-of-the-art methods, as well as its strong generaliza…▽ MoreIn this work, we present DeepEraser, an effective deep network for generic text removal. DeepEraser utilizes a recurrent architecture that erases the text in an image via iterative operations. Our idea comes from the process of erasing pencil script, where the text area designated for removal is subject to continuous monitoring and the text is attenuated progressively, ensuring a thorough and clean erasure. Technically, at each iteration, an innovative erasing module is deployed, which not only explicitly aggregates the previous erasing progress but also mines additional semantic context to erase the target text. Through iterative refinements, the text regions are progressively replaced with more appropriate content and finally converge to a relatively accurate status. Furthermore, a custom mask generation strategy is introduced to improve the capability of DeepEraser for adaptive text removal, as opposed to indiscriminately removing all the text in an image. Our DeepEraser is notably compact with only 1.4M parameters and trained in an end-to-end manner. To verify its effectiveness, extensive experiments are conducted on several prevalent benchmarks, including SCUT-Syn, SCUT-EnsText, andOxfordSynthetic text dataset. The quantitative and qualitative results demonstrate the effectiveness of our DeepEraser over the state-of-the-art methods, as well as its strong generalization ability in custom mask text removal. The codes and pre-trained models are available at https://github.com/fh2019ustc/DeepEraser△ Less"
ReViT: Enhancing Vision Transformers Feature Diversity with Attention Residual Connections,"Authors:Anxhelo Diko,Danilo Avola,Marco Cascio,Luigi Cinque","Abstract:…scene being analyzed. The effectiveness and robustness of the presented method are evaluated on five image classification benchmarks, including ImageNet1k, CIFAR10, CIFAR100,OxfordFlowers-102, andOxford-IIIT Pet, achieving improved performances. Additionally, experiments on the COCO2017 dataset show that the devised…▽ MoreVision Transformer (ViT) self-attention mechanism is characterized by feature collapse in deeper layers, resulting in the vanishing of low-level visual features. However, such features can be helpful to accurately represent and identify elements within an image and increase the accuracy and robustness of vision-based recognition systems. Following this rationale, we propose a novel residual attention learning method for improving ViT-based architectures, increasing their visual feature diversity and model robustness. In this way, the proposed network can capture and preserve significant low-level features, providing more details about the elements within the scene being analyzed. The effectiveness and robustness of the presented method are evaluated on five image classification benchmarks, including ImageNet1k, CIFAR10, CIFAR100,OxfordFlowers-102, andOxford-IIIT Pet, achieving improved performances. Additionally, experiments on the COCO2017 dataset show that the devised approach discovers and incorporates semantic and spatial relationships for object detection and instance segmentation when implemented into spatial-aware transformer models.△ Less"
The invisible black widow PSR J1720-0534: implications for the electron density towards the North Polar Spur,"Authors:Karri I. I. Koljonen,Sindre S. Lindseth,Manuel Linares,Alice K. Harding,Marco Turchetta","Abstract:Radio emission from pulsars can be used to map out their distances through dispersion measure (DM), which quantifies the amount of radio pulse dispersion. However, this method relies on accurately modelling the free electron density in the line of sight. Here, we present a detailed study of the multiwavelength emission from PSR J1720-0534, a black widow compact binary millisecond pulsar discover…▽ MoreRadio emission from pulsars can be used to map out their distances through dispersion measure (DM), which quantifies the amount of radio pulse dispersion. However, this method relies on accurately modelling the free electron density in the line of sight. Here, we present a detailed study of the multiwavelength emission from PSR J1720-0534, a black widow compact binary millisecond pulsar discovered in 2021, which the latest electron density model of the Galaxy (Yao et al. 2017) places at only 191 pc. We obtained and analysed deep multiwavelength observations in theγ-ray (Fermi-Large Area Telescope, 2008-2022), optical (Las Cumbres Observatory, 2.7 h), near-infrared (Nordic Optical Telescope, 3.5 h), and X-ray (Swift-X-Ray Telescope, 10 ks) bands. We found no significant detection ofγ-ray, optical, near-infrared, or X-ray counterparts around the radio-timing position of PSR J1720-0534, which we thus nickname 'the invisible black widow'. Employing the most constraining near-infrared limit (J>23.4mag), we established a lower limit on the source distance,d>1.1kpc, assuming conservative properties for the black widow companion star. This distance lower limit differs drastically (by a factor of more than 5) from the Yao et al. DM distance estimate. We attribute this difference to the inclusion in the Yao et al. model of a large and dense component towards the North Polar Spur. Considering our results and recent parallax distances to other pulsars in this direction, we argue that such a local and large component in the electron density model of the Galaxy is unnecessary.△ Less"
"What is a ""bug""? On subjectivity, epistemic power, and implications for software research","Authors:David Gray Widder,Claire Le Goues","Abstract:…in Software Engineering, Programming Languages, and beyond.
  But, what is a bug, exactly? While segmentation faults rarely spark joy, most bugs are not so clear cut. Per theOxfordEnglish Dictionary, the word ""bug"" has been a colloquialism for an engineering ""defect"" at least since the 1870s. Most modern software-oriented definitions speak…▽ MoreConsiderable effort in software research and practice is spent on bugs. Finding, reporting, tracking, triaging, attempting to fix them automatically, detecting ""bug smells"" -these comprise a substantial portion of large projects' time and development cost, and are of significant interest to researchers in Software Engineering, Programming Languages, and beyond.
  But, what is a bug, exactly? While segmentation faults rarely spark joy, most bugs are not so clear cut. Per theOxfordEnglish Dictionary, the word ""bug"" has been a colloquialism for an engineering ""defect"" at least since the 1870s. Most modern software-oriented definitions speak to a disconnect between what a developer intended and what a program actually does. Formal verification, from its inception, has developed means to identify deviations from a formal specification, expected to more or less fully encode desired behavior. However, software is rarely accompanied by full and formal specifications, and this intention is instead treated as implicit or partially-documented at best. The International Software Testing Qualifications board writes: ""A human being can make an error (mistake), which produces a defect (fault, bug) in the program code, or in a document. If a defect in code is executed, the system may fail to do what it should do (or do something it shouldn't), causing a failure. Defects may result in failures, but not all [do]"". Most sources forsake this precision. The influential paper ""Finding bugs is easy"" begins by saying ""bug patterns are code idioms that are often errors""-with no particular elaboration. Other work relies on imperfect practical proxies for specifications. For example, in automatic program repair research, a bug corresponds to a failing test case: when the test passes, the bug is considered fixed.
  However, when we interrogate fairly straightforward definitions, they start to break down...△ Less"
Contagion on Financial Networks: An Introduction,Authors:Sunday Akukodi Ugwu,"Abstract:This mini-project models propagation of shocks, in time point, through links in connected banks. In particular, financial network of 100 banks out of which 15 are shocked to default (that is, 85.00% of the banks are solvent) is modelled using Erdos and Renyi network -- directed, weighted and randomly generated network. Shocking some banks in a financial network implies removing their assets and re…▽ MoreThis mini-project models propagation of shocks, in time point, through links in connected banks. In particular, financial network of 100 banks out of which 15 are shocked to default (that is, 85.00% of the banks are solvent) is modelled using Erdos and Renyi network -- directed, weighted and randomly generated network. Shocking some banks in a financial network implies removing their assets and redistributing their liabilities to other connected ones in the network. The banks are nodes and two ranges of probability values determine tendency of having a link between a pair of banks. Our major finding shows that the ranges of probability values and banks' percentage solvency have positive correlation.△ Less"
Unsupervised Sign Language Translation and Generation,"Authors:Zhengsheng Guo,Zhiwei He,Wenxiang Jiao,Xing Wang,Rui Wang,Kehai Chen,Zhaopeng Tu,Yong Xu,Min Zhang","Abstract:…sign language translation and generation model capable of generating both natural language text and sign language video in a unified manner. Experimental results on the BBC-OxfordSign Language dataset (BOBSL) and Open-Domain American Sign Language dataset (OpenASL) reveal that USLNet achieves competitive results compared to supervised baseline models, indic…▽ MoreMotivated by the success of unsupervised neural machine translation (UNMT), we introduce an unsupervised sign language translation and generation network (USLNet), which learns from abundant single-modality (text and video) data without parallel sign language data. USLNet comprises two main components: single-modality reconstruction modules (text and video) that rebuild the input from its noisy version in the same modality and cross-modality back-translation modules (text-video-text and video-text-video) that reconstruct the input from its noisy version in the different modality using back-translation procedure.Unlike the single-modality back-translation procedure in text-based UNMT, USLNet faces the cross-modality discrepancy in feature representation, in which the length and the feature dimension mismatch between text and video sequences. We propose a sliding window method to address the issues of aligning variable-length text with video sequences. To our knowledge, USLNet is the first unsupervised sign language translation and generation model capable of generating both natural language text and sign language video in a unified manner. Experimental results on the BBC-OxfordSign Language dataset (BOBSL) and Open-Domain American Sign Language dataset (OpenASL) reveal that USLNet achieves competitive results compared to supervised baseline models, indicating its effectiveness in sign language translation and generation.△ Less"
Sardinia Radio Telescope observations of the Coma Cluster,"Authors:M. Murgia,F. Govoni,V. Vacca,F. Loi,L. Feretti,G. Giovannini,A. Melis,R. Concu,E. Carretti,S. Poppi,G. Valente,A. Bonafede,G. Bernardi,W. Boschin,M. Brienza,T. E. Clarke,F. de Gasperin,T. A. Ensslin,C. Ferrari,F. Gastaldello,M. Girardi,L. Gregorini,M. Johnston-Hollitt,E. Orru',P. Parma, et al. (3 additional authors not shown)","Abstract:We present deep total intensity and polarization observations of the Coma cluster at 1.4 and 6.6 GHz performed with the Sardinia Radio Telescope. By combining the single-dish 1.4 GHz data with archival Very Large Array observations we obtain new images of the central radio halo and of the peripheral radio relic where we properly recover the brightness from the large scale structures. At 6.6 GHz we…▽ MoreWe present deep total intensity and polarization observations of the Coma cluster at 1.4 and 6.6 GHz performed with the Sardinia Radio Telescope. By combining the single-dish 1.4 GHz data with archival Very Large Array observations we obtain new images of the central radio halo and of the peripheral radio relic where we properly recover the brightness from the large scale structures. At 6.6 GHz we detect both the relic and the central part of the halo in total intensity and polarization. These are the highest frequency images available to date for these radio sources in this galaxy cluster. In the halo, we find a localized spot of polarized signal, with fractional polarization of about 45%. The polarized emission possibly extends along the north-east side of the diffuse emission. The relic is highly polarized, up to 55%, as usually found for these sources. We confirm the halo spectrum is curved, in agreement with previous single-dish results. The spectral index is alpha=1.48 +/- 0.07 at a reference frequency of 1 GHz and varies from alpha ~1.1, at 0.1 GHz, up to alpha ~ 1.8, at 10 GHz. We compare the Coma radio halo surface brightness profile at 1.4 GHz (central brightness and e-folding radius) with the same properties of the other halos, and we find that it has one of the lowest emissivities observed so far. Reanalyzing the relic's spectrum in the light of the new data, we obtain a refined radio Mach number of M=2.9 +/- 0.1.△ Less"
ALE spaces and nodal curves,Authors:Nigel Hitchin,Abstract:We consider the twistor theory approach to Kronheimer's ALE metrics on resolutions of the quotient of C^2 by a finite subgroup of SU(2). The circle action on the 4-manifold induces a C^* action on a compactification of the twistor space and we identify the orbit of a generic twistor line as a nodal rational curve in a particular cohomology class of a projective rational surface. Using the results…▽ MoreWe consider the twistor theory approach to Kronheimer's ALE metrics on resolutions of the quotient of C^2 by a finite subgroup of SU(2). The circle action on the 4-manifold induces a C^* action on a compactification of the twistor space and we identify the orbit of a generic twistor line as a nodal rational curve in a particular cohomology class of a projective rational surface. Using the results of N.Honda et al we identify this surface with the minitwistor space for the Einstein-Weyl structure on the 3-dimensional quotient of the ALE space by the circle action.△ Less
Null geodesic structure for the Barriola-Vilenkin spacetime viak-essence,"Authors:Bivash Majumder,Saibal Ray,Goutam Manna","Abstract:Based on the work of Chandrasekhar [{\it The Mathematical Theory of Black Holes,OxfordUniv. Press (1992)}], we investigate the null geodesic structure of the emergent Barriola-Vilenkin spacetime in the context of {\bf k-}essence theory. For {\bf k-}essence, the emergent gravity metric is a one-to-one correspondence with the Barriola-Vilenkin (BV) metric co…▽ MoreBased on the work of Chandrasekhar [{\it The Mathematical Theory of Black Holes,OxfordUniv. Press (1992)}], we investigate the null geodesic structure of the emergent Barriola-Vilenkin spacetime in the context of {\bf k-}essence theory. For {\bf k-}essence, the emergent gravity metric is a one-to-one correspondence with the Barriola-Vilenkin (BV) metric connected to the Schwarzschild background, where the global monopole charge is replaced by the dark energy density. This equivalence holds specifically for a certain class of {\bf k-}essence scalar fields that have been constructed by Gangopadhyay and Manna [Euro. Phys. Lett., 100, 49001, (2012)]. We have traced out different trajectories for null geodesic in the presence of dark energy for the {\bf k-}essence emergent Barriola-Vilenkin spacetime. It is demonstrated that the outcomes deviate from the typical Schwarzchild spacetime owing to the fundamental configuration with a constant dark energy density.△ Less"
Open-RadVLAD: Fast and Robust Radar Place Recognition,"Authors:Matthew Gadd,Paul Newman","Abstract:…descriptors. Our method is more comprehensively tested than all prior radar place recognition work - over an exhaustive combination of all 870 pairs of trajectories from 30OxfordRadar RobotCar Dataset sequences (each approximately 10 km). Code and detailed results are provided at github.com/mttgdd/open-radvlad, as an open implementation and benchmark for f…▽ MoreRadar place recognition often involves encoding a live scan as a vector and matching this vector to a database in order to recognise that the vehicle is in a location that it has visited before. Radar is inherently robust to lighting or weather conditions, but place recognition with this sensor is still affected by: (1) viewpoint variation, i.e. translation and rotation, (2) sensor artefacts or ""noises"". For 360-degree scanning radar, rotation is readily dealt with by in some way aggregating across azimuths. Also, we argue in this work that it is more critical to deal with the richness of representation and sensor noises than it is to deal with translational invariance - particularly in urban driving where vehicles predominantly follow the same lane when repeating a route. In our method, for computational efficiency, we use only the polar representation. For partial translation invariance and robustness to signal noise, we use only a one-dimensional Fourier Transform along radial returns. We also achieve rotational invariance and a very discriminative descriptor space by building a vector of locally aggregated descriptors. Our method is more comprehensively tested than all prior radar place recognition work - over an exhaustive combination of all 870 pairs of trajectories from 30OxfordRadar RobotCar Dataset sequences (each approximately 10 km). Code and detailed results are provided at github.com/mttgdd/open-radvlad, as an open implementation and benchmark for future work in this area. We achieve a median of 91.52% in Recall@1, outstripping the 69.55% for the only other open implementation, RaPlace, and at a fraction of its computational cost (relying on fewer integral transforms e.g. Radon, Fourier, and inverse Fourier).△ Less"
1/f noise in quantum nanoscience,"Authors:Giuseppe Falci,Pertti J. Hakonen,Elisabetta Paladino","Abstract:Fundamental issues of 1/f noise in quantum nanoscience are reviewed starting from basic statistical noise processes. Fundamental noise models based on two-level systems (TLS) are described. We emphasize the importance of TLSs in materials parameter fluctuations, such as dielectric constant. The present understanding of 1/f noise in superconducting quantum interferometers and in single electron dev…▽ MoreFundamental issues of 1/f noise in quantum nanoscience are reviewed starting from basic statistical noise processes. Fundamental noise models based on two-level systems (TLS) are described. We emphasize the importance of TLSs in materials parameter fluctuations, such as dielectric constant. The present understanding of 1/f noise in superconducting quantum interferometers and in single electron devices is summarized. For coherent quantum nanoscience, we introduce superconducting qubits and the relation between decoherence and 1/f noise using the filter function formulation. We also clarify the qubit noise spectroscopy and emphasize the importance of materials with reduced 1/f noise for future quantum coherent nanodevices.△ Less"
Mechanisms of nearshore retention and offshore export of mussel larvae over the Agulhas Bank,"Authors:Nicolas Weidberg,Francesca Porri,Charles von der Meden,Jennifer M. Jackson,Wayne Goschen,Christopher McQuaid","Abstract:Ecological connectivity is critical for population dynamics but in many benthic species it is complicated by a planktonic larval phase, whose dispersal remains poorly understood. Using a plankton pump, we examine the distribution of intertidal mussel larvae along three axes: alongshore, cross-shelf and by depth during a large scale (600 km) cruise over the Agulhas Bank off southern Africa in Augus…▽ MoreEcological connectivity is critical for population dynamics but in many benthic species it is complicated by a planktonic larval phase, whose dispersal remains poorly understood. Using a plankton pump, we examine the distribution of intertidal mussel larvae along three axes: alongshore, cross-shelf and by depth during a large scale (600 km) cruise over the Agulhas Bank off southern Africa in August/September 2010. As a general pattern, higher veliger abundances were found close to the coast. Our analyses of the nearshore flow, estimated from ADCP data and the vertical distribution of larvae, show that onshore larval retention may be mediated by active vertical swimming through the water column guided by light and wind-induced turbulence. A massive offshore export of larvae off St Francis Bay was, however, observed during an Agulhas Current meander which influenced inner shelf waters. We hypothesize that, by increasing and homogenizing flow, the Agulhas Current may erase the effects of larval vertical positioning on onshore retention and transport larvae offshore. Our study highlights the need to integrate the effects of complex, region-specific physical dynamics with the swimming behaviour of larvae in order to explain their spatial distribution, population connectivity and the consequences for population dynamics.△ Less"
"Memory, Space, and Planning: Multiscale Predictive Representations",Authors:Ida Momennejad,"Abstract:Memory is inherently entangled with prediction and planning. Flexible behavior in biological and artificial agents depends on the interplay of learning from the past and predicting the future in ever-changing environments. This chapter reviews computational, behavioral, and neural evidence suggesting these processes rely on learning the relational structure of experiences, known as cognitive maps,…▽ MoreMemory is inherently entangled with prediction and planning. Flexible behavior in biological and artificial agents depends on the interplay of learning from the past and predicting the future in ever-changing environments. This chapter reviews computational, behavioral, and neural evidence suggesting these processes rely on learning the relational structure of experiences, known as cognitive maps, and draws two key takeaways. First, that these memory structures are organized as multiscale, compact predictive representations in hippocampal and prefrontal cortex, or PFC, hierarchies. Second, we argue that such predictive memory structures are crucial to the complementary functions of the hippocampus and PFC, both for enabling a recall of detailed and coherent past episodes as well as generalizing experiences at varying scales for efficient prediction and planning. These insights advance our understanding of memory and planning mechanisms in the brain and hold significant implications for advancing artificial intelligence systems.△ Less"
Hitomi-HXT deconvolution imaging of the Crab Nebula dazzled by the Crab pulsar,"Authors:Mikio Morii,Yoshitomo Maeda,Hisamitsu Awaki,Kouichi Hagino,Manabu Ishida,Koji Mori","Abstract:We develop a new deconvolution method to improve the angular resolution of the Crab Nebula image taken by the Hitomi HXT. Here, we extend the Richardson-Lucy method by introducing two components for the nebula and the Crab pulsar with regularization for smoothness and flux, respectively, and deconvolving multi-pulse-phase images simultaneously. The deconvolved nebular image at the lowest energy ba…▽ MoreWe develop a new deconvolution method to improve the angular resolution of the Crab Nebula image taken by the Hitomi HXT. Here, we extend the Richardson-Lucy method by introducing two components for the nebula and the Crab pulsar with regularization for smoothness and flux, respectively, and deconvolving multi-pulse-phase images simultaneously. The deconvolved nebular image at the lowest energy band of 3.6--15 keV looks consistent with the Chandra X-ray image. Above 15 keV, we confirm that the NuSTAR's findings that the nebula size decreases in higher energy bands. We find that the north-east side of the nebula becomes dark in higher energy bands. Our deconvolution method can be applicable for any telescope images of faint diffuse objects containing a bright point source.△ Less"
SETI at FAST in China,"Authors:Tong-Jie Zhang,Bo-Lun Huang,Jian-Kang Li,Zhen-Zhao Tao,Xiao-Hang Luan,Zhi-Song Zhang,Yu-Chen Wang","Abstract:Since the commencement of the first SETI observation in 2019, China's Search for Extraterrestrial Intelligence program has garnered momentum through domestic support and international collaborations. Several observations targeting exoplanets and nearby stars have been conducted with the FAST. In 2023, the introduction of the Far Neighbour Project(FNP) marks a substantial leap forward, driven by th…▽ MoreSince the commencement of the first SETI observation in 2019, China's Search for Extraterrestrial Intelligence program has garnered momentum through domestic support and international collaborations. Several observations targeting exoplanets and nearby stars have been conducted with the FAST. In 2023, the introduction of the Far Neighbour Project(FNP) marks a substantial leap forward, driven by the remarkable sensitivity of the FAST telescope and some of the novel observational techniques. The FNP seeks to methodically detect technosignatures from celestial bodies, including nearby stars, exoplanetary systems, Milky Way globular clusters, and more. This paper provides an overview of the progress achieved by SETI in China and offers insights into the distinct phases comprising the FNP. Additionally, it underscores the significance of this project's advancement and its potential contributions to the field.△ Less"
Proceedings of the Sixth International Conference on Applied Category Theory 2023,"Authors:Sam Staton,Christina Vasilakopoulou","Abstract:…Conference on Applied Category Theory took place at the University of Maryland, 31 July -- 4 August 2023. This conference follows the previous meetings at Leiden (2018),Oxford(2019), MIT (2020, fully online), Cambridge (2021) and Glasgow (2022). The conference comprised contributed talks, a poster session, an industry showcase session, and a session where…▽ MoreThe Sixth International Conference on Applied Category Theory took place at the University of Maryland, 31 July -- 4 August 2023. This conference follows the previous meetings at Leiden (2018),Oxford(2019), MIT (2020, fully online), Cambridge (2021) and Glasgow (2022). The conference comprised contributed talks, a poster session, an industry showcase session, and a session where junior researchers who had attended the Adjoint School presented the results of their research at the school. Information regarding the conference may be found at (https://act2023.github.io/). The contributions to ACT2023 ranged from pure to applied and included contributions in a wide range of disciplines in science and engineering. Submission to ACT 2023 had three tracks: extended abstracts, software demonstrations, and proceedings. Only papers submitted to the proceedings track were considered for publication in this volume.△ Less"
A Physics Lab Inside Your Head: Quantum Thought Experiments as an Educational Tool,Authors:Maria Violaris,"Abstract:…based on the ""quantum bomb tester"" for school students as young as 11. This paper draws upon my experience in developing and delivering quantum computing workshops inOxford, and in creating a quantum paradoxes content series with IBM Quantum of videos, blogs and code tutorials.▽ MoreThought experiments are where logical reasoning meets storytelling, catalysing progress in quantum science and technology. Schrödinger's famous cat brought quantum science to the public consciousness, while Deutsch's thought experiment to test the many-worlds and Copenhagen interpretations involved the first conception of a quantum computer. I will show how presenting thought experiments using quantum circuits can demystify apparent quantum paradoxes, and provide fun, conceptually important activities for learners to implement themselves on near-term quantum devices. Additionally, I will explain how thought experiments can be used as a first introduction to quantum, and outline a workshop based on the ""quantum bomb tester"" for school students as young as 11. This paper draws upon my experience in developing and delivering quantum computing workshops inOxford, and in creating a quantum paradoxes content series with IBM Quantum of videos, blogs and code tutorials.△ Less"
Spatiotemporal Event Graphs for Dynamic Scene Understanding,Authors:Salman Khan,"Abstract:Dynamic scene understanding is the ability of a computer system to interpret and make sense of the visual information present in a video of a real-world scene. In this thesis, we present a series of frameworks for dynamic scene understanding starting from road event detection from an autonomous driving perspective to complex video activity detection, followed by continual learning approaches for t…▽ MoreDynamic scene understanding is the ability of a computer system to interpret and make sense of the visual information present in a video of a real-world scene. In this thesis, we present a series of frameworks for dynamic scene understanding starting from road event detection from an autonomous driving perspective to complex video activity detection, followed by continual learning approaches for the life-long learning of the models. Firstly, we introduce the ROad event Awareness Dataset (ROAD) for Autonomous Driving, to our knowledge the first of its kind. Due to the lack of datasets equipped with formally specified logical requirements, we also introduce the ROad event Awareness Dataset with logical Requirements (ROAD-R), the first publicly available dataset for autonomous driving with requirements expressed as logical constraints, as a tool for driving neurosymbolic research in the area. Next, we extend event detection to holistic scene understanding by proposing two complex activity detection methods. In the first method, we present a deformable, spatiotemporal scene graph approach, consisting of three main building blocks: action tube detection, a 3D deformable RoI pooling layer designed for learning the flexible, deformable geometry of the constituent action tubes, and a scene graph constructed by considering all parts as nodes and connecting them based on different semantics. In a second approach evolving from the first, we propose a hybrid graph neural network that combines attention applied to a graph encoding of the local (short-term) dynamic scene with a temporal graph modelling the overall long-duration activity. Finally, the last part of the thesis is about presenting a new continual semi-supervised learning (CSSL) paradigm.△ Less"
RMS: Redundancy-Minimizing Point Cloud Sampling for Real-Time Pose Estimation,"Authors:Pavel Petracek,Kostas Alexis,Martin Saska","Abstract:…redundancy for robot ego-motion estimation. We integrate RMS into the point-based KISS-ICP and feature-based LOAM odometry pipelines and evaluate experimentally on KITTI, Hilti-Oxford, and custom datasets from multirotor UAVs. The experiments demonstrate that RMS outperforms state-of-the-art methods in speed, compression, and accuracy in well-conditioned as…▽ MoreThe typical point cloud sampling methods used in state estimation for mobile robots preserve a high level of point redundancy. This redundancy unnecessarily slows down the estimation pipeline and may cause drift under real-time constraints. Such undue latency becomes a bottleneck for resource-constrained robots (especially UAVs), requiring minimal delay for agile and accurate operation. We propose a novel, deterministic, uninformed, and single-parameter point cloud sampling method named RMS that minimizes redundancy within a 3D point cloud. In contrast to the state of the art, RMS balances the translation-space observability by leveraging the fact that linear and planar surfaces inherently exhibit high redundancy propagated into iterative estimation pipelines. We define the concept of gradient flow, quantifying the local surface underlying a point. We also show that maximizing the entropy of the gradient flow minimizes point redundancy for robot ego-motion estimation. We integrate RMS into the point-based KISS-ICP and feature-based LOAM odometry pipelines and evaluate experimentally on KITTI, Hilti-Oxford, and custom datasets from multirotor UAVs. The experiments demonstrate that RMS outperforms state-of-the-art methods in speed, compression, and accuracy in well-conditioned as well as in geometrically-degenerated settings.△ Less"
Mixture of Gaussian-distributed Prototypes with Generative Modelling for Interpretable and Trustworthy Image Recognition,"Authors:Chong Wang,Yuanhong Chen,Fengbei Liu,Yuyuan Liu,Davis James McCarthy,Helen Frazer,Gustavo Carneiro","Abstract:…promote model compactness, we further propose to prune MGProto by removing prototypes with low importance priors. Experiments on CUB-200-2011, Stanford Cars, Stanford Dogs, andOxford-IIIT Pets datasets show that MGProto achieves state-of-the-art image recognition and OoD detection performances, while providing encouraging interpretability results.▽ MorePrototypical-part methods, e.g., ProtoPNet, enhance interpretability in image recognition by linking predictions to training prototypes, thereby offering intuitive insights into their decision-making. Existing methods, which rely on a point-based learning of prototypes, typically face two critical issues: 1) the learned prototypes have limited representation power and are not suitable to detect Out-of-Distribution (OoD) inputs, reducing their decision trustworthiness; and 2) the necessary projection of the learned prototypes back into the space of training images causes a drastic degradation in the predictive performance. Furthermore, current prototype learning adopts an aggressive approach that considers only the most active object parts during training, while overlooking sub-salient object regions which still hold crucial classification information. In this paper, we present a new generative paradigm to learn prototype distributions, termed as Mixture of Gaussian-distributed Prototypes (MGProto). The distribution of prototypes from MGProto enables both interpretable image classification and trustworthy recognition of OoD inputs. The optimisation of MGProto naturally projects the learned prototype distributions back into the training image space, thereby addressing the performance degradation caused by prototype projection. Additionally, we develop a novel and effective prototype mining strategy that considers not only the most active but also sub-salient object parts. To promote model compactness, we further propose to prune MGProto by removing prototypes with low importance priors. Experiments on CUB-200-2011, Stanford Cars, Stanford Dogs, andOxford-IIIT Pets datasets show that MGProto achieves state-of-the-art image recognition and OoD detection performances, while providing encouraging interpretability results.△ Less"
Richard Feynman's talent for finding things out,Authors:David Broadhurst,"Abstract:…about Richard Feynman, given at a conference Polymaths across the Eras organized in November 2023 by the St Cross Centre for the History and Philosophy of Physics (HAPP) inOxford. It describes Feynman as an unconventional polymath, primarily concerned with his own understanding of nature, having little regard for previous authorities. His curiosity, respect…▽ MoreThis article is a summary of a talk about Richard Feynman, given at a conference Polymaths across the Eras organized in November 2023 by the St Cross Centre for the History and Philosophy of Physics (HAPP) inOxford. It describes Feynman as an unconventional polymath, primarily concerned with his own understanding of nature, having little regard for previous authorities. His curiosity, respect for experiment, ability to calculate and notable ingenuity led to important developments in quantum mechanics, quantum electrodynamics, and strong interactions. In particle physics, Feynman diagrams have been a mainstay of calculation for the last 75 years. His relish for the spoken word led to renown as an educator and raconteur. His perceptions were influential in a wide range of other scientific fields, including weak interactions, gravity, superconductivity, biology, nanotechnology, algorithmic computation and quantum computers. In all of this, he held to the idea that nature has an intrinsic simplicity and beauty that permit us to find things out, if only we will try hard enough.△ Less"
A novel CFA+EFA model to detect aberrant respondents,"Authors:Niccolò Cao,Livio Finos,Luigi Lombardi,Antonio Calcagnì","Abstract:Aberrant respondents are common but yet extremely detrimental to the quality of social surveys or questionnaires. Recently, factor mixture models have been employed to identify individuals providing deceptive or careless responses. We propose a comprehensive factor mixture model for continuous outcomes that combines confirmatory and exploratory factor models to classify both the non-aberrant and a…▽ MoreAberrant respondents are common but yet extremely detrimental to the quality of social surveys or questionnaires. Recently, factor mixture models have been employed to identify individuals providing deceptive or careless responses. We propose a comprehensive factor mixture model for continuous outcomes that combines confirmatory and exploratory factor models to classify both the non-aberrant and aberrant respondents. The flexibility of the proposed {classification model} allows for the identification of two of the most common aberrant response styles, namely faking and careless responding. We validated our approach by means of two simulations and two case studies. The results indicate the effectiveness of the proposed model in dealing with aberrant responses in social and behavioural surveys.△ Less"
Towards a Feminist Metaethics of AI,Authors:Anastasia Siapka,"Abstract:The proliferation of Artificial Intelligence (AI) has sparked an overwhelming number of AI ethics guidelines, boards and codes of conduct. These outputs primarily analyse competing theories, principles and values for AI development and deployment. However, as a series of recent problematic incidents about AI ethics/ethicists demonstrate, this orientation is insufficient. Before proceeding to evalu…▽ MoreThe proliferation of Artificial Intelligence (AI) has sparked an overwhelming number of AI ethics guidelines, boards and codes of conduct. These outputs primarily analyse competing theories, principles and values for AI development and deployment. However, as a series of recent problematic incidents about AI ethics/ethicists demonstrate, this orientation is insufficient. Before proceeding to evaluate other professions, AI ethicists should critically evaluate their own; yet, such an evaluation should be more explicitly and systematically undertaken in the literature. I argue that these insufficiencies could be mitigated by developing a research agenda for a feminist metaethics of AI. Contrary to traditional metaethics, which reflects on the nature of morality and moral judgements in a non-normative way, feminist metaethics expands its scope to ask not only what ethics is but also what our engagement with it should be like. Applying this perspective to the context of AI, I suggest that a feminist metaethics of AI would examine: (i) the continuity between theory and action in AI ethics; (ii) the real-life effects of AI ethics; (iii) the role and profile of those involved in AI ethics; and (iv) the effects of AI on power relations through methods that pay attention to context, emotions and narrative.△ Less"
Efficient Rotation Invariance in Deep Neural Networks through Artificial Mental Rotation,"Authors:Lukas Tuggener,Thilo Stadelmann,Jürgen Schmidhuber","Abstract:…by the neuro-psychological concept of mental rotation. Our simple AMR implementation works with all common CNN and ViT architectures. We test it on ImageNet, Stanford Cars, andOxfordPet. With a top-1 error (averaged across datasets and architectures) of0.743, AMR outperforms the current state of the art (rotational data augmentation, average top-1 error…▽ MoreHumans and animals recognize objects irrespective of the beholder's point of view, which may drastically change their appearances. Artificial pattern recognizers also strive to achieve this, e.g., through translational invariance in convolutional neural networks (CNNs). However, both CNNs and vision transformers (ViTs) perform very poorly on rotated inputs. Here we present artificial mental rotation (AMR), a novel deep learning paradigm for dealing with in-plane rotations inspired by the neuro-psychological concept of mental rotation. Our simple AMR implementation works with all common CNN and ViT architectures. We test it on ImageNet, Stanford Cars, andOxfordPet. With a top-1 error (averaged across datasets and architectures) of0.743, AMR outperforms the current state of the art (rotational data augmentation, average top-1 error of0.626) by19\%. We also easily transfer a trained AMR module to a downstream task to improve the performance of a pre-trained semantic segmentation model on rotated CoCo from32.7to55.2IoU.△ Less"
Proceedings 19th International Conference on Quantum Physics and Logic,"Authors:Stefano Gogioso,Matty Hoban","Abstract:…contains the proceedings of the 19th International Conference on Quantum Physics and Logic (QPL 2022), which was held June 27-July 1, 2022 at Wolfson College, University ofOxford, UK. QPL is an annual conference that brings together academic and industry researchers working on mathematical foundations of quantum computation, quantum physics, and related are…▽ MoreThis volume contains the proceedings of the 19th International Conference on Quantum Physics and Logic (QPL 2022), which was held June 27-July 1, 2022 at Wolfson College, University ofOxford, UK. QPL is an annual conference that brings together academic and industry researchers working on mathematical foundations of quantum computation, quantum physics, and related areas. The main focus is on the use of algebraic and categorical structures, formal languages, semantic methods, as well as other mathematical and computer scientific techniques applicable to the study of physical systems, physical processes, and their composition.△ Less"
Identification of Books That are Suitable for Middle School Students Using Artificial Neural Networks,"Authors:Alp Niksarli,Sadik Ozan Gorgu,Ege Gencer","Abstract:…trained using the data which had been preprocessed to construct an original dataset. To train this network, suitable books for middle school students were provided by the MEB,Oxfordand Cambridge and with content assessed based on the ""R"" criterion, and inappropriate books for middle school students in terms of content were included. This trained ne…▽ MoreReading right books contributes to children's imagination and brain development, enhances their language and emotional comprehension abilities, and strengthens their relationships with others. Building upon the critical role of reading books in individual development, this paper aims to develop an algorithm that determines the suitability of books for middle school students by analyzing their structural and semantic features. Using methods described, an algorithm will be created that can be utilized by institutions and individuals responsible for children's education, such as the Ministry of National Education officials and schools. This algorithm will facilitate the selection of books to be taught at the middle school level. With the algorithm, the book selection process for the middle school curriculum can be expedited, and it will serve as a preliminary reference source for those who evaluate books by reading them. In this paper, the Python programming language was employed, utilizing natural language processing methods. Additionally, an artificial neural network (ANN) was trained using the data which had been preprocessed to construct an original dataset. To train this network, suitable books for middle school students were provided by the MEB,Oxfordand Cambridge and with content assessed based on the ""R"" criterion, and inappropriate books for middle school students in terms of content were included. This trained neural network achieved a 90.06% consistency rate in determining the appropriateness of the test-provided books. Considering the obtained findings, it can be concluded that the developed software has achieved the desired objective.△ Less"
High-precision scattering amplitudes for LHC phenomenology,Authors:Piotr Bargiela,"Abstract:In this work, we consider scattering amplitudes relevant for high-precision Large Hadron Collider (LHC) phenomenology. We analyse the general structure of amplitudes, and we review state-of-the-art methods for computing them. We discuss advantages and shortcomings of these methods, and we point out the bottlenecks in modern amplitude computations. As a practical illustration, we present frontier a…▽ MoreIn this work, we consider scattering amplitudes relevant for high-precision Large Hadron Collider (LHC) phenomenology. We analyse the general structure of amplitudes, and we review state-of-the-art methods for computing them. We discuss advantages and shortcomings of these methods, and we point out the bottlenecks in modern amplitude computations. As a practical illustration, we present frontier applications relevant for multi-loop multi-scale processes. We compute the helicity amplitudes for diphoton production in gluon fusion and photon+jet production in proton scattering in three-loop massless Quantum Chromodynamics (QCD). We have adopted a new projector-based prescription to compute helicity amplitudes in the 't Hooft-Veltman scheme. We also rederived the minimal set of independent Feynman integrals for this problem using the differential equations method, and we confirmed their intricate analytic properties. By employing modern methods for integral reduction, we provide the final results in a compact form, which is appropriate for efficient numerical evaluation. Beyond QCD, we have computed the two-loop mixed QCD-electroweak amplitudes for Z+jet production in proton scattering in light-quark-initiated channels, without closed fermion loops. This process provides important insight into the high-precision studies of the Standard Model, as well as into Dark Matter searches at the LHC. We have employed a numerical approach based on high-precision evaluation of Feynman integrals with the modern Auxiliary Mass Flow method. The obtained numerical results in all relevant partonic channels are evaluated on a two-dimensional grid appropriate for further phenomenological applications.△ Less"
Osprey: Multi-Session Autonomous Aerial Mapping with LiDAR-based SLAM and Next Best View Planning,"Authors:Rowan Border,Nived Chebrolu,Yifu Tao,Jonathan D. Gammell,Maurice Fallon","Abstract:Aerial mapping systems are important for many surveying applications (e.g., industrial inspection or agricultural monitoring). Aerial platforms that can fly GPS-guided preplanned missions semi-autonomously are already widely available but fully autonomous systems can significantly improve efficiency. Autonomously mapping complex 3D structures requires a system that performs online mapping and miss…▽ MoreAerial mapping systems are important for many surveying applications (e.g., industrial inspection or agricultural monitoring). Aerial platforms that can fly GPS-guided preplanned missions semi-autonomously are already widely available but fully autonomous systems can significantly improve efficiency. Autonomously mapping complex 3D structures requires a system that performs online mapping and mission planning. This paper presents Osprey, an autonomous aerial mapping system with state-of-the-art multi-session LiDAR-based mapping capabilities. It enables a non-expert operator to specify a bounded target area that the aerial platform can then map autonomously over multiple flights. Field experiments with Osprey demonstrate that this system can achieve greater map coverage of large industrial sites than manual surveys with a pilot-flown aerial platform or a terrestrial laser scanner (TLS). Three sites, with a total ground coverage of2528m^2and a maximum height of27m, were mapped in separate missions using112minutes of autonomous flight time. True colour maps were created from images captured by Osprey using pointcloud and NeRF reconstruction methods. These maps provide useful data for structural inspection tasks.△ Less"
Scattering Vision Transformer: Spectral Mixing Matters,"Authors:Badri N. Patro,Vijay Srinivas Agneeswaran","Abstract:…results in other vision tasks such as instance segmentation. SVT also outperforms other transformers in transfer learning on standard datasets such as CIFAR10, CIFAR100,OxfordFlower, and Stanford Car datasets. The project page is available on this webpage.\url{https://badripatro.github.io/svt/}.▽ MoreVision transformers have gained significant attention and achieved state-of-the-art performance in various computer vision tasks, including image classification, instance segmentation, and object detection. However, challenges remain in addressing attention complexity and effectively capturing fine-grained information within images. Existing solutions often resort to down-sampling operations, such as pooling, to reduce computational cost. Unfortunately, such operations are non-invertible and can result in information loss. In this paper, we present a novel approach called Scattering Vision Transformer (SVT) to tackle these challenges. SVT incorporates a spectrally scattering network that enables the capture of intricate image details. SVT overcomes the invertibility issue associated with down-sampling operations by separating low-frequency and high-frequency components. Furthermore, SVT introduces a unique spectral gating network utilizing Einstein multiplication for token and channel mixing, effectively reducing complexity. We show that SVT achieves state-of-the-art performance on the ImageNet dataset with a significant reduction in a number of parameters and FLOPS. SVT shows 2\% improvement over LiTv2 and iFormer. SVT-H-S reaches 84.2\% top-1 accuracy, while SVT-H-B reaches 85.2\% (state-of-art for base versions) and SVT-H-L reaches 85.7\% (again state-of-art for large versions). SVT also shows comparable results in other vision tasks such as instance segmentation. SVT also outperforms other transformers in transfer learning on standard datasets such as CIFAR10, CIFAR100,OxfordFlower, and Stanford Car datasets. The project page is available on this webpage.\url{https://badripatro.github.io/svt/}.△ Less"
On the Prospects of a de Broglie-Bohm-Barbour-Bertotti Theory,"Authors:Antonio Vassallo,Pedro Naranjo",Abstract:Pure shape dynamics (PSD) is a novel implementation of the relational framework originally proposed by Julian Barbour and Bruno Bertotti. PSD represents a Leibnizian/Machian approach to physics in that it completely describes the dynamical evolution of a physical system without resorting to any structure external to the system itself. The chapter discusses how PSD effectively describes a de Brogli…▽ MorePure shape dynamics (PSD) is a novel implementation of the relational framework originally proposed by Julian Barbour and Bruno Bertotti. PSD represents a Leibnizian/Machian approach to physics in that it completely describes the dynamical evolution of a physical system without resorting to any structure external to the system itself. The chapter discusses how PSD effectively describes a de Broglie-Bohm N-body system and the conceptual benefits of such a relational description. The analysis will highlight the new directions in the quest for an understanding of the nature of the wave function that are opened up by a modern relationalist elaboration on de Broglie's and Bohm's original insights.△ Less
BioInstruct: Instruction Tuning of Large Language Models for Biomedical Natural Language Processing,"Authors:Hieu Tran,Zhichao Yang,Zonghai Yao,Hong Yu","Abstract:To enhance the performance of large language models (LLMs) in biomedical natural language processing (BioNLP) by introducing a domain-specific instruction dataset and examining its impact when combined with multi-task learning principles. We created the BioInstruct, comprising 25,005 instructions to instruction-tune LLMs(LLaMA 1 & 2, 7B & 13B version). The instructions were created by prompting th…▽ MoreTo enhance the performance of large language models (LLMs) in biomedical natural language processing (BioNLP) by introducing a domain-specific instruction dataset and examining its impact when combined with multi-task learning principles. We created the BioInstruct, comprising 25,005 instructions to instruction-tune LLMs(LLaMA 1 & 2, 7B & 13B version). The instructions were created by prompting the GPT-4 language model with three-seed samples randomly drawn from an 80 human curated instructions. We employed Low-Rank Adaptation(LoRA) for parameter-efficient fine-tuning. We then evaluated these instruction-tuned LLMs on several BioNLP tasks, which can be grouped into three major categories: question answering(QA), information extraction(IE), and text generation(GEN). We also examined whether categories(e.g., QA, IE, and generation) of instructions impact model performance. Comparing with LLMs without instruction-tuned, our instruction-tuned LLMs demonstrated marked performance gains: 17.3% in QA, 5.7% in IE, and 96% in Generation tasks. Our 7B-parameter instruction-tuned LLaMA 1 model was competitive or even surpassed other LLMs in the biomedical domain that were also fine-tuned from LLaMA 1 with vast domain-specific data or a variety of tasks. Our results also show that the performance gain is significantly higher when instruction fine-tuning is conducted with closely related tasks. Our findings align with the observations of multi-task learning, suggesting the synergies between two tasks. The BioInstruct dataset serves as a valuable resource and instruction tuned LLMs lead to the best performing BioNLP applications.△ Less"
Exploring the Emotional Landscape of Music: An Analysis of Valence Trends and Genre Variations in Spotify Music Data,"Authors:Shruti Dutta,Shashwat Mookherjee","Abstract:This paper conducts an intricate analysis of musical emotions and trends using Spotify music data, encompassing audio features and valence scores extracted through the Spotipi API. Employing regression modeling, temporal analysis, mood transitions, and genre investigation, the study uncovers patterns within music-emotion relationships. Regression models linear, support vector, random forest, and r…▽ MoreThis paper conducts an intricate analysis of musical emotions and trends using Spotify music data, encompassing audio features and valence scores extracted through the Spotipi API. Employing regression modeling, temporal analysis, mood transitions, and genre investigation, the study uncovers patterns within music-emotion relationships. Regression models linear, support vector, random forest, and ridge, are employed to predict valence scores. Temporal analysis reveals shifts in valence distribution over time, while mood transition exploration illuminates emotional dynamics within playlists. The research contributes to nuanced insights into music's emotional fabric, enhancing comprehension of the interplay between music and emotions through years.△ Less"
Nanostructured Superconductors,Authors:Wolfgang Lang,"Abstract:The relevant length scales for superconductivity are of the order of nanometers. By confining the superconducting condensate to such dimensions, many physical properties change substantially, and novel phenomena emerge, which are absent in the pristine material. We discuss various methods of creating artificial nanostructures by top-down approaches in metallic and copper-oxide superconductors and…▽ MoreThe relevant length scales for superconductivity are of the order of nanometers. By confining the superconducting condensate to such dimensions, many physical properties change substantially, and novel phenomena emerge, which are absent in the pristine material. We discuss various methods of creating artificial nanostructures by top-down approaches in metallic and copper-oxide superconductors and their applications. Such nanostructures can be used to control magnetic flux quanta in superconductors, anchoring them to engineered defects to avoid dissipation, guiding their motion, or building artificial flux-quanta arrangements. Nanopatterned superconductors are essential for creating model systems for basic research and enable building almost dissipationless and ultrafast electronic devices and highly sensitive sensors.△ Less"
What you see is what you get: Experience ranking with deep neural dataset-to-dataset similarity for topological localisation,"Authors:Matthew Gadd,Benjamin Ramtoula,Daniele De Martini,Paul Newman","Abstract:…to performance when localising using a past experience with the same appearance gap. We validate our approach over the Nordland cross-season dataset as well as data fromOxford'sUniversity Parks with lighting and mild seasonal change, showing excellent ability of our system to rank actual localisation performance across candidate experiences.▽ MoreRecalling the most relevant visual memories for localisation or understanding a priori the likely outcome of localisation effort against a particular visual memory is useful for efficient and robust visual navigation. Solutions to this problem should be divorced from performance appraisal against ground truth - as this is not available at run-time - and should ideally be based on generalisable environmental observations. For this, we propose applying the recently developed Visual DNA as a highly scalable tool for comparing datasets of images - in this work, sequences of map and live experiences. In the case of localisation, important dataset differences impacting performance are modes of appearance change, including weather, lighting, and season. Specifically, for any deep architecture which is used for place recognition by matching feature volumes at a particular layer, we use distribution measures to compare neuron-wise activation statistics between live images and multiple previously recorded past experiences, with a potentially large seasonal (winter/summer) or time of day (day/night) shift. We find that differences in these statistics correlate to performance when localising using a past experience with the same appearance gap. We validate our approach over the Nordland cross-season dataset as well as data fromOxford'sUniversity Parks with lighting and mild seasonal change, showing excellent ability of our system to rank actual localisation performance across candidate experiences.△ Less"
Advancements in Radar Odometry,"Authors:Matteo Frosi,Mirko Usuelli,Matteo Matteucci","Abstract:…of the contribution of each improvement to the localization accuracy, and we benchmark our system on the sequences of the main datasets for radar understanding, i.e., theOxfordRadar RobotCar, MulRan, and Boreas datasets. The proposed pipeline is able to achieve superior results, on all scenarios considered and under harsh environmental constraints.▽ MoreRadar odometry estimation has emerged as a critical technique in the field of autonomous navigation, providing robust and reliable motion estimation under various environmental conditions. Despite its potential, the complex nature of radar signals and the inherent challenges associated with processing these signals have limited the widespread adoption of this technology. This paper aims to address these challenges by proposing novel improvements to an existing method for radar odometry estimation, designed to enhance accuracy and reliability in diverse scenarios. Our pipeline consists of filtering, motion compensation, oriented surface points computation, smoothing, one-to-many radar scan registration, and pose refinement. The developed method enforces local understanding of the scene, by adding additional information through smoothing techniques, and alignment of consecutive scans, as a refinement posterior to the one-to-many registration. We present an in-depth investigation of the contribution of each improvement to the localization accuracy, and we benchmark our system on the sequences of the main datasets for radar understanding, i.e., theOxfordRadar RobotCar, MulRan, and Boreas datasets. The proposed pipeline is able to achieve superior results, on all scenarios considered and under harsh environmental constraints.△ Less"
Aluminium-26 production in low- and intermediate-mass binary systems,"Authors:Zara Osborn,Amanda I. Karakas,Alex J. Kemp,Robert G. Izzard","Abstract:Aluminium-26 is a radioactive isotope which can be synthesized within asymptotic giant branch (AGB) stars, primarily through hot bottom burning. Studies exploring^{26}Al production within AGB stars typically focus on single-stars; however, observations show that low- and intermediate-mass stars commonly exist in binaries. We use the binary population synthesis code binary_c to explore the impac…▽ MoreAluminium-26 is a radioactive isotope which can be synthesized within asymptotic giant branch (AGB) stars, primarily through hot bottom burning. Studies exploring^{26}Al production within AGB stars typically focus on single-stars; however, observations show that low- and intermediate-mass stars commonly exist in binaries. We use the binary population synthesis code binary_c to explore the impact of binary evolution on^{26}Al yields at solar metallicity both within individual AGB stars and a low/intermediate-mass stellar population. We find the key stellar structural condition achieving most^{26}Al overproduction is for stars to enter the thermally-pulsing AGB (TP-AGB) phase with small cores relative to their total masses, allowing those stars to spend abnormally long times on the TP-AGB compared to single-stars of identical mass. Our population with a binary fraction of 0.75 has an^{26}Al weighted population yield increase of25\%compared to our population of only single-stars. Stellar-models calculated from the Mt Stromlo/Monash Stellar Structure Program, which we use to test our results from binary_c and closely examine the interior structure of the overproducing stars, support our binary_c results only when the stellar envelope gains mass after core-He depletion. Stars which gain mass before core-He depletion still overproduce^{26}Al, but to a lesser extent. This introduces some physical uncertainty into our conclusions as55\%of our^{26}Al overproducing stars gain envelope mass through stellar wind accretion onto pre-AGB objects. Our work highlights the need to consider binary influence on the production of^{26}Al.△ Less"
"Red Teaming Generative AI/NLP, the BB84 quantum cryptography protocol and the NIST-approved Quantum-Resistant Cryptographic Algorithms","Authors:Petar Radanliev,David De Roure,Omar Santos","Abstract:…groundwork, which this study seeks to expand upon, aiming to translate theoretical insights into actionable, real-world cybersecurity solutions. Located at the University ofOxford'stechnology precinct, the research benefits from state-of-the-art infrastructure and a rich collaborative environment. The study's overarching goal is to ensure that as t…▽ MoreIn the contemporary digital age, Quantum Computing and Artificial Intelligence (AI) convergence is reshaping the cyber landscape, introducing unprecedented opportunities and potential vulnerabilities.This research, conducted over five years, delves into the cybersecurity implications of this convergence, with a particular focus on AI/Natural Language Processing (NLP) models and quantum cryptographic protocols, notably the BB84 method and specific NIST-approved algorithms. Utilising Python and C++ as primary computational tools, the study employs a ""red teaming"" approach, simulating potential cyber-attacks to assess the robustness of quantum security measures. Preliminary research over 12 months laid the groundwork, which this study seeks to expand upon, aiming to translate theoretical insights into actionable, real-world cybersecurity solutions. Located at the University ofOxford'stechnology precinct, the research benefits from state-of-the-art infrastructure and a rich collaborative environment. The study's overarching goal is to ensure that as the digital world transitions to quantum-enhanced operations, it remains resilient against AI-driven cyber threats. The research aims to foster a safer, quantum-ready digital future through iterative testing, feedback integration, and continuous improvement. The findings are intended for broad dissemination, ensuring that the knowledge benefits academia and the global community, emphasising the responsible and secure harnessing of quantum technology.△ Less"
AI Regulation in Europe: From the AI Act to Future Regulatory Challenges,Authors:Philipp Hacker,"Abstract:This chapter provides a comprehensive discussion on AI regulation in the European Union, contrasting it with the more sectoral and self-regulatory approach in the UK. It argues for a hybrid regulatory strategy that combines elements from both philosophies, emphasizing the need for agility and safe harbors to ease compliance. The paper examines the AI Act as a pioneering legislative effort to addre…▽ MoreThis chapter provides a comprehensive discussion on AI regulation in the European Union, contrasting it with the more sectoral and self-regulatory approach in the UK. It argues for a hybrid regulatory strategy that combines elements from both philosophies, emphasizing the need for agility and safe harbors to ease compliance. The paper examines the AI Act as a pioneering legislative effort to address the multifaceted challenges posed by AI, asserting that, while the Act is a step in the right direction, it has shortcomings that could hinder the advancement of AI technologies. The paper also anticipates upcoming regulatory challenges, such as the management of toxic content, environmental concerns, and hybrid threats. It advocates for immediate action to create protocols for regulated access to high-performance, potentially open-source AI systems. Although the AI Act is a significant legislative milestone, it needs additional refinement and global collaboration for the effective governance of rapidly evolving AI technologies.△ Less"
"How weak values illuminate the role of ""hidden""-variables as predictive tools","Authors:Xabier Oianguren-Asua,Albert Solé,Carlos F. Destefani,Xavier Oriols","Abstract:In this chapter we offer an introduction to weak values from a three-fold perspective: first, outlining the protocols that enable their experimental determination; next, deriving their correlates in the quantum formalism and, finally, discussing their ontological significance according to different quantum theories or interpretations. We argue that weak values have predictive power and provide nov…▽ MoreIn this chapter we offer an introduction to weak values from a three-fold perspective: first, outlining the protocols that enable their experimental determination; next, deriving their correlates in the quantum formalism and, finally, discussing their ontological significance according to different quantum theories or interpretations. We argue that weak values have predictive power and provide novel ways to characterise quantum systems. We show that this holds true regardless of ongoing ontological disputes. And, still, we contend that certain ""hidden"" variables theories like Bohmian mechanics constitute very valuable heuristic tools for identifying informative weak values or functions thereof. To illustrate these points, we present a case study concerning quantum thermalization. We show that certain weak values, singled out by Bohmian mechanics as physically relevant, play a crucial role in elucidating the thermalization time of certain systems, whereas standard expectation values are ""blind"" to the onset of thermalization.△ Less"
Dark Side Augmentation: Generating Diverse Night Examples for Metric Learning,"Authors:Albert Mohwald,Tomas Jenicek,Ondřej Chum","Abstract:…anchor mining.
  The proposed method improves over the state-of-the-art results on a standard Tokyo 24/7 day-night retrieval benchmark while preserving the performance onOxfordand Paris datasets. This is achieved without the need of training image pairs of matching day and night images. The source code is available at https://github.com/mohwald/gandtr .▽ MoreImage retrieval methods based on CNN descriptors rely on metric learning from a large number of diverse examples of positive and negative image pairs. Domains, such as night-time images, with limited availability and variability of training data suffer from poor retrieval performance even with methods performing well on standard benchmarks. We propose to train a GAN-based synthetic-image generator, translating available day-time image examples into night images. Such a generator is used in metric learning as a form of augmentation, supplying training data to the scarce domain. Various types of generators are evaluated and analyzed. We contribute with a novel light-weight GAN architecture that enforces the consistency between the original and translated image through edge consistency. The proposed architecture also allows a simultaneous training of an edge detector that operates on both night and day images. To further increase the variability in the training examples and to maximize the generalization of the trained model, we propose a novel method of diverse anchor mining.
  The proposed method improves over the state-of-the-art results on a standard Tokyo 24/7 day-night retrieval benchmark while preserving the performance onOxfordand Paris datasets. This is achieved without the need of training image pairs of matching day and night images. The source code is available at https://github.com/mohwald/gandtr .△ Less"
Learning Dense Flow Field for Highly-accurate Cross-view Camera Localization,"Authors:Zhenbo Song,Xianghui Ze,Jianfeng Lu,Yujiao Shi","Abstract:…improvements compared to state-of-the-art methods. Notably, our approach reduces the median localization error by 89%, 19%, 80% and 35% on the KITTI, Ford multi-AV, VIGOR andOxfordRobotCar datasets, respectively.▽ MoreThis paper addresses the problem of estimating the 3-DoF camera pose for a ground-level image with respect to a satellite image that encompasses the local surroundings. We propose a novel end-to-end approach that leverages the learning of dense pixel-wise flow fields in pairs of ground and satellite images to calculate the camera pose. Our approach differs from existing methods by constructing the feature metric at the pixel level, enabling full-image supervision for learning distinctive geometric configurations and visual appearances across views. Specifically, our method employs two distinct convolution networks for ground and satellite feature extraction. Then, we project the ground feature map to the bird's eye view (BEV) using a fixed camera height assumption to achieve preliminary geometric alignment. To further establish content association between the BEV and satellite features, we introduce a residual convolution block to refine the projected BEV feature. Optical flow estimation is performed on the refined BEV feature map and the satellite feature map using flow decoder networks based on RAFT. After obtaining dense flow correspondences, we apply the least square method to filter matching inliers and regress the ground camera pose. Extensive experiments demonstrate significant improvements compared to state-of-the-art methods. Notably, our approach reduces the median localization error by 89%, 19%, 80% and 35% on the KITTI, Ford multi-AV, VIGOR andOxfordRobotCar datasets, respectively.△ Less"
Can the natural system of viruses reconcile the current taxonomy with an alternative classification useful to clinicians?,Authors:Alexandr A. Ezhov,"Abstract:In 2022, a group of basic and clinical virologists, bioinformaticians, and evolutionary and structural biologists met inOxford, UK, to develop a consensus on methodologies used to classify viruses. They concluded that virus taxonomy, which is hierarchical and based on evolution, is only one of many possible ways to classify viruses. This taxonomy, while sat…▽ MoreIn 2022, a group of basic and clinical virologists, bioinformaticians, and evolutionary and structural biologists met inOxford, UK, to develop a consensus on methodologies used to classify viruses. They concluded that virus taxonomy, which is hierarchical and based on evolution, is only one of many possible ways to classify viruses. This taxonomy, while satisfying the four principles they set out, faces difficulties in coordinating with other classification systems useful to clinicians, infectious disease specialists, agronomists, etc. One example discussed is the grouping of different viral strains that cause different diseases into the species Enterovirus C. Here we show that the use of a previously proposed variant of a natural virus classification system based on the use of Neural Replicator Analysis can resolve this contradiction by establishing the fine structure of the Enterovirus C species, in which strains that cause different diseases are placed in several different cells of the binomial table of viruses. A key element in enabling this is the sophisticated preprocessing of the original viral genomes using neural replicators.△ Less"
VPRTempo: A Fast Temporally Encoded Spiking Neural Network for Visual Place Recognition,"Authors:Adam D. Hines,Peter G. Stratton,Michael Milford,Tobias Fischer","Abstract:…Dependent Plasticity and a supervised delta learning rule enforcing that each output spiking neuron responds to just a single place. We evaluate our system on the Nordland andOxfordRobotCar benchmark localization datasets, which include up to 27k places. We found that VPRTempo's accuracy is comparable to prior SNNs and the popular NetVLAD place recogni…▽ MoreSpiking Neural Networks (SNNs) are at the forefront of neuromorphic computing thanks to their potential energy-efficiency, low latencies, and capacity for continual learning. While these capabilities are well suited for robotics tasks, SNNs have seen limited adaptation in this field thus far. This work introduces a SNN for Visual Place Recognition (VPR) that is both trainable within minutes and queryable in milliseconds, making it well suited for deployment on compute-constrained robotic systems. Our proposed system, VPRTempo, overcomes slow training and inference times using an abstracted SNN that trades biological realism for efficiency. VPRTempo employs a temporal code that determines the timing of a single spike based on a pixel's intensity, as opposed to prior SNNs relying on rate coding that determined the number of spikes; improving spike efficiency by over 100%. VPRTempo is trained using Spike-Timing Dependent Plasticity and a supervised delta learning rule enforcing that each output spiking neuron responds to just a single place. We evaluate our system on the Nordland andOxfordRobotCar benchmark localization datasets, which include up to 27k places. We found that VPRTempo's accuracy is comparable to prior SNNs and the popular NetVLAD place recognition algorithm, while being several orders of magnitude faster and suitable for real-time deployment -- with inference speeds over 50 Hz on CPU. VPRTempo could be integrated as a loop closure component for online SLAM on resource-constrained systems such as space and underwater robots.△ Less"
How to foster innovation in the social sciences? Qualitative evidence from focus group workshops atOxfordUniversity,"Authors:Fabian Braesemann,Moritz Marpe","Abstract:This report addresses challenges and opportunities for innovation in the social sciences at the University ofOxford. It summarises findings from two focus group workshops with innovation experts from the University ecosystem. Experts included successful social science entrepreneurs and professional service staff from the University. The workshops focused on…▽ MoreThis report addresses challenges and opportunities for innovation in the social sciences at the University ofOxford. It summarises findings from two focus group workshops with innovation experts from the University ecosystem. Experts included successful social science entrepreneurs and professional service staff from the University. The workshops focused on four different dimensions related to innovative activities and commercialisation. The findings show several challenges at the institutional and individual level, together with features of the social scientific discipline that impede more innovation in the social sciences. Based on identifying these challenges, we present potential solutions and ways forward identified in the focus group discussions to foster social science innovation. The report aims to illustrate the potential of innovation and commercialisation of social scientific research for both researchers and the university.△ Less"
The life and entangled adventures of Schrodinger's cat,Authors:Anthony Sudbery,"Abstract:In this lecture, intended for a general audience, I describe Schrödinger's thought experiment which was designed to show the strange results of extending the formalism of quantum theory, particularly the idea of superposition, beyond the subatomic regime. I describe a way to understand superposition in the terms of formal logic. I trace the development of Schrodinger's ideas after this thought exp…▽ MoreIn this lecture, intended for a general audience, I describe Schrödinger's thought experiment which was designed to show the strange results of extending the formalism of quantum theory, particularly the idea of superposition, beyond the subatomic regime. I describe a way to understand superposition in the terms of formal logic. I trace the development of Schrodinger's ideas after this thought experiment, and briefly sketch some work which realises it in actual experiments, and proposals for taking it further.△ Less"
Towards Content-based Pixel Retrieval in RevisitedOxfordand Paris,"Authors:Guoyuan An,Woo Jae Kim,Saelyne Yang,Rong Li,Yuchi Huo,Sung-Eui Yoon","Abstract:This paper introduces the first two pixel retrieval benchmarks. Pixel retrieval is segmented instance retrieval. Like semantic segmentation extends classification to the pixel level, pixel retrieval is an extension of image retrieval and offers information about which pixels are related to the query object. In addition to retrieving images for the given query, it helps users quickly identify the q…▽ MoreThis paper introduces the first two pixel retrieval benchmarks. Pixel retrieval is segmented instance retrieval. Like semantic segmentation extends classification to the pixel level, pixel retrieval is an extension of image retrieval and offers information about which pixels are related to the query object. In addition to retrieving images for the given query, it helps users quickly identify the query object in true positive images and exclude false positive images by denoting the correlated pixels. Our user study results show pixel-level annotation can significantly improve the user experience.
  Compared with semantic and instance segmentation, pixel retrieval requires a fine-grained recognition capability for variable-granularity targets. To this end, we propose pixel retrieval benchmarks named PROxford and PRParis, which are based on the widely used image retrieval datasets, ROxford and RParis. Three professional annotators label 5,942 images with two rounds of double-checking and refinement. Furthermore, we conduct extensive experiments and analysis on the SOTA methods in image search, image matching, detection, segmentation, and dense matching using our pixel retrieval benchmarks. Results show that the pixel retrieval task is challenging to these approaches and distinctive from existing problems, suggesting that further research can advance the content-based pixel-retrieval and thus user search experience. The datasets can be downloaded from \href{https://github.com/anguoyuan/Pixel_retrieval-Segmented_instance_retrieval}{this link}.△ Less"
Evaluating Visual Odometry Methods for Autonomous Driving in Rain,"Authors:Yu Xiang Tan,Marcel Bartholomeus Prasetyo,Mohammad Alif Daffa,Deshpande Sunny Nitin,Malika Meghjani","Abstract:…rainy weather urban driving data to evaluate their robustness. We compiled a dataset comprising of a range of rainy weather conditions from different cities. This includes, theOxfordRobotcar dataset fromOxford, the 4Seasons dataset from Munich and an internal dataset collected in Singapore. We evaluated different vi…▽ MoreThe increasing demand for autonomous vehicles has created a need for robust navigation systems that can also operate effectively in adverse weather conditions. Visual odometry is a technique used in these navigation systems, enabling the estimation of vehicle position and motion using input from onboard cameras. However, visual odometry accuracy can be significantly impacted in challenging weather conditions, such as heavy rain, snow, or fog. In this paper, we evaluate a range of visual odometry methods, including our DROID-SLAM based heuristic approach. Specifically, these algorithms are tested on both clear and rainy weather urban driving data to evaluate their robustness. We compiled a dataset comprising of a range of rainy weather conditions from different cities. This includes, theOxfordRobotcar dataset fromOxford, the 4Seasons dataset from Munich and an internal dataset collected in Singapore. We evaluated different visual odometry algorithms for both monocular and stereo camera setups using the Absolute Trajectory Error (ATE). From the range of approaches evaluated, our findings suggest that the Depth and Flow for Visual Odometry (DF-VO) algorithm with monocular setup performed the best for short range distances (< 500m) and our proposed DROID-SLAM based heuristic approach for the stereo setup performed relatively well for long-term localization. Both VO algorithms suggested a need for a more robust sensor fusion based approach for localization in rain.△ Less"
"Gas clumping in the outskirts of galaxy clusters, an assessment of the sensitivity of STAR-X","Authors:Christian T. Norseth,Daniel R. Wik,John A. ZuHone,Eric D. Miller,Marshall W. Bautz,Michael McDonald","Abstract:In the outskirts of galaxy clusters, entropy profiles measured from X-ray observations of the hot intracluster medium (ICM) drops off unexpectedly. One possible explanation for this effect is gas clumping, where pockets of cooler and denser structures within the ICM are present. Current observatories are unable to directly detect these hypothetical gas clumps. One of the science drivers of the pro…▽ MoreIn the outskirts of galaxy clusters, entropy profiles measured from X-ray observations of the hot intracluster medium (ICM) drops off unexpectedly. One possible explanation for this effect is gas clumping, where pockets of cooler and denser structures within the ICM are present. Current observatories are unable to directly detect these hypothetical gas clumps. One of the science drivers of the proposed STAR-X observatory is to resolve these or similar structures. Its high spatial resolution, large effective area, and low instrumental background make STAR-X ideal for directly detecting and characterizing clumps and diffuse emission in cluster outskirts. The aim of this work is to simulate observations of clumping in clusters to determine how well STAR-X will be able to detect clumps, as well as what clumping properties reproduce observed entropy profiles. This is achieved by using yt, pyXSIM, SOXS, and other tools to inject ideally modeled clumps into three-dimensional models derived from actual clusters using their observed profiles from other X-ray missions. Radial temperature and surface brightness profiles are then extracted from mock observations using concentric annuli. We find that in simulated observations for STAR-X, a parameter space of clump properties exists where gas clumps can be successfully identified using wavdetect and masked, and are able to recover the true cluster profiles. This demonstrates that STAR-X could be capable of detecting substructure in the outskirts of nearby clusters and that the properties of both the outskirts and the clumps will be revealed.△ Less"
Sample Complexity of Robust Learning against Evasion Attacks,Authors:Pascale Gourdeau,"Abstract:It is becoming increasingly important to understand the vulnerability of machine learning models to adversarial attacks. One of the fundamental problems in adversarial machine learning is to quantify how much training data is needed in the presence of evasion attacks, where data is corrupted at test time. In this thesis, we work with the exact-in-the-ball notion of robustness and study the feasibi…▽ MoreIt is becoming increasingly important to understand the vulnerability of machine learning models to adversarial attacks. One of the fundamental problems in adversarial machine learning is to quantify how much training data is needed in the presence of evasion attacks, where data is corrupted at test time. In this thesis, we work with the exact-in-the-ball notion of robustness and study the feasibility of adversarially robust learning from the perspective of learning theory, considering sample complexity.
  We first explore the setting where the learner has access to random examples only, and show that distributional assumptions are essential. We then focus on learning problems with distributions on the input data that satisfy a Lipschitz condition and show that robustly learning monotone conjunctions has sample complexity at least exponential in the adversary's budget (the maximum number of bits it can perturb on each input). However, if the adversary is restricted to perturbingO(\log n)bits, then one can robustly learn conjunctions and decision lists w.r.t. log-Lipschitz distributions.
  We then study learning models where the learner is given more power. We first consider local membership queries, where the learner can query the label of points near the training sample. We show that, under the uniform distribution, the exponential dependence on the adversary's budget to robustly learn conjunctions remains inevitable. We then introduce a local equivalence query oracle, which returns whether the hypothesis and target concept agree in a given region around a point in the training sample, and a counterexample if it exists. We show that if the query radius is equal to the adversary's budget, we can develop robust empirical risk minimization algorithms in the distribution-free setting. We give general query complexity upper and lower bounds, as well as for concrete concept classes.△ Less"
From RATs to riches: mitigating anthropogenic and synanthropic noise in atom interferometer searches for ultra-light dark matter,"Authors:John Carlton,Christopher McCabe","Abstract:…noise sources and examine their influence on a year-long measurement campaign by AION-10, an upcoming atom interferometer experiment that will be located at the University ofOxford. We propose a data cleaning framework that identifies and then masks anthropogenic and synanthropic noise. With this framework, we demonstrate that even in noisy conditions, the…▽ MoreAtom interferometers offer promising new avenues for detecting ultra-light dark matter (ULDM). The exceptional sensitivity of atom interferometers to fluctuations in the local gravitational potential exposes them to sources of noise from human (anthropogenic) and animal (synanthropic) activity, which may obscure signals from ULDM. We characterise potential anthropogenic and synanthropic noise sources and examine their influence on a year-long measurement campaign by AION-10, an upcoming atom interferometer experiment that will be located at the University ofOxford. We propose a data cleaning framework that identifies and then masks anthropogenic and synanthropic noise. With this framework, we demonstrate that even in noisy conditions, the sensitivity to ULDM can be restored to within between 10% and 40% of an atom shot noise-limited experiment, depending on the specific composition of the anthropogenic and synanthropic noise. This work provides an important step towards creating robust noise reduction analysis strategies in the pursuit of ULDM detection with atom interferometers.△ Less"
Robust Monocular Depth Estimation under Challenging Conditions,"Authors:Stefano Gasperini,Nils Morbitzer,HyunJun Jung,Nassir Navab,Federico Tombari","Abstract:…model to recover information across diverse conditions without modifications at inference time. Extensive experiments on two challenging public datasets, namely nuScenes andOxfordRobotCar, demonstrate the effectiveness of our techniques, outperforming prior works by a large margin in both standard and challenging conditions. Source code and data are availa…▽ MoreWhile state-of-the-art monocular depth estimation approaches achieve impressive results in ideal settings, they are highly unreliable under challenging illumination and weather conditions, such as at nighttime or in the presence of rain. In this paper, we uncover these safety-critical issues and tackle them with md4all: a simple and effective solution that works reliably under both adverse and ideal conditions, as well as for different types of learning supervision. We achieve this by exploiting the efficacy of existing methods under perfect settings. Therefore, we provide valid training signals independently of what is in the input. First, we generate a set of complex samples corresponding to the normal training ones. Then, we train the model by guiding its self- or full-supervision by feeding the generated samples and computing the standard losses on the corresponding original images. Doing so enables a single model to recover information across diverse conditions without modifications at inference time. Extensive experiments on two challenging public datasets, namely nuScenes andOxfordRobotCar, demonstrate the effectiveness of our techniques, outperforming prior works by a large margin in both standard and challenging conditions. Source code and data are available at: https://md4all.github.io.△ Less"
Canonicity and Computability in Homotopy Type Theory,Authors:Dmitry Filippov,"Abstract:This dissertation gives an overview of Martin Lof's dependant type theory, focusing on its computational content and addressing a question of possibility of fully canonical and computable semantic presentation.This dissertation gives an overview of Martin Lof's dependant type theory, focusing on its computational content and addressing a question of possibility of fully canonical and computable semantic presentation.△ Less"
A Gaia view of the optical and X-ray luminosities of compact binary millisecond pulsars,"Authors:Karri I. I. Koljonen,Manuel Linares","Abstract:In this paper, we study compact binary millisecond pulsars with low- and very low-mass companion stars (spiders) in the Galactic field, using data from the latest Gaia data release (DR3). We infer the parallax distances of the optical counterparts to spiders, which we use to estimate optical and X-ray luminosities. We compare the parallax distances to those derived from radio pulse dispersion meas…▽ MoreIn this paper, we study compact binary millisecond pulsars with low- and very low-mass companion stars (spiders) in the Galactic field, using data from the latest Gaia data release (DR3). We infer the parallax distances of the optical counterparts to spiders, which we use to estimate optical and X-ray luminosities. We compare the parallax distances to those derived from radio pulse dispersion measures and find that they have systematically larger values, by 40% on average. We also test the correlation between X-ray and spin-down luminosities, finding that most redbacks have a spin-down to X-ray luminosity conversion efficiency of $\sim$0.1%, indicating a contribution from the intrabinary shock. On the other hand, most black widows have an efficiency of $\sim$0.01%, similar to the majority of the pulsar population. Finally, we find that the bolometric optical luminosity significantly correlates with the orbital period, with a large scatter due to different irradiated stellar temperatures and binary properties. We interpret this correlation as the effect of the increasing size of the Roche Lobe radius with the orbital period. With this newly found correlation, an estimate of the optical magnitude can be obtained from the orbital period and a distance estimate.△ Less"
Global Features are All You Need for Image Retrieval and Reranking,"Authors:Shihao Shao,Kaifeng Chen,Arjun Karpur,Qinghua Cui,Andre Araujo,Bingyi Cao","Abstract:…being very compute and memory efficient. Our experiments demonstrate substantial improvements compared to the state of the art in standard benchmarks. Notably, on the RevisitedOxford+1M Hard dataset, our single-stage results improve by 7.1%, while our two-stage gain reaches 3.7% with a strong 64,865x speedup. Our two-stage system surpasses the current singl…▽ MoreImage retrieval systems conventionally use a two-stage paradigm, leveraging global features for initial retrieval and local features for reranking. However, the scalability of this method is often limited due to the significant storage and computation cost incurred by local feature matching in the reranking stage. In this paper, we present SuperGlobal, a novel approach that exclusively employs global features for both stages, improving efficiency without sacrificing accuracy. SuperGlobal introduces key enhancements to the retrieval system, specifically focusing on the global feature extraction and reranking processes. For extraction, we identify sub-optimal performance when the widely-used ArcFace loss and Generalized Mean (GeM) pooling methods are combined and propose several new modules to improve GeM pooling. In the reranking stage, we introduce a novel method to update the global features of the query and top-ranked images by only considering feature refinement with a small set of images, thus being very compute and memory efficient. Our experiments demonstrate substantial improvements compared to the state of the art in standard benchmarks. Notably, on the RevisitedOxford+1M Hard dataset, our single-stage results improve by 7.1%, while our two-stage gain reaches 3.7% with a strong 64,865x speedup. Our two-stage system surpasses the current single-stage state-of-the-art by 16.3%, offering a scalable, accurate alternative for high-performing image retrieval systems with minimal time overhead. Code: https://github.com/ShihaoShao-GH/SuperGlobal.△ Less"
Coarse-to-Fine: Learning Compact Discriminative Representation for Single-Stage Image Retrieval,"Authors:Yunquan Zhu,Xinkai Gao,Bo Ke,Ruizhi Qiao,Xing Sun","Abstract:…experimental results have demonstrated the effectiveness of our method, which achieves state-of-the-art single-stage image retrieval performance on benchmarks such as RevisitedOxfordand Revisited Paris. Code is available at https://github.com/bassyess/CFCD.▽ MoreImage retrieval targets to find images from a database that are visually similar to the query image. Two-stage methods following retrieve-and-rerank paradigm have achieved excellent performance, but their separate local and global modules are inefficient to real-world applications. To better trade-off retrieval efficiency and accuracy, some approaches fuse global and local feature into a joint representation to perform single-stage image retrieval. However, they are still challenging due to various situations to tackle, $e.g.$, background, occlusion and viewpoint. In this work, we design a Coarse-to-Fine framework to learn Compact Discriminative representation (CFCD) for end-to-end single-stage image retrieval-requiring only image-level labels. Specifically, we first design a novel adaptive softmax-based loss which dynamically tunes its scale and margin within each mini-batch and increases them progressively to strengthen supervision during training and intra-class compactness. Furthermore, we propose a mechanism which attentively selects prominent local descriptors and infuse fine-grained semantic relations into the global representation by a hard negative sampling strategy to optimize inter-class distinctiveness at a global scale. Extensive experimental results have demonstrated the effectiveness of our method, which achieves state-of-the-art single-stage image retrieval performance on benchmarks such as RevisitedOxfordand Revisited Paris. Code is available at https://github.com/bassyess/CFCD.△ Less"
Quench Risk Increase With Irradiation Damage,"Authors:Jacob John,Mark Gilbert,Chris Hardie","Abstract:Superconducting material enables fusion reactor magnet concepts to operate with current densities that would melt materials with non-zero resistance. The application of superconducting material is considered essential for net-positive power machines. Catastrophic damage can occur when superconductivity is lost and the current generates heat. This scenario is called a quench. Stabilizer material ca…▽ MoreSuperconducting material enables fusion reactor magnet concepts to operate with current densities that would melt materials with non-zero resistance. The application of superconducting material is considered essential for net-positive power machines. Catastrophic damage can occur when superconductivity is lost and the current generates heat. This scenario is called a quench. Stabilizer material carries the magnet current (typically copper) during a quench and is the focus of this work. Irradiation-induced defects store energy in the Cu crystalline lattice. The release of stored energy in the magnet materials, combined with the associated magnet material property changes, can cause extreme off-normal events in superconducting magnets that worsen with fluence at an increasing rate. Stored energy can be released causing local heating and increasing the risk of a quench. For example, following irradiation at 4.6K and an estimated fluence of 0.45*10^18 n/cm^2, an energy release of 0.023 J/g was measured from Cu when increased in temperature from 10K to 18K, which would have been enough energy to create the same temperature increase spontaneously. Extrapolations of experimental data are used to estimate when spontaneous heating can occur due to the release of energy stored in irradiation-induced defects. Critical fluence values are estimated between 1.74*10^18 n/cm^2 and 2.85*10^19 n/cm^2 for neutron irradiation of Cu at a temperature of 20K. High-temperature ramp rate in-situ cryogenic calorimetry experiments of magnet materials following irradiation would provide more clarity to designers of fusion magnet systems. Due to the increased quench risk with superconducting magnet dose, magnetic confinement reactor designers should consider the frequency of maintenance temperature cycles to maintain an appropriate level of risk during operation.△ Less"
String Field Theory,Authors:Carlo Maccaferri,"Abstract:…non-perturbative structure and background independence. This article gives a concise overview on the subject and of some of the main recent progress.
  Note: Review article forOxfordResearch Encyclopedia of Physics▽ MoreString Field Theory is a formulation of String Theory as a Quantum Field Theory in target space. It allows to tame the infrared divergences of String Theory and to approach its non-perturbative structure and background independence. This article gives a concise overview on the subject and of some of the main recent progress.
  Note: Review article forOxfordResearch Encyclopedia of Physics△ Less"
Proceedings Fifth International Conference on Applied Category Theory,"Authors:Jade Master,Martha Lewis","Abstract:…on Applied Category Theory took place at the University of Strathclyde in Glasgow, Scotland on 18-22 July 2022. This conference follows the previous meetings at Leiden (2018),Oxford(2019), MIT (2020, fully online), and Cambridge (2021). The conference comprised 59 contributed talks, a poster session, an industry showcase session, and a session where junior…▽ MoreThe Fifth International Conference on Applied Category Theory took place at the University of Strathclyde in Glasgow, Scotland on 18-22 July 2022. This conference follows the previous meetings at Leiden (2018),Oxford(2019), MIT (2020, fully online), and Cambridge (2021). The conference comprised 59 contributed talks, a poster session, an industry showcase session, and a session where junior researchers who had attended the Adjoint School presented the results of their research at the school. Information regarding the conference may be found at (https://msp.cis.strath.ac.uk/act2022).
  The contributions to ACT2022 ranged from pure to applied and included contributions in a wide range of disciplines in science and engineering. ACT2022 included talks in linguistics, functional programming, classical mechanics, quantum physics, probability theory, electrical engineering, epidemiology, thermodynamics, engineering, and logic. ACT2022 was sponsored by Huawei, Protocol Labs, Cambridge Quantum, Conexus, Topos, and SICSA (Scottish Informatics and Computer Science Alliance).
  Submission to ACT2022 had three tracks: extended abstracts, software demonstrations, and proceedings. The extended abstract and software demonstration submissions had a page limit of 2 pages, and the proceedings track had a page limit of 14 pages. Only papers submitted to the proceedings track were considered for publication in this volume. In total, there were 97 submissions, of which 59 were accepted for presentation and 24 for publication in this volume. Publication of accepted submissions in the proceedings was determined by personal choice of the authors and not based on quality. Each submission received a review from three different members of the programming committee, and papers were selected based on discussion and consensus by these reviewers.△ Less"
Infinite Permutation Groups,Authors:Peter M. Neumann,Abstract:This is a transcript of a lecture course on Infinite Permutation Groups given by Peter M. Neumann (1940-2020) inOxfordduring the academic year 1988-1989. The field of Infinite Permutation Groups only emerged as an independent field of study in the 1980's. Most of the results described in these notes were at the time of the lectures brand new and had ei…▽ MoreThis is a transcript of a lecture course on Infinite Permutation Groups given by Peter M. Neumann (1940-2020) inOxfordduring the academic year 1988-1989. The field of Infinite Permutation Groups only emerged as an independent field of study in the 1980's. Most of the results described in these notes were at the time of the lectures brand new and had either just recently appeared in print or had not appeared formally. A large part of the results described is either due to Peter himself or heavily influenced by him. These notes offer Peter's personal take on a field that he was instrumental in creating and in many cases ideas and questions that can not be found in the published literature.△ Less
The De Broglie-Bohm theory {\it is} and {\it is not} a hidden variable theory,Authors:Jean Bricmont,"Abstract:We will first define what is meant by ``hidden variables"". Then, we will review various theorems proving the impossibility of theories introducing such variables and then show that the de Broglie-Bohm theory is not refuted by those theorems. We will also explain the relation between those theorems and nonlocality, with or without introducing Bell's inequalities.We will first define what is meant by ``hidden variables"". Then, we will review various theorems proving the impossibility of theories introducing such variables and then show that the de Broglie-Bohm theory is not refuted by those theorems. We will also explain the relation between those theorems and nonlocality, with or without introducing Bell's inequalities.△ Less"
Proceedings Nineteenth conference on Theoretical Aspects of Rationality and Knowledge,Authors:Rineke Verbrugge,"Abstract:…for presentation at the Nineteenth Conference on Theoretical Aspects of Rationality and Knowledge (TARK 2023), held between June 28 and June 30, 2023, at the University ofOxford, United Kingdom. The conference website can be found at https://sites.google.com/view/tark-2023▽ MoreThe TARK conference (Theoretical Aspects of Rationality and Knowledge) is a conference that aims to bring together researchers from a wide variety of fields, including computer science, artificial intelligence, game theory, decision theory, philosophy, logic, linguistics, and cognitive science. Its goal is to further our understanding of interdisciplinary issues involving reasoning about rationality and knowledge.
  Previous conferences have been held biennially around the world since 1986, on the initiative of Joe Halpern (Cornell University). Topics of interest include, but are not limited to, semantic models for knowledge, belief, awareness and uncertainty, bounded rationality and resource-bounded reasoning, commonsense epistemic reasoning, epistemic logic, epistemic game theory, knowledge and action, applications of reasoning about knowledge and other mental states, belief revision, computational social choice, algorithmic game theory, and foundations of multi-agent systems. Information about TARK, including conference proceedings, is available at http://www.tark.org/
  These proceedings contain the papers that have been accepted for presentation at the Nineteenth Conference on Theoretical Aspects of Rationality and Knowledge (TARK 2023), held between June 28 and June 30, 2023, at the University ofOxford, United Kingdom. The conference website can be found at https://sites.google.com/view/tark-2023△ Less"
"Audio-visual End-to-end Multi-channel Speech Separation, Dereverberation and Recognition","Authors:Guinan Li,Jiajun Deng,Mengzhe Geng,Zengrui Jin,Tianzi Wang,Shujie Hu,Mingyu Cui,Helen Meng,Xunying Liu","Abstract:…interpolation with the speech enhancement loss. Experiments were conducted on the mixture overlapped and reverberant speech data constructed using simulation or replay of theOxfordLRS2 dataset. The proposed audio-visual multi-channel speech separation, dereverberation and recognition systems consistently outperformed the comparable audio-only baseline by 9…▽ MoreAccurate recognition of cocktail party speech containing overlapping speakers, noise and reverberation remains a highly challenging task to date. Motivated by the invariance of visual modality to acoustic signal corruption, an audio-visual multi-channel speech separation, dereverberation and recognition approach featuring a full incorporation of visual information into all system components is proposed in this paper. The efficacy of the video input is consistently demonstrated in mask-based MVDR speech separation, DNN-WPE or spectral mapping (SpecM) based speech dereverberation front-end and Conformer ASR back-end. Audio-visual integrated front-end architectures performing speech separation and dereverberation in a pipelined or joint fashion via mask-based WPD are investigated. The error cost mismatch between the speech enhancement front-end and ASR back-end components is minimized by end-to-end jointly fine-tuning using either the ASR cost function alone, or its interpolation with the speech enhancement loss. Experiments were conducted on the mixture overlapped and reverberant speech data constructed using simulation or replay of theOxfordLRS2 dataset. The proposed audio-visual multi-channel speech separation, dereverberation and recognition systems consistently outperformed the comparable audio-only baseline by 9.1% and 6.2% absolute (41.7% and 36.0% relative) word error rate (WER) reductions. Consistent speech enhancement improvements were also obtained on PESQ, STOI and SRMR scores.△ Less"
A review of uranium-based thin films,"Authors:R. Springell,E. Lawrence Bright,D. A. Chaney,L. M. Harding,C. Bell,R. C. C. Ward,G. H. Lander","Abstract:…Thin films of uranium and uranium dioxide were fabricated in the 1960s and 1970s, but there was little sustained effort until the early 2000s. Significant programmes started atOxfordUniversity (transferring to Bristol University in 2011), and Los Alamos National Laboratory (LANL) in New Mexico, USA. In this review we cover the work that has been published…▽ MoreThin films based on silicon and transition-metal elements dominate the semiconducting industry and are ubiquitous in all modern devices. Films have also been produced in the rare-earth series of elements for both research and specialized applications. Thin films of uranium and uranium dioxide were fabricated in the 1960s and 1970s, but there was little sustained effort until the early 2000s. Significant programmes started atOxfordUniversity (transferring to Bristol University in 2011), and Los Alamos National Laboratory (LANL) in New Mexico, USA. In this review we cover the work that has been published over the last ~20 years with these materials. Important breakthroughs occurred with the fabrication of epitaxial thin films of initially uranium metal and UO2, but more recently of many other uranium compounds and alloys. These have led to a number of different experiments that are reviewed, as well as some important trends. The interaction with the substrate leads to differing strain and hence changes in properties. An important advantage is that epitaxial films can often be made of materials that are impossible to produce as bulk single crystals. Examples are U3O8, U2N3 and alloys of U-Mo, which form in a modified bcc structure. Epitaxial films may also be used in applied research. They represent excellent surfaces, and it is at the surfaces that most of the important reactions occur in the nuclear fuel cycle. For example, the fuel-cladding interactions, and the dissolution of fuel by water in the long-term storage of spent fuel. To conclude, we discuss possible future prospects, examples include bilayers containing uranium for spintronics, and superlattices that could be used in heterostructures. Such applications will require a more detailed knowledge of the interface interactions in these systems, and this is an important direction for future research.△ Less"
"UnLoc: A Universal Localization Method for Autonomous Vehicles using LiDAR, Radar and/or Camera Input","Authors:Muhammad Ibrahim,Naveed Akhtar,Saeed Anwar,Ajmal Mian","Abstract:…the Radar and image modalities. We introduce a unique learnable modality encoding scheme to distinguish between the input sensor data. Our method is extensively evaluated onOxfordRadar RobotCar, ApolloSouthBay and Perth-WA datasets. The results ascertain the efficacy of our technique.▽ MoreLocalization is a fundamental task in robotics for autonomous navigation. Existing localization methods rely on a single input data modality or train several computational models to process different modalities. This leads to stringent computational requirements and sub-optimal results that fail to capitalize on the complementary information in other data streams. This paper proposes UnLoc, a novel unified neural modeling approach for localization with multi-sensor input in all weather conditions. Our multi-stream network can handle LiDAR, Camera and RADAR inputs for localization on demand, i.e., it can work with one or more input sensors, making it robust to sensor failure. UnLoc uses 3D sparse convolutions and cylindrical partitioning of the space to process LiDAR frames and implements ResNet blocks with a slot attention-based feature filtering module for the Radar and image modalities. We introduce a unique learnable modality encoding scheme to distinguish between the input sensor data. Our method is extensively evaluated onOxfordRadar RobotCar, ApolloSouthBay and Perth-WA datasets. The results ascertain the efficacy of our technique.△ Less"
ProbVLM: Probabilistic Adapter for Frozen Vision-Language Models,"Authors:Uddeshya Upadhyay,Shyamgopal Karthik,Massimiliano Mancini,Zeynep Akata","Abstract:…VLMs via inter/intra-modal alignment in a post-hoc manner without needing large-scale datasets or computing. On four challenging datasets, i.e., COCO, Flickr, CUB, andOxford-flowers, we estimate the multi-modal embedding uncertainties for two VLMs, i.e., CLIP and BLIP, quantify the calibration of embedding uncertainties in retrieval tasks and show that Prob…▽ MoreLarge-scale vision-language models (VLMs) like CLIP successfully find correspondences between images and text. Through the standard deterministic mapping process, an image or a text sample is mapped to a single vector in the embedding space. This is problematic: as multiple samples (images or text) can abstract the same concept in the physical world, deterministic embeddings do not reflect the inherent ambiguity in the embedding space. We propose ProbVLM, a probabilistic adapter that estimates probability distributions for the embeddings of pre-trained VLMs via inter/intra-modal alignment in a post-hoc manner without needing large-scale datasets or computing. On four challenging datasets, i.e., COCO, Flickr, CUB, andOxford-flowers, we estimate the multi-modal embedding uncertainties for two VLMs, i.e., CLIP and BLIP, quantify the calibration of embedding uncertainties in retrieval tasks and show that ProbVLM outperforms other methods. Furthermore, we propose active learning and model selection as two real-world downstream tasks for VLMs and show that the estimated uncertainty aids both tasks. Lastly, we present a novel technique for visualizing the embedding distributions using a large-scale pre-trained latent diffusion model. Code is available at https://github.com/ExplainableML/ProbVLM.△ Less"
DisPlacing Objects: Improving Dynamic Vehicle Detection via Visual Place Recognition under Adverse Conditions,"Authors:Stephen Hausler,Sourav Garg,Punarjay Chakravarty,Shubham Shrivastava,Ankit Vora,Michael Milford","Abstract:…the performance of vehicle detection when combined with an existing off-the-shelf vehicle detector. We demonstrate our approach using standard datasets across two cities (Oxfordand Zurich) under different settings of train-test separation of map-query traverse pairs. We further emphasize the performance gains of our approach against alternative design choic…▽ MoreCan knowing where you are assist in perceiving objects in your surroundings, especially under adverse weather and lighting conditions? In this work we investigate whether a prior map can be leveraged to aid in the detection of dynamic objects in a scene without the need for a 3D map or pixel-level map-query correspondences. We contribute an algorithm which refines an initial set of candidate object detections and produces a refined subset of highly accurate detections using a prior map. We begin by using visual place recognition (VPR) to retrieve a reference map image for a given query image, then use a binary classification neural network that compares the query and mapping image regions to validate the query detection. Once our classification network is trained, on approximately 1000 query-map image pairs, it is able to improve the performance of vehicle detection when combined with an existing off-the-shelf vehicle detector. We demonstrate our approach using standard datasets across two cities (Oxfordand Zurich) under different settings of train-test separation of map-query traverse pairs. We further emphasize the performance gains of our approach against alternative design choices and show that VPR suffices for the task, eliminating the need for precise ground truth localization.△ Less"
Can Machines Garden? Systematically Comparing the AlphaGarden vs. Professional Horticulturalists,"Authors:Simeon Adebola,Rishi Parikh,Mark Presten,Satvik Sharma,Shrey Aeron,Ananth Rao,Sandeep Mukherjee,Tomson Qu,Christina Wistrom,Eugen Solowjow,Ken Goldberg","Abstract:…pruning tools and algorithms. In this paper, we systematically compare the performance of the AlphaGarden to professional horticulturalists on the staff of the UC BerkeleyOxfordTract Greenhouse. The humans and the machine tend side-by-side polyculture gardens with the same seed arrangement. We compare performance in terms of canopy coverage, plant diversit…▽ MoreThe AlphaGarden is an automated testbed for indoor polyculture farming which combines a first-order plant simulator, a gantry robot, a seed planting algorithm, plant phenotyping and tracking algorithms, irrigation sensors and algorithms, and custom pruning tools and algorithms. In this paper, we systematically compare the performance of the AlphaGarden to professional horticulturalists on the staff of the UC BerkeleyOxfordTract Greenhouse. The humans and the machine tend side-by-side polyculture gardens with the same seed arrangement. We compare performance in terms of canopy coverage, plant diversity, and water consumption. Results from two 60-day cycles suggest that the automated AlphaGarden performs comparably to professional horticulturalists in terms of coverage and diversity, and reduces water consumption by as much as 44%. Code, videos, and datasets are available at https://sites.google.com/berkeley.edu/systematiccomparison.△ Less"
Ontology Enrichment from Texts: A Biomedical Dataset for Concept Discovery and Placement,"Authors:Hang Dong,Jiaoyan Chen,Yuan He,Ian Horrocks","Abstract:Mentions of new concepts appear regularly in texts and require automated approaches to harvest and place them into Knowledge Bases (KB), e.g., ontologies and taxonomies. Existing datasets suffer from three issues, (i) mostly assuming that a new concept is pre-discovered and cannot support out-of-KB mention discovery; (ii) only using the concept label as the input along with the KB and thus lacking…▽ MoreMentions of new concepts appear regularly in texts and require automated approaches to harvest and place them into Knowledge Bases (KB), e.g., ontologies and taxonomies. Existing datasets suffer from three issues, (i) mostly assuming that a new concept is pre-discovered and cannot support out-of-KB mention discovery; (ii) only using the concept label as the input along with the KB and thus lacking the contexts of a concept label; and (iii) mostly focusing on concept placement w.r.t a taxonomy of atomic concepts, instead of complex concepts, i.e., with logical operators. To address these issues, we propose a new benchmark, adapting MedMentions dataset (PubMed abstracts) with SNOMED CT versions in 2014 and 2017 under the Diseases sub-category and the broader categories of Clinical finding, Procedure, and Pharmaceutical / biologic product. We provide usage on the evaluation with the dataset for out-of-KB mention discovery and concept placement, adapting recent Large Language Model based methods.△ Less"
Condensation Calculations in Planetary Science and Cosmochemistry,Authors:Denton S. Ebel,"Abstract:Cool a piece of the Sun to 1000 K at one millibar pressure to yield a mineral assemblage consistent with those found in the most primitive meteorites. This is an equilibrium or fractional condensation experiment simulated by calculations using equations of state for hundreds of gaseous molecules, condensed mineral solids, and silicate liquids, the products of a century of experimental measurements…▽ MoreCool a piece of the Sun to 1000 K at one millibar pressure to yield a mineral assemblage consistent with those found in the most primitive meteorites. This is an equilibrium or fractional condensation experiment simulated by calculations using equations of state for hundreds of gaseous molecules, condensed mineral solids, and silicate liquids, the products of a century of experimental measurements and theoretical studies. Such calculations have revolutionized our understanding of the chemistry of the cosmos.
  The mid-20th Century realization that meteorites are fossil records of the early Solar System made chemistry central to understanding planetary origins. Thus ""condensation"", the distribution of elements and isotopes between vapor and condensed solids and/or liquids at or approaching chemical equilibrium, deeply informs discussion of how meteor/comet compositions bear on planets.
  Condensation calculations have been applied to disks around young stars, to the mineral ""rain"" of mineral grains expected to form in cool dwarf star atmospheres, to the expanding envelopes of giant stars, to the vapor plumes that form in planetary impacts, and to the chemically and isotopically distinct ""shells"" computed and observed to exist in supernovae. As with all sophisticated calculations, there are inherent caveats, subtleties, and computational difficulties.
  Local chemistry has yet to be consistently integrated into dynamical astrophysical simulations so that effects like the blocking of radiation by grains, absorption and reemission of light by grains, and buffering of heat by grain evaporation/condensation feed back into the physics at each node of a gridded calculation over time. A deeper integration of thermochemistry with physical models makes the prospect of a general protoplanetary disk model as hopeful now as a general circulation model for global climate was in the early 1970's.△ Less"
Off the Radar: Uncertainty-Aware Radar Place Recognition with Introspective Querying and Map Maintenance,"Authors:Jianhao Yuan,Paul Newman,Matthew Gadd","Abstract:…and predicts the downstream uncertainty in metric-space-based place recognition. We prove the effectiveness of our method over extensive cross-validated tests of theOxfordRadar RobotCar and MulRan dataset. In this, we outperform the current state-of-the-art in radar place recognition and other uncertainty-aware methods when using only single nearest-neighb…▽ MoreLocalisation with Frequency-Modulated Continuous-Wave (FMCW) radar has gained increasing interest due to its inherent resistance to challenging environments. However, complex artefacts of the radar measurement process require appropriate uncertainty estimation to ensure the safe and reliable application of this promising sensor modality. In this work, we propose a multi-session map management system which constructs the best maps for further localisation based on learned variance properties in an embedding space. Using the same variance properties, we also propose a new way to introspectively reject localisation queries that are likely to be incorrect. For this, we apply robust noise-aware metric learning, which both leverages the short-timescale variability of radar data along a driven path (for data augmentation) and predicts the downstream uncertainty in metric-space-based place recognition. We prove the effectiveness of our method over extensive cross-validated tests of theOxfordRadar RobotCar and MulRan dataset. In this, we outperform the current state-of-the-art in radar place recognition and other uncertainty-aware methods when using only single nearest-neighbour queries. We also show consistent performance increases when rejecting queries based on uncertainty over a difficult test environment, which we did not observe for a competing uncertainty-aware place recognition system.△ Less"
Microquasar Cyg X-3 -- a unique jet-wind neutrino factory?,"Authors:Karri I. I. Koljonen,Konstancja Satalecka,Elina J. Lindfors,Ioannis Liodakis","Abstract:The origin of astrophysical neutrinos is one of the most debated topics today. Perhaps the most robust evidence of neutrino counterpart comes from supermassive black holes in active galactic nuclei associated with strongly collimated outflows, or jets, that can accelerate particles to relativistic energies and produce neutrinos through hadronic interactions. Similar outflows can also be found from…▽ MoreThe origin of astrophysical neutrinos is one of the most debated topics today. Perhaps the most robust evidence of neutrino counterpart comes from supermassive black holes in active galactic nuclei associated with strongly collimated outflows, or jets, that can accelerate particles to relativistic energies and produce neutrinos through hadronic interactions. Similar outflows can also be found from X-ray binaries, or `microquasars', that consist of a neutron star or a stellar-mass black hole accreting matter from a non-degenerate companion star. In some cases, these systems can accelerate particles up to GeV energies implying an efficient acceleration mechanism in their jets. Neutrino production in microquasar jets can be expected with suitable conditions and a hadronic particle population. Microquasar Cyg X-3 is a unique, short orbital period X-ray binary hosting a Wolf-Rayet companion star with a strong stellar wind. The interaction of the dense stellar wind with a relativistic jet leads to particle collisions followed by high-energy gamma-ray and potentially neutrino emission. Here, using the 10-year neutrino candidate sample of the IceCube neutrino observatory, we find that the events with the highest spatial association with Cyg X-3 occur during short-lived high-energy gamma-ray flaring periods indicating the possible astrophysical nature of these events.△ Less"
Localization with Anticipation for Autonomous Urban Driving in Rain,"Authors:Yu Xiang Tan,Malika Meghjani,Marcel Bartholomeus Prasetyo","Abstract:…information such as the angle of turn which can be potentially used to improve the localization accuracy especially when sensors are compromised. We experimented on theOxfordRobotcar Dataset and our internal dataset from Singapore to validate our localization algorithm in both clear and rain weather conditions. Our method improves localization accuracy by…▽ MoreThis paper presents a localization algorithm for autonomous urban vehicles under rain weather conditions. In adverse weather, human drivers anticipate the location of the ego-vehicle based on the control inputs they provide and surrounding road contextual information. Similarly, in our approach for localization in rain weather, we use visual data, along with a global reference path and vehicle motion model for anticipating and better estimating the pose of the ego-vehicle in each frame. The global reference path contains useful road contextual information such as the angle of turn which can be potentially used to improve the localization accuracy especially when sensors are compromised. We experimented on theOxfordRobotcar Dataset and our internal dataset from Singapore to validate our localization algorithm in both clear and rain weather conditions. Our method improves localization accuracy by 50.83% in rain weather and 34.32% in clear weather when compared to baseline algorithms.△ Less"
Evaluating the Social Impact of Generative AI Systems in Systems and Society,"Authors:Irene Solaiman,Zeerak Talat,William Agnew,Lama Ahmad,Dylan Baker,Su Lin Blodgett,Canyu Chen,Hal Daumé III,Jesse Dodge,Isabella Duan,Ellie Evans,Felix Friedrich,Avijit Ghosh,Usman Gohar,Sara Hooker,Yacine Jernite,Ria Kalluri,Alberto Lusoli,Alina Leidinger,Michelle Lin,Xiuzhu Lin,Sasha Luccioni,Jennifer Mickel,Margaret Mitchell,Jessica Newman, et al. (6 additional authors not shown)","Abstract:Generative AI systems across modalities, ranging from text (including code), image, audio, and video, have broad social impacts, but there is no official standard for means of evaluating those impacts or for which impacts should be evaluated. In this paper, we present a guide that moves toward a standard approach in evaluating a base generative AI system for any modality in two overarching categor…▽ MoreGenerative AI systems across modalities, ranging from text (including code), image, audio, and video, have broad social impacts, but there is no official standard for means of evaluating those impacts or for which impacts should be evaluated. In this paper, we present a guide that moves toward a standard approach in evaluating a base generative AI system for any modality in two overarching categories: what can be evaluated in a base system independent of context and what can be evaluated in a societal context. Importantly, this refers to base systems that have no predetermined application or deployment context, including a model itself, as well as system components, such as training data. Our framework for a base system defines seven categories of social impact: bias, stereotypes, and representational harms; cultural values and sensitive content; disparate performance; privacy and data protection; financial costs; environmental costs; and data and content moderation labor costs. Suggested methods for evaluation apply to listed generative modalities and analyses of the limitations of existing evaluations serve as a starting point for necessary investment in future evaluations. We offer five overarching categories for what can be evaluated in a broader societal context, each with its own subcategories: trustworthiness and autonomy; inequality, marginalization, and violence; concentration of authority; labor and creativity; and ecosystem and environment. Each subcategory includes recommendations for mitigating harm.△ Less"
Optimal sizing of solar photovoltaic and lithium battery storage to reduce grid electricity reliance in buildings,"Authors:Han Kun Ren,Malcolm McCulloch,David Wallom","Abstract:In alignment with the Paris Agreement, the city ofOxfordin the UK aims to become carbon neutral by 2040. Renewable energy help achieve this target by reducing the reliance on carbon-intensive grid electricity. This research seeks to optimally size solar photovoltaic and lithium battery storage systems, reducing…▽ MoreIn alignment with the Paris Agreement, the city ofOxfordin the UK aims to become carbon neutral by 2040. Renewable energy help achieve this target by reducing the reliance on carbon-intensive grid electricity. This research seeks to optimally size solar photovoltaic and lithium battery storage systems, reducingOxford'sgrid electricity reliance in buildings. The analysis starts with modeling the electricity demand. The model uses Elexon electricity settlement profiles, and assembles them into the demand profile according to the quantity and types of buildings inOxford. Then, solar generation is modeled using Pfenninger and Staffell's method. Solar photovoltaic and lithium storage systems are sized using a hybridized analytical and iterative method. First, the method calculates the solar system size search range, then iterates through the range. At each solar size, the method calculates and iterates through the storage system size search range. Within each iteration, the renewable system is simulated using demand and generation data with a simplified system set-up and the conventional operation strategy. The method outputs combinations of solar system capacity, storage system capacity, and grid electricity import. Each combination's levelized cost of electricity is calculated, and the lowest cost combination is the optimal sizing. Solar and storage system costs are projected from 2019 to 2100, and the optimal sizing is calculated for each year. The result shows that solar photovoltaic is economically competitive, but lithium storage cost is still too high. As solar and storage prices continue to drop, they will take up greater portions of the energy system. However, there will always be a need for the grid, as it provides flexibility and can meet demands that are too costly for solar and storage△ Less"
Impact of quasi-periodic and steep-spectrum timing noise on the measurement of pulsar timing parameters,"Authors:Michael J. Keith,Iuliana C. Niţu","Abstract:Timing noise in pulsars is often modelled with a Fourier-basis Gaussian process that follows a power law with periodic boundary conditions on the observation time, $T_\mathrm{span}$. However the actual noise processes can extend well below $1/T_\mathrm{span}$, and many pulsars are known to exhibit quasi-periodic timing noise. In this paper we investigate several adaptions that try to account for t…▽ MoreTiming noise in pulsars is often modelled with a Fourier-basis Gaussian process that follows a power law with periodic boundary conditions on the observation time, $T_\mathrm{span}$. However the actual noise processes can extend well below $1/T_\mathrm{span}$, and many pulsars are known to exhibit quasi-periodic timing noise. In this paper we investigate several adaptions that try to account for these differences between the observed behaviour and the simple power-law model. Firstly, we propose to include an additional term that models the quasi-periodic spin-down variations known to be present in many pulsars. Secondly, we show that a Fourier basis of $1/2T_\mathrm{span}$ can be more suited for estimating long term timing parameters such as the spin frequency second derivative (F2), and is required when the exponent of the power spectrum is greater than ~4. We also implement a Bayesian version of the generalised least squares `Cholesky' method which has different limitations at low frequency, but find that there is little advantage over Fourier-basis methods. We apply our quasi-periodic spin down model to a sample of pulsars with known spin-down variations and show that this improves parameter estimation of F2 and proper motion for the most pathological cases, but in general the results are consistent with a power-law model. The models are all made available through the run_enterprise software package.△ Less"
A systematic literature review on solution approaches for the index tracking problem in the last decade,"Authors:Julio Cezar Soares Silva,Adiel Teixeira de Almeida Filho","Abstract:The passive management approach offers conservative investors a way to reduce risk concerning the market. This investment strategy aims at replicating a specific index, such as the NASDAQ Composite or the FTSE100 index. The problem is that buying all the index's assets incurs high rebalancing costs, and this harms future returns. The index tracking problem concerns building a portfolio that follow…▽ MoreThe passive management approach offers conservative investors a way to reduce risk concerning the market. This investment strategy aims at replicating a specific index, such as the NASDAQ Composite or the FTSE100 index. The problem is that buying all the index's assets incurs high rebalancing costs, and this harms future returns. The index tracking problem concerns building a portfolio that follows a specific benchmark with fewer transaction costs. Since a subset of assets is required to solve the index problem this class of problems is NP-hard, and in the past years, researchers have been studying solution approaches to obtain tracking portfolios more practically. This work brings an analysis, spanning the last decade, of the advances in mathematical approaches for index tracking. The systematic literature review covered important issues, such as the most relevant research areas, solution methods, and model structures. Special attention was given to the exploration and analysis of metaheuristics applied to the index tracking problem.△ Less"
SelFLoc: Selective Feature Fusion for Large-scale Point Cloud-based Place Recognition,"Authors:Qibo Qiu,Haiming Gao,Wenxiao Wang,Zhiyi Su,Tian Xie,Wei Hua,Xiaofei He","Abstract:…for point cloud-based place recognition, which is termed SelFLoc. Comparative experimental results show that SelFLoc achieves the state-of-the-art (SOTA) performance on theOxfordand other three in-house benchmarks with an improvement of 1.6 absolute percentages on mean average recall@1.▽ MorePoint cloud-based place recognition is crucial for mobile robots and autonomous vehicles, especially when the global positioning sensor is not accessible. LiDAR points are scattered on the surface of objects and buildings, which have strong shape priors along different axes. To enhance message passing along particular axes, Stacked Asymmetric Convolution Block (SACB) is designed, which is one of the main contributions in this paper. Comprehensive experiments demonstrate that asymmetric convolution and its corresponding strategies employed by SACB can contribute to the more effective representation of point cloud feature. On this basis, Selective Feature Fusion Block (SFFB), which is formed by stacking point- and channel-wise gating layers in a predefined sequence, is proposed to selectively boost salient local features in certain key regions, as well as to align the features before fusion phase. SACBs and SFFBs are combined to construct a robust and accurate architecture for point cloud-based place recognition, which is termed SelFLoc. Comparative experimental results show that SelFLoc achieves the state-of-the-art (SOTA) performance on theOxfordand other three in-house benchmarks with an improvement of 1.6 absolute percentages on mean average recall@1.△ Less"
BAMF-SLAM: Bundle Adjusted Multi-Fisheye Visual-Inertial SLAM Using Recurrent Field Transforms,"Authors:Wei Zhang,Sen Wang,Xingliang Dong,Rongwei Guo,Norbert Haala","Abstract:…pose factors with loop closure factors, the global states can be adjusted efficiently with modest memory footprint while maintaining high accuracy. Evaluations on TUM-VI, Hilti-Oxfordand Newer College datasets show the superior performance of the proposed system over prior works. In the Hilti SLAM Challenge 2022, our VIO version achieves second place. In a…▽ MoreIn this paper, we present BAMF-SLAM, a novel multi-fisheye visual-inertial SLAM system that utilizes Bundle Adjustment (BA) and recurrent field transforms (RFT) to achieve accurate and robust state estimation in challenging scenarios. First, our system directly operates on raw fisheye images, enabling us to fully exploit the wide Field-of-View (FoV) of fisheye cameras. Second, to overcome the low-texture challenge, we explore the tightly-coupled integration of multi-camera inputs and complementary inertial measurements via a unified factor graph and jointly optimize the poses and dense depth maps. Third, for global consistency, the wide FoV of the fisheye camera allows the system to find more potential loop closures, and powered by the broad convergence basin of RFT, our system can perform very wide baseline loop closing with little overlap. Furthermore, we introduce a semi-pose-graph BA method to avoid the expensive full global BA. By combining relative pose factors with loop closure factors, the global states can be adjusted efficiently with modest memory footprint while maintaining high accuracy. Evaluations on TUM-VI, Hilti-Oxfordand Newer College datasets show the superior performance of the proposed system over prior works. In the Hilti SLAM Challenge 2022, our VIO version achieves second place. In a subsequent submission, our complete system, including the global BA backend, outperforms the winning approach.△ Less"
Precise measurement of the $D^+_s$ lifetime at Belle II,"Authors:Belle II Collaboration,I. Adachi,L. Aggarwal,H. Aihara,N. Akopov,A. Aloisio,N. Anh Ky,D. M. Asner,H. Atmacan,T. Aushev,V. Aushev,M. Aversano,V. Babu,H. Bae,S. Bahinipati,P. Bambade,Sw. Banerjee,M. Barrett,J. Baudot,M. Bauer,A. Baur,A. Beaubien,J. Becker,P. K. Behera,J. V. Bennett, et al. (337 additional authors not shown)","Abstract:We measure the lifetime of the $D_s^+$ meson using a data sample of 207 fb$^{-1}$ collected by the Belle II experiment running at the SuperKEKB asymmetric-energy $e^+ e^-$ collider. The lifetime is determined by fitting the decay-time distribution of a sample of $116\times 10^3$ $D_s^+\rightarrowφπ^+$ decays. Our result is $τ^{}_{D^+_s} = (499.5\pm 1.7\pm 0.9)$ fs, where the first uncertainty is s…▽ MoreWe measure the lifetime of the $D_s^+$ meson using a data sample of 207 fb$^{-1}$ collected by the Belle II experiment running at the SuperKEKB asymmetric-energy $e^+ e^-$ collider. The lifetime is determined by fitting the decay-time distribution of a sample of $116\times 10^3$ $D_s^+\rightarrowφπ^+$ decays. Our result is $τ^{}_{D^+_s} = (499.5\pm 1.7\pm 0.9)$ fs, where the first uncertainty is statistical and the second is systematic. This result is significantly more precise than previous measurements.△ Less"
Representation Learning on Hyper-Relational and Numeric Knowledge Graphs with Transformers,"Authors:Chanyoung Chung,Jaejun Lee,Joyce Jiyoung Whang","Abstract:…that the entities are discrete objects, some information should be represented using numeric values, e.g., (J.R.R., was born in, 1892). Also, a triplet (J.R.R., educated at,OxfordUniv.) can be associated with a qualifier such as (start time, 1911). In this paper, we propose a unified framework named HyNT that learns representations of a hyper-relational kn…▽ MoreA hyper-relational knowledge graph has been recently studied where a triplet is associated with a set of qualifiers; a qualifier is composed of a relation and an entity, providing auxiliary information for a triplet. While existing hyper-relational knowledge graph embedding methods assume that the entities are discrete objects, some information should be represented using numeric values, e.g., (J.R.R., was born in, 1892). Also, a triplet (J.R.R., educated at,OxfordUniv.) can be associated with a qualifier such as (start time, 1911). In this paper, we propose a unified framework named HyNT that learns representations of a hyper-relational knowledge graph containing numeric literals in either triplets or qualifiers. We define a context transformer and a prediction transformer to learn the representations based not only on the correlations between a triplet and its qualifiers but also on the numeric information. By learning compact representations of triplets and qualifiers and feeding them into the transformers, we reduce the computation cost of using transformers. Using HyNT, we can predict missing numeric values in addition to missing entities or relations in a hyper-relational knowledge graph. Experimental results show that HyNT significantly outperforms state-of-the-art methods on real-world datasets.△ Less"
Localization under consistent assumptions over dynamics,"Authors:Matti Pekkanen,Francesco Verdoja,Ville Kyrki","Abstract:…subtraction-based filtering method is used to remove dynamic measurements. Experimental comparison against a state-of-the-art baseline solution using real-world data fromOxfordRadar RobotCar data set shows that consistent assumptions over dynamics increase localization accuracy.▽ MoreAccurate maps are a prerequisite for virtually all autonomous vehicle tasks. Most state-of-the-art maps assume a static world, and therefore dynamic objects are filtered out of the measurements. However, this division ignores movable but non-moving, i.e. semi-static, objects, which are usually recorded in the map and treated as static objects, violating the static world assumption, causing error in the localization. In this paper, we present a method for modeling moving and movable objects for matching the map and the measurements consistently. This reduces the error resulting from inconsistent categorization and treatment of non-static measurements. A semantic segmentation network is used to categorize the measurements into static and semi-static classes, and a background subtraction-based filtering method is used to remove dynamic measurements. Experimental comparison against a state-of-the-art baseline solution using real-world data fromOxfordRadar RobotCar data set shows that consistent assumptions over dynamics increase localization accuracy.△ Less"
Density Functional Theory of Material Design: Fundamentals and Applications -- I,"Authors:Prashant Singh,Manoj K Harbola","Abstract:This article is part-I of a review of density-functional theory (DFT) that is the most widely used method for calculating electronic structure of materials. The accuracy and ease of numerical implementation of DFT methods has resulted in its extensive use for materials design and discovery and has thus ushered in the new field of computational material science. In this article we start with an int…▽ MoreThis article is part-I of a review of density-functional theory (DFT) that is the most widely used method for calculating electronic structure of materials. The accuracy and ease of numerical implementation of DFT methods has resulted in its extensive use for materials design and discovery and has thus ushered in the new field of computational material science. In this article we start with an introduction to Schrödinger equation and methods of its solutions. After presenting exact results for some well-known systems, difficulties encountered in solving the equation for interacting electrons are described. How these difficulties are handled using the variational principle for the energy to obtain approximate solutions of the Schrödinger equation is discussed. The resulting Hartree and Hartree-Fock theories are presented along with results they give for atomic and solid-state systems. We then describe Thomas-Fermi theory and its extensions which were the initial attempts to formulate many-electron problem in terms of electronic density of a system. Having described these theories, we introduce modern density functional theory by discussing Hohenberg-Kohn theorems that form its foundations. We then go on to discuss Kohn-Sham formulation of density-functional theory in its exact form. Next, local density approximation is introduced and solutions of Kohn-Sham equation for some representative systems, obtained using the local density approximation, are presented. We end part-I of the review describing the contents of part-II.△ Less"
Black holes and modular forms in string theory,Authors:Sameer Murthy,"Abstract:The study of black holes in string theory has led to the discovery of deep and surprising connections between black holes and modular forms -- which are two classical, a priori unrelated, subjects. This article explains the main physical and mathematical ideas behind these connections. It is known from the pioneering work of J.Bekenstein and S.Hawking in the 1970s that black holes have thermodynam…▽ MoreThe study of black holes in string theory has led to the discovery of deep and surprising connections between black holes and modular forms -- which are two classical, a priori unrelated, subjects. This article explains the main physical and mathematical ideas behind these connections. It is known from the pioneering work of J.Bekenstein and S.Hawking in the 1970s that black holes have thermodynamic entropy, and should therefore be made up of a collection of microscopic quantum states. Superstring theory provides a framework wherein we can associate a number of microscopic states that make up the quantum-statistical system underlying a black hole, thus explaining their thermodynamic behavior from a more fundamental point of view. %The above-mentioned connections arise from the observation that, i The basic connection to modular forms arises from the observation that, in the simplest superstring-theoretic construction, the generating function of the number of microscopic states is a modular form. In one direction, modular symmetry acts as a powerful guide to the calculation of quantum-gravitational effects on the black hole entropy. In the other direction, the connection has led to the discovery of surprising relations between Ramanujan's mock modular forms and a class of string-theoretic black holes, thus providing an infinite number of new examples of mock modular forms.△ Less"
Fusion-S2iGan: An Efficient and Effective Single-Stage Framework for Speech-to-Image Generation,"Authors:Zhenxing Zhang,Lambert Schomaker","Abstract:…to reinforce the visual feature maps at various hierarchical levels in the architecture. We conduct a series of experiments on four benchmark data sets, i.e., CUB birds,Oxford-102, Flickr8k and Places-subset. The experimental results demonstrate the superiority of the presented Fusion-S2iGan compared to the state-of-the-art models with a multi-stage archite…▽ MoreThe goal of a speech-to-image transform is to produce a photo-realistic picture directly from a speech signal. Recently, various studies have focused on this task and have achieved promising performance. However, current speech-to-image approaches are based on a stacked modular framework that suffers from three vital issues: 1) Training separate networks is time-consuming as well as inefficient and the convergence of the final generative model strongly depends on the previous generators; 2) The quality of precursor images is ignored by this architecture; 3) Multiple discriminator networks are required to be trained. To this end, we propose an efficient and effective single-stage framework called Fusion-S2iGan to yield perceptually plausible and semantically consistent image samples on the basis of given spoken descriptions. Fusion-S2iGan introduces a visual+speech fusion module (VSFM), constructed with a pixel-attention module (PAM), a speech-modulation module (SMM) and a weighted-fusion module (WFM), to inject the speech embedding from a speech encoder into the generator while improving the quality of synthesized pictures. Fusion-S2iGan spreads the bimodal information over all layers of the generator network to reinforce the visual feature maps at various hierarchical levels in the architecture. We conduct a series of experiments on four benchmark data sets, i.e., CUB birds,Oxford-102, Flickr8k and Places-subset. The experimental results demonstrate the superiority of the presented Fusion-S2iGan compared to the state-of-the-art models with a multi-stage architecture and a performance level that is close to traditional text-to-image approaches.△ Less"
Search for lepton-flavor-violating $τ^- \to \ell^-φ$ decays in 2019-2021 Belle II data,"Authors:Belle II Collaboration,F. Abudinén,I. Adachi,K. Adamczyk,L. Aggarwal,P. Ahlburg,H. Ahmed,J. K. Ahn,H. Aihara,N. Akopov,A. Aloisio,L. Andricek,N. Anh Ky,D. M. Asner,H. Atmacan,V. Aulchenko,T. Aushev,V. Aushev,M. Aversano,V. Babu,S. Bacher,H. Bae,S. Bahinipati,A. M. Bakich,P. Bambade, et al. (555 additional authors not shown)","Abstract:We report a search for lepton-flavor-violating decays $τ^- \to \ell^- φ$ ($\ell^- =e^-,μ^-$) at the Belle II experiment, using a sample of electron-positron data produced at the SuperKEKB collider in 2019-2021 and corresponding to an integrated luminosity of 190 fb$^{-1}$. We use a new untagged selection for $e^+e^- \to τ^+τ^-$ events, where the signal $τ$ is searched for as a neutrinoless final s…▽ MoreWe report a search for lepton-flavor-violating decays $τ^- \to \ell^- φ$ ($\ell^- =e^-,μ^-$) at the Belle II experiment, using a sample of electron-positron data produced at the SuperKEKB collider in 2019-2021 and corresponding to an integrated luminosity of 190 fb$^{-1}$. We use a new untagged selection for $e^+e^- \to τ^+τ^-$ events, where the signal $τ$ is searched for as a neutrinoless final state of a single charged lepton and a $φ$ meson and the other $τ$ is not reconstructed in any specific decay mode, in contrast to previous measurements by the BaBar and Belle experiments. We find no evidence for $τ^- \to \ell^- φ$ decays and obtain upper limits on the branching fractions at 90% confidence level of 23 $\times 10^{-8}$ and 9.7$\times 10^{-8}$ for $τ^- \rightarrow e^-φ$ and $τ^- \rightarrow μ^-φ$, respectively△ Less"
NGTS clusters survey $-$ V: Rotation in the Orion Star-forming Complex,"Authors:Gareth D. Smith,Edward Gillen,Simon T. Hodgkin,Douglas R. Alves,David R. Anderson,Matthew P. Battley,Matthew R. Burleigh,Sarah L. Casewell,Samuel Gill,Michael R. Goad,Beth A. Henderson,James S. Jenkins,Alicia Kendall,Maximiliano Moyano,Gavin Ramsay,Rosanna H. Tilbrook,Jose I. Vines,Richard G. West,Peter J. Wheatley","Abstract:We present a study of rotation across 30 square degrees of the Orion Star-forming Complex, following a $\sim$200 d photometric monitoring campaign by the Next Generation Transit Survey (NGTS). From 5749 light curves of Orion members, we report periodic signatures for 2268 objects and analyse rotation period distributions as a function of colour for 1789 stars with spectral types F0$-$M5. We select…▽ MoreWe present a study of rotation across 30 square degrees of the Orion Star-forming Complex, following a $\sim$200 d photometric monitoring campaign by the Next Generation Transit Survey (NGTS). From 5749 light curves of Orion members, we report periodic signatures for 2268 objects and analyse rotation period distributions as a function of colour for 1789 stars with spectral types F0$-$M5. We select candidate members of Orion using $\textit{Gaia}$ data and assign our targets to kinematic sub-groups. We correct for interstellar extinction on a star-by-star basis and determine stellar and cluster ages using magnetic and non-magnetic stellar evolutionary models. Rotation periods generally lie in the range 1$-$10 d, with only 1.5 per cent of classical T Tauri stars or Class I/II young stellar objects rotating with periods shorter than 1.8 d, compared with 14 per cent of weak-line T Tauri stars or Class III objects. In period$-$colour space, the rotation period distribution moves towards shorter periods among low-mass (>M2) stars of age 3$-$6 Myr, compared with those at 1$-$3 Myr, with no periods longer than 10 d for stars later than M3.5. This could reflect a mass-dependence for the dispersal of circumstellar discs. Finally, we suggest that the turnover (from increasing to decreasing periods) in the period$-$colour distributions may occur at lower mass for the older-aged population: $\sim$K5 spectral type at 1$-$3 Myr shifting to $\sim$M1 at 3$-$6 Myr.△ Less"
Mildly Relativistic Motion in the Radio Quiet Quasar PG 1351+640,"Authors:Ailing Wang,Tao An,Shaoguang Guo,Luis C. Ho,Willem A. Baan,Robert Braun,Sina Chen,Xiaopeng Cheng,Philippa Hartley,Jun Yang,Yingkang Zhang","Abstract:Measuring the proper motion of the emission component in radio-quiet quasars (RQQs) could help to distinguish between the origins of the radio emission and to understand whether the jet production mechanism is the same in radio-loud quasars (RLQs) and RQQs. PG 1351+640 is one of the few RQQs suitable for proper motion studies: it has two compact components on milli-arcsecond scales, a flat-spectru…▽ MoreMeasuring the proper motion of the emission component in radio-quiet quasars (RQQs) could help to distinguish between the origins of the radio emission and to understand whether the jet production mechanism is the same in radio-loud quasars (RLQs) and RQQs. PG 1351+640 is one of the few RQQs suitable for proper motion studies: it has two compact components on milli-arcsecond scales, a flat-spectrum core and a steep-spectrum jet; both components are >2 mJy at 5 GHz and are well suited for Very Long Baseline Array (VLBA) observations. We compare recent VLBA observations with that made seventeen years ago and find no significant change in the core-jet separation between 2005 and 2015 (a proper motion of 0.003 mas yr-1). However, the core-jet separation increased significantly between 2015 and 2022, inferring a jet proper motion velocity of 0.063 mas yr-1, which corresponds to an apparent transverse velocity of 0.37c. The result suggests that the jet of the RQQ PG 1351+640 is mildly relativistic and oriented at a relatively small viewing angle.△ Less"
"Local causality in the works of Einstein, Bohm and Bell",Authors:Aurélien Drezet,"Abstract:In this chapter we discuss the Einstein Podolsky Rosen theorem and its strong relation with Bell's theorem. The central role played by the concept of beable introduced by Bell is emphasized. In particular we stress that beables involved in EPR and Bell theorems are not limited to hidden supplementary variables (e.g., like in the de Broglie-Bohm (dBB) pilot-wave theory) but also include the wave fu…▽ MoreIn this chapter we discuss the Einstein Podolsky Rosen theorem and its strong relation with Bell's theorem. The central role played by the concept of beable introduced by Bell is emphasized. In particular we stress that beables involved in EPR and Bell theorems are not limited to hidden supplementary variables (e.g., like in the de Broglie-Bohm (dBB) pilot-wave theory) but also include the wave function. In full agreement with Bell this allows us the reformulate the EPR and Bell results as strong theorems concerning nonlocality for quantum mechanics itself and not only for hidden-variables approaches as it is often mistakenly assumed. Furthermore, we clarify some repeated ambiguities concerning `local-realism' and emphasize that neither realism nor determinism nor counterfactual definiteness are prerequisites of EPR and Bell theorems.△ Less"
Biomarker Investigation using Multiple Brain Measures from MRI through XAI in Alzheimer's Disease Classification,"Authors:Davide Coluzzi,Valentina Bordin,Massimo Walter Rivolta,Igor Fortel,Liang Zhang,Alex Leow,Giuseppe Baselli","Abstract:…(Grad-CAM) to measure the level of interpretability of both models. The XAI assessment was conducted across 132 brain parcels, extracted from a combination of the Harvard-Oxfordand AAL brain atlases, and compared to well-known pathological regions to measure adherence to domain knowledge. Results highlighted acceptable classification performance as compared…▽ MoreAlzheimer's Disease (AD) is the world leading cause of dementia, a progressively impairing condition leading to high hospitalization rates and mortality. To optimize the diagnostic process, numerous efforts have been directed towards the development of deep learning approaches (DL) for the automatic AD classification. However, their typical black box outline has led to low trust and scarce usage within clinical frameworks. In this work, we propose two state-of-the art DL models, trained respectively on structural MRI (ResNet18) and brain connectivity matrixes (BC-GCN-SE) derived from diffusion data. The models were initially evaluated in terms of classification accuracy. Then, results were analyzed using an Explainable Artificial Intelligence (XAI) approach (Grad-CAM) to measure the level of interpretability of both models. The XAI assessment was conducted across 132 brain parcels, extracted from a combination of the Harvard-Oxfordand AAL brain atlases, and compared to well-known pathological regions to measure adherence to domain knowledge. Results highlighted acceptable classification performance as compared to the existing literature (ResNet18: TPRmedian = 0.817, TNRmedian = 0.816; BC-GCN-SE: TPRmedian = 0.703, TNRmedian = 0.738). As evaluated through a statistical test (p < 0.05) and ranking of the most relevant parcels (first 15%), Grad-CAM revealed the involvement of target brain areas for both the ResNet18 and BC-GCN-SE models: the medial temporal lobe and the default mode network. The obtained interpretabilities were not without limitations. Nevertheless, results suggested that combining different imaging modalities may result in increased classification performance and model reliability. This could potentially boost the confidence laid in DL models and favor their wide applicability as aid diagnostic tools.△ Less"
Boundary-aware Backward-Compatible Representation via Adversarial Learning in Image Retrieval,"Authors:Tan Pan,Furong Xu,Xudong Yang,Sifeng He,Chen Jiang,Qingpei Guo,Feng Qian Xiaobo Zhang,Yuan Cheng,Lei Yang,Wei Chu","Abstract:…old model. Meanwhile, we add an elastic boundary constraint during training to improve compatibility and discrimination efficiently. Extensive experiments on GLDv2, RevisitedOxford(ROxford), and Revisited Paris (RParis) demonstrate that our method outperforms other BCT methods on both compatibility and discrimination. The implementation of AdvBCT will be p…▽ MoreImage retrieval plays an important role in the Internet world. Usually, the core parts of mainstream visual retrieval systems include an online service of the embedding model and a large-scale vector database. For traditional model upgrades, the old model will not be replaced by the new one until the embeddings of all the images in the database are re-computed by the new model, which takes days or weeks for a large amount of data. Recently, backward-compatible training (BCT) enables the new model to be immediately deployed online by making the new embeddings directly comparable to the old ones. For BCT, improving the compatibility of two models with less negative impact on retrieval performance is the key challenge. In this paper, we introduce AdvBCT, an Adversarial Backward-Compatible Training method with an elastic boundary constraint that takes both compatibility and discrimination into consideration. We first employ adversarial learning to minimize the distribution disparity between embeddings of the new model and the old model. Meanwhile, we add an elastic boundary constraint during training to improve compatibility and discrimination efficiently. Extensive experiments on GLDv2, RevisitedOxford(ROxford), and Revisited Paris (RParis) demonstrate that our method outperforms other BCT methods on both compatibility and discrimination. The implementation of AdvBCT will be publicly available at https://github.com/Ashespt/AdvBCT.△ Less"
Computed microwave spectra of C$_{70}^+$,Authors:L. Nemes,"Abstract:The apolar fullerenes C60 and C70 are not accessible for radio astronomy. Upon ionization static Jahn-Teller effects occur in C70+ that distort the D5h neutral symmetry to Cs. This point group is polar thus ionization induces a permanent electric dipole moment in C70. The goal of the present calculations is to compute the equilibrium geometry and dipole moment of the C70+ cation by various DFT met…▽ MoreThe apolar fullerenes C60 and C70 are not accessible for radio astronomy. Upon ionization static Jahn-Teller effects occur in C70+ that distort the D5h neutral symmetry to Cs. This point group is polar thus ionization induces a permanent electric dipole moment in C70. The goal of the present calculations is to compute the equilibrium geometry and dipole moment of the C70+ cation by various DFT methods and to simulate microwave spectra. Using quantum chemistry rotational constants, Cartesian dipole moment components and the resultant dipole, as well as Jahn-Teller stabilization energies and HOMO-LUMO gaps were obtained. Microwave rotational spectrum simulations for the slightly asymmetric top ion were carried out for gas phase temperatures 2.73 K and 10 K. These spectra may serve as starting point for laboratory microwave measurements and as screening guide in radio astronomical searches. In addition it was found that the static Jahn-Teller effect in C70+ is the consequence of the mixing of the two highest ground state occupied orbitals, thus it is a pseudo Jahn-Teller effect.△ Less"
Observation of ${B\to D^{(*)} K^- K^{0}_S}$ decays using the 2019-2022 Belle II data sample,"Authors:Belle II Collaboration,F. Abudinén,I. Adachi,K. Adamczyk,L. Aggarwal,P. Ahlburg,H. Ahmed,J. K. Ahn,H. Aihara,N. Akopov,A. Aloisio,L. Andricek,N. Anh Ky,D. M. Asner,H. Atmacan,V. Aulchenko,T. Aushev,V. Aushev,M. Aversano,V. Babu,S. Bacher,H. Bae,S. Bahinipati,A. M. Bakich,P. Bambade, et al. (555 additional authors not shown)","Abstract:We present a measurement of the branching fractions of four $B^{0,-}\to D^{(*)+,0} K^- K^{0}_S$ decay modes. The measurement is based on data from SuperKEKB electron-positron collisions at the $Υ(4S)$ resonance collected with the Belle II detector and corresponding to an integrated luminosity of ${362~\text{fb}^{-1}}$. The event yields are extracted from fits to the distributions of the difference…▽ MoreWe present a measurement of the branching fractions of four $B^{0,-}\to D^{(*)+,0} K^- K^{0}_S$ decay modes. The measurement is based on data from SuperKEKB electron-positron collisions at the $Υ(4S)$ resonance collected with the Belle II detector and corresponding to an integrated luminosity of ${362~\text{fb}^{-1}}$. The event yields are extracted from fits to the distributions of the difference between expected and observed $B$ meson energy to separate signal and background, and are efficiency-corrected as a function of the invariant mass of the $K^-K_S^0$ system. We find the branching fractions to be: \[ \text{B}(B^-\to D^0K^-K_S^0)=(1.89\pm 0.16\pm 0.10)\times 10^{-4}, \] \[ \text{B}(\overline B{}^0\to D^+K^-K_S^0)=(0.85\pm 0.11\pm 0.05)\times 10^{-4},\] \[ \text{B}(B^-\to D^{*0}K^-K_S^0)=(1.57\pm 0.27\pm 0.12)\times 10^{-4}, \] \[ \text{B}(\overline B{}^0\to D^{*+}K^-K_S^0)=(0.96\pm 0.18\pm 0.06)\times 10^{-4},\] where the first uncertainty is statistical and the second systematic. These results include the first observation of $\overline B{}^0\to D^+K^-K_S^0$, $B^-\to D^{*0}K^-K_S^0$, and $\overline B{}^0\to D^{*+}K^-K_S^0$ decays and a significant improvement in the precision of $\text{B}(B^-\to D^0K^-K_S^0)$ compared to previous measurements.△ Less"
Strange metals and black holes: insights from the Sachdev-Ye-Kitaev model,Authors:Subir Sachdev,"Abstract:Complex many-particle quantum entanglement is a central theme in two distinct major topics in physics: the strange metal state found in numerous correlated electron compounds, and the quantum theory of black holes in Einstein gravity. The Sachdev-Ye-Kitaev model provides a solvable theory of entangled many-particle quantum states without quasiparticle excitations. This article reviews how this toy…▽ MoreComplex many-particle quantum entanglement is a central theme in two distinct major topics in physics: the strange metal state found in numerous correlated electron compounds, and the quantum theory of black holes in Einstein gravity. The Sachdev-Ye-Kitaev model provides a solvable theory of entangled many-particle quantum states without quasiparticle excitations. This article reviews how this toy model has led to realistic universal models of strange metals, and to new insights on the quantum states of black holes.△ Less"
Directly Optimizing IoU for Bounding Box Localization,"Authors:Mofassir ul Islam Arif,Mohsan Jameel,Lars Schmidt-Thieme","Abstract:…nature. In this paper, we have formulated a novel loss namely, the Smooth IoU, which directly optimizes the IoUs for the bounding boxes. This loss has been evaluated on theOxfordIIIT Pets, Udacity self-driving car, PASCAL VOC, and VWFS Car Damage datasets and has shown performance gains over the standard Huber loss.▽ MoreObject detection has seen remarkable progress in recent years with the introduction of Convolutional Neural Networks (CNN). Object detection is a multi-task learning problem where both the position of the objects in the images as well as their classes needs to be correctly identified. The idea here is to maximize the overlap between the ground-truth bounding boxes and the predictions i.e. the Intersection over Union (IoU). In the scope of work seen currently in this domain, IoU is approximated by using the Huber loss as a proxy but this indirect method does not leverage the IoU information and treats the bounding box as four independent, unrelated terms of regression. This is not true for a bounding box where the four coordinates are highly correlated and hold a semantic meaning when taken together. The direct optimization of the IoU is not possible due to its non-convex and non-differentiable nature. In this paper, we have formulated a novel loss namely, the Smooth IoU, which directly optimizes the IoUs for the bounding boxes. This loss has been evaluated on theOxfordIIIT Pets, Udacity self-driving car, PASCAL VOC, and VWFS Car Damage datasets and has shown performance gains over the standard Huber loss.△ Less"
SpectFormer: Frequency and Attention is what you need in a Vision Transformer,"Authors:Badri N. Patro,Vinay P. Namboodiri,Vijay Srinivas Agneeswaran","Abstract:…version of the transformers. We further ensure that we obtain reasonable results in other scenarios such as transfer learning on standard datasets such as CIFAR-10, CIFAR-100,Oxford-IIIT-flower, and Standford Car datasets. We then investigate its use in downstream tasks such of object detection and instance segmentation on the MS-COCO dataset and observe th…▽ MoreVision transformers have been applied successfully for image recognition tasks. There have been either multi-headed self-attention based (ViT \cite{dosovitskiy2020image}, DeIT, \cite{touvron2021training}) similar to the original work in textual models or more recently based on spectral layers (Fnet\cite{lee2021fnet}, GFNet\cite{rao2021global}, AFNO\cite{guibas2021efficient}). We hypothesize that both spectral and multi-headed attention plays a major role. We investigate this hypothesis through this work and observe that indeed combining spectral and multi-headed attention layers provides a better transformer architecture. We thus propose the novel Spectformer architecture for transformers that combines spectral and multi-headed attention layers. We believe that the resulting representation allows the transformer to capture the feature representation appropriately and it yields improved performance over other transformer representations. For instance, it improves the top-1 accuracy by 2\% on ImageNet compared to both GFNet-H and LiT. SpectFormer-S reaches 84.25\% top-1 accuracy on ImageNet-1K (state of the art for small version). Further, Spectformer-L achieves 85.7\% that is the state of the art for the comparable base version of the transformers. We further ensure that we obtain reasonable results in other scenarios such as transfer learning on standard datasets such as CIFAR-10, CIFAR-100,Oxford-IIIT-flower, and Standford Car datasets. We then investigate its use in downstream tasks such of object detection and instance segmentation on the MS-COCO dataset and observe that Spectformer shows consistent performance that is comparable to the best backbones and can be further optimized and improved. Hence, we believe that combined spectral and attention layers are what are needed for vision transformers.△ Less"
Novel method for the identification of the production flavor of neutral charmed mesons,"Authors:Belle II Collaboration,I. Adachi,L. Aggarwal,H. Ahmed,H. Aihara,N. Akopov,A. Aloisio,N. Anh Ky,D. M. Asner,H. Atmacan,T. Aushev,V. Aushev,M. Aversano,V. Babu,H. Bae,S. Bahinipati,P. Bambade,Sw. Banerjee,M. Barrett,J. Baudot,M. Bauer,A. Baur,A. Beaubien,J. Becker,P. K. Behera, et al. (374 additional authors not shown)","Abstract:We propose a new algorithm for the identification of the production flavor of neutral $D$ mesons in the Belle II experiment. The algorithm exploits the correlation between the flavor of a reconstructed neutral $D$ meson (signal $D$ meson) and the electric charges of particles reconstructed in the rest of the $e^+e^-\to c \bar{c}$ event. These include those originating from the decay of the other c…▽ MoreWe propose a new algorithm for the identification of the production flavor of neutral $D$ mesons in the Belle II experiment. The algorithm exploits the correlation between the flavor of a reconstructed neutral $D$ meson (signal $D$ meson) and the electric charges of particles reconstructed in the rest of the $e^+e^-\to c \bar{c}$ event. These include those originating from the decay of the other charm hadron produced in the event, as well as those possibly produced in association with the signal $D$ meson. We develop the algorithm using simulation and calibrate it in data using decay modes that identify the flavor of the decaying neutral $D$ meson. We use a data sample of $e^+e^-$ collisions, corresponding to $362\,\mathrm{fb}^{-1}$ of integrated luminosity, collected by Belle II at center-of-mass energies near the $Υ(4S)$ mass. The effective tagging efficiency in data is $(47.91 \pm 0.07 \mathrm{(stat)} \pm 0.51 \mathrm{(syst)})\%$, independent of the neutral-$D$-meson decay mode. This charm flavor tagger will approximately double the effective sample size of many $CP$-violation and charm-mixing measurements that so far have exclusively relied on neutral $D$ mesons originating from $D^{*\pm}$ decays. While developed for Belle II, the basic principles underlying the charm flavor tagger can be used in other experiments, including those at hadron colliders.△ Less"
FinderNet: A Data Augmentation Free Canonicalization aided Loop Detection and Closure technique for Point clouds in 6-DOF separation,"Authors:Sudarshan S Harithas,Gurkirat Singh,Aneesh Chavan,Sarthak Sharma,Suraj Patni,Chetan Arora,K. Madhava Krishna","Abstract:…performance gain in terms of Average Precision for loop detection and absolute translation/rotation error for relative pose estimation (or loop closure) on Kitti, GPR andOxfordRobot Car over multiple SOTA LDC methods. Our encoder technique allows to compress the original point cloud by over 830 times. To further test the robustness of our technique we crea…▽ MoreWe focus on the problem of LiDAR point cloud based loop detection (or Finding) and closure (LDC) in a multi-agent setting. State-of-the-art (SOTA) techniques directly generate learned embeddings of a given point cloud, require large data transfers, and are not robust to wide variations in 6 Degrees-of-Freedom (DOF) viewpoint. Moreover, absence of strong priors in an unstructured point cloud leads to highly inaccurate LDC. In this original approach, we propose independent roll and pitch canonicalization of the point clouds using a common dominant ground plane. Discretization of the canonicalized point cloud along the axis perpendicular to the ground plane leads to an image similar to Digital Elevation Maps (DEMs), which exposes strong spatial priors in the scene. Our experiments show that LDC based on learnt embeddings of such DEMs is not only data efficient but also significantly more robust, and generalizable than the current SOTA. We report significant performance gain in terms of Average Precision for loop detection and absolute translation/rotation error for relative pose estimation (or loop closure) on Kitti, GPR andOxfordRobot Car over multiple SOTA LDC methods. Our encoder technique allows to compress the original point cloud by over 830 times. To further test the robustness of our technique we create and opensource a custom dataset called Lidar-UrbanFly Dataset (LUF) which consists of point clouds obtained from a LiDAR mounted on a quadrotor.△ Less"
Risk-Sensitive and Robust Model-Based Reinforcement Learning and Planning,Authors:Marc Rigter,"Abstract:Many sequential decision-making problems that are currently automated, such as those in manufacturing or recommender systems, operate in an environment where there is either little uncertainty, or zero risk of catastrophe. As companies and researchers attempt to deploy autonomous systems in less constrained environments, it is increasingly important that we endow sequential decision-making algorit…▽ MoreMany sequential decision-making problems that are currently automated, such as those in manufacturing or recommender systems, operate in an environment where there is either little uncertainty, or zero risk of catastrophe. As companies and researchers attempt to deploy autonomous systems in less constrained environments, it is increasingly important that we endow sequential decision-making algorithms with the ability to reason about uncertainty and risk.
  In this thesis, we will address both planning and reinforcement learning (RL) approaches to sequential decision-making. In the planning setting, it is assumed that a model of the environment is provided, and a policy is optimised within that model. Reinforcement learning relies upon extensive random exploration, and therefore usually requires a simulator in which to perform training. In many real-world domains, it is impossible to construct a perfectly accurate model or simulator. Therefore, the performance of any policy is inevitably uncertain due to the incomplete knowledge about the environment. Furthermore, in stochastic domains, the outcome of any given run is also uncertain due to the inherent randomness of the environment. These two sources of uncertainty are usually classified as epistemic, and aleatoric uncertainty, respectively. The over-arching goal of this thesis is to contribute to developing algorithms that mitigate both sources of uncertainty in sequential decision-making problems.
  We make a number of contributions towards this goal, with a focus on model-based algorithms...△ Less"
CrossLoc3D: Aerial-Ground Cross-Source 3D Place Recognition,"Authors:Tianrui Guan,Aswath Muthuselvam,Montana Hoover,Xijun Wang,Jing Liang,Adarsh Jagan Sathyamoorthy,Damon Conover,Dinesh Manocha","Abstract:…of 4.74% - 15.37% in terms of the top 1 average recall on our CS-Campus3D benchmark and achieves performance comparable to state-of-the-art 3D place recognition method on theOxfordRobotCar. The code and CS-CAMPUS3D benchmark will be available at github.com/rayguan97/crossloc3d.▽ MoreWe present CrossLoc3D, a novel 3D place recognition method that solves a large-scale point matching problem in a cross-source setting. Cross-source point cloud data corresponds to point sets captured by depth sensors with different accuracies or from different distances and perspectives. We address the challenges in terms of developing 3D place recognition methods that account for the representation gap between points captured by different sources. Our method handles cross-source data by utilizing multi-grained features and selecting convolution kernel sizes that correspond to most prominent features. Inspired by the diffusion models, our method uses a novel iterative refinement process that gradually shifts the embedding spaces from different sources to a single canonical space for better metric learning. In addition, we present CS-Campus3D, the first 3D aerial-ground cross-source dataset consisting of point cloud data from both aerial and ground LiDAR scans. The point clouds in CS-Campus3D have representation gaps and other features like different views, point densities, and noise patterns. We show that our CrossLoc3D algorithm can achieve an improvement of 4.74% - 15.37% in terms of the top 1 average recall on our CS-Campus3D benchmark and achieves performance comparable to state-of-the-art 3D place recognition method on theOxfordRobotCar. The code and CS-CAMPUS3D benchmark will be available at github.com/rayguan97/crossloc3d.△ Less"
CCL: Continual Contrastive Learning for LiDAR Place Recognition,"Authors:Jiafeng Cui,Xieyuanli Chen","Abstract:…distillation to maintain the retrieval ability of the past data while continually learning to recognize new places from the new data. We thoroughly evaluate our approach onOxford, MulRan, and PNV datasets using three different LPR methods. The experimental results show that our CCL consistently improves the performance of different methods in different envi…▽ MorePlace recognition is an essential and challenging task in loop closing and global localization for robotics and autonomous driving applications. Benefiting from the recent advances in deep learning techniques, the performance of LiDAR place recognition (LPR) has been greatly improved. However, current deep learning-based methods suffer from two major problems: poor generalization ability and catastrophic forgetting. In this paper, we propose a continual contrastive learning method, named CCL, to tackle the catastrophic forgetting problem and generally improve the robustness of LPR approaches. Our CCL constructs a contrastive feature pool and utilizes contrastive loss to train more transferable representations of places. When transferred into new environments, our CCL continuously reviews the contrastive memory bank and applies a distribution-based knowledge distillation to maintain the retrieval ability of the past data while continually learning to recognize new places from the new data. We thoroughly evaluate our approach onOxford, MulRan, and PNV datasets using three different LPR methods. The experimental results show that our CCL consistently improves the performance of different methods in different environments outperforming the state-of-the-art continual learning method. The implementation of our method has been released at https://github.com/cloudcjf/CCL.△ Less"
The origin of optical emission lines in the soft state of X-ray binary outbursts: the case of MAXI J1820+070,"Authors:K. I. I. Koljonen,K. S. Long,J. H. Matthews,C. Knigge","Abstract:The optical emission line spectra of X-ray binaries (XRBs) are thought to be produced in an irradiated atmosphere, possibly the base of a wind, located above the outer accretion disc. However, the physical nature of - and physical conditions in - the line-forming region remain poorly understood. Here, we test the idea that the optical spectrum is formed in the transition region between the cool, g…▽ MoreThe optical emission line spectra of X-ray binaries (XRBs) are thought to be produced in an irradiated atmosphere, possibly the base of a wind, located above the outer accretion disc. However, the physical nature of - and physical conditions in - the line-forming region remain poorly understood. Here, we test the idea that the optical spectrum is formed in the transition region between the cool, geometrically thin part of the disc near the mid-plane and a hot, vertically extended atmosphere or outflow produced by X-ray irradiation. We first present a VLT X-Shooter spectrum of XRB MAXI J1820+070 in the soft state associated with its 2018 outburst, which displays a rich set of double-peaked hydrogen and helium recombination lines. Aided by ancillary X-ray spectra and reddening estimates, we then model this spectrum with the Monte Carlo radiative transfer code Python, using a simple biconical disc wind model inspired by radiation-hydrodynamic simulations of irradiation-driven outflows from XRB discs. Such a model can qualitatively reproduce the observed features; nearly all of the optical emission arising from the transonic 'transition region' near the base of the wind. In this region, characteristic electron densities are on the order of 10$^{12-13}$ cm$^{-3}$, in line with the observed flat Balmer decrement (H$α$/H$β\approx 1.3$). We conclude that strong irradiation can naturally give rise to both the optical line-forming layer in XRB discs and an overlying outflow/atmosphere that produces X-ray absorption lines.△ Less"
"Workforce Development Through Research-Based, Plasma-Focused Activities","Authors:E G Kostadinova,Shannon Greco,Maajida Murdock,Ernesto Barraza-Valdez,Hannah R Hasson,Imani Z West-Abdallah,Cheryl A Harper,Katrina Brown,Earl Scime,Franklin Dollar,Carl Greninger,Bryan Stanley,Elizabeth Oxford,David Schaffner,Laura Provenzani,Chandra Breanne Curry,Claudia Fracchiolla,Shams El-Adawy,Saikat Chakraborty Thakur,Dmitri Orlov,Caroline Anderson","Abstract:This report is a summary of the mini-conference Workforce Development Through Research-Based, Plasma-Focused Science Education and Public Engagement held during the 2022 American Physical Society Division of Plasma Physics (APS DPP) annual meeting. The motivation for organizing this mini-conference originates from recent studies and community-based reports highlighting important issues with the cu…▽ MoreThis report is a summary of the mini-conference Workforce Development Through Research-Based, Plasma-Focused Science Education and Public Engagement held during the 2022 American Physical Society Division of Plasma Physics (APS DPP) annual meeting. The motivation for organizing this mini-conference originates from recent studies and community-based reports highlighting important issues with the current state of the plasma workforce. Here we summarize the main findings presented in the two speaker sessions of the mini-conference, the challenges and recommendations identified in the discussion sessions, and the results from a post-conference survey. We further provide information on initiatives and studies presented at the mini-conference, along with references to further resources.△ Less"
Convolutional Cross-View Pose Estimation,"Authors:Zimin Xia,Olaf Booij,Julian F. P. Kooij","Abstract:…possible erroneous predictions. Without re-training, the model can infer on ground images with different field of views and utilize orientation priors if available. On theOxfordRobotCar dataset, our method can reliably estimate the ego-vehicle's pose over time, achieving a median localization error under 1 meter and a median orientation error of around…▽ MoreWe propose a novel end-to-end method for cross-view pose estimation. Given a ground-level query image and an aerial image that covers the query's local neighborhood, the 3 Degrees-of-Freedom camera pose of the query is estimated by matching its image descriptor to descriptors of local regions within the aerial image. The orientation-aware descriptors are obtained by using a translationally equivariant convolutional ground image encoder and contrastive learning. The Localization Decoder produces a dense probability distribution in a coarse-to-fine manner with a novel Localization Matching Upsampling module. A smaller Orientation Decoder produces a vector field to condition the orientation estimate on the localization. Our method is validated on the VIGOR and KITTI datasets, where it surpasses the state-of-the-art baseline by 72% and 36% in median localization error for comparable orientation estimation accuracy. The predicted probability distribution can represent localization ambiguity, and enables rejecting possible erroneous predictions. Without re-training, the model can infer on ground images with different field of views and utilize orientation priors if available. On theOxfordRobotCar dataset, our method can reliably estimate the ego-vehicle's pose over time, achieving a median localization error under 1 meter and a median orientation error of around 1 degree at 14 FPS.△ Less"
Basic ZX-calculus for students and professionals,Authors:Bob Coecke,"Abstract:These are the lecture notes of guest lectures for Artur Ekert's course Introduction to Quantum Information at the Mathematical Institute ofOxfordUniversity, Hilary Term 2023. Some basic familiarity with Dirac notation is assumed. For the readers of Quantum in Pictures (QiP) who have some basic quantum background, these notes also constitute the shortes…▽ MoreThese are the lecture notes of guest lectures for Artur Ekert's course Introduction to Quantum Information at the Mathematical Institute ofOxfordUniversity, Hilary Term 2023. Some basic familiarity with Dirac notation is assumed. For the readers of Quantum in Pictures (QiP) who have some basic quantum background, these notes also constitute the shortest path to an explanation of how what they learn in QIP relates to the traditional quantum formalism.△ Less"
BPT: Binary Point Cloud Transformer for Place Recognition,"Authors:Zhixing Hou,Yuzhang Shang,Tian Gao,Yan Yan","Abstract:…model and even outperform some full-precision deep learning methods. For example, the proposed method achieves 93.28% at the top @1% and 85.74% at the top @1% on theOxfordRobotCar dataset in terms of the metric of the average recall rate. Meanwhile, the size and floating point operations of the model with the same transformer structure reduce 56.1% and 34.…▽ MorePlace recognition, an algorithm to recognize the re-visited places, plays the role of back-end optimization trigger in a full SLAM system. Many works equipped with deep learning tools, such as MLP, CNN, and transformer, have achieved great improvements in this research field. Point cloud transformer is one of the excellent frameworks for place recognition applied in robotics, but with large memory consumption and expensive computation, it is adverse to widely deploy the various point cloud transformer networks in mobile or embedded devices. To solve this issue, we propose a binary point cloud transformer for place recognition. As a result, a 32-bit full-precision model can be reduced to a 1-bit model with less memory occupation and faster binarized bitwise operations. To our best knowledge, this is the first binary point cloud transformer that can be deployed on mobile devices for online applications such as place recognition. Experiments on several standard benchmarks demonstrate that the proposed method can get comparable results with the corresponding full-precision transformer model and even outperform some full-precision deep learning methods. For example, the proposed method achieves 93.28% at the top @1% and 85.74% at the top @1% on theOxfordRobotCar dataset in terms of the metric of the average recall rate. Meanwhile, the size and floating point operations of the model with the same transformer structure reduce 56.1% and 34.1% respectively from original precision to binary precision.△ Less"
Entity-Level Text-Guided Image Manipulation,"Authors:Yikai Wang,Jianan Wang,Guansong Lu,Hang Xu,Zhenguo Li,Wei Zhang,Yanwei Fu","Abstract:…and the continuous denoising generation with diffusion models, yielding SeMani-Trans and SeMani-Diff, respectively. We conduct extensive experiments on the real datasets CUB,Oxford, and COCO datasets to verify that SeMani can distinguish the entity-relevant and -irrelevant regions and achieve more precise and flexible manipulation in a zero-shot manner comp…▽ MoreExisting text-guided image manipulation methods aim to modify the appearance of the image or to edit a few objects in a virtual or simple scenario, which is far from practical applications. In this work, we study a novel task on text-guided image manipulation on the entity level in the real world (eL-TGIM). The task imposes three basic requirements, (1) to edit the entity consistent with the text descriptions, (2) to preserve the entity-irrelevant regions, and (3) to merge the manipulated entity into the image naturally. To this end, we propose an elegant framework, dubbed as SeMani, forming the Semantic Manipulation of real-world images that can not only edit the appearance of entities but also generate new entities corresponding to the text guidance. To solve eL-TGIM, SeMani decomposes the task into two phases: the semantic alignment phase and the image manipulation phase. In the semantic alignment phase, SeMani incorporates a semantic alignment module to locate the entity-relevant region to be manipulated. In the image manipulation phase, SeMani adopts a generative model to synthesize new images conditioned on the entity-irrelevant regions and target text descriptions. We discuss and propose two popular generation processes that can be utilized in SeMani, the discrete auto-regressive generation with transformers and the continuous denoising generation with diffusion models, yielding SeMani-Trans and SeMani-Diff, respectively. We conduct extensive experiments on the real datasets CUB,Oxford, and COCO datasets to verify that SeMani can distinguish the entity-relevant and -irrelevant regions and achieve more precise and flexible manipulation in a zero-shot manner compared with baseline methods. Our codes and models will be released at https://github.com/Yikai-Wang/SeMani.△ Less"
Automotive RADAR sub-sampling via object detection networks: Leveraging prior signal information,"Authors:Madhumitha Sakthi,Ahmed Tewfik,Marius Arvinte,Haris Vikalo","Abstract:…enabling near-optimal performance at considerably lower effective sampling rates. Designed to robustly perform under variable weather conditions, the algorithm was shown on theOxfordraw radar and RADIATE dataset to achieve accurate reconstruction utilizing only 10% of the original samples in good weather and 20% in extreme (snow, fog) weather conditions. A…▽ MoreAutomotive radar has increasingly attracted attention due to growing interest in autonomous driving technologies. Acquiring situational awareness using multimodal data collected at high sampling rates by various sensing devices including cameras, LiDAR, and radar requires considerable power, memory and compute resources which are often limited at an edge device. In this paper, we present a novel adaptive radar sub-sampling algorithm designed to identify regions that require more detailed/accurate reconstruction based on prior environmental conditions' knowledge, enabling near-optimal performance at considerably lower effective sampling rates. Designed to robustly perform under variable weather conditions, the algorithm was shown on theOxfordraw radar and RADIATE dataset to achieve accurate reconstruction utilizing only 10% of the original samples in good weather and 20% in extreme (snow, fog) weather conditions. A further modification of the algorithm incorporates object motion to enable reliable identification of important regions. This includes monitoring possible future occlusions caused by objects detected in the present frame. Finally, we train a YOLO network on the RADIATE dataset to perform object detection directly on RADAR data and obtain a 6.6% AP50 improvement over the baseline Faster R-CNN network.△ Less"
pykanto: a python library to accelerate research on wild bird song,Authors:Nilo Merino Recalde,"Abstract:…similarity between individuals and populations. To demonstrate its capabilities, I put the library to the test on the vocalisations of male great tits in Wytham Woods, nearOxford, UK. The results show that the identities of individual birds can be accurately determined from their songs and that the use of pykanto improves the efficiency and reproducibility…▽ MoreStudying the vocalisations of wild animals can be a challenge due to the limitations of traditional computational methods, which often are time-consuming and lack reproducibility. Here, I present pykanto, a new software package that provides a set of tools to build, manage, and explore large sound databases. It can automatically find discrete units in animal vocalisations, perform semi-supervised labelling of individual repertoires with a new interactive web app, and feed data to deep learning models to study things like individual signatures and acoustic similarity between individuals and populations. To demonstrate its capabilities, I put the library to the test on the vocalisations of male great tits in Wytham Woods, nearOxford, UK. The results show that the identities of individual birds can be accurately determined from their songs and that the use of pykanto improves the efficiency and reproducibility of the process.△ Less"
GlocalFuse-Depth: Fusing Transformers and CNNs for All-day Self-supervised Monocular Depth Estimation,"Authors:Zezheng Zhang,Ryan K. Y. Chan,Kenneth K. Y. Wong","Abstract:…to fuse multi-dimensional features from the two branches. Extensive experiments demonstrate that GlocalFuse-Depth achieves state-of-the-art results for all-day images on theOxfordRobotCar dataset, which proves the superiority of our method.▽ MoreIn recent years, self-supervised monocular depth estimation has drawn much attention since it frees of depth annotations and achieved remarkable results on standard benchmarks. However, most of existing methods only focus on either daytime or nighttime images, thus their performance degrades on the other domain because of the large domain shift between daytime and nighttime images. To address this problem, in this paper we propose a two-branch network named GlocalFuse-Depth for self-supervised depth estimation of all-day images. The daytime and nighttime image in input image pair are fed into the two branches: CNN branch and Transformer branch, respectively, where both fine-grained details and global dependency can be efficiently captured. Besides, a novel fusion module is proposed to fuse multi-dimensional features from the two branches. Extensive experiments demonstrate that GlocalFuse-Depth achieves state-of-the-art results for all-day images on theOxfordRobotCar dataset, which proves the superiority of our method.△ Less"
Hard-aware Instance Adaptive Self-training for Unsupervised Cross-domain Semantic Segmentation,"Authors:Chuang Zhu,Kebin Liu,Wenqi Tang,Ke Mei,Jiaqi Zou,Tiejun Huang","Abstract:…Our method is so concise and efficient that it is easy to be generalized to other UDA methods. Experiments on GTA5 to Cityscapes, SYNTHIA to Cityscapes, and Cityscapes toOxfordRobotCar demonstrate the superior performance of our approach compared with the state-of-the-art methods.▽ MoreThe divergence between labeled training data and unlabeled testing data is a significant challenge for recent deep learning models. Unsupervised domain adaptation (UDA) attempts to solve such problem. Recent works show that self-training is a powerful approach to UDA. However, existing methods have difficulty in balancing the scalability and performance. In this paper, we propose a hard-aware instance adaptive self-training framework for UDA on the task of semantic segmentation. To effectively improve the quality and diversity of pseudo-labels, we develop a novel pseudo-label generation strategy with an instance adaptive selector. We further enrich the hard class pseudo-labels with inter-image information through a skillfully designed hard-aware pseudo-label augmentation. Besides, we propose the region-adaptive regularization to smooth the pseudo-label region and sharpen the non-pseudo-label region. For the non-pseudo-label region, consistency constraint is also constructed to introduce stronger supervision signals during model optimization. Our method is so concise and efficient that it is easy to be generalized to other UDA methods. Experiments on GTA5 to Cityscapes, SYNTHIA to Cityscapes, and Cityscapes toOxfordRobotCar demonstrate the superior performance of our approach compared with the state-of-the-art methods.△ Less"
Language Model Analysis for Ontology Subsumption Inference,"Authors:Yuan He,Jiaoyan Chen,Ernesto Jiménez-Ruiz,Hang Dong,Ian Horrocks","Abstract:Investigating whether pre-trained language models (LMs) can function as knowledge bases (KBs) has raised wide research interests recently. However, existing works focus on simple, triple-based, relational KBs, but omit more sophisticated, logic-based, conceptualised KBs such as OWL ontologies. To investigate an LM's knowledge of ontologies, we propose OntoLAMA, a set of inference-based probing tas…▽ MoreInvestigating whether pre-trained language models (LMs) can function as knowledge bases (KBs) has raised wide research interests recently. However, existing works focus on simple, triple-based, relational KBs, but omit more sophisticated, logic-based, conceptualised KBs such as OWL ontologies. To investigate an LM's knowledge of ontologies, we propose OntoLAMA, a set of inference-based probing tasks and datasets from ontology subsumption axioms involving both atomic and complex concepts. We conduct extensive experiments on ontologies of different domains and scales, and our results demonstrate that LMs encode relatively less background knowledge of Subsumption Inference (SI) than traditional Natural Language Inference (NLI) but can improve on SI significantly when a small number of samples are given. We will open-source our code and datasets.△ Less"
An Introduction to the Theory of Spin Glasses,"Authors:Ada Altieri,Marco Baity-Jesi","Abstract:We review the main methods used to study spin glasses. In the first part, we focus on methods for fully connected models and systems defined on a tree, such as the replica method, the Thouless-Anderson-Palmer formalism, the cavity method, and the dynamical mean-field theory. In the second part, we deal with the description of low-dimensional systems, mostly in three spatial dimensions, which are m…▽ MoreWe review the main methods used to study spin glasses. In the first part, we focus on methods for fully connected models and systems defined on a tree, such as the replica method, the Thouless-Anderson-Palmer formalism, the cavity method, and the dynamical mean-field theory. In the second part, we deal with the description of low-dimensional systems, mostly in three spatial dimensions, which are mostly studied through numerical simulations. We conclude by mentioning some of the main open problems in the field.△ Less"
Real-time LIDAR localization in natural and urban environments,"Authors:Georgi Tinchev,Adrian Penate-Sanchez,Maurice Fallon","Abstract:…on nine scenarios from six datasets varying between urban, park, forest, and industrial environments. Part of which includes post-processed data from 30 sequences of theOxfordRobotCar dataset, which we make publicly available. Our experiments demonstrate a factor of three reduction of computation, 70% lower memory consumption with marginal loss in localiza…▽ MoreLocalization is a key challenge in many robotics applications. In this work we explore LIDAR-based global localization in both urban and natural environments and develop a method suitable for online application. Our approach leverages efficient deep learning architecture capable of learning compact point cloud descriptors directly from 3D data. The method uses an efficient feature space representation of a set of segmented point clouds to match between the current scene and the prior map. We show that down-sampling in the inner layers of the network can significantly reduce computation time without sacrificing performance. We present substantial evaluation of LIDAR-based global localization methods on nine scenarios from six datasets varying between urban, park, forest, and industrial environments. Part of which includes post-processed data from 30 sequences of theOxfordRobotCar dataset, which we make publicly available. Our experiments demonstrate a factor of three reduction of computation, 70% lower memory consumption with marginal loss in localization frequency. The proposed method allows the full pipeline to run on robots with limited computation payload such as drones, quadrupeds, and UGVs as it does not require a GPU at run time.△ Less"
Magnetospheric accretion at the late phases of the Pre-Main-Sequence evolution. The case of RZ Psc,"Authors:D. V. Dmitriev,V. P. Grinin,I. S. Potravnov,T. A. Ermolaeva","Abstract:It has been shown that during the outburst of accretion activity observed in UX Ori type star RZ Psc in 2013, the accretion rate increased approximately by an order of magnitude. This means that the accretion process at the late stages of the Pre-Main Sequence evolution is very unstable. Using the spectra obtained during this episode we have studied the magnetospheric emission in the Hαline. Mo…▽ MoreIt has been shown that during the outburst of accretion activity observed in UX Ori type star RZ Psc in 2013, the accretion rate increased approximately by an order of magnitude. This means that the accretion process at the late stages of the Pre-Main Sequence evolution is very unstable. Using the spectra obtained during this episode we have studied the magnetospheric emission in the Hαline. Models of magnetospheric accretion are calculated to obtain the parameters of the magnetosphere from this observation. In present work we have taken into account the influence of the recombination delay effect during gas motion in the stellar magnetosphere. The accounting for this effect and the presence of the magnetospheric absorption in the IR CaII triplet lines and its absence in D Na I resonance lines allowed us to place a lower limit on the temperature in the magnetosphere at\approx10000 K, which significantly improved precision of our estimate of accretion rate. According to the best fit model the logarithm of accretion rate is\log\dot{M} = -10.1\pm0.3(\dot{M} \approx 7\times10^{-11}M_\odotyr^{-1}) and the inclination angle of RZ Psc is43\pm 3^\circ. It is less than the inclination, typical for the UX Ori stars (about 70^\circ), that explains the weak photometric variability of this star. Using the obtained accretion rate and magnetosphere radius we estimate the strength of the dipole component of the magnetic field of RZ Psc\approx0.1 kGs.△ Less"
Excitons and trions with negative effective masses in two-dimensional semiconductors,"Authors:M. A. Semina,J. V. Mamedov,M. M. Glazov","Abstract:We study theoretically fundamental Coulomb-correlated complexes: neutral and charged excitons, also known as trions, in transition metal dichalogenides monolayers. We focus on the situation where one of the electrons occupies excited, high-lying, conduction band characterized by a negative effective mass. We develop the theory of such high-lying excitons and trions with negative effective mass and…▽ MoreWe study theoretically fundamental Coulomb-correlated complexes: neutral and charged excitons, also known as trions, in transition metal dichalogenides monolayers. We focus on the situation where one of the electrons occupies excited, high-lying, conduction band characterized by a negative effective mass. We develop the theory of such high-lying excitons and trions with negative effective mass and demonstrate the key role of the non-parabolicity of the high-lying conduction band dispersion in formation of the bound exciton and trion states. We present simple, accurate and physically justified trial wavefunctions for calculating the binding energies of Coulomb-bound complexes and compare the results of variational calculations with those of a fully numerical approach. Within the developed model we discuss recent experimental results on observation of high-lying negative effective mass trions [K.-Q. Lin et al., Nat. Commun. 13, 6980 (2022)].△ Less"
When do Default Nudges Work?,"Authors:Carl Bonander,Mats Ekman,Niklas Jakobsson","Abstract:Nudging is a burgeoning topic in science and in policy, but evidence on the effectiveness of nudges among differentially-incentivized groups is lacking. This paper exploits regional variations in the roll-out of the Covid-19 vaccine in Sweden to examine the effect of a nudge on groups whose intrinsic incentives are different: 16-17-year-olds, for whom Covid-19 is not dangerous, and 50-59-year-olds…▽ MoreNudging is a burgeoning topic in science and in policy, but evidence on the effectiveness of nudges among differentially-incentivized groups is lacking. This paper exploits regional variations in the roll-out of the Covid-19 vaccine in Sweden to examine the effect of a nudge on groups whose intrinsic incentives are different: 16-17-year-olds, for whom Covid-19 is not dangerous, and 50-59-year-olds, who face a substantial risk of death or severe dis-ease. We find a significantly stronger response in the younger group, consistent with the theory that nudges are more effective for choices that are not meaningful to the individual.△ Less"
A Workflow Model for Holistic Data Management and Semantic Interoperability in Quantitative Archival Research,"Authors:Pavlos Fafalios,Yannis Marketakis,Anastasia Axaridou,Yannis Tzitzikas,Martin Doerr","Abstract:Archival research is a complicated task that involves several diverse activities for the extraction of evidence and knowledge from a set of archival documents. The involved activities are usually unconnected, in terms of data connection and flow, making difficult their recursive revision and execution, as well as the inspection of provenance information at data element level. This paper proposes a…▽ MoreArchival research is a complicated task that involves several diverse activities for the extraction of evidence and knowledge from a set of archival documents. The involved activities are usually unconnected, in terms of data connection and flow, making difficult their recursive revision and execution, as well as the inspection of provenance information at data element level. This paper proposes a workflow model for holistic data management in archival research; from transcribing and documenting a set of archival documents, to curating the transcribed data, integrating it to a rich semantic network (knowledge graph), and then exploring the integrated data quantitatively. The workflow is provenance-aware, highly-recursive and focuses on semantic interoperability, aiming at the production of sustainable data of high value and long-term validity. We provide implementation details for each step of the workflow and present its application in maritime history research. We also discuss relevant quality aspects and lessons learned from its application in a real context.△ Less"
Measurement of theB^{0} \rightarrow D^{*-} \ell^{+} ν_{\ell}branching ratio and|V_{cb}|with a fully reconstructed accompanyingBmeson in 2019-2021 Belle II data,"Authors:F. Abudinén,I. Adachi,K. Adamczyk,L. Aggarwal,P. Ahlburg,H. Ahmed,J. K. Ahn,H. Aihara,N. Akopov,A. Aloisio,F. Ameli,L. Andricek,N. Anh Ky,D. M. Asner,H. Atmacan,V. Aulchenko,T. Aushev,V. Aushev,T. Aziz,V. Babu,H. Bae,S. Baehr,S. Bahinipati,A. M. Bakich,P. Bambade, et al. (561 additional authors not shown)","Abstract:We present a measurement of theB^{0} \rightarrow D^{*-} \ell^{+} ν_{\ell}(\ell=e,μ) branching ratio and of the CKM parameter|V_{cb}|using signal decays accompanied by a fully reconstructedBmeson. The Belle II data set of electron-positron collisions at theΥ(4S)resonance, corresponding to 189.3\,fb^{-1}of integrated luminosity, is analyzed. With the Caprini-Lellouch-Neubert f…▽ MoreWe present a measurement of theB^{0} \rightarrow D^{*-} \ell^{+} ν_{\ell}(\ell=e,μ) branching ratio and of the CKM parameter|V_{cb}|using signal decays accompanied by a fully reconstructedBmeson. The Belle II data set of electron-positron collisions at theΥ(4S)resonance, corresponding to 189.3\,fb^{-1}of integrated luminosity, is analyzed. With the Caprini-Lellouch-Neubert form factor parameterization, the parametersη_{\rm EW} F(1) |V_{cb}|andρ^{2}are extracted, whereη_{\rm EW}is an electroweak correction,F(1)is a normalization factor andρ^{2}is a form factor shape parameter. We reconstruct 516 signal decays and thereby obtain\mathcal{B} (B^{0} \rightarrow D^{*-} \ell^{+} ν_{\ell} ) = \left(5.27 \pm 0.22~\rm{\left(stat\right)} \pm 0.38~\rm{\left(syst\right)}\right) \%,η_{EW} F(1) |V_{cb}| \times 10^{3} = 34.6 \pm 1.8~\rm{\left(stat\right)} \pm 1.7~\rm{\left(syst\right)}, andρ^{2} = 0.94 \pm 0.18~\rm{\left(stat\right)} \pm 0.11~\rm{\left(syst\right)}.△ Less"
Characterizing quantile-varying covariate effects under the accelerated failure time model,"Authors:Harrison T. Reeder,Kyu Ha Lee,Sebastien Haneuse","Abstract:An important task in survival analysis is choosing a structure for the relationship between covariates of interest and the time-to-event outcome. For example, the accelerated failure time (AFT) model structures each covariate effect as a constant multiplicative shift in the outcome distribution across all survival quantiles. Though parsimonious, this structure cannot detect or capture effects that…▽ MoreAn important task in survival analysis is choosing a structure for the relationship between covariates of interest and the time-to-event outcome. For example, the accelerated failure time (AFT) model structures each covariate effect as a constant multiplicative shift in the outcome distribution across all survival quantiles. Though parsimonious, this structure cannot detect or capture effects that differ across quantiles of the distribution, a limitation that is analogous to only permitting proportional hazards in the Cox model. To address this, we propose a general framework for quantile-varying multiplicative effects under the AFT model. Specifically, we embed flexible regression structures within the AFT model, and derive a novel formula for interpretable effects on the quantile scale. A regression standardization scheme based on the g-formula is proposed to enable estimation of both covariate-conditional and marginal effects for an exposure of interest. We implement a user-friendly Bayesian approach for estimation and quantification of uncertainty, while accounting for left truncation and complex censoring. We emphasize the intuitive interpretation of this model through numerical and graphical tools, and illustrate its performance by application to a study of Alzheimer's disease and dementia.△ Less"
Mathematical Foundations for a Compositional Account of the Bayesian Brain,Authors:Toby St Clere Smithe,"Abstract:This dissertation reports some first steps towards a compositional account of active inference and the Bayesian brain. Specifically, we use the tools of contemporary applied category theory to supply functorial semantics for approximate inference. To do so, we define on the `syntactic' side the new notion of Bayesian lens and show that Bayesian updating composes according to the compositional lens…▽ MoreThis dissertation reports some first steps towards a compositional account of active inference and the Bayesian brain. Specifically, we use the tools of contemporary applied category theory to supply functorial semantics for approximate inference. To do so, we define on the `syntactic' side the new notion of Bayesian lens and show that Bayesian updating composes according to the compositional lens pattern. Using Bayesian lenses, and inspired by compositional game theory, we define fibrations of statistical games and classify various problems of statistical inference as corresponding sections: the chain rule of the relative entropy is formalized as a strict section, while maximum likelihood estimation and the free energy give lax sections. In the process, we introduce a new notion of `copy-composition'.
  On the `semantic' side, we present a new formalization of general open dynamical systems (particularly: deterministic, stochastic, and random; and discrete- and continuous-time) as certain coalgebras of polynomial functors, which we show collect into monoidal opindexed categories (or, alternatively, into algebras for multicategories of generalized polynomial functors). We use these opindexed categories to define monoidal bicategories of cilia: dynamical systems which control lenses, and which supply the target for our functorial semantics. Accordingly, we construct functors which explain the bidirectional compositional structure of predictive coding neural circuits under the free energy principle, thereby giving a formal mathematical underpinning to the bidirectionality observed in the cortex. Along the way, we explain how to compose rate-coded neural circuits using an algebra for a multicategory of linear circuit diagrams, showing subsequently that this is subsumed by lenses and polynomial functors.△ Less"
On the Optimum Scenarios for Single Row Equidistant Facility Layout Problem,"Authors:Shrouq Gamal,Ahmed A. Hawam,Ahmed M. El-Kassas","Abstract:Single Row Equidistant Facility Layout Problem SREFLP is with an NP-Hard nature to mimic material handling costs along with equally spaced straight-line facilities layout. Based on literature, it is obvious that efforts of researchers for solving SREFLP turn from exact methods into release the running time tracing the principle of the approximate methods in time race, regardless searching their ti…▽ MoreSingle Row Equidistant Facility Layout Problem SREFLP is with an NP-Hard nature to mimic material handling costs along with equally spaced straight-line facilities layout. Based on literature, it is obvious that efforts of researchers for solving SREFLP turn from exact methods into release the running time tracing the principle of the approximate methods in time race, regardless searching their time complexity release in conjunction with a provable quality of solutions. This study focuses on Lower bounding LB techniques as an independent potential solution tool for SREFLP. In particular, Best-known SREFLP LBs are reported from literature and significantly LBs optimum scenarios are highlighted. Initially, one gap of the SREFLP bidirectional LB is enhanced. From the integration between the enhanced LB and the best-known Gilmore-Lawler GL bounding, a new SREFLP optimum scenario is provided. Further improvements to GLB lead to guarantee an exact Shipping/Receiving Facility assignment and propose a conjecture of at most 4/3 approximation scheme for SREFLP.△ Less"
Homonymy Information for English WordNet,"Authors:Rowan Hall Maudslay,Simone Teufel","Abstract:…to synthesise homonymy annotation for Princeton WordNet. Previous approaches treat the problem using clustering methods; by contrast, our method works by linking WordNet to theOxfordEnglish Dictionary, which contains the information we need. To perform this alignment, we pair definitions based on their proximity in an embedding space produced by a Transfor…▽ MoreA widely acknowledged shortcoming of WordNet is that it lacks a distinction between word meanings which are systematically related (polysemy), and those which are coincidental (homonymy). Several previous works have attempted to fill this gap, by inferring this information using computational methods. We revisit this task, and exploit recent advances in language modelling to synthesise homonymy annotation for Princeton WordNet. Previous approaches treat the problem using clustering methods; by contrast, our method works by linking WordNet to theOxfordEnglish Dictionary, which contains the information we need. To perform this alignment, we pair definitions based on their proximity in an embedding space produced by a Transformer model. Despite the simplicity of this approach, our best model attains an F1 of .97 on an evaluation set that we annotate. The outcome of our work is a high-quality homonymy annotation layer for Princeton WordNet, which we release.△ Less"
Category Theory for Quantum Natural Language Processing,Authors:Alexis Toumi,"Abstract:This thesis introduces quantum natural language processing (QNLP) models based on a simple yet powerful analogy between computational linguistics and quantum mechanics: grammar as entanglement. The grammatical structure of text and sentences connects the meaning of words in the same way that entanglement structure connects the states of quantum systems. Category theory allows to make this language…▽ MoreThis thesis introduces quantum natural language processing (QNLP) models based on a simple yet powerful analogy between computational linguistics and quantum mechanics: grammar as entanglement. The grammatical structure of text and sentences connects the meaning of words in the same way that entanglement structure connects the states of quantum systems. Category theory allows to make this language-to-qubit analogy formal: it is a monoidal functor from grammar to vector spaces. We turn this abstract analogy into a concrete algorithm that translates the grammatical structure onto the architecture of parameterised quantum circuits. We then use a hybrid classical-quantum algorithm to train the model so that evaluating the circuits computes the meaning of sentences in data-driven tasks.
  The implementation of QNLP models motivated the development of DisCoPy (Distributional Compositional Python), the toolkit for applied category theory of which the first chapter gives a comprehensive overview. String diagrams are the core data structure of DisCoPy, they allow to reason about computation at a high level of abstraction. We show how they can encode both grammatical structures and quantum circuits, but also logical formulae, neural networks or arbitrary Python code. Monoidal functors allow to translate these abstract diagrams into concrete computation, interfacing with optimised task-specific libraries.
  The second chapter uses DisCopy to implement QNLP models as parameterised functors from grammar to quantum circuits. It gives a first proof-of-concept for the more general concept of functorial learning: generalising machine learning from functions to functors by learning from diagram-like data. In order to learn optimal functor parameters via gradient descent, we introduce the notion of diagrammatic differentiation: a graphical calculus for computing the gradients of parameterised diagrams.△ Less"
Epidemiological Agent-Based Modelling Software (Epiabm),"Authors:Kit Gallagher,Ioana Bouros,Nicholas Fan,Elizabeth Hayman,Luke Heirene,Patricia Lamirande,Annabelle Lemenuel-Diot,Ben Lambert,David Gavaghan,Richard Creswell","Abstract:…Analysis at Imperial College London. It has been developed as part of the first-year training programme in the EPSRC SABS:R3 Centre for Doctoral Training at the University ofOxford. The model builds an age-stratified, spatially heterogeneous population and offers a modular approach to configure and run epidemic scenarios, allowing for a broad scope of inves…▽ MoreEpiabm is a fully tested, open-source software package for epidemiological agent-based modelling, re-implementing the well-known CovidSim model from the MRC Centre for Global Infectious Disease Analysis at Imperial College London. It has been developed as part of the first-year training programme in the EPSRC SABS:R3 Centre for Doctoral Training at the University ofOxford. The model builds an age-stratified, spatially heterogeneous population and offers a modular approach to configure and run epidemic scenarios, allowing for a broad scope of investigative and comparative studies. Two simulation backends are provided: a pedagogical Python backend (with full functionality) and a high performance C++ backend for use with larger population simulations. Both are highly modular, with comprehensive testing and documentation for ease of understanding and extensibility. Epiabm is publicly available through GitHub at https://github.com/SABS-R3-Epidemiology/epiabm.△ Less"
Diffusion Art or Digital Forgery? Investigating Data Replication in Diffusion Models,"Authors:Gowthami Somepalli,Vasu Singla,Micah Goldblum,Jonas Geiping,Tom Goldstein","Abstract:…compare generated images with training samples and detect when content has been replicated. Applying our frameworks to diffusion models trained on multiple datasets includingOxfordflowers, Celeb-A, ImageNet, and LAION, we discuss how factors such as training set size impact rates of content replication. We also identify cases where diffusion models, includ…▽ MoreCutting-edge diffusion models produce images with high quality and customizability, enabling them to be used for commercial art and graphic design purposes. But do diffusion models create unique works of art, or are they replicating content directly from their training sets? In this work, we study image retrieval frameworks that enable us to compare generated images with training samples and detect when content has been replicated. Applying our frameworks to diffusion models trained on multiple datasets includingOxfordflowers, Celeb-A, ImageNet, and LAION, we discuss how factors such as training set size impact rates of content replication. We also identify cases where diffusion models, including the popular Stable Diffusion model, blatantly copy from their training data.△ Less"
Modelling Correlated Bernoulli Data Part II: Inference,"Authors:Louise Kimpton,Peter Challenor,Henry Wynn","Abstract:…an extension to Markov chains. Given the DBP and an observed sequence of binary data, we present a method of inference using Bayes' factors. Results are applied to theOxfordand Cambridge annual boat race.▽ MoreBinary data are highly common in many applications, however it is usually modelled with the assumption that the data are independently and identically distributed. This is typically not the case in many real-world examples and such the probability of a success can be dependent on the outcome successes of past events. The de Bruijn process (DBP) was introduced in Kimpton et al. [2022]. This is a correlated Bernoulli process which can be used to model binary data with known correlation. The correlation structures are included through the use of de Bruijn graphs, giving an extension to Markov chains. Given the DBP and an observed sequence of binary data, we present a method of inference using Bayes' factors. Results are applied to theOxfordand Cambridge annual boat race.△ Less"
Automated Identification of Eviction Status from Electronic Health Record Notes,"Authors:Zonghai Yao,Jack Tsai,Weisong Liu,David A. Levy,Emily Druhl,Joel I Reisman,Hong Yu","Abstract:Objective: Evictions are important social and behavioral determinants of health. Evictions are associated with a cascade of negative events that can lead to unemployment, housing insecurity/homelessness, long-term poverty, and mental health problems. In this study, we developed a natural language processing system to automatically detect eviction status from electronic health record (EHR) notes.…▽ MoreObjective: Evictions are important social and behavioral determinants of health. Evictions are associated with a cascade of negative events that can lead to unemployment, housing insecurity/homelessness, long-term poverty, and mental health problems. In this study, we developed a natural language processing system to automatically detect eviction status from electronic health record (EHR) notes.
  Materials and Methods: We first defined eviction status (eviction presence and eviction period) and then annotated eviction status in 5000 EHR notes from the Veterans Health Administration (VHA). We developed a novel model, KIRESH, that has shown to substantially outperform other state-of-the-art models such as fine-tuning pre-trained language models like BioBERT and BioClinicalBERT. Moreover, we designed a novel prompt to further improve the model performance by using the intrinsic connection between the two sub-tasks of eviction presence and period prediction. Finally, we used the Temperature Scaling-based Calibration on our KIRESH-Prompt method to avoid over-confidence issues arising from the imbalance dataset.
  Results: KIRESH-Prompt substantially outperformed strong baseline models including fine-tuning the BioClinicalBERT model to achieve 0.74672 MCC, 0.71153 Macro-F1, and 0.83396 Micro-F1 in predicting eviction period and 0.66827 MCC, 0.62734 Macro-F1, and 0.7863 Micro-F1 in predicting eviction presence. We also conducted additional experiments on a benchmark social determinants of health (SBDH) dataset to demonstrate the generalizability of our methods.
  Conclusion and Future Work: KIRESH-Prompt has substantially improved eviction status classification. We plan to deploy KIRESH-Prompt to the VHA EHRs as an eviction surveillance system to help address the US Veterans' housing insecurity.△ Less"
Binary De Bruijn Processes,"Authors:Louise Kimpton,Peter Challenor,Henry Wynn","Abstract:…is, the run lengths of letters are observed along with run length properties. Inference is also presented along with two application examples: precipitation data and theOxfordand Cambridge boat race.▽ MoreBinary time series data are very common in many applications, and are typically modelled independently via a Bernoulli process with a single probability of success. However, the probability of a success can be dependent on the outcome successes of past events. Presented here is a novel approach for modelling binary time series data called a binary de Bruijn process which takes into account temporal correlation. The structure is derived from de Bruijn Graphs - a directed graph, where given a set of symbols, V, and a 'word' length, m, the nodes of the graph consist of all possible sequences of V of length m. De Bruijn Graphs are equivalent to mth order Markov chains, where the 'word' length controls the number of states that each individual state is dependent on. This increases correlation over a wider area. To quantify how clustered a sequence generated from a de Bruijn process is, the run lengths of letters are observed along with run length properties. Inference is also presented along with two application examples: precipitation data and theOxfordand Cambridge boat race.△ Less"
Reconstruction ofB \to ρ\ell ν_\elldecays identified using hadronic decays of the recoilBmeson in 2019 -- 2021 Belle II data,"Authors:Belle II Collaboration,F. Abudinén,I. Adachi,K. Adamczyk,L. Aggarwal,P. Ahlburg,H. Ahmed,J. K. Ahn,H. Aihara,N. Akopov,A. Aloisio,F. Ameli,L. Andricek,N. Anh Ky,D. M. Asner,H. Atmacan,V. Aulchenko,T. Aushev,V. Aushev,V. Babu,S. Bacher,H. Bae,S. Baehr,S. Bahinipati,A. M. Bakich, et al. (560 additional authors not shown)","Abstract:We present results on the semileptonic decaysB^0 \to ρ^- \ell^+ ν_\ellandB^+ \to ρ^0 \ell^+ ν_\ellin a sample corresponding to 189.9/fb of Belle II data at the SuperKEKBe^- e^+collider. Signal decays are identified using full reconstruction of the recoilBmeson in hadronic final states. We determine the total branching fractions via fits to the distributions of the square of the ""mi…▽ MoreWe present results on the semileptonic decaysB^0 \to ρ^- \ell^+ ν_\ellandB^+ \to ρ^0 \ell^+ ν_\ellin a sample corresponding to 189.9/fb of Belle II data at the SuperKEKBe^- e^+collider. Signal decays are identified using full reconstruction of the recoilBmeson in hadronic final states. We determine the total branching fractions via fits to the distributions of the square of the ""missing"" mass in the event and the dipion mass in the signal candidate and find{\mathcal{B}(B^0\toρ^-\ell^+ ν_\ell) = (4.12 \pm 0.64(\mathrm{stat}) \pm 1.16(\mathrm{syst})) \times 10^{-4}}and{\mathcal{B}({B^+\toρ^0\ell^+ν_\ell}) = (1.77 \pm 0.23 (\mathrm{stat}) \pm 0.36 (\mathrm{syst})) \times 10^{-4}}where the dominant systematic uncertainty comes from modeling the nonresonantB\to (ππ)\ell^+ν_\ellcontribution.△ Less"
CASSPR: Cross Attention Single Scan Place Recognition,"Authors:Yan Xia,Mariia Gladkova,Rui Wang,Qianyun Li,Uwe Stilla,João F. Henriques,Daniel Cremers","Abstract:…both to inform the output global descriptor of the point cloud. Extensive experiments show that CASSPR surpasses the state-of-the-art by a large margin on several datasets (OxfordRobotCar, TUM, USyd). For instance, it achieves AR@1 of 85.6% on the TUM dataset, surpassing the strongest prior model by ~15%. Our code is publicly available.▽ MorePlace recognition based on point clouds (LiDAR) is an important component for autonomous robots or self-driving vehicles. Current SOTA performance is achieved on accumulated LiDAR submaps using either point-based or voxel-based structures. While voxel-based approaches nicely integrate spatial context across multiple scales, they do not exhibit the local precision of point-based methods. As a result, existing methods struggle with fine-grained matching of subtle geometric features in sparse single-shot Li- DAR scans. To overcome these limitations, we propose CASSPR as a method to fuse point-based and voxel-based approaches using cross attention transformers. CASSPR leverages a sparse voxel branch for extracting and aggregating information at lower resolution and a point-wise branch for obtaining fine-grained local information. CASSPR uses queries from one branch to try to match structures in the other branch, ensuring that both extract self-contained descriptors of the point cloud (rather than one branch dominating), but using both to inform the output global descriptor of the point cloud. Extensive experiments show that CASSPR surpasses the state-of-the-art by a large margin on several datasets (OxfordRobotCar, TUM, USyd). For instance, it achieves AR@1 of 85.6% on the TUM dataset, surpassing the strongest prior model by ~15%. Our code is publicly available.△ Less"
Injectable Bubbles for Physiological Pressure Measurement,Authors:Prashant Pandey,"Abstract:Microbubbles - used as contrast agents in ultrasound imaging - are important tools in biomedical research, having been used together with ultrasound to develop significant diagnostic and therapeutic techniques. It has been suggested that the dynamic behaviour of microbubbles is dependent on the surrounding fluid's ambient (hydrostatic) pressure, and the potential to non-invasively determine blood…▽ MoreMicrobubbles - used as contrast agents in ultrasound imaging - are important tools in biomedical research, having been used together with ultrasound to develop significant diagnostic and therapeutic techniques. It has been suggested that the dynamic behaviour of microbubbles is dependent on the surrounding fluid's ambient (hydrostatic) pressure, and the potential to non-invasively determine blood pressure has numerous medical applications. To study this dependence, a computational mathematical model was created based on Marmottant's dynamic model of a microbubble. A pulse inversion (PI) protocol was incorporated into the model to emphasize the nonlinear behaviour of the microbubble's response. The mathematical model was also used to assess the sensitivity of the microbubbles to undesirable changes in parameters other than the ambient pressure. It found that a variation in the microbubble's initial radius and surface tension would cause the most significant changes in signal energy and hence pose a risk to ambient pressure measurements. To test the practicality of detecting a change in the dynamic behaviour of microbubbles, in vitro experiments were designed and carried out using clinically available contrast agent with two different ultrasound systems. The experiments, while possessing certain limitations, confirmed that there is a change in microbubbles' dynamic behaviour when the ambient pressure is varied, in cases by as little as 7.36 mmHg (981 Pa - a 0.98% change in atmospheric pressure). The experimental results establish a proof-of-principle that future experimental work can build upon to verify the mathematical model, and hence aid in developing a non-invasive blood pressure measurement procedure.△ Less"
Concentration of CMC Surfaces in a 3-manifold,Authors:Paul Laurain,Abstract:We prove that simply connected H-surfaces with small diameter in a 3-manifold necessarily concentrate at a critical point of the scalar curvature.We prove that simply connected H-surfaces with small diameter in a 3-manifold necessarily concentrate at a critical point of the scalar curvature.△ Less
Discretisations and Preconditioners for Magnetohydrodynamics Models,Authors:Fabian Laakmann,"Abstract:The magnetohydrodynamics (MHD) equations are generally known to be difficult to solve numerically, due to their highly nonlinear structure and the strong coupling between the electromagnetic and hydrodynamic variables, especially for high Reynolds and coupling numbers.
  In the first part of this work, we present a scalable augmented Lagrangian preconditioner for a finite element discretisation of…▽ MoreThe magnetohydrodynamics (MHD) equations are generally known to be difficult to solve numerically, due to their highly nonlinear structure and the strong coupling between the electromagnetic and hydrodynamic variables, especially for high Reynolds and coupling numbers.
  In the first part of this work, we present a scalable augmented Lagrangian preconditioner for a finite element discretisation of the\mathbf{B}-\mathbf{E}formulation of the incompressible viscoresistive MHD equations. For stationary problems, our solver achieves robust performance with respect to the Reynolds and coupling numbers in two dimensions and good results in three dimensions. Our approach relies on specialised parameter-robust multigrid methods for the hydrodynamic and electromagnetic blocks. The scheme ensures exactly divergence-free approximations of both the velocity and the magnetic field up to solver tolerances.
  In the second part, we focus on incompressible, resistive Hall MHD models and derive structure-preserving finite element methods for these equations. We present a variational formulation of Hall MHD that enforces the magnetic Gauss's law precisely (up to solver tolerances) and prove the well-posedness of a Picard linearisation. For the transient problem, we present time discretisations that preserve the energy and magnetic and hybrid helicity precisely in the ideal limit for two types of boundary conditions.
  In the third part, we investigate anisothermal MHD models. We start by performing a bifurcation analysis for a magnetic Rayleigh--Bénard problem at a high coupling numberS=1{,}000by choosing the Rayleigh number in the range between 0 and100{,}000as the bifurcation parameter. We study the effect of the coupling number on the bifurcation diagram and outline how we create initial guesses to obtain complex solution patterns and disconnected branches for high coupling numbers.△ Less"
System architecture of a four-wheel drive Formula Student vehicle,"Authors:Adriano Schommer,Gordana Collier,Robert Norris,Denise Morrey,Ludmila Nesi Maria,Chris Johnston","Abstract:…The interaction between software and hardware is complex and imposes additional challenges for systems integration. This paper provides a structured introduction to the OBR22OxfordBrookes Racing Formula Student electric vehicle. From a system architecture perspective, the four-wheel drive in-hub motors topology is described. Diagrams of the hardware compo…▽ MoreFormula Student vehicles are becoming increasingly complex, especially with the current shift from internal combustion engines toward electric powertrains. The interaction between software and hardware is complex and imposes additional challenges for systems integration. This paper provides a structured introduction to the OBR22OxfordBrookes Racing Formula Student electric vehicle. From a system architecture perspective, the four-wheel drive in-hub motors topology is described. Diagrams of the hardware components, the architecture of the high voltage and communication systems are presented. This paper also demonstrates the model-based development process, including an overview of the model-in-the-loop (MiL) and hardware-in-the-loop (HiL) control design phases.△ Less"
Large-Z atoms in the strong-interaction limit of DFT: Implications for gradient expansions and for the Lieb-Oxfordbound,"Authors:Kimberly J. Daas,Derk P. Kooi,Tarik Benyahia,Michael Seidl,Paola Gori-Giorgi","Abstract:…coefficient at strong coupling, while generalized gradient approximations such as PBE and PBEsol severely underestimate it. We then use our results to analyse the Lieb-Oxfordbound from the point of view of slowly-varying densities, clarifying some aspects on the bound at fixed number of electrons.▽ MoreWe study numerically the strong-interaction limit of the exchange-correlation functional for neutral atoms and for Bohr atoms as the number of electrons increases. Using a compact representation, we analyse the second-order gradient expansion, comparing it with the one for exchange (weak interaction limit). The two gradient expansions, at strong and weak interaction, turn out to be very similar in magnitude, but with opposite signs. We find that the point-charge plus continuum model is surprisingly accurate for the gradient expansion coefficient at strong coupling, while generalized gradient approximations such as PBE and PBEsol severely underestimate it. We then use our results to analyse the Lieb-Oxfordbound from the point of view of slowly-varying densities, clarifying some aspects on the bound at fixed number of electrons.△ Less"
Understanding Approximation for Bayesian Inference in Neural Networks,Authors:Sebastian Farquhar,"Abstract:Bayesian inference has theoretical attractions as a principled framework for reasoning about beliefs. However, the motivations of Bayesian inference which claim it to be the only 'rational' kind of reasoning do not apply in practice. They create a binary split in which all approximate inference is equally 'irrational'. Instead, we should ask ourselves how to define a spectrum of more- and less-rat…▽ MoreBayesian inference has theoretical attractions as a principled framework for reasoning about beliefs. However, the motivations of Bayesian inference which claim it to be the only 'rational' kind of reasoning do not apply in practice. They create a binary split in which all approximate inference is equally 'irrational'. Instead, we should ask ourselves how to define a spectrum of more- and less-rational reasoning that explains why we might prefer one Bayesian approximation to another. I explore approximate inference in Bayesian neural networks and consider the unintended interactions between the probabilistic model, approximating distribution, optimization algorithm, and dataset. The complexity of these interactions highlights the difficulty of any strategy for evaluating Bayesian approximations which focuses entirely on the method, outside the context of specific datasets and decision-problems. For given applications, the expected utility of the approximate posterior can measure inference quality. To assess a model's ability to incorporate different parts of the Bayesian framework we can identify desirable characteristic behaviours of Bayesian reasoning and pick decision-problems that make heavy use of those behaviours. Here, we use continual learning (testing the ability to update sequentially) and active learning (testing the ability to represent credence). But existing continual and active learning set-ups pose challenges that have nothing to do with posterior quality which can distort their ability to evaluate Bayesian approximations. These unrelated challenges can be removed or reduced, allowing better evaluation of approximate inference methods.△ Less"
RaLiBEV: Radar and LiDAR BEV Fusion Learning for Anchor Box Free Object Detection Systems,"Authors:Yanlong Yang,Jianan Liu,Tao Huang,Qing-Long Han,Gang Ma,Bing Zhu","Abstract:…enhanced by employing a novel interactive transformer module. The superior performance of the methods proposed in this paper has been demonstrated using the recently publishedOxfordRadar RobotCar dataset. Our system's average precision significantly outperforms the state-of-the-art method by 13.1% and 19.0% at Intersection of Union (IoU) of 0.8 under &…▽ MoreIn autonomous driving, LiDAR and radar are crucial for environmental perception. LiDAR offers precise 3D spatial sensing information but struggles in adverse weather like fog. Conversely, radar signals can penetrate rain or mist due to their specific wavelength but are prone to noise disturbances. Recent state-of-the-art works reveal that the fusion of radar and LiDAR can lead to robust detection in adverse weather. The existing works adopt convolutional neural network architecture to extract features from each sensor data, then align and aggregate the two branch features to predict object detection results. However, these methods have low accuracy of predicted bounding boxes due to a simple design of label assignment and fusion strategies. In this paper, we propose a bird's-eye view fusion learning-based anchor box-free object detection system, which fuses the feature derived from the radar range-azimuth heatmap and the LiDAR point cloud to estimate possible objects. Different label assignment strategies have been designed to facilitate the consistency between the classification of foreground or background anchor points and the corresponding bounding box regressions. Furthermore, the performance of the proposed object detector is further enhanced by employing a novel interactive transformer module. The superior performance of the methods proposed in this paper has been demonstrated using the recently publishedOxfordRadar RobotCar dataset. Our system's average precision significantly outperforms the state-of-the-art method by 13.1% and 19.0% at Intersection of Union (IoU) of 0.8 under 'Clear+Foggy' training conditions for 'Clear' and 'Foggy' testing, respectively.△ Less"
Optical Spectroscopy of Blazars for the Cherenkov Telescope Array -- II,"Authors:E. Kasai,P. Goldoni,S. Pita,D. A. Williams,W. Max-Moerbeck,O. Hervet,G. Cotter,M. Backes,C. Boisson,J. Becerra González,U. Barres de Almeida,F. D'Ammando,V. Fallah Ramazani,E. Lindfors","Abstract:Active galactic nuclei (AGNs) make up about 35 per cent of the more than 250 sources detected in very-high-energy (VHE) gamma rays to date with Imaging Atmospheric Cherenkov Telescopes. Apart from four nearby radio galaxies and two AGNs of unknown type, all known VHE AGNs are blazars. Knowledge of the cosmological redshift of gamma-ray blazars is key to enabling the study of their intrinsic emissi…▽ MoreActive galactic nuclei (AGNs) make up about 35 per cent of the more than 250 sources detected in very-high-energy (VHE) gamma rays to date with Imaging Atmospheric Cherenkov Telescopes. Apart from four nearby radio galaxies and two AGNs of unknown type, all known VHE AGNs are blazars. Knowledge of the cosmological redshift of gamma-ray blazars is key to enabling the study of their intrinsic emission properties, as the interaction between gamma rays and the extragalactic background light (EBL) results in a spectral softening. Therefore, the redshift determination exercise is crucial to indirectly placing tight constraints on the EBL density and to studying blazar population evolution across cosmic time. Due to the powerful relativistic jets in blazars, most of their host galaxies' spectral features are outshined, and dedicated high signal-to-noise spectroscopic observations are required. Deep medium- to high-resolution spectroscopy of 33 gamma-ray blazar optical counterparts was performed with the European Southern Observatory New Technology Telescope, Keck II telescope, Shane 3-meter telescope and the Southern African Large Telescope. From the sample, spectra from 25 objects display spectral features or are featureless and have high signal-to-noise. The other eight objects have low quality featureless spectra. We systematically searched for absorption and emission features and estimated, when possible, the fractional host galaxy flux in the measured total flux. Our measurements yielded 14 firm spectroscopic redshifts, ranging from 0.0838 to 0.8125, one tentative redshift, and two lower limits: one at z > 0.382 and the other at z > 0.629.△ Less"
"Lidar-level localization with radar? The CFEAR approach to accurate, fast and robust large-scale radar odometry in diverse environments","Authors:Daniel Adolfsson,Martin Magnusson,Anas Alhashimi,Achim J. Lilienthal,Henrik Andreasson","Abstract:…our state-of-the-art by 38%, thus, surprisingly, outperforming radar SLAM and approaching lidar SLAM. The most accurate configuration achieves 1.09% error at 5Hz on theOxfordbenchmark, and the fastest achieves 1.79% error at 160Hz.▽ MoreThis paper presents an accurate, highly efficient, and learning-free method for large-scale odometry estimation using spinning radar, empirically found to generalize well across very diverse environments -- outdoors, from urban to woodland, and indoors in warehouses and mines - without changing parameters. Our method integrates motion compensation within a sweep with one-to-many scan registration that minimizes distances between nearby oriented surface points and mitigates outliers with a robust loss function. Extending our previous approach CFEAR, we present an in-depth investigation on a wider range of data sets, quantifying the importance of filtering, resolution, registration cost and loss functions, keyframe history, and motion compensation. We present a new solving strategy and configuration that overcomes previous issues with sparsity and bias, and improves our state-of-the-art by 38%, thus, surprisingly, outperforming radar SLAM and approaching lidar SLAM. The most accurate configuration achieves 1.09% error at 5Hz on theOxfordbenchmark, and the fastest achieves 1.79% error at 160Hz.△ Less"
Data-driven discovery of Green's functions,Authors:Nicolas Boullé,"Abstract:Discovering hidden partial differential equations (PDEs) and operators from data is an important topic at the frontier between machine learning and numerical analysis. This doctoral thesis introduces theoretical results and deep learning algorithms to learn Green's functions associated with linear partial differential equations and rigorously justify PDE learning techniques. A theoretically rigoro…▽ MoreDiscovering hidden partial differential equations (PDEs) and operators from data is an important topic at the frontier between machine learning and numerical analysis. This doctoral thesis introduces theoretical results and deep learning algorithms to learn Green's functions associated with linear partial differential equations and rigorously justify PDE learning techniques. A theoretically rigorous algorithm is derived to obtain a learning rate, which characterizes the amount of training data needed to approximately learn Green's functions associated with elliptic PDEs. The construction connects the fields of PDE learning and numerical linear algebra by extending the randomized singular value decomposition to non-standard Gaussian vectors and Hilbert--Schmidt operators, and exploiting the low-rank hierarchical structure of Green's functions using hierarchical matrices. Rational neural networks (NNs) are introduced and consist of neural networks with trainable rational activation functions. The highly compositional structure of these networks, combined with rational approximation theory, implies that rational functions have higher approximation power than standard activation functions. In addition, rational NNs may have poles and take arbitrarily large values, which is ideal for approximating functions with singularities such as Green's functions. Finally, theoretical results on Green's functions and rational NNs are combined to design a human-understandable deep learning method for discovering Green's functions from data. This approach complements state-of-the-art PDE learning techniques, as a wide range of physics can be captured from the learned Green's functions such as dominant modes, symmetries, and singularity locations.△ Less"
TPFNet: A Novel Text In-painting Transformer for Text Removal,"Authors:Onkar Susladkar,Dhruv Makwana,Gayatri Deshmukh,Sparsh Mittal,Sai Chandra Teja R,Rekha Singhal","Abstract:…in learning the image structure. To precisely locate the text, TPFNet employs an adversarial loss that is conditional on the segmentation map rather than the input image. OnOxford, SCUT, and SCUT-EnsText datasets, our network outperforms recently proposed networks on nearly all the metrics. For example, on SCUT-EnsText dataset, TPFNet has a PSNR (higher is…▽ MoreText erasure from an image is helpful for various tasks such as image editing and privacy preservation. In this paper, we present TPFNet, a novel one-stage (end-toend) network for text removal from images. Our network has two parts: feature synthesis and image generation. Since noise can be more effectively removed from low-resolution images, part 1 operates on low-resolution images. The output of part 1 is a low-resolution text-free image. Part 2 uses the features learned in part 1 to predict a high-resolution text-free image. In part 1, we use ""pyramidal vision transformer"" (PVT) as the encoder. Further, we use a novel multi-headed decoder that generates a high-pass filtered image and a segmentation map, in addition to a text-free image. The segmentation branch helps locate the text precisely, and the high-pass branch helps in learning the image structure. To precisely locate the text, TPFNet employs an adversarial loss that is conditional on the segmentation map rather than the input image. OnOxford, SCUT, and SCUT-EnsText datasets, our network outperforms recently proposed networks on nearly all the metrics. For example, on SCUT-EnsText dataset, TPFNet has a PSNR (higher is better) of 39.0 and text-detection precision (lower is better) of 21.1, compared to the best previous technique, which has a PSNR of 32.3 and precision of 53.2. The source code can be obtained from https://github.com/CandleLabAI/TPFNet△ Less"
MAROAM: Map-based Radar SLAM through Two-step Feature Selection,"Authors:Dequan Wang,Yifan Duan,Xiaoran Fan,Chengzhen Meng,Jianmin Ji,Yanyong Zhang","Abstract:…we perform loop closure and graph optimization in the back-end, further reducing the accumulated drift error.
  We evaluate the performance of MAROAM on the three datasets: theOxfordRadar RobotCar Dataset, the MulRan Dataset and the Boreas Dataset. We consider a variety of experimental settings with different scenery, weather, and road conditions. The expe…▽ MoreIn this letter, we propose MAROAM, a millimeter wave radar-based SLAM framework, which employs a two-step feature selection process to build the global consistent map. Specifically, we first extract feature points from raw data based on their local geometric properties to filter out those points that violate the principle of millimeter-wave radar imaging. Then, we further employ another round of probabilistic feature selection by examining how often and how recent the feature point has been detected in the proceeding frames. With such a two-step feature selection, we establish a global consistent map for accurate and robust pose estimation as well as other downstream tasks. At last, we perform loop closure and graph optimization in the back-end, further reducing the accumulated drift error.
  We evaluate the performance of MAROAM on the three datasets: theOxfordRadar RobotCar Dataset, the MulRan Dataset and the Boreas Dataset. We consider a variety of experimental settings with different scenery, weather, and road conditions. The experimental results show that the accuracy of MAROAM is 7.95%, 37.0% and 8.9% higher than the currently best-performing algorithms on these three datasets, respectively. The ablation results also show that our map-based odometry performs 28.6% better than the commonly used scan-to-frames method. Finally, as devoted contributors to the open-source community, we will open source the algorithm after the paper is accepted.△ Less"
Determination of|V_{cb}|fromB\to D\ellνdecays using 2019-2021 Belle II data,"Authors:Belle II collaboration,F. Abudinén,I. Adachi,K. Adamczyk,L. Aggarwal,P. Ahlburg,H. Ahmed,J. K. Ahn,H. Aihara,N. Akopov,A. Aloisio,F. Ameli,L. Andricek,N. Anh Ky,D. M. Asner,H. Atmacan,V. Aulchenko,T. Aushev,V. Aushev,T. Aziz,V. Babu,S. Bacher,H. Bae,S. Baehr,S. Bahinipati, et al. (570 additional authors not shown)","Abstract:We present a determination of the magnitude of the Cabibbo-Kobayashi-Maskawa (CKM) matrix elementV_{cb}usingB\to D\ellνdecays. The result is based one^+e^-\toΥ(4S)data recorded by the Belle II detector corresponding to 189.2/fb of integrated luminosity. The semileptonic decaysB^0\to D^-(\to K^+π^-π^-)\ell^+ν_\ellandB^+\to\bar D^0(\to K^+π^-)\ell^+ν_\ellare reconstructed, where…▽ MoreWe present a determination of the magnitude of the Cabibbo-Kobayashi-Maskawa (CKM) matrix elementV_{cb}usingB\to D\ellνdecays. The result is based one^+e^-\toΥ(4S)data recorded by the Belle II detector corresponding to 189.2/fb of integrated luminosity. The semileptonic decaysB^0\to D^-(\to K^+π^-π^-)\ell^+ν_\ellandB^+\to\bar D^0(\to K^+π^-)\ell^+ν_\ellare reconstructed, where\ellis either electron or a muon. The secondBmeson in theΥ(4S)event is not explicitly reconstructed. Using the diamond-frame method, we determine theBmeson four-momentum and thus the hadronic recoil. We extract the partial decay rates as functions ofwand perform a fit to the decay form-factor and the CKM parameter|V_{cb}|using the BGL parameterization of the form factor and lattice QCD input from the FNAL/MILC and HPQCD collaborations. We obtainη_{EW}|V_{cb}|=(38.53\pm 1.15)\times 10^{-3}, whereη_{EW}is an electroweak correction, and the error accounts for theoretical and experimental sources of uncertainty.△ Less"
"""Covid vaccine is against Covid butOxfordvaccine is made atOxford!"" Semantic Interpretation of Proper Noun Compounds","Authors:Keshav Kolluru,Gabriel Stanovsky,Mausam","Abstract:Proper noun compounds, e.g., ""Covid vaccine"", convey information in a succinct manner (a ""Covid vaccine"" is a ""vaccine that immunizes against the Covid disease""). These are commonly used in short-form domains, such as news headlines, but are largely ignored in information-seeking applications. To address this limitation, we release a new manually annotated dataset, ProNCI, consisting of 22.5K prop…▽ MoreProper noun compounds, e.g., ""Covid vaccine"", convey information in a succinct manner (a ""Covid vaccine"" is a ""vaccine that immunizes against the Covid disease""). These are commonly used in short-form domains, such as news headlines, but are largely ignored in information-seeking applications. To address this limitation, we release a new manually annotated dataset, ProNCI, consisting of 22.5K proper noun compounds along with their free-form semantic interpretations. ProNCI is 60 times larger than prior noun compound datasets and also includes non-compositional examples, which have not been previously explored. We experiment with various neural models for automatically generating the semantic interpretations from proper noun compounds, ranging from few-shot prompting to supervised learning, with varying degrees of knowledge about the constituent nouns. We find that adding targeted knowledge, particularly about the common noun, results in performance gains of upto 2.8%. Finally, we integrate our model generated interpretations with an existing Open IE system and observe an 7.5% increase in yield at a precision of 85%. The dataset and code are available at https://github.com/dair-iitd/pronci.△ Less"
The background information about perturbative quantum gravity,Authors:Ilya L. Shapiro,"Abstract:…is a modified version of the corresponding chapters of Part II of the recent textbook on quantum field theory (QFT) and QG, co-authored with I.L. Buchbinder and published inOxfordUniversity Press. We discuss the choice of the starting action in the QG models, degrees of freedom and propagator of metric perturbations, power counting and renormalizability of…▽ MoreThe purpose of this Chapter is to give a general introduction and status review on the perturbative approach to quantum gravity (QG). This text is a modified version of the corresponding chapters of Part II of the recent textbook on quantum field theory (QFT) and QG, co-authored with I.L. Buchbinder and published inOxfordUniversity Press. We discuss the choice of the starting action in the QG models, degrees of freedom and propagator of metric perturbations, power counting and renormalizability of these models, the problems related to higher derivative theories and ghosts, such as quantum unitarity and the stability of classical solutions in general relativity; and the perspective to overcome these problems. The gauge fixing and parametrization dependencies are discussed in detail using the corresponding general QFT theorems developed in gauge theories. On top of that, we present a basic example of deriving the one-loop divergences and discuss an important example of the renormalization group in QG. The gauge invariant renormalizability of QG is considered in another Chapter of the Handbook, written together with P.M. Lavrov.△ Less"
"DIICAN: Dual Time-scale State-Coupled Co-estimation of SOC, SOH and RUL for Lithium-Ion Batteries","Authors:Ningbo Cai,Yuwen Qin,Xin Chen,Kai Wu","Abstract:…battery aging on the SOC estimation, the battery degradation-related state is incorporated in the SOC estimation for capacity calibration. The DIICAN method is validated on theOxfordbattery dataset. The experimental results show that the proposed method can achieve SOH and RUL co-estimation with high accuracy and effectively improve SOC estimation accuracy…▽ MoreAccurate co-estimations of battery states, such as state-of-charge (SOC), state-of-health (SOH,) and remaining useful life (RUL), are crucial to the battery management systems to assure safe and reliable management. Although the external properties of the battery charge with the aging degree, batteries' degradation mechanism shares similar evolving patterns. Since batteries are complicated chemical systems, these states are highly coupled with intricate electrochemical processes. A state-coupled co-estimation method named Deep Inter and Intra-Cycle Attention Network (DIICAN) is proposed in this paper to estimate SOC, SOH, and RUL, which organizes battery measurement data into the intra-cycle and inter-cycle time scales. And to extract degradation-related features automatically and adapt to practical working conditions, the convolutional neural network is applied. The state degradation attention unit is utilized to extract the battery state evolution pattern and evaluate the battery degradation degree. To account for the influence of battery aging on the SOC estimation, the battery degradation-related state is incorporated in the SOC estimation for capacity calibration. The DIICAN method is validated on theOxfordbattery dataset. The experimental results show that the proposed method can achieve SOH and RUL co-estimation with high accuracy and effectively improve SOC estimation accuracy for the whole lifespan.△ Less"
NEREL-BIO: A Dataset of Biomedical Abstracts Annotated with Nested Named Entities,"Authors:Natalia Loukachevitch,Suresh Manandhar,Elina Baral,Igor Rozhkov,Pavel Braslavski,Vladimir Ivanov,Tatiana Batura,Elena Tutubalina","Abstract:This paper describes NEREL-BIO -- an annotation scheme and corpus of PubMed abstracts in Russian and smaller number of abstracts in English. NEREL-BIO extends the general domain dataset NEREL by introducing domain-specific entity types. NEREL-BIO annotation scheme covers both general and biomedical domains making it suitable for domain transfer experiments. NEREL-BIO provides annotation for nested…▽ MoreThis paper describes NEREL-BIO -- an annotation scheme and corpus of PubMed abstracts in Russian and smaller number of abstracts in English. NEREL-BIO extends the general domain dataset NEREL by introducing domain-specific entity types. NEREL-BIO annotation scheme covers both general and biomedical domains making it suitable for domain transfer experiments. NEREL-BIO provides annotation for nested named entities as an extension of the scheme employed for NEREL. Nested named entities may cross entity boundaries to connect to shorter entities nested within longer entities, making them harder to detect.
  NEREL-BIO contains annotations for 700+ Russian and 100+ English abstracts. All English PubMed annotations have corresponding Russian counterparts. Thus, NEREL-BIO comprises the following specific features: annotation of nested named entities, it can be used as a benchmark for cross-domain (NEREL -> NEREL-BIO) and cross-language (English -> Russian) transfer. We experiment with both transformer-based sequence models and machine reading comprehension (MRC) models and report their results.
  The dataset is freely available at https://github.com/nerel-ds/NEREL-BIO.△ Less"
Measurement of the photon-energy spectrum in inclusiveB\rightarrow X_{s}γdecays identified using hadronic decays of the recoilBmeson in 2019-2021 Belle II data,"Authors:Belle II Collaboration,F. Abudinén,I. Adachi,K. Adamczyk,L. Aggarwal,P. Ahlburg,H. Ahmed,J. K. Ahn,H. Aihara,N. Akopov,A. Aloisio,F. Ameli,L. Andricek,N. Anh Ky,D. M. Asner,H. Atmacan,V. Aulchenko,T. Aushev,V. Aushev,T. Aziz,V. Babu,S. Bacher,H. Bae,S. Baehr,S. Bahinipati, et al. (573 additional authors not shown)","Abstract:We measure the photon-energy spectrum in radiative bottom-meson (B) decays into inclusive final states involving a strange hadron and a photon. We use SuperKEKB electron-positron collisions corresponding to189~\mathrm{fb}^{-1}of integrated luminosity collected at theΥ(4S)resonance by the Belle II experiment. The partnerBcandidates are fully reconstructed using a large number of hadro…▽ MoreWe measure the photon-energy spectrum in radiative bottom-meson (B) decays into inclusive final states involving a strange hadron and a photon. We use SuperKEKB electron-positron collisions corresponding to189~\mathrm{fb}^{-1}of integrated luminosity collected at theΥ(4S)resonance by the Belle II experiment. The partnerBcandidates are fully reconstructed using a large number of hadronic channels. TheB \rightarrow X_s γpartial branching fractions are measured as a function of photon energy in the signalBmeson rest frame in eight bins above1.8~\mathrm{GeV}. The background-subtracted signal yield for this photon energy region is343 \pm 122events. Integrated branching fractions for three photon energy thresholds of1.8~\mathrm{GeV},2.0~\mathrm{GeV}, and2.1~\mathrm{GeV}are also reported, and found to be in agreement with world averages.△ Less"
Learning Self-Regularized Adversarial Views for Self-Supervised Vision Transformers,"Authors:Tao Tang,Changlin Li,Guangrun Wang,Kaicheng Yu,Xiaojun Chang,Xiaodan Liang","Abstract:…+1.3% k-NN accuracy). Extensive experiments show that AutoView pretraining also benefits downstream tasks (+1.2% mAcc on ADE20K Semantic Segmentation and +2.8% mAP on revisitedOxfordImage Retrieval benchmark) and improves model robustness (+2.3% Top-1 Acc on ImageNet-A and +1.0% AUPR on ImageNet-O). Code and models will be available at https://github.com/T…▽ MoreAutomatic data augmentation (AutoAugment) strategies are indispensable in supervised data-efficient training protocols of vision transformers, and have led to state-of-the-art results in supervised learning. Despite the success, its development and application on self-supervised vision transformers have been hindered by several barriers, including the high search cost, the lack of supervision, and the unsuitable search space. In this work, we propose AutoView, a self-regularized adversarial AutoAugment method, to learn views for self-supervised vision transformers, by addressing the above barriers. First, we reduce the search cost of AutoView to nearly zero by learning views and network parameters simultaneously in a single forward-backward step, minimizing and maximizing the mutual information among different augmented views, respectively. Then, to avoid information collapse caused by the lack of label supervision, we propose a self-regularized loss term to guarantee the information propagation. Additionally, we present a curated augmentation policy search space for self-supervised learning, by modifying the generally used search space designed for supervised learning. On ImageNet, our AutoView achieves remarkable improvement over RandAug baseline (+10.2% k-NN accuracy), and consistently outperforms sota manually tuned view policy by a clear margin (up to +1.3% k-NN accuracy). Extensive experiments show that AutoView pretraining also benefits downstream tasks (+1.2% mAcc on ADE20K Semantic Segmentation and +2.8% mAP on revisitedOxfordImage Retrieval benchmark) and improves model robustness (+2.3% Top-1 Acc on ImageNet-A and +1.0% AUPR on ImageNet-O). Code and models will be available at https://github.com/Trent-tangtao/AutoView.△ Less"
Scale Equivariant U-Net,"Authors:Mateus Sangalli,Samy Blusseau,Santiago Velasco-Forero,Jesus Angulo","Abstract:…in order to improve generalization to different scales in approximately scale-equivariant architectures. The proposed SEU-Net is trained for semantic segmentation of theOxfordPet IIIT and the DIC-C2DH-HeLa dataset for cell segmentation. The generalization metric to unseen scales is dramatically improved in comparison to the U-Net, even when the U-Net is tr…▽ MoreIn neural networks, the property of being equivariant to transformations improves generalization when the corresponding symmetry is present in the data. In particular, scale-equivariant networks are suited to computer vision tasks where the same classes of objects appear at different scales, like in most semantic segmentation tasks. Recently, convolutional layers equivariant to a semigroup of scalings and translations have been proposed. However, the equivariance of subsampling and upsampling has never been explicitly studied even though they are necessary building blocks in some segmentation architectures. The U-Net is a representative example of such architectures, which includes the basic elements used for state-of-the-art semantic segmentation. Therefore, this paper introduces the Scale Equivariant U-Net (SEU-Net), a U-Net that is made approximately equivariant to a semigroup of scales and translations through careful application of subsampling and upsampling layers and the use of aforementioned scale-equivariant layers. Moreover, a scale-dropout is proposed in order to improve generalization to different scales in approximately scale-equivariant architectures. The proposed SEU-Net is trained for semantic segmentation of theOxfordPet IIIT and the DIC-C2DH-HeLa dataset for cell segmentation. The generalization metric to unseen scales is dramatically improved in comparison to the U-Net, even when the U-Net is trained with scale jittering, and to a scale-equivariant architecture that does not perform upsampling operators inside the equivariant pipeline. The scale-dropout induces better generalization on the scale-equivariant models in the Pet experiment, but not on the cell segmentation experiment.△ Less"
Determination of|V_{ub}|from untaggedB^0\toπ^- \ell^+ ν_{\ell}decays using 2019-2021 Belle II data,"Authors:Belle II Collaboration,K. Adamczyk,L. Aggarwal,P. Ahlburg,H. Ahmed,J. K. Ahn,H. Aihara,N. Akopov,A. Aloisio,F. Ameli,L. Andricek,N. Anh Ky,D. M. Asner,H. Atmacan,V. Aulchenko,T. Aushev,V. Aushev,T. Aziz,V. Babu,S. Bacher,H. Bae,S. Baehr,S. Bahinipati,A. M. Bakich,P. Bambade, et al. (568 additional authors not shown)","Abstract:We present an analysis of the charmless semileptonic decayB^0\toπ^- \ell^+ ν_{\ell}, where\ell = e, μ, from 198.0 million pairs ofB\bar{B}mesons recorded by the Belle II detector at the SuperKEKB electron-positron collider. The decay is reconstructed without identifying the partnerBmeson. The partial branching fractions are measured independently forB^0\toπ^- e^+ ν_{e}and…▽ MoreWe present an analysis of the charmless semileptonic decayB^0\toπ^- \ell^+ ν_{\ell}, where\ell = e, μ, from 198.0 million pairs ofB\bar{B}mesons recorded by the Belle II detector at the SuperKEKB electron-positron collider. The decay is reconstructed without identifying the partnerBmeson. The partial branching fractions are measured independently forB^0\toπ^- e^+ ν_{e}andB^0\toπ^- μ^+ ν_μas functions ofq^{2}(momentum transfer squared), using 3896B^0\toπ^- e^+ ν_{e}and 5466B^0\toπ^- μ^+ ν_μdecays. The total branching fraction is found to be(1.426 \pm 0.056 \pm 0.125) \times 10^{-4}forB^0\toπ^- \ell^+ ν_{\ell}decays, where the uncertainties are statistical and systematic, respectively. By fitting the measured partial branching fractions as functions ofq^{2}, together with constraints on the nonperturbative hadronic contribution from lattice QCD calculations, the magnitude of the Cabibbo-Kobayashi-Maskawa matrix elementV_{ub},(3.55 \pm 0.12 \pm 0.13 \pm 0.17) \times 10^{-3}, is extracted. Here, the first uncertainty is statistical, the second is systematic and the third is theoretical.△ Less"
T2CI-GAN: Text to Compressed Image generation using Generative Adversarial Network,"Authors:Bulla Rajesh,Nandakishore Dusa,Mohammed Javed,Shiv Ram Dubey,P. Nagabhushan","Abstract:…trained with RGB images (pixel domain) to generate JPEG compressed DCT representation from text descriptions. The proposed models are tested on an open source benchmark datasetOxford-102 Flower images using both RGB and JPEG compressed versions, and accomplished the state-of-the-art performance in the JPEG compressed domain. The code will be publicly releas…▽ MoreThe problem of generating textual descriptions for the visual data has gained research attention in the recent years. In contrast to that the problem of generating visual data from textual descriptions is still very challenging, because it requires the combination of both Natural Language Processing (NLP) and Computer Vision techniques. The existing methods utilize the Generative Adversarial Networks (GANs) and generate the uncompressed images from textual description. However, in practice, most of the visual data are processed and transmitted in the compressed representation. Hence, the proposed work attempts to generate the visual data directly in the compressed representation form using Deep Convolutional GANs (DCGANs) to achieve the storage and computational efficiency. We propose GAN models for compressed image generation from text. The first model is directly trained with JPEG compressed DCT images (compressed domain) to generate the compressed images from text descriptions. The second model is trained with RGB images (pixel domain) to generate JPEG compressed DCT representation from text descriptions. The proposed models are tested on an open source benchmark datasetOxford-102 Flower images using both RGB and JPEG compressed versions, and accomplished the state-of-the-art performance in the JPEG compressed domain. The code will be publicly released at GitHub after acceptance of paper.△ Less"
The Nuclear Physics of Neutron Stars,Authors:J. Piekarewicz,"Abstract:Neutron stars -- compact objects with masses similar to that of our Sun but radii comparable to the size of a city -- contain the densest form of matter in the universe that can be probed in terrestrial laboratories as well as in earth- and space-based observatories. The historical detection of gravitational waves from a binary neutron star merger has opened the brand new era of multimessenger ast…▽ MoreNeutron stars -- compact objects with masses similar to that of our Sun but radii comparable to the size of a city -- contain the densest form of matter in the universe that can be probed in terrestrial laboratories as well as in earth- and space-based observatories. The historical detection of gravitational waves from a binary neutron star merger has opened the brand new era of multimessenger astronomy and has propelled neutron stars to the center of a variety of disciplines, such as astrophysics, general relativity, nuclear physics, and particle physics. The main input required to study the structure of neutron stars is the pressure support generated by its constituents against gravitational collapse. These include neutrons, protons, electrons, and perhaps even more exotic constituents. As such, nuclear physics plays a prominent role in elucidating the fascinating structure, dynamics, and composition of neutron stars.△ Less"
Learning Deep Representations via Contrastive Learning for Instance Retrieval,"Authors:Tao Wu,Tie Luo,Donald Wunsch","Abstract:…to IIR. Our empirical evaluation demonstrates significant performance enhancement over the off-the-shelf features learned from a pre-trained DNN classifier on the challengingOxfordand Paris datasets.▽ MoreInstance-level Image Retrieval (IIR), or simply Instance Retrieval, deals with the problem of finding all the images within an dataset that contain a query instance (e.g. an object). This paper makes the first attempt that tackles this problem using instance-discrimination based contrastive learning (CL). While CL has shown impressive performance for many computer vision tasks, the similar success has never been found in the field of IIR. In this work, we approach this problem by exploring the capability of deriving discriminative representations from pre-trained and fine-tuned CL models. To begin with, we investigate the efficacy of transfer learning in IIR, by comparing off-the-shelf features learned by a pre-trained deep neural network (DNN) classifier with features learned by a CL model. The findings inspired us to propose a new training strategy that optimizes CL towards learning IIR-oriented features, by using an Average Precision (AP) loss together with a fine-tuning method to learn contrastive feature representations that are tailored to IIR. Our empirical evaluation demonstrates significant performance enhancement over the off-the-shelf features learned from a pre-trained DNN classifier on the challengingOxfordand Paris datasets.△ Less"
A detailed star formation history for the extremely diffuse Andromeda XIX dwarf galaxy,"Authors:Michelle L. M. Collins,Benjamin F. Williams,Erik J. Tollerud,Eduardo Balbinot,Karoline M. Gilbert,Andrew Dolphin","Abstract:We present deep imaging of the ultra-diffuse Andromeda XIX dwarf galaxy from the Advance Camera for Surveys on the Hubble Space Telescope which resolves its stellar populations to below the oldest main sequence turn-off. We derive a full star formation history for the galaxy using MATCH, and find no evidence of star formation in the past 8 Gyr. We calculate a quenching time of $τ_{90}=9.7\pm0.2$~G…▽ MoreWe present deep imaging of the ultra-diffuse Andromeda XIX dwarf galaxy from the Advance Camera for Surveys on the Hubble Space Telescope which resolves its stellar populations to below the oldest main sequence turn-off. We derive a full star formation history for the galaxy using MATCH, and find no evidence of star formation in the past 8 Gyr. We calculate a quenching time of $τ_{90}=9.7\pm0.2$~Gyr, suggesting Andromeda~XIX ceased forming stars very early on. This early quenching, combined with its extremely large half-light radius, low density dark matter halo and lower than expected metallicity make it a unique galaxy within the Local Group and raises questions about how it formed. The early quenching time allows us to rule out feedback from bursty star formation as a means to explain its diffuse stellar population and low density dark matter halo. We find that the extended stellar population, low density halo and star formation could be explained by either tidal interactions (such as tidal shocking) or by late dry mergers, with the latter also explaining its low metallicity. Proper motions and detailed abundances would allow us to distinguish between these two scenarios.△ Less"
Measurement of decay-time dependent $CP$ violation in $B^0 \rightarrow K^0_S K^0_S K^0_S$ using 2019--2021 Belle II data,"Authors:Belle II Collaboration,F. Abudinén,I. Adachi,K. Adamczyk,L. Aggarwal,P. Ahlburg,H. Ahmed,J. K. Ahn,H. Aihara,N. Akopov,A. Aloisio,F. Ameli,L. Andricek,N. Anh Ky,D. M. Asner,H. Atmacan,V. Aulchenko,T. Aushev,V. Aushev,T. Aziz,V. Babu,S. Bacher,H. Bae,S. Baehr,S. Bahinipati, et al. (570 additional authors not shown)","Abstract:We report a measurement of decay-time dependent $CP$-violating parameters in $B^0 \rightarrow K^0_S K^0_S K^0_S$ decays. We use $(198.0 \pm 3.0) \times 10^6\ B\overline{B}$ pairs collected at the $Υ(4S)$ resonance with the Belle II detector at the SuperKEKB asymmetric-energy $e^+e^-$ collider. The observed mixing-induced and direct $CP$ violation parameters are…▽ MoreWe report a measurement of decay-time dependent $CP$-violating parameters in $B^0 \rightarrow K^0_S K^0_S K^0_S$ decays. We use $(198.0 \pm 3.0) \times 10^6\ B\overline{B}$ pairs collected at the $Υ(4S)$ resonance with the Belle II detector at the SuperKEKB asymmetric-energy $e^+e^-$ collider. The observed mixing-induced and direct $CP$ violation parameters are $\mathcal{S} = -1.86\ _{-0.46}^{+0.91}~{\rm (stat)} \pm 0.09~{\rm (syst)}$ and $\mathcal{A} = -0.22\ _{-0.27}^{+0.30}~{\rm (stat)} \pm 0.04~{\rm (syst)}$, respectively.△ Less"
"Ensembles of Compact, Region-specific & Regularized Spiking Neural Networks for Scalable Place Recognition","Authors:Somayeh Hussaini,Michael Milford,Tobias Fischer","Abstract:…problematic hyperactive neurons during the initial environmental learning phase. We evaluate this new scalable modular system on benchmark localization datasets Nordland andOxfordRobotCar, with comparisons to standard techniques NetVLAD, DenseVLAD, and SAD, and a previous spiking neural network system. Our system substantially outperforms the previous SNN…▽ MoreSpiking neural networks have significant potential utility in robotics due to their high energy efficiency on specialized hardware, but proof-of-concept implementations have not yet typically achieved competitive performance or capability with conventional approaches. In this paper, we tackle one of the key practical challenges of scalability by introducing a novel modular ensemble network approach, where compact, localized spiking networks each learn and are solely responsible for recognizing places in a local region of the environment only. This modular approach creates a highly scalable system. However, it comes with a high-performance cost where a lack of global regularization at deployment time leads to hyperactive neurons that erroneously respond to places outside their learned region. Our second contribution introduces a regularization approach that detects and removes these problematic hyperactive neurons during the initial environmental learning phase. We evaluate this new scalable modular system on benchmark localization datasets Nordland andOxfordRobotCar, with comparisons to standard techniques NetVLAD, DenseVLAD, and SAD, and a previous spiking neural network system. Our system substantially outperforms the previous SNN system on its small dataset, but also maintains performance on 27 times larger benchmark datasets where the operation of the previous system is computationally infeasible, and performs competitively with the conventional localization systems.△ Less"
SICRET: Supernova Ia Cosmology with truncated marginal neural Ratio EsTimation,"Authors:Konstantin Karchev,Roberto Trotta,Christoph Weniger","Abstract:Type Ia supernovae (SNae Ia), standardisable candles that allow tracing the expansion history of the Universe, are instrumental in constraining cosmological parameters, particularly dark energy. State-of-the-art likelihood-based analyses scale poorly to future large datasets, are limited to simplified probabilistic descriptions, and must explicitly sample a high-dimensional latent posterior to inf…▽ MoreType Ia supernovae (SNae Ia), standardisable candles that allow tracing the expansion history of the Universe, are instrumental in constraining cosmological parameters, particularly dark energy. State-of-the-art likelihood-based analyses scale poorly to future large datasets, are limited to simplified probabilistic descriptions, and must explicitly sample a high-dimensional latent posterior to infer the few parameters of interest, which makes them inefficient.
  Marginal likelihood-free inference, on the other hand, is based on forward simulations of data, and thus can fully account for complicated redshift uncertainties, contamination from non-SN Ia sources, selection effects, and a realistic instrumental model. All latent parameters, including instrumental and survey-related ones, per-object and population-level properties, are implicitly marginalised, while the cosmological parameters of interest are inferred directly.
  As a proof of concept, we apply truncated marginal neural ratio estimation (TMNRE), a form of marginal likelihood-free inference, to BAHAMAS, a Bayesian hierarchical model for SALT parameters. We verify that TMNRE produces unbiased and precise posteriors for cosmological parameters from up to 100 000 SNae Ia. With minimal additional effort, we train a network to infer simultaneously the O(100 000) latent parameters of the supernovae (e.g. absolute brightnesses). In addition, we describe and apply a procedure that utilises local amortisation of the inference to convert the approximate Bayesian posteriors into frequentist confidence regions with exact coverage. Finally, we discuss the planned improvements to the model that are enabled by using a likelihood-free inference framework, like selection effects and non-Ia contamination.△ Less"
SIPGI: an interactive pipeline for spectroscopic data reduction,"Authors:A. Gargiulo,M. Fumana,S. Bisogni,P. Franzetti,L. P. Cassarà,B. Garilli,M. Scodeggio,G. Vietri","Abstract:We present SIPGI, a spectroscopic pipeline to reduce optical/near-infrared data from slit-based spectrographs. SIPGI is a complete spectroscopic data reduction environment which retains the high level of flexibility and accuracy typical of the standard ""by-hand"" reduction methods but is characterized by a significantly higher level of efficiency. This is obtained by exploiting three main concepts:…▽ MoreWe present SIPGI, a spectroscopic pipeline to reduce optical/near-infrared data from slit-based spectrographs. SIPGI is a complete spectroscopic data reduction environment which retains the high level of flexibility and accuracy typical of the standard ""by-hand"" reduction methods but is characterized by a significantly higher level of efficiency. This is obtained by exploiting three main concepts: $i)$ the instrument model: at the core of the data reduction is an analytic description of the main calibration relations (e.g. spectra location and wavelength calibration) that can be easily checked and adjusted on data using a graphical tool; $ii)$ a built-in data organizer that classifies the data, together with a graphical interface that helps in providing the recipes with the correct input; $iii)$ the design and flexibility of the reduction recipes: the number of tasks required to perform a complete reduction is minimized, while preserving the possibility of verifying the accuracy of the main stages of data-reduction process with provided tools. The current version of SIPGI manages data from the MODS and LUCI spectrographs mounted at the Large Binocular Telescope, and it is our plan to extend SIPGI to support other through-slit spectrographs. Meanwhile, to allow using the same approach based on the instrument model with other instruments, we have developed SpectraPy, a spectrograph independent Python library working on through-slit spectra. In its current version, SpectraPy produces two-dimensional wavelength calibrated spectra corrected by instrument distortions. The current release of SIPGI and its documentation can by downloaded from http://pandora.lambrate.inaf.it/sipgi/, while SpectraPy can be found at http://pandora.lambrate.inaf.it/SpectraPy/.△ Less"
Measurement of the branching fractions and $CP$ asymmetries of $B^+ \rightarrow π^+ π^0$ and $B^+ \rightarrow K^+ π^0$ decays in 2019-2021 Belle II data,"Authors:Belle II Collaboration,F. Abudinén,I. Adachi,K. Adamczyk,L. Aggarwal,P. Ahlburg,H. Ahmed,J. K. Ahn,H. Aihara,N. Akopov,A. Aloisio,F. Ameli,L. Andricek,N. Anh Ky,D. M. Asner,H. Atmacan,V. Aulchenko,T. Aushev,V. Aushev,T. Aziz,V. Babu,H. Bae,S. Baehr,S. Bahinipati,A. M. Bakich, et al. (562 additional authors not shown)","Abstract:We determine the branching fractions ${\mathcal{B}}$ and $CP$ asymmetries ${\mathcal{A}_{\it CP}}$ of the decays $B^+ \rightarrow π^+ π^0$ and $B^+ \rightarrow K^+ π^0$. The results are based on a data set containing 198 million bottom-antibottom meson pairs corresponding to an integrated luminosity of $190\;\text{fb}^{-1}$ recorded by the Belle II detector in energy-asymmetric electron-positron c…▽ MoreWe determine the branching fractions ${\mathcal{B}}$ and $CP$ asymmetries ${\mathcal{A}_{\it CP}}$ of the decays $B^+ \rightarrow π^+ π^0$ and $B^+ \rightarrow K^+ π^0$. The results are based on a data set containing 198 million bottom-antibottom meson pairs corresponding to an integrated luminosity of $190\;\text{fb}^{-1}$ recorded by the Belle II detector in energy-asymmetric electron-positron collisions at the $Υ(4S)$ resonance. We measure ${\mathcal{B}(B^+ \rightarrow π^+ π^0) = (6.12 \pm 0.53 \pm 0.53)\times 10^{-6}}$, ${\mathcal{B}(B^+ \rightarrow K^+ π^0) = (14.30 \pm 0.69 \pm 0.79)\times 10^{-6}}$, ${\mathcal{A}_{\it CP}(B^+ \rightarrow π^+ π^0) = -0.085 \pm 0.085 \pm 0.019}$, and ${\mathcal{A}_{\it CP}(B^+ \rightarrow K^+ π^0) = 0.014 \pm 0.047 \pm 0.010}$, where the first uncertainties are statistical and the second are systematic. These results improve a previous Belle II measurement and agree with the world averages.△ Less"
BON: An extended public domain dataset for human activity recognition,"Authors:Girmaw Abebe Tadesse,Oliver Bent,Komminist Weldemariam,Md. Abrar Istiak,Taufiq Hasan,Andrea Cavallaro","Abstract:…provides details of a large and publicly available office activity dataset (BON) collected in different office settings across three geographical locations: Barcelona (Spain),Oxford(UK) and Nairobi (Kenya), using a chest-mounted GoPro Hero camera. The BON dataset contains eighteen common office activities that can be categorised into person-to-person inter…▽ MoreBody-worn first-person vision (FPV) camera enables to extract a rich source of information on the environment from the subject's viewpoint. However, the research progress in wearable camera-based egocentric office activity understanding is slow compared to other activity environments (e.g., kitchen and outdoor ambulatory), mainly due to the lack of adequate datasets to train more sophisticated (e.g., deep learning) models for human activity recognition in office environments. This paper provides details of a large and publicly available office activity dataset (BON) collected in different office settings across three geographical locations: Barcelona (Spain),Oxford(UK) and Nairobi (Kenya), using a chest-mounted GoPro Hero camera. The BON dataset contains eighteen common office activities that can be categorised into person-to-person interactions (e.g., Chat with colleagues), person-to-object (e.g., Writing on a whiteboard), and proprioceptive (e.g., Walking). Annotation is provided for each segment of video with 5-seconds duration. Generally, BON contains 25 subjects and 2639 total segments. In order to facilitate further research in the sub-domain, we have also provided results that could be used as baselines for future studies.△ Less"
How Descriptive are GMRES Convergence Bounds?,Authors:Mark Embree,"Abstract:GMRES is a popular Krylov subspace method for solving linear systems of equations involving a general non-Hermitian coefficient matrix. The conventional bounds on GMRES convergence involve polynomial approximation problems in the complex plane. Three popular approaches pose this approximation problem on the spectrum, the field of values, or pseudospectra of the coefficient matrix. We analyze and c…▽ MoreGMRES is a popular Krylov subspace method for solving linear systems of equations involving a general non-Hermitian coefficient matrix. The conventional bounds on GMRES convergence involve polynomial approximation problems in the complex plane. Three popular approaches pose this approximation problem on the spectrum, the field of values, or pseudospectra of the coefficient matrix. We analyze and compare these bounds, illustrating with six examples the success and failure of each. When the matrix departs from normality due only to a low-dimensional invariant subspace, we discuss how these bounds can be adapted to exploit this structure. Since the Arnoldi process that underpins GMRES provides approximations to the pseudospectra, one can estimate the GMRES convergence bounds as an iteration proceeds.△ Less"
Signatures of generalized ALP interactions in SM decays of mesons,"Authors:Subhajit Ghosh,Triparno Bandyopadhyay,Tuhin S. Roy","Abstract:In addition to giving rise to spectacular new physics signals in the final states of meson decays, Axion-like-particles also induce modifications in the standard model decays of mesons. These `indirect' signatures can be parametrized as the modifications of the hadronic form factors and can be probed using meson decay width and decay distribution measurements. Starting with a generalized ALP Lagra…▽ MoreIn addition to giving rise to spectacular new physics signals in the final states of meson decays, Axion-like-particles also induce modifications in the standard model decays of mesons. These `indirect' signatures can be parametrized as the modifications of the hadronic form factors and can be probed using meson decay width and decay distribution measurements. Starting with a generalized ALP Lagrangian, we demonstrate these effects for semileptonic Kaon decays and derived bounds using NA48/2 data. We also briefly discuss other indirect signatures such as modification of meson mass spectrum and `sum rules' comprised of meson decay amplitudes which show deviation in presence of ALP.△ Less"
Bayesian Neural Network Language Modeling for Speech Recognition,"Authors:Boyang Xue,Shoukang Hu,Junhao Xu,Mengzhe Geng,Xunying Liu,Helen Meng","Abstract:…These allow the computational costs incurred in Bayesian NNLM training and evaluation to be minimized. Experiments are conducted on two tasks: AMI meeting transcription andOxford-BBC LipReading Sentences 2 (LRS2) overlapped speech recognition using state-of-the-art LF-MMI trained factored TDNN systems featuring data augmentation, speaker adaptation and aud…▽ MoreState-of-the-art neural network language models (NNLMs) represented by long short term memory recurrent neural networks (LSTM-RNNs) and Transformers are becoming highly complex. They are prone to overfitting and poor generalization when given limited training data. To this end, an overarching full Bayesian learning framework encompassing three methods is proposed in this paper to account for the underlying uncertainty in LSTM-RNN and Transformer LMs. The uncertainty over their model parameters, choice of neural activations and hidden output representations are modeled using Bayesian, Gaussian Process and variational LSTM-RNN or Transformer LMs respectively. Efficient inference approaches were used to automatically select the optimal network internal components to be Bayesian learned using neural architecture search. A minimal number of Monte Carlo parameter samples as low as one was also used. These allow the computational costs incurred in Bayesian NNLM training and evaluation to be minimized. Experiments are conducted on two tasks: AMI meeting transcription andOxford-BBC LipReading Sentences 2 (LRS2) overlapped speech recognition using state-of-the-art LF-MMI trained factored TDNN systems featuring data augmentation, speaker adaptation and audio-visual multi-channel beamforming for overlapped speech. Consistent performance improvements over the baseline LSTM-RNN and Transformer LMs with point estimated model parameters and drop-out regularization were obtained across both tasks in terms of perplexity and word error rate (WER). In particular, on the LRS2 data, statistically significant WER reductions up to 1.3% and 1.2% absolute (12.1% and 11.3% relative) were obtained over the baseline LSTM-RNN and Transformer LMs respectively after model combination between Bayesian NNLMs and their respective baselines.△ Less"
Infinitesimally helicoidal motions with fixed pitch of oriented geodesics of a space form,"Authors:Mateo Anarella,Marcos Salvai","Abstract:…spherical case with alpha = (+/-) 1, an admissible curve remains in the set of fibers of a fixed Hopf fibration of S^3.
  We also address and solve a sort of Kendall's (akaOxford) problem in this setting: Finding the minimum number of switches of piecewise continuous curves joininig two arbitrary oriented lines, with pieces in some distinguished familie…▽ MoreLet L be the manifold of all (unparametrized) oriented lines of R^3. We study the controllability of the control system in L given by the condition that a curve in L describes at each instant, at the infinitesimal level, an helicoid with prescribed angular speed alpha. Actually, we pose the analogous more general problem by means of a control system on the manifold G_kappa of all the oriented complete geodesics of the three dimensional space form of curvature kappa: R^3 for kappa = 0, S^3 for kappa = 1 and hyperbolic 3-space for kappa = -1. We obtain that the system is controllable if and only if alpha ^2 not equal kappa. In the spherical case with alpha = (+/-) 1, an admissible curve remains in the set of fibers of a fixed Hopf fibration of S^3.
  We also address and solve a sort of Kendall's (akaOxford) problem in this setting: Finding the minimum number of switches of piecewise continuous curves joininig two arbitrary oriented lines, with pieces in some distinguished families of admissible curves.△ Less"
(3200) Phaethon Polarimetry in the Negative Branch: New Evidence for the Anhydrous Nature of the DESTINY+ Target Asteroid,"Authors:Jooyeon Geem,Masateru Ishiguro,Jun Takahashi,Hiroshi Akitaya,Koji S. Kawabata,Tatsuya Nakaoka,Ryo Imazawa,Fumiki Mori,Sunho Jin,Yoonsoo P. Bach,Hangbin Jo,Daisuke Kuroda,Sunao Hasegawa,Fumi Yoshida,Ko Ishibashi,Tomohiko Sekiguchi,Jin Beniyama,Tomoko Arai,Yuji Ikeda,Yoshiharu Shinnaka,Mikael Granvik,Lauri Siltala,Anlaug A. Djupvik,Anni Kasikov,Viktoria Pinter, et al. (1 additional authors not shown)","Abstract:We report on the first polarimetric study of (3200) Phaethon, the target of JAXA's DESTINY$^+$ mission, in the negative branch to ensure its anhydrous nature and to derive an accurate geometric albedo. We conducted observations at low phase angles (Sun-target-observer angle, alpha = 8.8-32.4 deg) from 2021 October to 2022 January and found that Phaethon has a minimum polarization degree $P_{min}$…▽ MoreWe report on the first polarimetric study of (3200) Phaethon, the target of JAXA's DESTINY$^+$ mission, in the negative branch to ensure its anhydrous nature and to derive an accurate geometric albedo. We conducted observations at low phase angles (Sun-target-observer angle, alpha = 8.8-32.4 deg) from 2021 October to 2022 January and found that Phaethon has a minimum polarization degree $P_{min}$ = -1.3 +- 0.1 %, a polarimetric slope h = 0.22 +- 0.02 % deg$^{-1}$, and an inversion angle alpha$_0$ = 19.9 +- 0.3 deg. The derived geometric albedo is $p_V$ = 0.11 (in the range of 0.08-0.13). These polarimetric properties are consistent with anhydrous chondrites, and contradict hydrous chondrites and typical cometary nuclei.△ Less"
Relative Facts of Relational Quantum Mechanics are Incompatible with Quantum Mechanics,"Authors:Jay Lawrence,Marcin Markiewicz,Marek Żukowski","Abstract:Relational Quantum Mechanics (RQM) claims to be an interpretation of quantum theory [see arXiv:2109.09170, which appears in theOxfordHandbook of the History of Interpretation of Quantum Physics]. However, there are significant departures from quantum theory: (i) in RQM measurement outcomes arise from interactions which entangle a system $S$ and an observer…▽ MoreRelational Quantum Mechanics (RQM) claims to be an interpretation of quantum theory [see arXiv:2109.09170, which appears in theOxfordHandbook of the History of Interpretation of Quantum Physics]. However, there are significant departures from quantum theory: (i) in RQM measurement outcomes arise from interactions which entangle a system $S$ and an observer $A$ without decoherence, and (ii) such an outcome is a ""fact"" relative to the observer $A$, but it is not a fact relative to another observer $B$ who has not interacted with $S$ or $A$ during the foregoing measurement process. For $B$ the system $S \otimes A$ remains entangled. We derive a GHZ-like contradiction showing that relative facts described by these statements are incompatible with quantum theory. Hence Relational Quantum Mechanics should not be considered an interpretation of quantum theory, according to a criterion for interpretations that we have introduced. The criterion states that whenever an interpretation introduces a notion of outcomes, these outcomes, whatever they are, must follow the probability distribution specified by the Born rule.△ Less"
Constraint-driven multi-task learning,"Authors:Bogdan Cretu,Andrew Cropper","Abstract:Inductive logic programming is a form of machine learning based on mathematical logic that generates logic programs from given examples and background knowledge.
  In this project, we extend the Popper ILP system to make use of multi-task learning. We implement the state-of-the-art approach and several new strategies to improve search performance. Furthermore, we introduce constraint preservation,…▽ MoreInductive logic programming is a form of machine learning based on mathematical logic that generates logic programs from given examples and background knowledge.
  In this project, we extend the Popper ILP system to make use of multi-task learning. We implement the state-of-the-art approach and several new strategies to improve search performance. Furthermore, we introduce constraint preservation, a technique that improves overall performance for all approaches.
  Constraint preservation allows the system to transfer knowledge between updates on the background knowledge set. Consequently, we reduce the amount of repeated work performed by the system. Additionally, constraint preservation allows us to transition from the current state-of-the-art iterative deepening search approach to a more efficient breadth first search approach.
  Finally, we experiment with curriculum learning techniques and show their potential benefit to the field.△ Less"
Hilti-OxfordDataset: A Millimetre-Accurate Benchmark for Simultaneous Localization and Mapping,"Authors:Lintong Zhang,Michael Helmberger,Lanke Frank Tarimo Fu,David Wisth,Marco Camurri,Davide Scaramuzza,Maurice Fallon","Abstract:…A key necessity in progressing SLAM research is the availability of high-quality datasets and fair and transparent benchmarking. To this end, we have created the Hilti-OxfordDataset, to push state-of-the-art SLAM systems to their limits. The dataset has a variety of challenges ranging from sparse and regular construction sites to a 17th century neoclassica…▽ MoreSimultaneous Localization and Mapping (SLAM) is being deployed in real-world applications, however many state-of-the-art solutions still struggle in many common scenarios. A key necessity in progressing SLAM research is the availability of high-quality datasets and fair and transparent benchmarking. To this end, we have created the Hilti-OxfordDataset, to push state-of-the-art SLAM systems to their limits. The dataset has a variety of challenges ranging from sparse and regular construction sites to a 17th century neoclassical building with fine details and curved surfaces. To encourage multi-modal SLAM approaches, we designed a data collection platform featuring a lidar, five cameras, and an IMU (Inertial Measurement Unit). With the goal of benchmarking SLAM algorithms for tasks where accuracy and robustness are paramount, we implemented a novel ground truth collection method that enables our dataset to accurately measure SLAM pose errors with millimeter accuracy. To further ensure accuracy, the extrinsics of our platform were verified with a micrometer-accurate scanner, and temporal calibration was managed online using hardware time synchronization. The multi-modality and diversity of our dataset attracted a large field of academic and industrial researchers to enter the second edition of the Hilti SLAM challenge, which concluded in June 2022. The results of the challenge show that while the top three teams could achieve an accuracy of 2cm or better for some sequences, the performance dropped off in more difficult sequences.△ Less"
JVLDLoc: a Joint Optimization of Visual-LiDAR Constraints and Direction Priors for Localization in Driving Scenario,"Authors:Longrui Dong,Gang Zeng","Abstract:…Moving Least Square(IMLS) surface of scan constraints, and solves them jointly along with direction projection error at global optimization. Experiments on KITTI, KITTI-360 andOxfordRadar Robotcar show that we achieve lower localization error or Absolute Pose Error (APE) than prior map, which validates our method is effective.▽ MoreThe ability for a moving agent to localize itself in environment is the basic demand for emerging applications, such as autonomous driving, etc. Many existing methods based on multiple sensors still suffer from drift. We propose a scheme that fuses map prior and vanishing points from images, which can establish an energy term that is only constrained on rotation, called the direction projection error. Then we embed these direction priors into a visual-LiDAR SLAM system that integrates camera and LiDAR measurements in a tightly-coupled way at backend. Specifically, our method generates visual reprojection error and point to Implicit Moving Least Square(IMLS) surface of scan constraints, and solves them jointly along with direction projection error at global optimization. Experiments on KITTI, KITTI-360 andOxfordRadar Robotcar show that we achieve lower localization error or Absolute Pose Error (APE) than prior map, which validates our method is effective.△ Less"
Measurement of the $Ω_c^0$ lifetime at Belle II,"Authors:Belle II Collaboration,F. Abudinén,I. Adachi,L. Aggarwal,H. Ahmed,H. Aihara,N. Akopov,A. Aloisio,N. Anh Ky,T. Aushev,V. Aushev,H. Bae,P. Bambade,Sw. Banerjee,J. Baudot,M. Bauer,A. Beaubien,J. Becker,P. K. Behera,J. V. Bennett,E. Bernieri,F. U. Bernlochner,V. Bertacchi,M. Bertemes,E. Bertholet, et al. (331 additional authors not shown)","Abstract:We report on a measurement of the $Ω_c^0$ lifetime using $Ω_c^0 \to Ω^-π^+$ decays reconstructed in $e^+e^-\to c\bar{c}$ data collected by the Belle II experiment and corresponding to $207~{\rm fb^{-1}}$ of integrated luminosity. The result, $\rmτ(Ω_c^0)=243\pm48( stat)\pm11(syst)~fs$, agrees with recent measurements indicating that the $Ω_c^0$ is not the shortest-lived weakly decaying charmed bar…▽ MoreWe report on a measurement of the $Ω_c^0$ lifetime using $Ω_c^0 \to Ω^-π^+$ decays reconstructed in $e^+e^-\to c\bar{c}$ data collected by the Belle II experiment and corresponding to $207~{\rm fb^{-1}}$ of integrated luminosity. The result, $\rmτ(Ω_c^0)=243\pm48( stat)\pm11(syst)~fs$, agrees with recent measurements indicating that the $Ω_c^0$ is not the shortest-lived weakly decaying charmed baryon.△ Less"
Visual Cross-View Metric Localization with Dense Uncertainty Estimates,"Authors:Zimin Xia,Olaf Booij,Marco Manfredi,Julian F. P. Kooij","Abstract:…against a state-of-the-art regression baseline that uses global image descriptors. Quantitative and qualitative experimental results on the recently proposed VIGOR and theOxfordRobotCar datasets validate our design. The produced probabilities are correlated with localization accuracy, and can even be used to roughly estimate the ground camera's heading…▽ MoreThis work addresses visual cross-view metric localization for outdoor robotics. Given a ground-level color image and a satellite patch that contains the local surroundings, the task is to identify the location of the ground camera within the satellite patch. Related work addressed this task for range-sensors (LiDAR, Radar), but for vision, only as a secondary regression step after an initial cross-view image retrieval step. Since the local satellite patch could also be retrieved through any rough localization prior (e.g. from GPS/GNSS, temporal filtering), we drop the image retrieval objective and focus on the metric localization only. We devise a novel network architecture with denser satellite descriptors, similarity matching at the bottleneck (rather than at the output as in image retrieval), and a dense spatial distribution as output to capture multi-modal localization ambiguities. We compare against a state-of-the-art regression baseline that uses global image descriptors. Quantitative and qualitative experimental results on the recently proposed VIGOR and theOxfordRobotCar datasets validate our design. The produced probabilities are correlated with localization accuracy, and can even be used to roughly estimate the ground camera's heading when its orientation is unknown. Overall, our method reduces the median metric localization error by 51%, 37%, and 28% compared to the state-of-the-art when generalizing respectively in the same area, across areas, and across time.△ Less"
FCNC in Concurrent Dark Photon and Dark $Z$ Models,Authors:Lopamudra Mukherjee,"Abstract:In this work we fit the available binned data of the differential decay distribution of the exclusive $B \to K^{(*)} \ell^+ \ell^-$ and $B_s \to φμ^+ μ^-$ decays to the mass and mixing parameters of a light dark vector boson model. Due to an incorrect assessment of the dominant contribution of the dark $Z$ model to the FCNC B-meson decays, a previous work in literature reported that…▽ MoreIn this work we fit the available binned data of the differential decay distribution of the exclusive $B \to K^{(*)} \ell^+ \ell^-$ and $B_s \to φμ^+ μ^-$ decays to the mass and mixing parameters of a light dark vector boson model. Due to an incorrect assessment of the dominant contribution of the dark $Z$ model to the FCNC B-meson decays, a previous work in literature reported that $\mathcal{O}(1)$ mixings were allowed by the data. In this talk we report the correct calculations and constraints on the mixing parameters as well as the mass of the dark vector boson using the marginalization technique. We also study other relevant bounds on the parameter space from low energy experiments such as the atomic parity violation, $K^+ \to μ^+ + invisible$ decay, $B_s - \overline{B}_s$ mixing etc and find that inspite of obtaining a good fit to the experimental $b \to s \ell^+ \ell^-$ data, the entire parameter space gets ruled out from some of the above bounds. In case of a model with tiny mixing and additional interaction of the dark $Z$ to the muon, some bounds are relaxed while some others are violated.△ Less"
Theoretical Review of Rare B Decays,Authors:F. Mahmoudi,"Abstract:We present an overview of the recent neutral current $B$ decays focusing on the deviations with respect to the Standard Model predictions that are observed in $b \to s \ell^+\ell^-$ transitions, and discuss their implications for new physics scenarios in a model-independent way by means of global statistical fits. The prospects for future discoveries in the individual channels, in particular using…▽ MoreWe present an overview of the recent neutral current $B$ decays focusing on the deviations with respect to the Standard Model predictions that are observed in $b \to s \ell^+\ell^-$ transitions, and discuss their implications for new physics scenarios in a model-independent way by means of global statistical fits. The prospects for future discoveries in the individual channels, in particular using the theoretically clean observables are also addressed.△ Less"
Separable Shape Tensors for Aerodynamic Design,"Authors:Zachary Grey,Olga Doronina,Andrew Glaws","Abstract:Airfoil shape design is a classical problem in engineering and manufacturing. In this work, we combine principled physics-based considerations for the shape design problem with modern computational techniques using a data-driven approach. Modern and traditional analyses of 2D and 3D aerodynamic shapes reveal a flow-based sensitivity to specific deformations that can be represented generally by aff…▽ MoreAirfoil shape design is a classical problem in engineering and manufacturing. In this work, we combine principled physics-based considerations for the shape design problem with modern computational techniques using a data-driven approach. Modern and traditional analyses of 2D and 3D aerodynamic shapes reveal a flow-based sensitivity to specific deformations that can be represented generally by affine transformations (rotation, scaling, shearing, translation). We present a novel representation of shapes that decouples affine-style deformations over a submanifold and a product submanifold principally of the Grassmannian. As an analytic generative model, the separable representation, informed by a database of physically relevant airfoils, offers (i) a rich set of novel 2D airfoil deformations not previously captured in the data, (ii) an improved low-dimensional parameter domain for inferential statistics informing design/manufacturing, and (iii) consistent 3D blade representation and perturbation over a sequence of nominal 2D shapes.△ Less"
Potterian Economics,"Authors:Daniel Levy,Avichai Snir","Abstract:Recent studies in psychology and neuroscience offer systematic evidence that fictional works exert a surprisingly strong influence on readers and have the power to shape their opinions and worldviews. Building on these findings, we study what we term Potterian economics, the economic ideas, insights, and structure, found in Harry Potter books, to assess how the books might affect economic literacy…▽ MoreRecent studies in psychology and neuroscience offer systematic evidence that fictional works exert a surprisingly strong influence on readers and have the power to shape their opinions and worldviews. Building on these findings, we study what we term Potterian economics, the economic ideas, insights, and structure, found in Harry Potter books, to assess how the books might affect economic literacy. A conservative estimate suggests that more than 7.3 percent of the world population has read the Harry Potter books, and millions more have seen their movie adaptations. These extraordinary figures underscore the importance of the messages the books convey. We explore the Potterian economic model and compare it to professional economic models to assess the consistency of the Potterian economic principles with the existing economic models. We find that some of the principles of Potterian economics are consistent with economists models. Many other principles, however, are distorted and contain numerous inaccuracies, contradicting professional economists views and insights. We conclude that Potterian economics can teach us about the formation and dissemination of folk economics, the intuitive notions of naive individuals who see market transactions as a zero-sum game, who care about distribution but fail to understand incentives and efficiency, and who think of prices as allocating wealth but not resources or their efficient use.△ Less"
Measurement of Branching Fraction and Longitudinal Polarization in $B^0 \to ρ^+ ρ^-$ Decays at Belle II,"Authors:Belle II Collaboration,F. Abudinén,I. Adachi,K. Adamczyk,L. Aggarwal,P. Ahlburg,H. Ahmed,J. K. Ahn,H. Aihara,N. Akopov,A. Aloisio,F. Ameli,L. Andricek,N. Anh Ky,D. M. Asner,H. Atmacan,V. Aulchenko,T. Aushev,V. Aushev,T. Aziz,V. Babu,H. Bae,S. Baehr,S. Bahinipati,A. M. Bakich, et al. (564 additional authors not shown)","Abstract:We present a measurement of the branching fraction and longitudinal polarization of $B^0 \to ρ^+ ρ^-$ decays. SuperKEKB electron-positron collision data corresponding to 189~fb$^{-1}$ of integrated luminosity and containing $198 \times 10^6 B\bar{B}$ pairs collected with the Belle II detector are used. We obtain \begin{eqnarray*}
  \mathcal{B}(B^0\toρ^+ρ^-) &=& [2.67\pm0.28\,(\mathrm{stat})\,\pm0.…▽ MoreWe present a measurement of the branching fraction and longitudinal polarization of $B^0 \to ρ^+ ρ^-$ decays. SuperKEKB electron-positron collision data corresponding to 189~fb$^{-1}$ of integrated luminosity and containing $198 \times 10^6 B\bar{B}$ pairs collected with the Belle II detector are used. We obtain \begin{eqnarray*}
  \mathcal{B}(B^0\toρ^+ρ^-) &=& [2.67\pm0.28\,(\mathrm{stat})\,\pm0.28\,(\mathrm{syst})] \times 10^{-5}, \end{eqnarray*} \begin{eqnarray*}
  f_L &=& 0.956\pm0.035\,(\mathrm{stat})\,\pm 0.033\,(\mathrm{syst}), \end{eqnarray*} These results are consistent with previous measurements and can be used to constrain penguin pollution and to extract the quark-mixing angle $φ_2$.△ Less"
CP Violation in Charmed and Bottom Baryon Physics,Authors:John M Yelton,"Abstract:This is an experimentalist's view of the recent results on, and prospects for, CP Violation in charmed and bottom baryonsThis is an experimentalist's view of the recent results on, and prospects for, CP Violation in charmed and bottom baryons△ Less"
Dominant Eigenvalue-Eigenvector Pair Estimation via Graph Infection,"Authors:Kaiyuan Yang,Li Xia,Y. C. Tay","Abstract:We present a novel method to estimate the dominant eigenvalue and eigenvector pair of any non-negative real matrix via graph infection. The key idea in our technique lies in approximating the solution to the first-order matrix ordinary differential equation (ODE) with the Euler method. Graphs, which can be weighted, directed, and with loops, are first converted to its adjacency matrix A. Then by a…▽ MoreWe present a novel method to estimate the dominant eigenvalue and eigenvector pair of any non-negative real matrix via graph infection. The key idea in our technique lies in approximating the solution to the first-order matrix ordinary differential equation (ODE) with the Euler method. Graphs, which can be weighted, directed, and with loops, are first converted to its adjacency matrix A. Then by a naive infection model for graphs, we establish the corresponding first-order matrix ODE, through which A's dominant eigenvalue is revealed by the fastest growing term. When there are multiple dominant eigenvalues of the same magnitude, the classical power iteration method can fail. In contrast, our method can converge to the dominant eigenvalue even when same-magnitude counterparts exist, be it complex or opposite in sign. We conduct several experiments comparing the convergence between our method and power iteration. Our results show clear advantages over power iteration for tree graphs, bipartite graphs, directed graphs with periods, and Markov chains with spider-traps. To our knowledge, this is the first work that estimates dominant eigenvalue and eigenvector pair from the perspective of a dynamical system and matrix ODE. We believe our method can be adopted as an alternative to power iteration, especially for graphs.△ Less"
Photonic quantum Hall effects,"Authors:Daniel Leykam,Daria Smirnova","Abstract:This article reviews the development of photonic analogues of quantum Hall effects, which have given rise to broad interest in topological phenomena in photonic systems over the past decade. We cover early investigations of geometric phases, analogies between electronic systems and the spectra of periodic photonic media including photonic crystals, efforts to generalize topological band theory to…▽ MoreThis article reviews the development of photonic analogues of quantum Hall effects, which have given rise to broad interest in topological phenomena in photonic systems over the past decade. We cover early investigations of geometric phases, analogies between electronic systems and the spectra of periodic photonic media including photonic crystals, efforts to generalize topological band theory to open, dissipative, and nonlinear wave systems, pursuit of useful device applications, and ongoing studies of photonic Hall effects in classical nonlinear optics and the quantum regime of strong photon-photon interactions.△ Less"
Energy Demand Unawareness and the Popularity of Bitcoin: Evidence from Nigeria,"Authors:Moritz Platt,Stephen Ojeka,Andreea-Elena Drăgnoiu,Oserere Ejemen Ibelegbu,Francesco Pierangeli,Johannes Sedlmeir,Zixin Wang","Abstract:Decentralized cryptocurrency networks, notably those with high energy demand, have faced significant criticism and subsequent regulatory scrutiny. Despite these concerns, policy interventions targeting cryptocurrency operations in the pursuit of sustainability have largely been ineffective. Some were abandoned for fear of jeopardizing innovation, whereas others failed due to the highly globalized…▽ MoreDecentralized cryptocurrency networks, notably those with high energy demand, have faced significant criticism and subsequent regulatory scrutiny. Despite these concerns, policy interventions targeting cryptocurrency operations in the pursuit of sustainability have largely been ineffective. Some were abandoned for fear of jeopardizing innovation, whereas others failed due to the highly globalized nature of blockchain systems. In search of a more effective angle for energy policy measures, this study adopts a consumer-centric perspective, examining the sentiments of Nigerian cryptocurrency users (${n=158}$) toward Bitcoin's sustainability, a representative cryptocurrency known for its high electricity demand. Three main findings emerged: 1) Even among those self-identifying as highly knowledgeable, most considerably underestimated Bitcoin's electricity consumption. 2) Participants with a more accurate understanding of Bitcoin's energy demand were more inclined to support sustainability measures. 3) Most of this supportive cohort viewed private entities as the primary stakeholders for implementing such measures. Given these findings, we suggest that consumer education should be at the forefront of policy initiatives aimed at cryptocurrency sustainability.△ Less"
Massive Right-handed Neutrinos in B Decays,Authors:Hongkai Liu,"Abstract:In this paper, we present the differential decay distributions for $\bar B \to D^{(*)} \ell \bar{X}$ decays with a massive right-handed neutrino in the low-energy effective field theory framework and show how the massive effects of the RH neutrinos can explain the positive value of the difference in forward-backward asymmetries, $ΔA_{\text{FB}}\equiv A_{\text{FB}}^μ-A_{\text{FB}}^e$, tentatively i…▽ MoreIn this paper, we present the differential decay distributions for $\bar B \to D^{(*)} \ell \bar{X}$ decays with a massive right-handed neutrino in the low-energy effective field theory framework and show how the massive effects of the RH neutrinos can explain the positive value of the difference in forward-backward asymmetries, $ΔA_{\text{FB}}\equiv A_{\text{FB}}^μ-A_{\text{FB}}^e$, tentatively inferred from Belle data. We also make predictions for $q^2$ dependent angular observables to motivate future measurements.△ Less"
CDF Measurement of $M_W$: Theory implications,Authors:S. Heinemeyer,"Abstract:The CDF collaboration recently reported a measurement of the $W$-bosos mass, $M_W$, showing a large positive deviation from the Standard Model (SM) prediction. The question arises whether extensions of the SM exist that can accommodate such large values, and what further phenomenological consequences arise from this. We give a brief review of the implications of the new CDF measurement on the SM,…▽ MoreThe CDF collaboration recently reported a measurement of the $W$-bosos mass, $M_W$, showing a large positive deviation from the Standard Model (SM) prediction. The question arises whether extensions of the SM exist that can accommodate such large values, and what further phenomenological consequences arise from this. We give a brief review of the implications of the new CDF measurement on the SM, as well as on Higgs-sector extensions. In particular, we review the compatibility of the $M_W$ measurement of CDF with excesses observed in the light Higgs-boson searches at $\sim 95$ GeV, as well as with the Minimal Supersymmetric Standard Model in conjunction with the anomalous magnetic moment of the muon, $(g-2)_μ$.△ Less"
Rare Charm Decays,Authors:M. Destefanis,"Abstract:Although the Standard Model has been firmly established, the search for physics beyond the SM is ongoing by investigating new experimental probes. Rare charm decays are a unique tool to access New Physics studies. The high luminosity achieved by the modern experiments and their high precision allow for rare charm decay studies in different scenarios. In this work, some of the most recent experimen…▽ MoreAlthough the Standard Model has been firmly established, the search for physics beyond the SM is ongoing by investigating new experimental probes. Rare charm decays are a unique tool to access New Physics studies. The high luminosity achieved by the modern experiments and their high precision allow for rare charm decay studies in different scenarios. In this work, some of the most recent experimental results will be discussed in detail.△ Less"
Anomalies in charged-current $B$ decays,Authors:Ryoutaro Watanabe,Abstract:This paper reviews the recent progresses of the flavor and collider searches that can probe New Physics effects responsible for the current discrepancy in the lepton flavor universality ratio of $R_{D^{(*)}}$ between the experimental measurements and SM values.This paper reviews the recent progresses of the flavor and collider searches that can probe New Physics effects responsible for the current discrepancy in the lepton flavor universality ratio of $R_{D^{(*)}}$ between the experimental measurements and SM values.△ Less
The $B$ Anomalies and non-SMEFT New Physics,Authors:David London,"Abstract:The modern viewpoint is that the Standard Model is the leading part of an effective field theory that obeys the symmetry $SU(3)_C \times SU(2)_L \times U(1)_Y$. Since the discovery of the Higgs boson, it is generally assumed that this symmetry is realized linearly (SMEFT), but a nonlinear realization (e.g., HEFT) is still possible. The two differ in their predictions for the size of certain low-en…▽ MoreThe modern viewpoint is that the Standard Model is the leading part of an effective field theory that obeys the symmetry $SU(3)_C \times SU(2)_L \times U(1)_Y$. Since the discovery of the Higgs boson, it is generally assumed that this symmetry is realized linearly (SMEFT), but a nonlinear realization (e.g., HEFT) is still possible. The two differ in their predictions for the size of certain low-energy dimension-6 four-fermion operators: for these, HEFT allows $O(1)$ couplings, while in SMEFT they are suppressed by a factor $v^2/Λ_{\rm NP}^2$, where $v$ is the Higgs vev. In this talk, I argue that (i) such non-SMEFT operators contribute to both $b \to s \ell^+ \ell^-$ and $b \to c \,τ^- {\barν}_τ$, transitions involved in the present-day $B$ anomalies, (ii) the contributions to $b \to s \ell^+ \ell^-$ are constrained to be small, at the SMEFT level, and (iii) the contribution to $b \to c \,τ^- {\barν}_τ$ can be sizeable. I show that the angular distribution in ${\bar B} \to D^* (\to D π') \, τ^{-} (\to π^- ν_τ) {\barν}_τ$ contains enough information to extract the coefficients of all new-physics operators. The measurement of this angular distribution can tell us if non-SMEFT new physics is present.△ Less"
Mind the hubris: complexity can misfire,"Authors:Arnald Puy,Andrea Saltelli","Abstract:Here we briefly reflect on the philosophical foundations that ground the quest towards ever-detailed models and identify four practical dangers derived from this pursuit: explosion of the model's uncertainty space, model black-boxing, computational exhaustion and model attachment. We argue that the growth of a mathematical model should be carefully and continuously pondered lest models become extr…▽ MoreHere we briefly reflect on the philosophical foundations that ground the quest towards ever-detailed models and identify four practical dangers derived from this pursuit: explosion of the model's uncertainty space, model black-boxing, computational exhaustion and model attachment. We argue that the growth of a mathematical model should be carefully and continuously pondered lest models become extraneous constructs chasing the Cartesian dream.△ Less"
"Measurements of the branching fraction, isospin asymmetry, and lepton-universality ratio in $B \to J/ψK$ decays at Belle II","Authors:Belle II Collaboration,F. Abudinén,I. Adachi,R. Adak,K. Adamczyk,L. Aggarwal,P. Ahlburg,H. Ahmed,J. K. Ahn,H. Aihara,N. Akopov,A. Aloisio,F. Ameli,L. Andricek,N. Anh Ky,D. M. Asner,H. Atmacan,V. Aulchenko,T. Aushev,V. Aushev,T. Aziz,V. Babu,S. Bacher,H. Bae,S. Baehr, et al. (570 additional authors not shown)","Abstract:We report a study of $B \to J/ψ(\ell^{+}\ell^{-})K$ decays, where $\ell$ represents an electron or a muon, using $e^{+}e^{-}$ collisions at the $Υ(4S)$ resonance. The data were collected by the Belle II experiment at the SuperKEKB asymmetric-energy collider during 2019-2021, corresponding to an integrated luminosity of $189$ fb$^{-1}$. The measured quantities are the branching fractions (…▽ MoreWe report a study of $B \to J/ψ(\ell^{+}\ell^{-})K$ decays, where $\ell$ represents an electron or a muon, using $e^{+}e^{-}$ collisions at the $Υ(4S)$ resonance. The data were collected by the Belle II experiment at the SuperKEKB asymmetric-energy collider during 2019-2021, corresponding to an integrated luminosity of $189$ fb$^{-1}$. The measured quantities are the branching fractions (${\mathcal B}$) of the decay channels $B^{+} \to J/ψ(e^{+}e^{-})K^{+}$, $B^{+} \to J/ψ(μ^{+}μ^{-}) K^{+}$, $B^{0} \to J/ψ(e^{+}e^{-}) K^{0}_{S}$, and $B^{0} \to J/ψ(μ^{+}μ^{-})K^{0}_{S}$; the lepton-flavor-dependent isospin asymmetries for the electron [$A_{I}\left(B \to J/ψ(e^{+}e^{-}) K\right)$] and muon [$A_{I}\left(B \to J/ψ(μ^{+} μ^{-}) K\right)$] channels; and the ratios of branching fractions between the muon and electron channels for the charged [$R_{K^{+}}\left(J/ψ\right)$] and neutral kaon [$R_{K^{0}}\left(J/ψ\right)$] case. The measurements are consistent with the world-average values.△ Less"
Mimetic Models: Ethical Implications of AI that Acts Like You,"Authors:Reid McIlroy-Young,Jon Kleinberg,Siddhartha Sen,Solon Barocas,Ashton Anderson","Abstract:An emerging theme in artificial intelligence research is the creation of models to simulate the decisions and behavior of specific people, in domains including game-playing, text generation, and artistic expression. These models go beyond earlier approaches in the way they are tailored to individuals, and the way they are designed for interaction rather than simply the reproduction of fixed, pre-c…▽ MoreAn emerging theme in artificial intelligence research is the creation of models to simulate the decisions and behavior of specific people, in domains including game-playing, text generation, and artistic expression. These models go beyond earlier approaches in the way they are tailored to individuals, and the way they are designed for interaction rather than simply the reproduction of fixed, pre-computed behaviors. We refer to these as mimetic models, and in this paper we develop a framework for characterizing the ethical and social issues raised by their growing availability. Our framework includes a number of distinct scenarios for the use of such models, and considers the impacts on a range of different participants, including the target being modeled, the operator who deploys the model, and the entities that interact with it.△ Less"
Seven Useful Questions in Density Functional Theory,"Authors:Steven Crisostomo,Ryan Pederson,John Kozlowski,Bhupalee Kalita,Antonio C. Cancio,Kiril Datchev,Adam Wasserman,Suhwan Song,Kieron Burke","Abstract:…the shape of adiabatic connection curves, using the constrained search with input densities, densities of states, the semiclassical expansion of energies, the tightness of Lieb-Oxfordbounds, and how we decide the accuracy of an approximate density.▽ MoreWe explore a variety of unsolved problems in density functional theory, where mathematicians might prove useful. We give the background and context of the different problems, and why progress toward resolving them would help those doing computations using density functional theory. Subjects covered include the magnitude of the kinetic energy in Hartree-Fock calculations, the shape of adiabatic connection curves, using the constrained search with input densities, densities of states, the semiclassical expansion of energies, the tightness of Lieb-Oxfordbounds, and how we decide the accuracy of an approximate density.△ Less"
Text to Image Synthesis using Stacked Conditional Variational Autoencoders and Conditional Generative Adversarial Networks,"Authors:Haileleol Tibebu,Aadil Malik,Varuna De Silva","Abstract:…benefits from a conditioning augmentation and a residual block on the Conditional GAN network to achieve the results. Multiple experiments were conducted using CUB andOxford-102 dataset and the result of the proposed approach is compared against state-ofthe-art techniques such as StackGAN. The experiments illustrate that the proposed method generates a high…▽ MoreSynthesizing a realistic image from textual description is a major challenge in computer vision. Current text to image synthesis approaches falls short of producing a highresolution image that represent a text descriptor. Most existing studies rely either on Generative Adversarial Networks (GANs) or Variational Auto Encoders (VAEs). GANs has the capability to produce sharper images but lacks the diversity of outputs, whereas VAEs are good at producing a diverse range of outputs, but the images generated are often blurred. Taking into account the relative advantages of both GANs and VAEs, we proposed a new stacked Conditional VAE (CVAE) and Conditional GAN (CGAN) network architecture for synthesizing images conditioned on a text description. This study uses Conditional VAEs as an initial generator to produce a high-level sketch of the text descriptor. This high-level sketch output from first stage and a text descriptor is used as an input to the conditional GAN network. The second stage GAN produces a 256x256 high resolution image. The proposed architecture benefits from a conditioning augmentation and a residual block on the Conditional GAN network to achieve the results. Multiple experiments were conducted using CUB andOxford-102 dataset and the result of the proposed approach is compared against state-ofthe-art techniques such as StackGAN. The experiments illustrate that the proposed method generates a high-resolution image conditioned on text descriptions and yield competitive results based on Inception and Frechet Inception Score using both datasets△ Less"
Measurement of the $Λ_c^+$ lifetime,"Authors:Belle II Collaboration,F. Abudinén,L. Aggarwal,H. Ahmed,J. K. Ahn,H. Aihara,N. Akopov,A. Aloisio,N. Anh Ky,D. M. Asner,H. Atmacan,T. Aushev,V. Aushev,V. Babu,H. Bae,P. Bambade,Sw. Banerjee,S. Bansal,J. Baudot,M. Bauer,A. Baur,A. Beaubien,J. Becker,J. V. Bennett,E. Bernieri, et al. (338 additional authors not shown)","Abstract:An absolute measurement of the $Λ^{+}_c$ lifetime is reported using $Λ_c^+\rightarrow pK^-π^+$ decays in events reconstructed from data collected by the Belle II experiment at the SuperKEKB asymmetric-energy electron-positron collider. The total integrated luminosity of the data sample, which was collected at center-of-mass energies at or near the $Υ(4S)$ resonance, is $207.2~\mbox{fb}^{-1}$. The…▽ MoreAn absolute measurement of the $Λ^{+}_c$ lifetime is reported using $Λ_c^+\rightarrow pK^-π^+$ decays in events reconstructed from data collected by the Belle II experiment at the SuperKEKB asymmetric-energy electron-positron collider. The total integrated luminosity of the data sample, which was collected at center-of-mass energies at or near the $Υ(4S)$ resonance, is $207.2~\mbox{fb}^{-1}$. The result, $τ(Λ^{+}_c) = 203.20 \pm 0.89 \,\mathrm{(stat)} \pm 0.77 \,\mathrm{(syst)}$ fs, is the most precise measurement to date and is consistent with previous determinations.△ Less"
When the Sun Goes Down: Repairing Photometric Losses for All-Day Depth Estimation,"Authors:Madhu Vankadari,Stuart Golodetz,Sourav Garg,Sangyun Shin,Andrew Markham,Niki Trigoni","Abstract:…and nighttime images without needing separate encoders or extra feature networks like existing methods. We perform extensive experiments and ablation studies on the challengingOxfordRobotCar dataset to demonstrate the efficacy of our approach for both day and nighttime sequences.▽ MoreSelf-supervised deep learning methods for joint depth and ego-motion estimation can yield accurate trajectories without needing ground-truth training data. However, as they typically use photometric losses, their performance can degrade significantly when the assumptions these losses make (e.g. temporal illumination consistency, a static scene, and the absence of noise and occlusions) are violated. This limits their use for e.g. nighttime sequences, which tend to contain many point light sources (including on dynamic objects) and low signal-to-noise ratio (SNR) in darker image regions. In this paper, we show how to use a combination of three techniques to allow the existing photometric losses to work for both day and nighttime images. First, we introduce a per-pixel neural intensity transformation to compensate for the light changes that occur between successive frames. Second, we predict a per-pixel residual flow map that we use to correct the reprojection correspondences induced by the estimated ego-motion and depth from the networks. And third, we denoise the training images to improve the robustness and accuracy of our approach. These changes allow us to train a single model for both day and nighttime images without needing separate encoders or extra feature networks like existing methods. We perform extensive experiments and ablation studies on the challengingOxfordRobotCar dataset to demonstrate the efficacy of our approach for both day and nighttime sequences.△ Less"
"Angular analysis of $B^+ \to ρ^+ρ^0$ decays reconstructed in 2019, 2020, and 2021 Belle II data","Authors:Belle II Collaboration,F. Abudinén,I. Adachi,K. Adamczyk,L. Aggarwal,P. Ahlburg,H. Ahmed,J. K. Ahn,H. Aihara,N. Akopov,A. Aloisio,F. Ameli,L. Andricek,N. Anh Ky,D. M. Asner,H. Atmacan,V. Aulchenko,T. Aushev,V. Aushev,T. Aziz,V. Babu,S. Bacher,H. Bae,S. Baehr,S. Bahinipati, et al. (570 additional authors not shown)","Abstract:We report on a Belle II measurement of the branching fraction ($\mathcal{B}$), longitudinal polarization fraction ($f_L$), and CP asymmetry ($\mathcal{A}_{CP}$) of $B^+\to ρ^+ρ^0$ decays. We reconstruct $B^+\to ρ^+(\to π^+π^0(\to γγ))ρ^0(\to π^+π^-)$ decays in a sample of SuperKEKB electron-positron collisions collected by the Belle II experiment in 2019, 2020, and 2021 at the $Υ$(4S) resonance an…▽ MoreWe report on a Belle II measurement of the branching fraction ($\mathcal{B}$), longitudinal polarization fraction ($f_L$), and CP asymmetry ($\mathcal{A}_{CP}$) of $B^+\to ρ^+ρ^0$ decays. We reconstruct $B^+\to ρ^+(\to π^+π^0(\to γγ))ρ^0(\to π^+π^-)$ decays in a sample of SuperKEKB electron-positron collisions collected by the Belle II experiment in 2019, 2020, and 2021 at the $Υ$(4S) resonance and corresponding to 190 fb$^{-1}$ of integrated luminosity. We fit the distributions of the difference between expected and observed $B$ candidate energy, continuum-suppression discriminant, dipion masses, and decay angles of the selected samples, to determine a signal yield of $345 \pm 31$ events. The signal yields are corrected for efficiencies determined from simulation and control data samples to obtain $\mathcal{B}(B^+ \to ρ^+ρ^0) = [23.2^{+\ 2.2}_{-\ 2.1} (\rm stat) \pm 2.7 (\rm syst)]\times 10^{-6}$, $f_L = 0.943 ^{+\ 0.035}_{-\ 0.033} (\rm stat)\pm 0.027(\rm syst)$, and $\mathcal{A}_{CP}=-0.069 \pm 0.068(\rm stat) \pm 0.060 (\rm syst)$. The results agree with previous measurements. This is the first measurement of $\mathcal{A}_{CP}$ in $B^+\to ρ^+ρ^0$ decays reported by Belle II.△ Less"
AI Challenges for Society and Ethics,"Authors:Jess Whittlestone,Sam Clarke","Abstract:Artificial intelligence is already being applied in and impacting many important sectors in society, including healthcare, finance, and policing. These applications will increase as AI capabilities continue to progress, which has the potential to be highly beneficial for society, or to cause serious harm. The role of AI governance is ultimately to take practical steps to mitigate this risk of harm…▽ MoreArtificial intelligence is already being applied in and impacting many important sectors in society, including healthcare, finance, and policing. These applications will increase as AI capabilities continue to progress, which has the potential to be highly beneficial for society, or to cause serious harm. The role of AI governance is ultimately to take practical steps to mitigate this risk of harm while enabling the benefits of innovation in AI. This requires answering challenging empirical questions about current and potential risks and benefits of AI: assessing impacts that are often widely distributed and indirect, and making predictions about a highly uncertain future. It also requires thinking through the normative question of what beneficial use of AI in society looks like, which is equally challenging. Though different groups may agree on high-level principles that uses of AI should respect (e.g., privacy, fairness, and autonomy), challenges arise when putting these principles into practice. For example, it is straightforward to say that AI systems must protect individual privacy, but there is presumably some amount or type of privacy that most people would be willing to give up to develop life-saving medical treatments. Despite these challenges, research can and has made progress on these questions. The aim of this chapter will be to give readers an understanding of this progress, and of the challenges that remain.△ Less"
"The Lieb-OxfordLower Bounds on the Coulomb Energy, Their Importance to Electron Density Functional Theory, and a Conjectured Tight Bound on Exchange","Authors:John P. Perdew,Jianwei Sun","Abstract:Lieb andOxford(1981) derived rigorous lower bounds, in the form of local functionals of the electron density, on the indirect part of the Coulomb repulsion energy. The greatest lower bound for a given electron number N depends monotonically upon N, and the N-> infinity limit is a bound for all N. These bounds have been shown to apply to the exact densit…▽ MoreLieb andOxford(1981) derived rigorous lower bounds, in the form of local functionals of the electron density, on the indirect part of the Coulomb repulsion energy. The greatest lower bound for a given electron number N depends monotonically upon N, and the N-> infinity limit is a bound for all N. These bounds have been shown to apply to the exact density functionals for the exchange- and exchange-correlation energies that must be approximated for an accurate and computationally efficient description of atoms, molecules, and solids. A tight bound on the exact exchange energy has been derived therefrom for two-electron ground states, and is conjectured to apply to all spin-unpolarized electronic ground states. Some of these and other exact constraints have been used to construct two generations of non-empirical density functionals beyond the local density approximation: the Perdew-Burke-Ernzerhof (PBE) generalized gradient approximation (GGA), and the strongly constrained and appropriately normed (SCAN) meta-GGA.△ Less"
How to Solve Big Problems: Bespoke Versus Platform Strategies,"Authors:Atif Ansar,Bent Flyvbjerg","Abstract:How should government and business solve big problems? In bold leaps or in many smaller moves? We show that bespoke, one-off projects are prone to poorer outcomes than projects built on a repeatable platform. Repeatable projects are cheaper, faster, and scale at lower risk of failure. We compare evidence from 203 space missions at NASA and SpaceX, on cost, speed-to-market, schedule, and scalabilit…▽ MoreHow should government and business solve big problems? In bold leaps or in many smaller moves? We show that bespoke, one-off projects are prone to poorer outcomes than projects built on a repeatable platform. Repeatable projects are cheaper, faster, and scale at lower risk of failure. We compare evidence from 203 space missions at NASA and SpaceX, on cost, speed-to-market, schedule, and scalability. We find that SpaceX's platform strategy was 10X cheaper and 2X faster than NASA's bespoke strategy. Moreover, SpaceX's platform strategy was financially less risky, virtually eliminating cost overruns. Finally, we show that achieving platform repeatability is a strategically diligent process involving experimental learning sequences. Sectors of the economy where governments find it difficult to control spending or timeframes or to realize planned benefits - e.g., health, education, climate, defence - are ripe for a platform rethink.△ Less"
Measurement of the branching fraction of the $B^0 \to K_S^0 π^0 γ$ decay using 190 fb$^{-1}$ of Belle II data,"Authors:Belle II Collaboration,F. Abudinén,I. Adachi,K. Adamczyk,L. Aggarwal,P. Ahlburg,H. Ahmed,J. K. Ahn,H. Aihara,N. Akopov,A. Aloisio,F. Ameli,L. Andricek,N. Anh Ky,D. M. Asner,H. Atmacan,V. Aulchenko,T. Aushev,V. Aushev,T. Aziz,V. Babu,S. Bacher,H. Bae,S. Baehr,S. Bahinipati, et al. (570 additional authors not shown)","Abstract:We report the measurement of the branching fraction of the $B^0 \to K_S^0 π^0 γ$ decay in $e^+ e^- \to Υ(4S) \to B \overline{B}$ data recorded by the Belle II experiment at the SuperKEKB asymmetric-energy collider and corresponding to 190 fb$^{-1}$ of integrated luminosity. The signal yield is measured to be $121\pm 29\,\hbox{(stat.)}$, leading to the branching fraction…▽ MoreWe report the measurement of the branching fraction of the $B^0 \to K_S^0 π^0 γ$ decay in $e^+ e^- \to Υ(4S) \to B \overline{B}$ data recorded by the Belle II experiment at the SuperKEKB asymmetric-energy collider and corresponding to 190 fb$^{-1}$ of integrated luminosity. The signal yield is measured to be $121\pm 29\,\hbox{(stat.)}$, leading to the branching fraction ${\cal B}\left(B^0 \to K_S^0 π^0 γ\right) = \left(7.3 \pm 1.8\,\hbox{(stat.)} \pm 1.0\,\hbox{(syst.)} \right)\times 10^{-6}$, which agrees with the known value.△ Less"
Study of Exclusive $B \to πe^+ ν_e$ Decays with Hadronic Full-event-interpretation Tagging in 189.3 fb$^{-1}$ of Belle II Data,"Authors:Belle II Collaboration,F. Abudinén,I. Adachi,K. Adamczyk,L. Aggarwal,P. Ahlburg,H. Ahmed,J. K. Ahn,H. Aihara,N. Akopov,A. Aloisio,F. Ameli,L. Andricek,N. Anh Ky,D. M. Asner,H. Atmacan,V. Aulchenko,T. Aushev,V. Aushev,T. Aziz,V. Babu,S. Bacher,H. Bae,S. Baehr,S. Bahinipati, et al. (570 additional authors not shown)","Abstract:We present a reconstruction of the semileptonic decays $B^0 \to π^- e^+ ν_e$ and $B^+ \to π^0 e^+ ν_e$ in a sample corresponding to 189.3 fb$^{-1}$ of Belle II data, using events where the partner $B$-meson is reconstructed from a large variety of hadronic channels via a tagging algorithm known as the full-event-interpretation. We determine the partial branching fractions in three bins of the squa…▽ MoreWe present a reconstruction of the semileptonic decays $B^0 \to π^- e^+ ν_e$ and $B^+ \to π^0 e^+ ν_e$ in a sample corresponding to 189.3 fb$^{-1}$ of Belle II data, using events where the partner $B$-meson is reconstructed from a large variety of hadronic channels via a tagging algorithm known as the full-event-interpretation. We determine the partial branching fractions in three bins of the squared momentum transfer to the leptonic system using fits to the distribution of the square of the missing mass. The partial branching fractions are summed to determine $\mathcal{B}(B^0 \to π^- e^+ ν_e)$ = (1.43 $\pm$ 0.27(stat) $\pm$ 0.07(syst)) $\times 10^{-4}$ and $\mathcal{B}(B^+ \to π^0 e^+ ν_e)$ = (8.33 $\pm$ 1.67(stat) $\pm$ 0.55(syst)) $\times 10^{-5}$. We extract a first Belle II measurement of the magnitude of the Cabibbo-Kobayashi-Maskawa matrix element $|V_{\mathrm{ub}}|$, with $|V_{\mathrm{ub}}|$ = (3.88 $\pm$ 0.45) $\times 10^{-3}$.△ Less"
First decay-time-dependent analysis of $B^{0} \to K_{S}^{0} π^{0}$ at Belle II,"Authors:Belle II Collaboration,F. Abudinén,I. Adachi,R. Adak,K. Adamczyk,L. Aggarwal,P. Ahlburg,H. Ahmed,J. K. Ahn,H. Aihara,N. Akopov,A. Aloisio,F. Ameli,L. Andricek,N. Anh Ky,D. M. Asner,H. Atmacan,V. Aulchenko,T. Aushev,V. Aushev,T. Aziz,V. Babu,S. Bacher,H. Bae,S. Baehr, et al. (569 additional authors not shown)","Abstract:We report measurements of the branching fraction ($\mathcal{B}$) and direct $CP$-violating asymmetry ($A_{CP}$) of the charmless decay $B^{0} \to K^0 π^0$ at Belle II. A sample of $e^{+} e^{-}$ collisions, corresponding to $189.8 fb^{-1}$ of integrated luminosity, recorded at the $Υ(4S)$ resonance is used for the first decay-time-dependent analysis of these decays within the experiment. We reconst…▽ MoreWe report measurements of the branching fraction ($\mathcal{B}$) and direct $CP$-violating asymmetry ($A_{CP}$) of the charmless decay $B^{0} \to K^0 π^0$ at Belle II. A sample of $e^{+} e^{-}$ collisions, corresponding to $189.8 fb^{-1}$ of integrated luminosity, recorded at the $Υ(4S)$ resonance is used for the first decay-time-dependent analysis of these decays within the experiment. We reconstruct about 135 signal candidates, and measure $\mathcal{B}(B^{0} \to K^{0} π^{0})= [11.0 \pm 1.2 (stat) \pm 1.0 (syst)] \times 10^{-6}$ and $A_{CP} (B^{0} \to K^{0} π^{0})= -0.41_{-0.32}^{+0.30} (stat) \pm 0.09 (syst)$.△ Less"
Measurement of the branching fraction for the decay $B \to K^{\ast}(892)\ell^+\ell^-$ at Belle II,"Authors:Belle II Collaboration,F. Abudinén,I. Adachi,R. Adak,K. Adamczyk,L. Aggarwal,P. Ahlburg,H. Ahmed,J. K. Ahn,H. Aihara,N. Akopov,A. Aloisio,F. Ameli,L. Andricek,N. Anh Ky,D. M. Asner,H. Atmacan,V. Aulchenko,T. Aushev,V. Aushev,T. Aziz,V. Babu,S. Bacher,H. Bae,S. Baehr, et al. (569 additional authors not shown)","Abstract:We report a measurement of the branching fraction of $B \to K^{\ast}(892)\ell^+\ell^-$ decays, where $\ell^+\ell^- = μ^+μ^-$ or $e^+e^-$, using electron-positron collisions recorded at an energy at or near the $Υ(4S)$ mass and corresponding to an integrated luminosity of $189$ fb$^{-1}$. The data was collected during 2019--2021 by the Belle II experiment at the SuperKEKB $e^{+}e^{-}$ asymmetric-en…▽ MoreWe report a measurement of the branching fraction of $B \to K^{\ast}(892)\ell^+\ell^-$ decays, where $\ell^+\ell^- = μ^+μ^-$ or $e^+e^-$, using electron-positron collisions recorded at an energy at or near the $Υ(4S)$ mass and corresponding to an integrated luminosity of $189$ fb$^{-1}$. The data was collected during 2019--2021 by the Belle II experiment at the SuperKEKB $e^{+}e^{-}$ asymmetric-energy collider. We reconstruct $K^{\ast}(892)$ candidates in the $K^+π^-$, $K_{S}^{0}π^+$, and $K^+π^0$ final states. The signal yields with statistical uncertainties are $22\pm 6$, $18 \pm 6$, and $38 \pm 9$ for the decays $B \to K^{\ast}(892)μ^+μ^-$, $B \to K^{\ast}(892)e^+e^-$, and $B \to K^{\ast}(892)\ell^+\ell^-$, respectively. We measure the branching fractions of these decays for the entire range of the dilepton mass, excluding the very low mass region to suppress the $B \to K^{\ast}(892)γ(\to e^+e^-)$ background and regions compatible with decays of charmonium resonances, to be \begin{equation} {\cal B}(B \to K^{\ast}(892)μ^+μ^-) = (1.19 \pm 0.31 ^{+0.08}_{-0.07}) \times 10^{-6}, {\cal B}(B \to K^{\ast}(892)e^+e^-) = (1.42 \pm 0.48 \pm 0.09)\times 10^{-6}, {\cal B}(B \to K^{\ast}(892)\ell^+\ell^-) = (1.25 \pm 0.30 ^{+0.08}_{-0.07}) \times 10^{-6}, \end{equation} where the first and second uncertainties are statistical and systematic, respectively. These results, limited by sample size, are the first measurements of $B \to K^{\ast}(892)\ell^+\ell^-$ branching fractions from the Belle II experiment.△ Less"
"How Algorithms Shape the Distribution of Political Advertising: Case Studies of Facebook, Google, and TikTok","Authors:Orestis Papakyriakopoulos,Christelle Tessono,Arvind Narayanan,Mihir Kshirsagar","Abstract:Online platforms play an increasingly important role in shaping democracy by influencing the distribution of political information to the electorate. In recent years, political campaigns have spent heavily on the platforms' algorithmic tools to target voters with online advertising. While the public interest in understanding how platforms perform the task of shaping the political discourse has nev…▽ MoreOnline platforms play an increasingly important role in shaping democracy by influencing the distribution of political information to the electorate. In recent years, political campaigns have spent heavily on the platforms' algorithmic tools to target voters with online advertising. While the public interest in understanding how platforms perform the task of shaping the political discourse has never been higher, the efforts of the major platforms to make the necessary disclosures to understand their practices falls woefully short. In this study, we collect and analyze a dataset containing over 800,000 ads and 2.5 million videos about the 2020 U.S. presidential election from Facebook, Google, and TikTok. We conduct the first large scale data analysis of public data to critically evaluate how these platforms amplified or moderated the distribution of political advertisements. We conclude with recommendations for how to improve the disclosures so that the public can hold the platforms and political advertisers accountable.△ Less"
Learning Vehicle Trajectory Uncertainty,"Authors:Barak Or,Itzik Klein","Abstract:…can suffer from degraded performance due to uncertainty in the vehicle kinematic trajectory modeling. Our method is evaluated and compared to other adaptive filters using theOxfordRobotCar dataset, and has shown to be effective in accurately determining the process noise covariance in real-time scenarios. Overall, this approach can be implemented in other…▽ MoreA novel approach for vehicle tracking using a hybrid adaptive Kalman filter is proposed. The filter utilizes recurrent neural networks to learn the vehicle's geometrical and kinematic features, which are then used in a supervised learning model to determine the actual process noise covariance in the Kalman framework. This approach addresses the limitations of traditional linear Kalman filters, which can suffer from degraded performance due to uncertainty in the vehicle kinematic trajectory modeling. Our method is evaluated and compared to other adaptive filters using theOxfordRobotCar dataset, and has shown to be effective in accurately determining the process noise covariance in real-time scenarios. Overall, this approach can be implemented in other estimation problems to improve performance.△ Less"
Rapid X-ray Variability in Mkn 421 during a Multiwavelength Campaign,"Authors:Alex G. Markowitz,Krzysztof Nalewajko,Gopal Bhatta,Gulab C. Dewangan,Sunil Chandra,Daniela Dorner,Bernd Schleicher,Urszula Pajdosz-Smierciak,Lukasz Stawarz,Staszek Zola,Michal Ostrowski,Daniele Carosati,Saikruba Krishnan,Rumen Bachev,Erika Benitez,Kosmas Gazeas,David Hiriart,Shao-Ming Hu,Valeri Larionov,Alessandro Marchini,Katsura Matsumoto,A. A. Nikiforova,Tapio Pursimo,Claudia M. Raiteri,Daniel E. Reichart, et al. (25 additional authors not shown)","Abstract:The study of short-term variability properties in AGN jets has the potential to shed light on their particle acceleration and emission mechanisms. We report results from a four-day coordinated multi-wavelength campaign on the highly-peaked blazar (HBL) Mkn 421 in 2019 January. We obtained X-ray data from AstroSAT, BVRI photometry with the Whole Earth Blazar Telescope (WEBT), and TeV data from FACT…▽ MoreThe study of short-term variability properties in AGN jets has the potential to shed light on their particle acceleration and emission mechanisms. We report results from a four-day coordinated multi-wavelength campaign on the highly-peaked blazar (HBL) Mkn 421 in 2019 January. We obtained X-ray data from AstroSAT, BVRI photometry with the Whole Earth Blazar Telescope (WEBT), and TeV data from FACT to explore short-term multi-wavelength variability in this HBL. The X-ray continuum is rapidly variable on time-scales of tens of ks. Fractional variability amplitude increases with energy across the synchrotron hump, consistent with previous studies; we interpret this observation in the context of a model with multiple cells whose emission spectra contain cutoffs that follow a power-law distribution. We also performed time-averaged and time-resolved (time-scales of 6 ks) spectral fits; a broken power-law model fits all spectra well; time-resolved spectral fitting reveals the usual hardening when brightening behaviour. Intra-X-ray cross correlations yield evidence for the 0.6-0.8 keV band to likely lead the other bands by an average of 4.6 +- 2.6 ks, but only during the first half of the observation. The source displayed minimal night-to-night variability at all wavebands thus precluding significant interband correlations during our campaign. The broadband SED is modeled well with a standard one-zone leptonic model, yielding jet parameters consistent with those obtained from previous SEDs of this source.△ Less"
Oxford-style Debates in Telecommunication and Computer Science Education,Authors:Marcin Niemiec,"Abstract:Oxford-style debating is a well-known tool in social sciences. Such formal discussions on particular topics are widely used by historians and sociologists. However, when we try to go beyond standard thinking, it turns out that…▽ MoreOxford-style debating is a well-known tool in social sciences. Such formal discussions on particular topics are widely used by historians and sociologists. However, when we try to go beyond standard thinking, it turns out thatOxford-style debating can be a great educational tool in telecommunication and computer science. This article presents this unusual method of education at technical universities and in the IT industry, and describes its features and challenges. Best practices and examples of debating are provided, taking into account emerging topics in telecommunications and computer science, such as cybersecurity. The article also contains feedback from IT engineers who participated inOxford-style debates. All this aims to encourage this form of education in telecommunication and computer science.△ Less"
Terms-we-Serve-with: a feminist-inspired social imaginary for improved transparency and engagement in AI,"Authors:Bogdana Rakova,Megan Ma,Renee Shelby","Abstract:Power and information asymmetries between people and digital technology companies have predominantly been legitimized through contractual agreements that have failed to provide diverse people with meaningful consent and contestability. We offer an interdisciplinary multidimensional perspective on the future of regulatory frameworks - the Terms-we-Serve-with (TwSw) social, computational, and legal…▽ MorePower and information asymmetries between people and digital technology companies have predominantly been legitimized through contractual agreements that have failed to provide diverse people with meaningful consent and contestability. We offer an interdisciplinary multidimensional perspective on the future of regulatory frameworks - the Terms-we-Serve-with (TwSw) social, computational, and legal contract for restructuring power asymmetries and center-periphery dynamics to enable improved human agency in individual and collective experiences of algorithmic harms.△ Less"
Collisional Charging in the Low Pressure Range of Protoplanetary Disks,"Authors:T. Becker,T. Steinpilz,J. Teiser,G. Wurm","Abstract:In recent years, collisional charging has been proposed to promote the growth of pebbles in early phases of planet formation. Ambient pressure in protoplanetary disks spans a wide range from below $10^{-9}$ mbar up to way beyond mbar. Yet, experiments on collisional charging of same material surfaces have only been conducted under Earth atmospheric pressure, Martian pressure and more generally dow…▽ MoreIn recent years, collisional charging has been proposed to promote the growth of pebbles in early phases of planet formation. Ambient pressure in protoplanetary disks spans a wide range from below $10^{-9}$ mbar up to way beyond mbar. Yet, experiments on collisional charging of same material surfaces have only been conducted under Earth atmospheric pressure, Martian pressure and more generally down to $10^{-2}$ mbar thus far. This work presents first pressure dependent charge measurements of same material collisions between $10^{-8}$ and $10^3$ mbar. Strong charging occurs down to the lowest pressure. In detail, our observations show a strong similarity to the pressure dependence of the breakdown voltage between two electrodes and we suggest that breakdown also determines the maximum charge on colliding grains in protoplanetary disks. We conclude that collisional charging can occur in all parts of protoplanetary disks relevant for planet formation.△ Less"
Biological Evolution and Genetic Algorithms: Exploring the Space of Abstract Tile Self-Assembly,Authors:Christian Schroeder de Witt,"Abstract:A physically-motivated genetic algorithm (GA) and full enumeration for a tile-based model of self-assembly (JaTAM) is implemented using a graphics processing unit (GPU). We observe performance gains with respect to state-of-the-art implementations on CPU of factor 7.7 for the GA and 2.9 for JaTAM. The correctness of our GA implementation is demonstrated using a test-bed fitness function, and our J…▽ MoreA physically-motivated genetic algorithm (GA) and full enumeration for a tile-based model of self-assembly (JaTAM) is implemented using a graphics processing unit (GPU). We observe performance gains with respect to state-of-the-art implementations on CPU of factor 7.7 for the GA and 2.9 for JaTAM. The correctness of our GA implementation is demonstrated using a test-bed fitness function, and our JaTAM implementation is verified by classifying a well-known search space $S_{2,8}$ based on two tile types. The performance gains achieved allow for the classification of a larger search space $S^{32}_{3,8}$ based on three tile types. The prevalence of structures based on two tile types demonstrates that simple organisms emerge preferrably even in complex ecosystems. The modularity of the largest structures found motivates the assumption that to first order, $S_{2,8}$ forms the building blocks of $S_{3,8}$. We conclude that GPUs may play an important role in future studies of evolutionary dynamics.△ Less"
What are People Talking about in #BlackLivesMatter and #StopAsianHate? Exploring and Categorizing Twitter Topics Emerging in Online Social Movements through the Latent Dirichlet Allocation Model,"Authors:Xin Tong,Yixuan Li,Jiayi Li,Rongqi Bei,Luyao Zhang","Abstract:Minority groups have been using social media to organize social movements that create profound social impacts. Black Lives Matter (BLM) and Stop Asian Hate (SAH) are two successful social movements that have spread on Twitter that promote protests and activities against racism and increase the public's awareness of other social challenges that minority groups face. However, previous studies have m…▽ MoreMinority groups have been using social media to organize social movements that create profound social impacts. Black Lives Matter (BLM) and Stop Asian Hate (SAH) are two successful social movements that have spread on Twitter that promote protests and activities against racism and increase the public's awareness of other social challenges that minority groups face. However, previous studies have mostly conducted qualitative analyses of tweets or interviews with users, which may not comprehensively and validly represent all tweets. Very few studies have explored the Twitter topics within BLM and SAH dialogs in a rigorous, quantified and data-centered approach. Therefore, in this research, we adopted a mixed-methods approach to comprehensively analyze BLM and SAH Twitter topics. We implemented (1) the latent Dirichlet allocation model to understand the top high-level words and topics and (2) open-coding analysis to identify specific themes across the tweets. We collected more than one million tweets with the #blacklivesmatter and #stopasianhate hashtags and compared their topics. Our findings revealed that the tweets discussed a variety of influential topics in depth, and social justice, social movements, and emotional sentiments were common topics in both movements, though with unique subtopics for each movement. Our study contributes to the topic analysis of social movements on social media platforms in particular and the literature on the interplay of AI, ethics, and society in general.△ Less"
Snowmass 2021 White Paper on Upgrading SuperKEKB with a Polarized Electron Beam: Discovery Potential and Proposed Implementation,"Authors:A. Accardi,D. M. Asner,H. Atmacan,R. Baartman,Sw. Banerjee,A. Beaubien,J. V. Bennett,M. Bertemes,M. Bessner,D. Biswas,G. Bonvicini,N. Brenny,R. A. Briere,T. E. Browder,C. Chen,S. Choudhury,D. Cinabro,J. Cochran,L. M. Cremaldi,W. Deconinck,A. Di Canto,S. Dubey,K. Flood,B. G. Fulsom,V. Gaur, et al. (83 additional authors not shown)","Abstract:Upgrading the SuperKEKB electron-positron collider with polarized electron beams opens a new program of precision physics at a center-of-mass energy of 10.58 GeV. This white paper describes the physics potential of this `Chiral Belle' program. It includes projections for precision measurements of $\sin^2θ_W$ that can be obtained from independent left-right asymmetry measurements of $e^+e^-$ transi…▽ MoreUpgrading the SuperKEKB electron-positron collider with polarized electron beams opens a new program of precision physics at a center-of-mass energy of 10.58 GeV. This white paper describes the physics potential of this `Chiral Belle' program. It includes projections for precision measurements of $\sin^2θ_W$ that can be obtained from independent left-right asymmetry measurements of $e^+e^-$ transitions to pairs of electrons, muons, taus, charm and b-quarks. The $\sin^2θ_W$ precision obtainable at SuperKEKB will match that of the LEP/SLC world average, but at the centre-of-mass energy of 10.58 GeV. Measurements of the couplings for muons, charm, and $b$-quarks will be substantially improved and the existing $3σ$ discrepancy between the SLC $A_{LR}$ and LEP $A_{FB}^b$ measurements will be addressed. Precision measurements of neutral current universality will be more than an order of magnitude more precise than currently available. As the energy scale is well away from the $Z^0$-pole, the precision measurements will have sensitivity to the presence of a parity-violating dark sector gauge boson, $Z_{\rm dark}$. The program also enables the measurement of the anomalous magnetic moment $g-2$ form factor of the $τ$ to be made at an unprecedented level of precision. A precision of $10^{-5}$ level is accessible with 40~ab$^{-1}$ and with more data it would start to approach the $10^{-6}$ level. This technique would provide the most precise information from the third generation about potential new physics explanations of the muon $g-2$ $4σ$ anomaly. Additional $τ$ and QCD physics programs enabled or enhanced with having polarized electron beams are also discussed in this White Paper. This paper includes a summary of the path forward in R&D and next steps required to implement this upgrade and access its exciting discovery potential.△ Less"
Deep Apprenticeship Learning for Playing Games,Authors:Dejan Markovikj,"Abstract:In the last decade, deep learning has achieved great success in machine learning tasks where the input data is represented with different levels of abstractions. Driven by the recent research in reinforcement learning using deep neural networks, we explore the feasibility of designing a learning model based on expert behaviour for complex, multidimensional tasks where reward function is not availa…▽ MoreIn the last decade, deep learning has achieved great success in machine learning tasks where the input data is represented with different levels of abstractions. Driven by the recent research in reinforcement learning using deep neural networks, we explore the feasibility of designing a learning model based on expert behaviour for complex, multidimensional tasks where reward function is not available. We propose a novel method for apprenticeship learning based on the previous research on supervised learning techniques in reinforcement learning. Our method is applied to video frames from Atari games in order to teach an artificial agent to play those games. Even though the reported results are not comparable with the state-of-the-art results in reinforcement learning, we demonstrate that such an approach has the potential to achieve strong performance in the future and is worthwhile for further research.△ Less"
CorAl: Introspection for Robust Radar and Lidar Perception in Diverse Environments Using Differential Entropy,"Authors:Daniel Adolfsson,Manuel Castellano-Quero,Martin Magnusson,Achim J. Lilienthal,Henrik Andreasson","Abstract:…experiments demonstrate that CorAl outperforms previous methods both on the ETH lidar benchmark, which includes several indoor and outdoor environments, and the large-scaleOxfordand MulRan radar data sets for urban traffic scenarios The results also demonstrate that CorAl generalizes very well across substantially different environments without the need of…▽ MoreRobust perception is an essential component to enable long-term operation of mobile robots. It depends on failure resilience through reliable sensor data and preprocessing, as well as failure awareness through introspection, for example the ability to self-assess localization performance. This paper presents CorAl: a principled, intuitive, and generalizable method to measure the quality of alignment between pairs of point clouds, which learns to detect alignment errors in a self-supervised manner. CorAl compares the differential entropy in the point clouds separately with the entropy in their union to account for entropy inherent to the scene. By making use of dual entropy measurements, we obtain a quality metric that is highly sensitive to small alignment errors and still generalizes well to unseen environments. In this work, we extend our previous work on lidar-only CorAl to radar data by proposing a two-stage filtering technique that produces high-quality point clouds from noisy radar scans. Thus we target robust perception in two ways: by introducing a method that introspectively assesses alignment quality, and applying it to an inherently robust sensor modality. We show that our filtering technique combined with CorAl can be applied to the problem of alignment classification, and that it detects small alignment errors in urban settings with up to 98% accuracy, and with up to 96% if trained only in a different environment. Our lidar and radar experiments demonstrate that CorAl outperforms previous methods both on the ETH lidar benchmark, which includes several indoor and outdoor environments, and the large-scaleOxfordand MulRan radar data sets for urban traffic scenarios The results also demonstrate that CorAl generalizes very well across substantially different environments without the need of retraining.△ Less"
Building Facade Parsing R-CNN,"Authors:Sijie Wang,Qiyu Kang,Rui She,Wee Peng Tay,Diego Navarro Navarro,Andreas Hartmannsgruber","Abstract:…than the current state-of-the-art facade parsing models, which are primarily developed for frontal views. We also publish a new building facade parsing dataset derived from theOxfordRobotCar dataset, which we call the…▽ MoreBuilding facade parsing, which predicts pixel-level labels for building facades, has applications in computer vision perception for autonomous vehicle (AV) driving. However, instead of a frontal view, an on-board camera of an AV captures a deformed view of the facade of the buildings on both sides of the road the AV is travelling on, due to the camera perspective. We propose Facade R-CNN, which includes a transconv module, generalized bounding box detection, and convex regularization, to perform parsing of deformed facade views. Experiments demonstrate that Facade R-CNN achieves better performance than the current state-of-the-art facade parsing models, which are primarily developed for frontal views. We also publish a new building facade parsing dataset derived from theOxfordRobotCar dataset, which we call theOxfordRobotCar Facade dataset. This dataset contains 500 street-view images from theOxfordRobotCar dataset augmented with accurate annotations of building facade objects. The published dataset is available at https://github.com/sijieaaa/Oxford-RobotCar-Facade△ Less"
Aligned with Whom? Direct and social goals for AI systems,"Authors:Anton Korinek,Avital Balwit","Abstract:As artificial intelligence (AI) becomes more powerful and widespread, the AI alignment problem - how to ensure that AI systems pursue the goals that we want them to pursue - has garnered growing attention. This article distinguishes two types of alignment problems depending on whose goals we consider, and analyzes the different solutions necessitated by each. The direct alignment problem considers…▽ MoreAs artificial intelligence (AI) becomes more powerful and widespread, the AI alignment problem - how to ensure that AI systems pursue the goals that we want them to pursue - has garnered growing attention. This article distinguishes two types of alignment problems depending on whose goals we consider, and analyzes the different solutions necessitated by each. The direct alignment problem considers whether an AI system accomplishes the goals of the entity operating it. In contrast, the social alignment problem considers the effects of an AI system on larger groups or on society more broadly. In particular, it also considers whether the system imposes externalities on others. Whereas solutions to the direct alignment problem center around more robust implementation, social alignment problems typically arise because of conflicts between individual and group-level goals, elevating the importance of AI governance to mediate such conflicts. Addressing the social alignment problem requires both enforcing existing norms on their developers and operators and designing new norms that apply directly to AI systems.△ Less"
Einstein'sOxfordBlackboard: A Unique Historical Artefact,Authors:Cormac O'Raifeartaigh,"Abstract:Einstein's blackboard is a well-known exhibit at the History of Science Museum atOxfordUniversity. However, it is much less well known that the writing on the board provides a neat summary of a work of historic importance, Einstein's 1931 model of the expanding universe. As a visual representation of one of the earliest models of the universe to be…▽ MoreEinstein's blackboard is a well-known exhibit at the History of Science Museum atOxfordUniversity. However, it is much less well known that the writing on the board provides a neat summary of a work of historic importance, Einstein's 1931 model of the expanding universe. As a visual representation of one of the earliest models of the universe to be proposed in the wake of Hubble's observations of the nebulae, the blackboard provides an intriguing snapshot of a key moment in modern astronomy and cosmology. In addition, one line on the blackboard that is not in Einstein's 1931 paper casts useful light on some anomalies in the calculations of that paper.△ Less"
"Artificial Intelligence and Structural Injustice: Foundations for Equity, Values, and Responsibility","Authors:Johannes Himmelreich,Désirée Lim","Abstract:This chapter argues for a structural injustice approach to the governance of AI. Structural injustice has an analytical and an evaluative component. The analytical component consists of structural explanations that are well-known in the social sciences. The evaluative component is a theory of justice. Structural injustice is a powerful conceptual tool that allows researchers and practitioners to i…▽ MoreThis chapter argues for a structural injustice approach to the governance of AI. Structural injustice has an analytical and an evaluative component. The analytical component consists of structural explanations that are well-known in the social sciences. The evaluative component is a theory of justice. Structural injustice is a powerful conceptual tool that allows researchers and practitioners to identify, articulate, and perhaps even anticipate, AI biases. The chapter begins with an example of racial bias in AI that arises from structural injustice. The chapter then presents the concept of structural injustice as introduced by the philosopher Iris Marion Young. The chapter moreover argues that structural injustice is well suited as an approach to the governance of AI and compares this approach to alternative approaches that start from analyses of harms and benefits or from value statements. The chapter suggests that structural injustice provides methodological and normative foundations for the values and concerns of Diversity, Equity, and Inclusion. The chapter closes with an outlook onto the idea of structure and on responsibility. The idea of a structure is central to justice. An open theoretical research question is to what extent AI is itself part of the structure of society. Finally, the practice of responsibility is central to structural injustice. Even if they cannot be held responsible for the existence of structural injustice, every individual and every organization has some responsibility to address structural injustice going forward.△ Less"
Forecasting Electricity Prices,"Authors:Katarzyna Maciejowska,Bartosz Uniejewski,Rafał Weron","Abstract:Forecasting electricity prices is a challenging task and an active area of research since the 1990s and the deregulation of the traditionally monopolistic and government-controlled power sectors. Although it aims at predicting both spot and forward prices, the vast majority of research is focused on short-term horizons which exhibit dynamics unlike in any other market. The reason is that power sys…▽ MoreForecasting electricity prices is a challenging task and an active area of research since the 1990s and the deregulation of the traditionally monopolistic and government-controlled power sectors. Although it aims at predicting both spot and forward prices, the vast majority of research is focused on short-term horizons which exhibit dynamics unlike in any other market. The reason is that power system stability calls for a constant balance between production and consumption, while being weather (both demand and supply) and business activity (demand only) dependent. The recent market innovations do not help in this respect. The rapid expansion of intermittent renewable energy sources is not offset by the costly increase of electricity storage capacities and modernization of the grid infrastructure. On the methodological side, this leads to three visible trends in electricity price forecasting research as of 2022. Firstly, there is a slow, but more noticeable with every year, tendency to consider not only point but also probabilistic (interval, density) or even path (also called ensemble) forecasts. Secondly, there is a clear shift from the relatively parsimonious econometric (or statistical) models towards more complex and harder to comprehend, but more versatile and eventually more accurate statistical/machine learning approaches. Thirdly, statistical error measures are nowadays regarded as only the first evaluation step. Since they may not necessarily reflect the economic value of reducing prediction errors, more and more often, they are complemented by case studies comparing profits from scheduling or trading strategies based on price forecasts obtained from different models.△ Less"
Recurrent Affine Transformation for Text-to-image Synthesis,"Authors:Senmao Ye,Fei Liu,Minkui Tan","Abstract:…discriminator. Being aware of matching image regions, text descriptions supervise the generator to synthesize more relevant image contents. Extensive experiments on the CUB,Oxford-102 and COCO datasets demonstrate the superiority of the proposed model in comparison to state-of-the-art models \footnote{https://github.com/senmaoy/Recurrent-Affine-Transformati…▽ MoreText-to-image synthesis aims to generate natural images conditioned on text descriptions. The main difficulty of this task lies in effectively fusing text information into the image synthesis process. Existing methods usually adaptively fuse suitable text information into the synthesis process with multiple isolated fusion blocks (e.g., Conditional
  Batch Normalization and Instance Normalization). However, isolated fusion blocks not only conflict with each other but also increase the difficulty of training (see first page of the supplementary). To address these issues, we propose a Recurrent Affine Transformation (RAT) for Generative Adversarial Networks that connects all the fusion blocks with a recurrent neural network to model their long-term dependency. Besides, to improve semantic consistency between texts and synthesized images, we incorporate a spatial attention model in the discriminator. Being aware of matching image regions, text descriptions supervise the generator to synthesize more relevant image contents. Extensive experiments on the CUB,Oxford-102 and COCO datasets demonstrate the superiority of the proposed model in comparison to state-of-the-art models \footnote{https://github.com/senmaoy/Recurrent-Affine-Transformation-for-Text-to-image-Synthesis.git}△ Less"
The importance of X-ray frequency in driving photoevaporative winds,"Authors:Andrew D. Sellek,Cathie J. Clarke,Barbara Ercolano","Abstract:Photoevaporative winds are a promising mechanism for dispersing protoplanetary discs, but so far theoretical models have been unable to agree on the relative roles that the X-ray, Extreme Ultraviolet or Far Ultraviolet play in driving the winds. This has been attributed to a variety of methodological differences between studies, including their approach to radiative transfer and thermal balance, t…▽ MorePhotoevaporative winds are a promising mechanism for dispersing protoplanetary discs, but so far theoretical models have been unable to agree on the relative roles that the X-ray, Extreme Ultraviolet or Far Ultraviolet play in driving the winds. This has been attributed to a variety of methodological differences between studies, including their approach to radiative transfer and thermal balance, the choice of irradiating spectrum employed, and the processes available to cool the gas. We use the \textsc{mocassin} radiative transfer code to simulate wind heating for a variety of spectra on a static density grid taken from simulations of an EUV-driven wind. We explore the impact of choosing a single representative X-ray frequency on their ability to drive a wind by measuring the maximum heated column as a function of photon energy. We demonstrate that for reasonable luminosities and spectra, the most effective energies are at a few $100~\mathrm{eV}$, firmly in the softer regions of the X-ray spectrum, while X-rays with energies $\sim1000~\mathrm{eV}$ interact too weakly with disc gas to provide sufficient heating to drive a wind. We develop a simple model to explain these findings. We argue that further increases in the cooling above our models - for example due to molecular rovibrational lines - may further restrict the heating to the softer energies but are unlikely to prevent X-ray heated winds from launching entirely; increasing the X-ray luminosity has the opposite effect. The various results of photoevaporative wind models should therefore be understood in terms of the choice of irradiating spectrum.△ Less"
Practical considerations for specifying a super learner,"Authors:Rachael V. Phillips,Mark J. van der Laan,Hana Lee,Susan Gruber","Abstract:Common tasks encountered in epidemiology, including disease incidence estimation and causal inference, rely on predictive modeling. Constructing a predictive model can be thought of as learning a prediction function, i.e., a function that takes as input covariate data and outputs a predicted value. Many strategies for learning these functions from data are available, from parametric regressions to…▽ MoreCommon tasks encountered in epidemiology, including disease incidence estimation and causal inference, rely on predictive modeling. Constructing a predictive model can be thought of as learning a prediction function, i.e., a function that takes as input covariate data and outputs a predicted value. Many strategies for learning these functions from data are available, from parametric regressions to machine learning algorithms. It can be challenging to choose an approach, as it is impossible to know in advance which one is the most suitable for a particular dataset and prediction task at hand. The super learner (SL) is an algorithm that alleviates concerns over selecting the one ""right"" strategy while providing the freedom to consider many of them, such as those recommended by collaborators, used in related research, or specified by subject-matter experts. It is an entirely pre-specified and data-adaptive strategy for predictive modeling. To ensure the SL is well-specified for learning the prediction function, the analyst does need to make a few important choices. In this Education Corner article, we provide step-by-step guidelines for making these choices, walking the reader through each of them and providing intuition along the way. In doing so, we aim to empower the analyst to tailor the SL specification to their prediction task, thereby ensuring their SL performs as well as possible. A flowchart provides a concise, easy-to-follow summary of key suggestions and heuristics, based on our accumulated experience, and guided by theory.△ Less"
HiTPR: Hierarchical Transformer for Place Recognition in Point Cloud,"Authors:Zhixing Hou,Yan Yan,Chengzhong Xu,Hui Kong","Abstract:…Experiments on several standard benchmarks demonstrate the superiority of the HiTPR in terms of average recall rate, achieving 93.71% at top 1% and 86.63% at top 1 on theOxfordRobotCar dataset for example.▽ MorePlace recognition or loop closure detection is one of the core components in a full SLAM system. In this paper, aiming at strengthening the relevancy of local neighboring points and the contextual dependency among global points simultaneously, we investigate the exploitation of transformer-based network for feature extraction, and propose a Hierarchical Transformer for Place Recognition (HiTPR). The HiTPR consists of four major parts: point cell generation, short-range transformer (SRT), long-range transformer (LRT) and global descriptor aggregation. Specifically, the point cloud is initially divided into a sequence of small cells by downsampling and nearest neighbors searching. In the SRT, we extract the local feature for each point cell. While in the LRT, we build the global dependency among all of the point cells in the whole point cloud. Experiments on several standard benchmarks demonstrate the superiority of the HiTPR in terms of average recall rate, achieving 93.71% at top 1% and 86.63% at top 1 on theOxfordRobotCar dataset for example.△ Less"
ManiTrans: Entity-Level Text-Guided Image Manipulation via Token-wise Semantic Alignment and Generation,"Authors:Jianan Wang,Guansong Lu,Hang Xu,Zhenguo Li,Chunjing Xu,Yanwei Fu","Abstract:…image regions to be manipulated, and a semantic loss to help align the relationship between the vision and language. We conduct extensive experiments on the real datasets, CUB,Oxford, and COCO datasets to verify that our method can distinguish the relevant and irrelevant regions and achieve more precise and flexible manipulation compared with baseline metho…▽ MoreExisting text-guided image manipulation methods aim to modify the appearance of the image or to edit a few objects in a virtual or simple scenario, which is far from practical application. In this work, we study a novel task on text-guided image manipulation on the entity level in the real world. The task imposes three basic requirements, (1) to edit the entity consistent with the text descriptions, (2) to preserve the text-irrelevant regions, and (3) to merge the manipulated entity into the image naturally. To this end, we propose a new transformer-based framework based on the two-stage image synthesis method, namely \textbf{ManiTrans}, which can not only edit the appearance of entities but also generate new entities corresponding to the text guidance. Our framework incorporates a semantic alignment module to locate the image regions to be manipulated, and a semantic loss to help align the relationship between the vision and language. We conduct extensive experiments on the real datasets, CUB,Oxford, and COCO datasets to verify that our method can distinguish the relevant and irrelevant regions and achieve more precise and flexible manipulation compared with baseline methods. The project homepage is \url{https://jawang19.github.io/manitrans}.△ Less"
Local Knowledge and Natural Resource Management in a Peasant Farming Community Facing Rapid Change: A Critical Examination,Authors:Jules R. Siedenburg,"Abstract:Environmental degradation is a major global problem. Its impacts are not just environmental, but also economic, with degradation recognised as a key cause of reduced agricultural productivity and rural poverty in the developing world. The degradation literature typically emphasises common property or open access natural resources, and how perverse incentives or missing institutions lead optimising…▽ MoreEnvironmental degradation is a major global problem. Its impacts are not just environmental, but also economic, with degradation recognised as a key cause of reduced agricultural productivity and rural poverty in the developing world. The degradation literature typically emphasises common property or open access natural resources, and how perverse incentives or missing institutions lead optimising private actors to degrade them. By contrast, the present paper considers degradation occurring on private farms in peasant communities. This is a critical yet delicate issue, given the poverty of such areas and questions about the role of farmers in either degrading or regenerating rural lands. The paper examines natural resource management by peasant farmers in Tanzania. Its key concern is how the local knowledge informing their management decisions adapts to challenges associated with environmental degradation and market liberalisation. Given their poverty, this question could have direct implications for the capacity of households to meet their livelihood needs. Based on fresh empirical data, the paper finds that differential farmer knowledge helps explain the large differences in how households respond to the degradation challenge. The implication is that some farmers adapt more effectively to emerging challenges than others, despite all being rational, optimising agents who follow the strategies they deem best. The paper thus provides a critique of local knowledge, implying that some farmers experience adaptation slippages while others race ahead with effective adaptations. The paper speaks to the chronic poverty that plagues many rural communities in the developing world. It helps explain the failure of proven sustainable agriculture technologies to disseminate readily beyond early innovators. Its key policy implication is to inform improved capacity building for such communities.△ Less"
Quantum Dots / Spin Qubits,Authors:Shannon Harvey,"Abstract:Spin qubits in semiconductor quantum dots represent a prominent family of solid-state qubits in the effort to build a quantum computer. They are formed when electrons or holes are confined in a static potential well in a semiconductor, giving them a quantized energy spectrum. The simplest spin qubit is a single electron spin located in a quantum dot, but many additional varieties have been develop…▽ MoreSpin qubits in semiconductor quantum dots represent a prominent family of solid-state qubits in the effort to build a quantum computer. They are formed when electrons or holes are confined in a static potential well in a semiconductor, giving them a quantized energy spectrum. The simplest spin qubit is a single electron spin located in a quantum dot, but many additional varieties have been developed, some containing multiple spins in multiple quantum dots, each of which has different benefits and drawbacks. While these spins act as simple quantum systems in many ways, they also experience complex effects due to their semiconductor environment. They can be controlled by both magnetic and electric fields depending on their configuration and are therefore dephased by magnetic and electric field noise, with different types of spin qubits having different control mechanisms and noise susceptibilities. While initial experiments were primarily performed in gallium arsenide (GaAs) based materials, silicon qubits have developed substantially and research on qubits in metal-oxide-semiconductor (Si-MOS), silicon/silicon germanium (Si/SiGe) heterostructures, and donors in silicon is also being pursued. An increasing number of spin qubit varieties have attained error rates that are low enough to be compatible with quantum error correction for single-qubit gates and two-qubit gates have been performed in several with success rates, or fidelities, of 90-95%.△ Less"
Combined spectroscopy and intensity interferometry to determine the distances of the blue supergiants P Cygni and Rigel,"Authors:E. S. G. de Almeida,Mathilde Hugbart,Armando Domiciano de Souza,Jean-Pierre Rivet,Farrokh Vakili,Antonin Siciak,Guillaume Labeyrie,Olivier Garde,Nolan Matthews,Olivier Lai,David Vernet,Robin Kaiser,William Guerin","Abstract:In this paper we report on spatial intensity interferometry measurements within the H$α$ line on two stars: the Luminous Blue Variable supergiant \PCygni\,and the late-type B supergiant Rigel. The experimental setup was upgraded to allow simultaneous measurement of two polarization channels, instead of one in our previous setup, and the zero baseline correlation function on-sky to validate indepen…▽ MoreIn this paper we report on spatial intensity interferometry measurements within the H$α$ line on two stars: the Luminous Blue Variable supergiant \PCygni\,and the late-type B supergiant Rigel. The experimental setup was upgraded to allow simultaneous measurement of two polarization channels, instead of one in our previous setup, and the zero baseline correlation function on-sky to validate independent estimates obtained from the stellar spectrum and the instrumental spectral throughput. Combined with simultaneous spectra measurements and based on radiative transfer models calculated with the code CMFGEN, we were able to fit our measured visibility curves to extract the stellar distances. Our distance determinations for both \PCygni\ (1.61 $\pm$ 0.18 kpc) and Rigel (0.26 $\pm$ 0.02 kpc) agree very well with the values provided by astrometry with the Gaia and Hipparcos missions, respectively. This result for Rigel was obtained by adopting a stellar luminosity of $L_{\star}$ = 123000 $L_{\odot}$, which is reported in the literature as being consistent with the Hipparcos distance to Rigel. However, due to the lack of consensus on Rigel's luminosity, we also explore how the adoption of the stellar luminosity in our models affects our distance determination for Rigel. In conclusion, we support, in an independent way, the distance to Rigel as the one provided by the Hipparcos mission, when taking the luminosity of 123000 $L_{\odot}$ at face value. This study is the first successful step towards extending the application of the Wind Momentum Luminosity Relation method for distance calibration from an LBV supergiant to a more normal late-type B supergiant.△ Less"
Kepler K2 Campaign 9: II. First space-based discovery of an exoplanet using microlensing,"Authors:D. Specht,R. Poleski,M. T. Penny,E. Kerins,I. McDonald,Chung-Uk Lee,A. Udalski,I. A. Bond,Y. Shvartzvald,Weicheng Zang,R. A. Street,D. W. Hogg,B. S. Gaudi,T. Barclay,G. Barentsen,S. B. Howell,F. Mullally,C. B. Henderson,S. T. Bryson,D. A. Caldwell,M. R. Haas,J. E. Van Cleve,K. Larson,K. McCalmont,C. Peterson, et al. (61 additional authors not shown)","Abstract:We present K2-2016-BLG-0005Lb, a densely sampled, planetary binary caustic-crossing microlensing event found from a blind search of data gathered from Campaign 9 of the Kepler K2 mission (K2C9). K2-2016-BLG-0005Lb is the first bound microlensing exoplanet discovered from space-based data. The event has caustic entry and exit points that are resolved in the K2C9 data, enabling the lens--source rela…▽ MoreWe present K2-2016-BLG-0005Lb, a densely sampled, planetary binary caustic-crossing microlensing event found from a blind search of data gathered from Campaign 9 of the Kepler K2 mission (K2C9). K2-2016-BLG-0005Lb is the first bound microlensing exoplanet discovered from space-based data. The event has caustic entry and exit points that are resolved in the K2C9 data, enabling the lens--source relative proper motion to be measured. We have fitted a binary microlens model to the Kepler data, and to simultaneous observations from multiple ground-based surveys. Whilst the ground-based data only sparsely sample the binary caustic, they provide a clear detection of parallax that allows us to break completely the microlensing mass--position--velocity degeneracy and measure the planet's mass directly. We find a host mass of $0.58\pm0.04 ~{\rm M}_\odot$ and a planetary mass of $1.1\pm0.1 ~{\rm M_J}$. The system lies at a distance of $5.2\pm0.2~$kpc from Earth towards the Galactic bulge, more than twice the distance of the previous most distant planet found by Kepler. The sky-projected separation of the planet from its host is found to be $4.2\pm0.3~$au which, for circular orbits, deprojects to a host separation $a = 4.4^{+1.9}_{-0.4}~$au and orbital period $P = 13^{+9}_{-2}~$yr. This makes K2-2016-BLG-0005Lb a close Jupiter analogue orbiting a low-mass host star. According to current planet formation models, this system is very close to the host mass threshold below which Jupiters are not expected to form. Upcoming space-based exoplanet microlensing surveys by NASA's Nancy Grace Roman Space Telescope and, possibly, ESA's Euclid mission, will provide demanding tests of current planet formation models.△ Less"
The complex of free factors of a free group,"Authors:Allen Hatcher,Karen Vogtmann",Abstract:This paper corrects an error in a proof in the original version of the paper published in 1998 in theOxfordQuarterly. The main theorem remains the same: The geometric realization of the partially ordered set of proper free factors in a finitely generated free group of rank $n$ is homotopy equivalent to a wedge of spheres of dimension $n-2$.This paper corrects an error in a proof in the original version of the paper published in 1998 in theOxfordQuarterly. The main theorem remains the same: The geometric realization of the partially ordered set of proper free factors in a finitely generated free group of rank $n$ is homotopy equivalent to a wedge of spheres of dimension $n-2$.△ Less
Improved Lieb-Oxfordbound on the indirect and exchange energies,"Authors:Mathieu Lewin,Elliott H. Lieb,Robert Seiringer","Abstract:The Lieb-Oxfordinequality provides a lower bound on the Coulomb energy of a classical system of $N$ identical charges only in terms of their one-particle density. We prove here a new estimate on the best constant in this inequality. Numerical evaluation provides the value 1.58, which is a significant improvement to the previously known value 1.64. The best…▽ MoreThe Lieb-Oxfordinequality provides a lower bound on the Coulomb energy of a classical system of $N$ identical charges only in terms of their one-particle density. We prove here a new estimate on the best constant in this inequality. Numerical evaluation provides the value 1.58, which is a significant improvement to the previously known value 1.64. The best constant has recently been shown to be larger than 1.44. In a second part, we prove that the constant can be reduced to 1.25 when the inequality is restricted to Hartree-Fock states. This is the first proof that the exchange term is always much lower than the full indirect Coulomb energy.△ Less"
Belle II Executive Summary,"Authors:D. M. Asner,H. Atmacan,Sw. Banerjee,J. V. Bennett,M. Bertemes,M. Bessner,D. Biswas,G. Bonvicini,N. Brenny,R. A. Briere,T. E. Browder,C. Chen,S. Choudhury,D. Cinabro,J. Cochran,L. M. Cremaldi,A. Di Canto,S. Dubey,K. Flood,B. G. Fulsom,V. Gaur,R. Godang,T. Gu,Y. Guan,J. Guilliams, et al. (56 additional authors not shown)","Abstract:Belle II is a Super $B$ Factory experiment, expected to record 50 ab$^{-1}$ of $e^+e^-$ collisions at the SuperKEKB accelerator until 2035. The large samples of $B$ mesons, charm hadrons, and tau leptons produced in the clean experimental environment of $e^+e^-$ collisions will provide the basis of a broad and unique flavor-physics program. Belle II will pursue physics beyond the Standard Model in…▽ MoreBelle II is a Super $B$ Factory experiment, expected to record 50 ab$^{-1}$ of $e^+e^-$ collisions at the SuperKEKB accelerator until 2035. The large samples of $B$ mesons, charm hadrons, and tau leptons produced in the clean experimental environment of $e^+e^-$ collisions will provide the basis of a broad and unique flavor-physics program. Belle II will pursue physics beyond the Standard Model in many ways, for example: improving the precision of weak interaction parameters, particularly Cabibbo-Kobayashi-Maskawa (CKM) matrix elements and phases, and thus more rigorously test the CKM paradigm, measuring lepton-flavor-violating parameters, and performing unique searches for missing-mass dark matter events. Many key measurements will be made with world-leading precision.△ Less"
Constraining Tidal Quality Factor using Spin Period in Eclipsing Binaries,"Authors:Ruskin Patel,Kaloyan Penev","Abstract:Evolution of binary objects under the influence of tides drastically affects the expected observational properties of the system. With the discovery of a large number of close-in hot Jupiter systems and eclipsing binaries from missions such as Kepler and TESS, it has become imperative to understand the extent of tidal influence on their formation and observed properties. In the case of binary syst…▽ MoreEvolution of binary objects under the influence of tides drastically affects the expected observational properties of the system. With the discovery of a large number of close-in hot Jupiter systems and eclipsing binaries from missions such as Kepler and TESS, it has become imperative to understand the extent of tidal influence on their formation and observed properties. In the case of binary systems, an efficient tidal dissipation can lead to either spin up or spin down of the stars and/or spin-orbit synchronization, depending upon the exchange of angular momentum between the star and the orbit. We combine the eclipsing binary systems from the Kepler mission with stellar and orbital parameters available in the literature to create a catalog of 41 eclipsing binaries suitable for analysis of tidal dissipation. Empirically, the efficiency of tidal dissipation is parameterized using a modified Tidal Quality Factor($Q_{\star}^{'}$). We find constraints on $Q_{\star}^{'}$ using the observed rotation period of the primary star in the eclipsing binary systems. We calculate detailed evolutions of binary systems under the combined influence of tides, stellar evolution, and loss of stellar angular momentum to magnetic winds, and perform Markov Chain Monte Carlo simulations to account for the uncertainties in the observed data. Our analysis shows that $\log_{10}{Q^{'}_{\star}}=7.818\pm0.035$ can reproduce the observed primary star spin in almost all systems in our sample.△ Less"
Quantum Volume in Practice: What Users Can Expect from NISQ Devices,"Authors:Elijah Pelofske,Andreas Bärtschi,Stephan Eidenbenz","Abstract:…QV values are often reported by NISQ providers for their systems, we perform our own series of QV calculations on 24 NISQ devices currently offered by IBM Q, IonQ, Rigetti,OxfordQuantum Circuits, and Quantinuum (formerly Honeywell). Our approach characterizes the performances that an advanced user of these NISQ devices can expect to achieve with a reasonab…▽ MoreQuantum volume (QV) has become the de-facto standard benchmark to quantify the capability of Noisy Intermediate-Scale Quantum (NISQ) devices. While QV values are often reported by NISQ providers for their systems, we perform our own series of QV calculations on 24 NISQ devices currently offered by IBM Q, IonQ, Rigetti,OxfordQuantum Circuits, and Quantinuum (formerly Honeywell). Our approach characterizes the performances that an advanced user of these NISQ devices can expect to achieve with a reasonable amount of optimization, but without white-box access to the device. In particular, we compile QV circuits to standard gate sets of the vendor using compiler optimization routines where available, and we perform experiments across different qubit subsets. We find that running QV tests requires very significant compilation cycles, QV values achieved in our tests typically lag behind officially reported results and also depend significantly on the classical compilation effort invested.△ Less"
Maximal Hörmander Functional Calculus on Lp Spaces and UMD Lattices,"Authors:Luc Deleaval,Christoph Kriegler","Abstract:Let $A$ be a generator of an analytic semigroup having a H{ö}rmander functional calculus on $X = L^p(Ω,Y)$, where $Y$ is a UMD lattice. Using methods from Banach space geometry in connection with functional calculus, we show that for H{ö}rmander spectral multipliers decaying sufficiently fast at $\infty$, there holds a maximal estimate…▽ MoreLet $A$ be a generator of an analytic semigroup having a H{ö}rmander functional calculus on $X = L^p(Ω,Y)$, where $Y$ is a UMD lattice. Using methods from Banach space geometry in connection with functional calculus, we show that for H{ö}rmander spectral multipliers decaying sufficiently fast at $\infty$, there holds a maximal estimate $\| \sup_{t \geq 0} |m(tA)f|\, \|_{L^p(Ω,Y)} \lesssim \|f\|_{L^p(Ω,Y)}$. We also show square function estimates $\left\| \left( \sum_k \sup _{t \geq 0} |m_k(tA)f_k|^2 \right)^{\frac12} \right\|_{L^p(Ω,Y)} \lesssim \left\| \left( \sum _k |f_k|^2 \right)^{\frac12} \right\|_{L^p(Ω,Y)}$ for suitable families of spectral multipliers $m_k$, which are even new for the euclidean Laplacian on scalar valued $L^p(\mathbb{R}^d)$. As corollaries, we obtain maximal estimates for wave propagators and Bochner--Riesz means. Finally, we illustrate the results by giving several examples of operators $A$ that admit a H{ö}rmander functional calculus on some $L^p(Ω,Y)$ and discuss examples of lattices $Y$ and non-self-adjoint operators $A$ fitting our context.△ Less"
Real-Time Hybrid Mapping of Populated Indoor Scenes using a Low-Cost Monocular UAV,"Authors:Stuart Golodetz,Madhu Vankadari,Aluna Everitt,Sangyun Shin,Andrew Markham,Niki Trigoni","Abstract:…our component-level design choices via extensive experiments on the large-scale ScanNet and GTA-IM datasets. To evaluate our system-level performance, we also construct a newOxfordHybrid Mapping dataset of populated indoor scenes.▽ MoreUnmanned aerial vehicles (UAVs) have been used for many applications in recent years, from urban search and rescue, to agricultural surveying, to autonomous underground mine exploration. However, deploying UAVs in tight, indoor spaces, especially close to humans, remains a challenge. One solution, when limited payload is required, is to use micro-UAVs, which pose less risk to humans and typically cost less to replace after a crash. However, micro-UAVs can only carry a limited sensor suite, e.g. a monocular camera instead of a stereo pair or LiDAR, complicating tasks like dense mapping and markerless multi-person 3D human pose estimation, which are needed to operate in tight environments around people. Monocular approaches to such tasks exist, and dense monocular mapping approaches have been successfully deployed for UAV applications. However, despite many recent works on both marker-based and markerless multi-UAV single-person motion capture, markerless single-camera multi-person 3D human pose estimation remains a much earlier-stage technology, and we are not aware of existing attempts to deploy it in an aerial context. In this paper, we present what is thus, to our knowledge, the first system to perform simultaneous mapping and multi-person 3D human pose estimation from a monocular camera mounted on a single UAV. In particular, we show how to loosely couple state-of-the-art monocular depth estimation and monocular 3D human pose estimation approaches to reconstruct a hybrid map of a populated indoor scene in real time. We validate our component-level design choices via extensive experiments on the large-scale ScanNet and GTA-IM datasets. To evaluate our system-level performance, we also construct a newOxfordHybrid Mapping dataset of populated indoor scenes.△ Less"
Gene-Environment Interplay in the Social Sciences,"Authors:Rita Dias Pereira,Pietro Biroli,Titus Galama,Stephanie von Hinke,Hans van Kippersluis,Cornelius A. Rietveld,Kevin Thom","Abstract:Nature (one's genes) and nurture (one's environment) jointly contribute to the formation and evolution of health and human capital over the life cycle. This complex interplay between genes and environment can be estimated and quantified using genetic information readily available in a growing number of social science data sets. Using genetic data to improve our understanding of individual decision…▽ MoreNature (one's genes) and nurture (one's environment) jointly contribute to the formation and evolution of health and human capital over the life cycle. This complex interplay between genes and environment can be estimated and quantified using genetic information readily available in a growing number of social science data sets. Using genetic data to improve our understanding of individual decision making, inequality, and to guide public policy is possible and promising, but requires a grounding in essential genetic terminology, knowledge of the literature in economics and social-science genetics, and a careful discussion of the policy implications and prospects of the use of genetic data in the social sciences and economics.△ Less"
InCloud: Incremental Learning for Point Cloud Place Recognition,"Authors:Joshua Knights,Peyman Moghadam,Milad Ramezani,Sridha Sridharan,Clinton Fookes","Abstract:…which preserves the higher-order structure of the network's embedding space. We introduce several challenging new benchmarks on four popular and large-scale LiDAR datasets (Oxford, MulRan, In-house and KITTI) showing broad improvements in point cloud place recognition performance over a variety of network architectures. To the best of our knowledge, this…▽ MorePlace recognition is a fundamental component of robotics, and has seen tremendous improvements through the use of deep learning models in recent years. Networks can experience significant drops in performance when deployed in unseen or highly dynamic environments, and require additional training on the collected data. However naively fine-tuning on new training distributions can cause severe degradation of performance on previously visited domains, a phenomenon known as catastrophic forgetting. In this paper we address the problem of incremental learning for point cloud place recognition and introduce InCloud, a structure-aware distillation-based approach which preserves the higher-order structure of the network's embedding space. We introduce several challenging new benchmarks on four popular and large-scale LiDAR datasets (Oxford, MulRan, In-house and KITTI) showing broad improvements in point cloud place recognition performance over a variety of network architectures. To the best of our knowledge, this work is the first to effectively apply incremental learning for point cloud place recognition. Data pre-processing, training and evaluation code for this paper can be found at https://github.com/csiro-robotics/InCloud.△ Less"
Fast-MbyM: Leveraging Translational Invariance of the Fourier Transform for Efficient and Accurate Radar Odometry,"Authors:Robert Weston,Matthew Gadd,Daniele De Martini,Paul Newman,Ingmar Posner","Abstract:…remains accurate and competitive with the best radar odometry variants available in the literature -- achieving an end-point drift of 2.01% in translation and 6.3deg/km on theOxfordRadar RobotCar Dataset.▽ MoreMasking By Moving (MByM), provides robust and accurate radar odometry measurements through an exhaustive correlative search across discretised pose candidates. However, this dense search creates a significant computational bottleneck which hinders real-time performance when high-end GPUs are not available. Utilising the translational invariance of the Fourier Transform, in our approach, f-MByM, we decouple the search for angle and translation. By maintaining end-to-end differentiability a neural network is used to mask scans and trained by supervising pose prediction directly. Training faster and with less memory, utilising a decoupled search allows f-MByM to achieve significant run-time performance improvements on a CPU (168%) and to run in real-time on embedded devices, in stark contrast to MByM. Throughout, our approach remains accurate and competitive with the best radar odometry variants available in the literature -- achieving an end-point drift of 2.01% in translation and 6.3deg/km on theOxfordRadar RobotCar Dataset.△ Less"
Bridge the Gap between Supervised and Unsupervised Learning for Fine-Grained Classification,"Authors:Jiabao Wang,Yang Li,Xiu-Shen Wei,Hang Li,Zhuang Miao,Rui Zhang","Abstract:…labels with inevitable noise, which can improve the optimization process of learning the parameters of the network. The effectiveness of our UFCL is verified on CUB-200-2011,Oxford-Flowers,Oxford-Pets, Stanford-Dogs, Stanford-Cars and FGVC-Aircraft datasets. Under the unsupervised FGVC setting, we achieve state-of-th…▽ MoreUnsupervised learning technology has caught up with or even surpassed supervised learning technology in general object classification (GOC) and person re-identification (re-ID). However, it is found that the unsupervised learning of fine-grained visual classification (FGVC) is more challenging than GOC and person re-ID. In order to bridge the gap between unsupervised and supervised learning for FGVC, we investigate the essential factors (including feature extraction, clustering, and contrastive learning) for the performance gap between supervised and unsupervised FGVC. Furthermore, we propose a simple, effective, and practical method, termed as UFCL, to alleviate the gap. Three key issues are concerned and improved: First, we introduce a robust and powerful backbone, ResNet50-IBN, which has an ability of domain adaptation when we transfer ImageNet pre-trained models to FGVC tasks. Next, we propose to introduce HDBSCAN instead of DBSCAN to do clustering, which can generate better clusters for adjacent categories with fewer hyper-parameters. Finally, we propose a weighted feature agent and its updating mechanism to do contrastive learning by using the pseudo labels with inevitable noise, which can improve the optimization process of learning the parameters of the network. The effectiveness of our UFCL is verified on CUB-200-2011,Oxford-Flowers,Oxford-Pets, Stanford-Dogs, Stanford-Cars and FGVC-Aircraft datasets. Under the unsupervised FGVC setting, we achieve state-of-the-art results, and analyze the key factors and the important parameters to provide a practical guidance.△ Less"
Some questions about Conformal Cyclic Cosmology,Authors:Paul Tod,Abstract:This article is an extended version of a talk given inOxfordin June 2021 as part of an online meeting `Ninety minutes of CCC' to mark the 90th birthday of Sir Roger Penrose. I assemble some questions that I have been asked or have asked myself about CCC.This article is an extended version of a talk given inOxfordin June 2021 as part of an online meeting `Ninety minutes of CCC' to mark the 90th birthday of Sir Roger Penrose. I assemble some questions that I have been asked or have asked myself about CCC.△ Less
The Lieb-Oxfordbound and the optimal transport limit of DFT,"Authors:Michael Seidl,Tarik Benyahia,Derk P. Kooi,Paola Gori-Giorgi",Abstract:We review and illustrate with several examples the connection between the search for lower bounds for the optimal constant in the Lieb-Oxfordinequality and the optimal transport limit (or strictly-correlated-electrons limit) of density functional theory. We focus in particular on several contributions from Elliott Lieb which already hinted at this connectio…▽ MoreWe review and illustrate with several examples the connection between the search for lower bounds for the optimal constant in the Lieb-Oxfordinequality and the optimal transport limit (or strictly-correlated-electrons limit) of density functional theory. We focus in particular on several contributions from Elliott Lieb which already hinted at this connection.△ Less
Benchmarking missing-values approaches for predictive models on health databases,"Authors:Alexandre Perez-Lebel,Gaël Varoquaux,Marine Le Morvan,Julie Josse,Jean-Baptiste Poline","Abstract:BACKGROUND: As databases grow larger, it becomes harder to fully control their collection, and they frequently come with missing values: incomplete observations. These large databases are well suited to train machine-learning models, for instance for forecasting or to extract biomarkers in biomedical settings. Such predictive approaches can use discriminative -- rather than generative -- modeling,…▽ MoreBACKGROUND: As databases grow larger, it becomes harder to fully control their collection, and they frequently come with missing values: incomplete observations. These large databases are well suited to train machine-learning models, for instance for forecasting or to extract biomarkers in biomedical settings. Such predictive approaches can use discriminative -- rather than generative -- modeling, and thus open the door to new missing-values strategies. Yet existing empirical evaluations of strategies to handle missing values have focused on inferential statistics. RESULTS: Here we conduct a systematic benchmark of missing-values strategies in predictive models with a focus on large health databases: four electronic health record datasets, a population brain imaging one, a health survey and two intensive care ones. Using gradient-boosted trees, we compare native support for missing values with simple and state-of-the-art imputation prior to learning. We investigate prediction accuracy and computational time. For prediction after imputation, we find that adding an indicator to express which values have been imputed is important, suggesting that the data are missing not at random. Elaborate missing values imputation can improve prediction compared to simple strategies but requires longer computational time on large data. Learning trees that model missing values-with missing incorporated attribute-leads to robust, fast, and well-performing predictive modeling. CONCLUSIONS: Native support for missing values in supervised machine learning predicts better than state-of-the-art imputation with much less computational cost. When using imputation, it is important to add indicator columns expressing which values have been imputed.△ Less"
System Safety and Artificial Intelligence,Authors:Roel I. J. Dobbe,"Abstract:This chapter formulates seven lessons for preventing harm in artificial intelligence (AI) systems based on insights from the field of system safety for software-based automation in safety-critical domains. New applications of AI across societal domains and public organizations and infrastructures come with new hazards, which lead to new forms of harm, both grave and pernicious. The text addresses…▽ MoreThis chapter formulates seven lessons for preventing harm in artificial intelligence (AI) systems based on insights from the field of system safety for software-based automation in safety-critical domains. New applications of AI across societal domains and public organizations and infrastructures come with new hazards, which lead to new forms of harm, both grave and pernicious. The text addresses the lack of consensus for diagnosing and eliminating new AI system hazards. For decades, the field of system safety has dealt with accidents and harm in safety-critical systems governed by varying degrees of software-based automation and decision-making. This field embraces the core assumption of systems and control that AI systems cannot be safeguarded by technical design choices on the model or algorithm alone, instead requiring an end-to-end hazard analysis and design frame that includes the context of use, impacted stakeholders and the formal and informal institutional environment in which the system operates. Safety and other values are then inherently socio-technical and emergent system properties that require design and control measures to instantiate these across the technical, social and institutional components of a system. This chapter honors system safety pioneer Nancy Leveson, by situating her core lessons for today's AI system safety challenges. For every lesson, concrete tools are offered for rethinking and reorganizing the safety management of AI systems, both in design and governance. This history tells us that effective AI safety management requires transdisciplinary approaches and a shared language that allows involvement of all levels of society.△ Less"
Attention-based Deep Neural Networks for Battery Discharge Capacity Forecasting,"Authors:Yadong Zhang,Chenye Zou,Xin Chen","Abstract:…error of the capacity estimation is 1.3 mAh. The mean absolute percentage error of the proposed DDN model is 0.06{\%}. The DDN model also performance well in theOxfordBattery Degradation Dataset with dynamic load profiles. Therefore, the high accuracy and strong robustness of the proposed algorithm are verified.▽ MoreBattery discharge capacity forecasting is critically essential for the applications of lithium-ion batteries. The capacity degeneration can be treated as the memory of the initial battery state of charge from the data point of view. The streaming sensor data collected by battery management systems (BMS) reflect the usable battery capacity degradation rates under various operational working conditions. The battery capacity in different cycles can be measured with the temporal patterns extracted from the streaming sensor data based on the attention mechanism. The attention-based similarity regarding the first cycle can describe the battery capacity degradation in the following cycles. The deep degradation network (DDN) is developed with the attention mechanism to measure similarity and predict battery capacity. The DDN model can extract the degeneration-related temporal patterns from the streaming sensor data and perform the battery capacity prediction efficiently online in real-time. Based on the MIT-Stanford open-access battery aging dataset, the root-mean-square error of the capacity estimation is 1.3 mAh. The mean absolute percentage error of the proposed DDN model is 0.06{\%}. The DDN model also performance well in theOxfordBattery Degradation Dataset with dynamic load profiles. Therefore, the high accuracy and strong robustness of the proposed algorithm are verified.△ Less"
Bias-Eliminated Semantic Refinement for Any-Shot Learning,"Authors:Liangjun Feng,Chunhui Zhao,Xi Li","Abstract:…and observe state-of-the-art results for any-shot learning; eg, we obtain 70.2% harmonic accuracy for the Caltech UCSD Birds (CUB) dataset and 82.2% harmonic accuracy for theOxfordFlowers (FLO) dataset in the standard GZSL setting. Various visualizations are also provided to show the bias-eliminated generation of SRWGAN. Our code is available.▽ MoreWhen training samples are scarce, the semantic embedding technique, ie, describing class labels with attributes, provides a condition to generate visual features for unseen objects by transferring the knowledge from seen objects. However, semantic descriptions are usually obtained in an external paradigm, such as manual annotation, resulting in weak consistency between descriptions and visual features. In this paper, we refine the coarse-grained semantic description for any-shot learning tasks, ie, zero-shot learning (ZSL), generalized zero-shot learning (GZSL), and few-shot learning (FSL). A new model, namely, the semantic refinement Wasserstein generative adversarial network (SRWGAN) model, is designed with the proposed multihead representation and hierarchical alignment techniques. Unlike conventional methods, semantic refinement is performed with the aim of identifying a bias-eliminated condition for disjoint-class feature generation and is applicable in both inductive and transductive settings. We extensively evaluate model performance on six benchmark datasets and observe state-of-the-art results for any-shot learning; eg, we obtain 70.2% harmonic accuracy for the Caltech UCSD Birds (CUB) dataset and 82.2% harmonic accuracy for theOxfordFlowers (FLO) dataset in the standard GZSL setting. Various visualizations are also provided to show the bias-eliminated generation of SRWGAN. Our code is available.△ Less"
On Neural Differential Equations,Authors:Patrick Kidger,"Abstract:The conjoining of dynamical systems and deep learning has become a topic of great interest. In particular, neural differential equations (NDEs) demonstrate that neural networks and differential equation are two sides of the same coin. Traditional parameterised differential equations are a special case. Many popular neural network architectures, such as residual networks and recurrent networks, are…▽ MoreThe conjoining of dynamical systems and deep learning has become a topic of great interest. In particular, neural differential equations (NDEs) demonstrate that neural networks and differential equation are two sides of the same coin. Traditional parameterised differential equations are a special case. Many popular neural network architectures, such as residual networks and recurrent networks, are discretisations.
  NDEs are suitable for tackling generative problems, dynamical systems, and time series (particularly in physics, finance, ...) and are thus of interest to both modern machine learning and traditional mathematical modelling. NDEs offer high-capacity function approximation, strong priors on model space, the ability to handle irregular data, memory efficiency, and a wealth of available theory on both sides.
  This doctoral thesis provides an in-depth survey of the field.
  Topics include: neural ordinary differential equations (e.g. for hybrid neural/mechanistic modelling of physical systems); neural controlled differential equations (e.g. for learning functions of irregular time series); and neural stochastic differential equations (e.g. to produce generative models capable of representing complex stochastic dynamics, or sampling from complex high-dimensional distributions).
  Further topics include: numerical methods for NDEs (e.g. reversible differential equations solvers, backpropagation through differential equations, Brownian reconstruction); symbolic regression for dynamical systems (e.g. via regularised evolution); and deep implicit models (e.g. deep equilibrium models, differentiable optimisation).
  We anticipate this thesis will be of interest to anyone interested in the marriage of deep learning with dynamical systems, and hope it will provide a useful reference for the current state of the art.△ Less"
Beyond synchronization: Body gestures and gaze direction in duo performance,"Authors:Laura Bishop,Carlos Cancino-Chacón,Werner Goebl","Abstract:In this chapter, we focus on two main categories of visual interaction: body gestures and gaze direction. Our focus on body gestures is motivated by research showing that gesture patterns often change during joint action tasks to become more predictable (van der Wel et al., 2016). Moreover, coordination sometimes emerges between musicians at the level of body sway (Chang et al., 2017). Our focus o…▽ MoreIn this chapter, we focus on two main categories of visual interaction: body gestures and gaze direction. Our focus on body gestures is motivated by research showing that gesture patterns often change during joint action tasks to become more predictable (van der Wel et al., 2016). Moreover, coordination sometimes emerges between musicians at the level of body sway (Chang et al., 2017). Our focus on gaze direction was motivated by the fact that gaze can serve simultaneously as a means of obtaining information about the world and as a means of communicating one's own attention and intent.△ Less"
A Fourier method for the determination of focus for telescopes with stars,"Authors:C. Y. Tan,B. Schulz","Abstract:We introduce a Fourier method (Fm) for the determination of best focus for telescopes with stars. Our method fits a power function, that we will derive in this paper, to a set of images taken as a function of focuser position. The best focus position is where the power is maximum. Fm was first tested with small refractor and Schmidt-Cassegrain (SCT) telescopes. After the successful small telescope…▽ MoreWe introduce a Fourier method (Fm) for the determination of best focus for telescopes with stars. Our method fits a power function, that we will derive in this paper, to a set of images taken as a function of focuser position. The best focus position is where the power is maximum. Fm was first tested with small refractor and Schmidt-Cassegrain (SCT) telescopes. After the successful small telescope tests, we then tested Fm with a 2 m Ritchey-Chrétien-Coudé (RCC). Our tests show that Fm is immune to the problems inherent in the popular half-flux diameter method.△ Less"
Indicative Image Retrieval: Turning Blackbox Learning into Grey,"Authors:Xulu Zhang,Zhenqun Yang,Hao Tian,Qing Li,Xiaoyong Wei","Abstract:…later for generating the matching evidence indications. It can improve the explainability of deep inference. Our method obtains a best performance in literature on bothOxford-5k and Paris-6k, and sets a new record of 97.77% onOxford-5k (97.81% on Paris-6k) without extracting any deep features.▽ MoreDeep learning became the game changer for image retrieval soon after it was introduced. It promotes the feature extraction (by representation learning) as the core of image retrieval, with the relevance/matching evaluation being degenerated into simple similarity metrics. In many applications, we need the matching evidence to be indicated rather than just have the ranked list (e.g., the locations of the target proteins/cells/lesions in medical images). It is like the matched words need to be highlighted in search engines. However, this is not easy to implement without explicit relevance/matching modeling. The deep representation learning models are not feasible because of their blackbox nature. In this paper, we revisit the importance of relevance/matching modeling in deep learning era with an indicative retrieval setting. The study shows that it is possible to skip the representation learning and model the matching evidence directly. By removing the dependency on the pre-trained models, it has avoided a lot of related issues (e.g., the domain gap between classification and retrieval, the detail-diffusion caused by convolution, and so on). More importantly, the study demonstrates that the matching can be explicitly modeled and backtracked later for generating the matching evidence indications. It can improve the explainability of deep inference. Our method obtains a best performance in literature on bothOxford-5k and Paris-6k, and sets a new record of 97.77% onOxford-5k (97.81% on Paris-6k) without extracting any deep features.△ Less"
Motor neuron pathology in CANVAS due to RFC1 expansions,"Authors:Vincent Huin,Giulia Coarelli,Clément Guemy,Susana Boluda,Rabab Debs,Fanny Mochel,Tanya Stojkovic,David Grabli,Thierry Maisonobe,Bertrand Gaymard,Timothée Lenglet,Céline Tard,Jean-Baptiste Davion,Bernard Sablonnière,Marie-Lorraine Monin,Claire Ewenczyk,Karine Viala,Perrine Charles,Isabelle Le Ber,Mary Reilly,Henry Houlden,Andrea Cortese,Danielle Seilhean,Alexis Brice,Alexandra Durr","Abstract:CANVAS caused by RFC1 biallelic expansions is a major cause of inherited sensory neuronopathy. Detection of RFC1 expansion is challenging and CANVAS can be associated with atypical features. We clinically and genetically characterized 50 patients, selected based on the presence of sensory neuronopathy confirmed by EMG. We screened RFC1 expansion by PCR, repeat-primed PCR, and Southern blotting of…▽ MoreCANVAS caused by RFC1 biallelic expansions is a major cause of inherited sensory neuronopathy. Detection of RFC1 expansion is challenging and CANVAS can be associated with atypical features. We clinically and genetically characterized 50 patients, selected based on the presence of sensory neuronopathy confirmed by EMG. We screened RFC1 expansion by PCR, repeat-primed PCR, and Southern blotting of long-range PCR products, a newly developed method. Neuropathological characterization was performed on the brain and spinal cord of one patient. Most patients (88%) carried a biallelic (AAGGG)n expansion in RFC1. In addition to the core CANVAS phenotype (sensory neuronopathy, cerebellar syndrome, and vestibular impairment), we observed chronic cough (97%), oculomotor signs (85%), motor neuron involvement (55%), dysautonomia (50%), and parkinsonism (10%). Motor neuron involvement was found for 24 of 38 patients (63.1%). First motor neuron signs, such as brisk reflexes, extensor plantar responses, and/or spasticity, were present in 29% of patients, second motor neuron signs, such as fasciculations, wasting, weakness, or a neurogenic pattern on EMG in 18%, and both in 16%. Mixed motor and sensory neuronopathy was observed in 19% of patients. Among six non-RFC1 patients, one carried a heterozygous AAGGG expansion and a pathogenic variant in GRM1. Neuropathological examination of one RFC1 patient with an enriched phenotype, including parkinsonism, dysautonomia, and cognitive decline, showed posterior column and lumbar posterior root atrophy. Degeneration of the vestibulospinal and spinocerebellar tracts was mild. We observed marked astrocytic gliosis and axonal swelling of the synapse between first and second motor neurons in the anterior horn at the lumbar level. The cerebellum showed mild depletion of Purkinje cells, with empty baskets, torpedoes, and astrogliosis characterized by a disorganization of the Bergmann's radial glia. We found neuronal loss in the vagal nucleus. The pars compacta of the substantia nigra was depleted, with widespread Lewy bodies in the locus coeruleus, substantia nigra, hippocampus, entorhinal cortex, and amygdala. We propose new guidelines for the screening of RFC1 expansion, considering different expansion motifs. Here, we developed a new method to more easily detect pathogenic RFC1 expansions. We report frequent motor neuron involvement and different neuronopathy subtypes. Parkinsonism was more prevalent in this cohort than in the general population, 10% versus the expected 1% (p < .001). We describe, for the first time, the spinal cord pathology in CANVAS, showing the alteration of posterior columns and roots, astrocytic gliosis and axonal swelling, suggesting motor neuron synaptic dysfunction.△ Less"
Motion Planning in Dynamic Environments Using Context-Aware Human Trajectory Prediction,"Authors:Mark Nicholas Finean,Luka Petrović,Wolfgang Merkt,Ivan Marković,Ioannis Havoutis","Abstract:…Toyota Human Support Robot (HSR) using live RGB-D sensor data from the onboard camera. In addition to providing analysis on a publicly available dataset, we release theOxfordIndoor Human Motion (…▽ MoreOver the years, the separate fields of motion planning, mapping, and human trajectory prediction have advanced considerably. However, the literature is still sparse in providing practical frameworks that enable mobile manipulators to perform whole-body movements and account for the predicted motion of moving obstacles. Previous optimisation-based motion planning approaches that use distance fields have suffered from the high computational cost required to update the environment representation. We demonstrate that GPU-accelerated predicted composite distance fields significantly reduce the computation time compared to calculating distance fields from scratch. We integrate this technique with a complete motion planning and perception framework that accounts for the predicted motion of humans in dynamic environments, enabling reactive and pre-emptive motion planning that incorporates predicted motions. To achieve this, we propose and implement a novel human trajectory prediction method that combines intention recognition with trajectory optimisation-based motion planning. We validate our resultant framework on a real-world Toyota Human Support Robot (HSR) using live RGB-D sensor data from the onboard camera. In addition to providing analysis on a publicly available dataset, we release theOxfordIndoor Human Motion (Oxford-IHM) dataset and demonstrate state-of-the-art performance in human trajectory prediction. TheOxford-IHM dataset is a human trajectory prediction dataset in which people walk between regions of interest in an indoor environment. Both static and robot-mounted RGB-D cameras observe the people while tracked with a motion-capture system.△ Less"
Privacy-aware Early Detection of COVID-19 through Adversarial Training,"Authors:Omid Rohanian,Samaneh Kouchaki,Andrew Soltan,Jenny Yang,Morteza Rohanian,Yang Yang,David Clifton","Abstract:…we examine in this work are intended to preserve sensitive information against adversarial attacks and information leakage. In a series of experiments using datasets from theOxfordUniversity Hospitals, Bedfordshire Hospitals NHS Foundation Trust, University Hospitals Birmingham NHS Foundation Trust, and Portsmouth Hospitals University NHS Trust we train an…▽ MoreEarly detection of COVID-19 is an ongoing area of research that can help with triage, monitoring and general health assessment of potential patients and may reduce operational strain on hospitals that cope with the coronavirus pandemic. Different machine learning techniques have been used in the literature to detect coronavirus using routine clinical data (blood tests, and vital signs). Data breaches and information leakage when using these models can bring reputational damage and cause legal issues for hospitals. In spite of this, protecting healthcare models against leakage of potentially sensitive information is an understudied research area. In this work, we examine two machine learning approaches, intended to predict a patient's COVID-19 status using routinely collected and readily available clinical data. We employ adversarial training to explore robust deep learning architectures that protect attributes related to demographic information about the patients. The two models we examine in this work are intended to preserve sensitive information against adversarial attacks and information leakage. In a series of experiments using datasets from theOxfordUniversity Hospitals, Bedfordshire Hospitals NHS Foundation Trust, University Hospitals Birmingham NHS Foundation Trust, and Portsmouth Hospitals University NHS Trust we train and test two neural networks that predict PCR test results using information from basic laboratory blood tests, and vital signs performed on a patients' arrival to hospital. We assess the level of privacy each one of the models can provide and show the efficacy and robustness of our proposed architectures against a comparable baseline. One of our main contributions is that we specifically target the development of effective COVID-19 detection models with built-in mechanisms in order to selectively protect sensitive attributes against adversarial attacks.△ Less"
Thin film growth of MAX phases as functional materials,"Authors:Abhijit Biswas,Varun Natu,Anand B. Puthirath","Abstract:Layered nanolaminate ternary carbides, nitrides and carbonitrides with general formula Mn+1AXn or MAX (n = 1, 2, or 3, M is an early transition metal, A is mostly group 13 or 14 element, and X is C and/or N) has revolutionized the world of nanomaterials, due to the coexistence of both ceramic and metallic nature, giving rise to exceptional mechanical, thermal, electrical, chemical properties and w…▽ MoreLayered nanolaminate ternary carbides, nitrides and carbonitrides with general formula Mn+1AXn or MAX (n = 1, 2, or 3, M is an early transition metal, A is mostly group 13 or 14 element, and X is C and/or N) has revolutionized the world of nanomaterials, due to the coexistence of both ceramic and metallic nature, giving rise to exceptional mechanical, thermal, electrical, chemical properties and wide range of applications. Although several solid-state bulk synthesis methods have been developed to produce a variety of MAX phases, however, for certain applications, the growth of MAX phases, especially in its high-quality epitaxial thin films form is of increasing interest. Here, we summarize the progress made thus far in epitaxial growth and property evaluation of MAX phase thin films grown by various deposition techniques. We also address the important future research directions to be made in terms of thin-film growth. Overall, in the future, high-quality single-phase epitaxial thin film growth and engineering of chemically diverse MAX phases may open up interesting new avenues for next-generation technology.△ Less"
Operation of a Continuous Flow Liquid Helium Magnetic Microscopy Cryostat as a Closed Cycle System,"Authors:K. Barr,T. Cookson,K. G. Lagoudakis","Abstract:We demonstrate successful operation of a continuous flow liquid helium magnetic cryostat (OxfordInstruments, Microstat MO) in closed cycle operation using a modular cryocooling system (ColdEdge Technologies, Stinger). For the system operation, we have developed a custom gas handling manifold and we show that despite the lower cooling power of the cryocooler…▽ MoreWe demonstrate successful operation of a continuous flow liquid helium magnetic cryostat (OxfordInstruments, Microstat MO) in closed cycle operation using a modular cryocooling system (ColdEdge Technologies, Stinger). For the system operation, we have developed a custom gas handling manifold and we show that despite the lower cooling power of the cryocooler with respect to the nominal cryostat cooling power requirements, the magnetic cryostat can be operated in a stable manner. We provide the design of the gas handling manifold, and a detailed analysis of the system performance in terms of cooling times, magnetic field ramping rates and vibrations at the sample. Base temperature can be reached within 10 hours while the superconducting magnet can be energized at a ramping rate of 0.5 T/min. Vibrations are measured interferometrically and show amplitudes with a root mean square on the order of 5 nm permitting the use of the system for sensitive magnetic microscopy experiments.△ Less"
Detection of anomalous element distribution in the extremely slowly rotating magnetic O9.7 V star HD 54879,"Authors:S. P. Järvinen,S. Hubrig.,M. Schöller,A. Cikota,I. Ilyin,C. A. Hummel.,M. Küker","Abstract:The O9.7 V star HD 54879 is currently the only massive magnetic star whose magnetic field geometry and rotation period are not constrained. Over the last three years, we gathered additional observations of this star, obtained using various instruments at several astronomical facilities with, the aim to constrain the rotation period and the magnetic field geometry. The new data include the first fu…▽ MoreThe O9.7 V star HD 54879 is currently the only massive magnetic star whose magnetic field geometry and rotation period are not constrained. Over the last three years, we gathered additional observations of this star, obtained using various instruments at several astronomical facilities with, the aim to constrain the rotation period and the magnetic field geometry. The new data include the first full Stokes vector observations with the PEPSI spectropolarimeter, installed at the Large Binocular Telescope. The acquired spectropolarimetric observations show a very slow magnetic field variability related to the extremely slow rotation of HD 54879, which is also indicated in a dynamical spectrum, displaying variability of the H$α$ line. The most intriguing result of our study is the discovery of differences in longitudinal magnetic field strengths measured using different LSD masks containing lines belonging to different elements. It is the first time that such a differential analysis of the field strength in dependence of the used lines is carried out for a magnetic O-type star. Since the LSD Stokes $I$ profiles of the studied O, Si, and He line masks remain stable over all observing epochs, we conclude that the detection of different field strengths using lines belonging to these elements is related to the different formation depths, with the He lines formed much higher in the stellar atmosphere compared to the silicon and the oxygen lines, and NLTE effects. Our numerical magnetospherical model suggests the presence of enhanced gas density that fills the volume inside the field lines close to the star.△ Less"
Predictive Coding Theories of Cortical Function,"Authors:Linxing Preston Jiang,Rajesh P. N. Rao","Abstract:Predictive coding is a unifying framework for understanding perception, action and neocortical organization. In predictive coding, different areas of the neocortex implement a hierarchical generative model of the world that is learned from sensory inputs. Cortical circuits are hypothesized to perform Bayesian inference based on this generative model. Specifically, the Rao-Ballard hierarchical pred…▽ MorePredictive coding is a unifying framework for understanding perception, action and neocortical organization. In predictive coding, different areas of the neocortex implement a hierarchical generative model of the world that is learned from sensory inputs. Cortical circuits are hypothesized to perform Bayesian inference based on this generative model. Specifically, the Rao-Ballard hierarchical predictive coding model assumes that the top-down feedback connections from higher to lower order cortical areas convey predictions of lower-level activities. The bottom-up, feedforward connections in turn convey the errors between top-down predictions and actual activities. These errors are used to correct current estimates of the state of the world and generate new predictions. Through the objective of minimizing prediction errors, predictive coding provides a functional explanation for a wide range of neural responses and many aspects of brain organization.△ Less"
CLIN-X: pre-trained language models and a study on cross-task transfer for concept extraction in the clinical domain,"Authors:Lukas Lange,Heike Adel,Jannik Strötgen,Dietrich Klakow","Abstract:The field of natural language processing (NLP) has recently seen a large change towards using pre-trained language models for solving almost any task. Despite showing great improvements in benchmark datasets for various tasks, these models often perform sub-optimal in non-standard domains like the clinical domain where a large gap between pre-training documents and target documents is observed. In…▽ MoreThe field of natural language processing (NLP) has recently seen a large change towards using pre-trained language models for solving almost any task. Despite showing great improvements in benchmark datasets for various tasks, these models often perform sub-optimal in non-standard domains like the clinical domain where a large gap between pre-training documents and target documents is observed. In this paper, we aim at closing this gap with domain-specific training of the language model and we investigate its effect on a diverse set of downstream tasks and settings. We introduce the pre-trained CLIN-X (Clinical XLM-R) language models and show how CLIN-X outperforms other pre-trained transformer models by a large margin for ten clinical concept extraction tasks from two languages. In addition, we demonstrate how the transformer model can be further improved with our proposed task- and language-agnostic model architecture based on ensembles over random splits and cross-sentence context. Our studies in low-resource and transfer settings reveal stable model performance despite a lack of annotated data with improvements of up to 47 F1 points when only 250 labeled sentences are available. Our results highlight the importance of specialized language models as CLIN-X for concept extraction in non-standard domains, but also show that our task-agnostic model architecture is robust across the tested tasks and languages so that domain- or task-specific adaptations are not required.△ Less"
Inferring the morphology of AGN torus using X-ray spectra: A reliability study,"Authors:Tathagata Saha,Alex G. Markowitz,Johannes Buchner","Abstract:Numerous X-ray spectral models have been developed to model emission reprocessed by the torus of an active galactic nucleus (AGN), e.g., UXCLUMPY, CTORUS, and MYTORUS. They span a range of assumed torus geometries and morphologies-some posit smooth gas distributions, and others posit distributions of clouds. It is suspected that given the quality of currently available data, certain model paramete…▽ MoreNumerous X-ray spectral models have been developed to model emission reprocessed by the torus of an active galactic nucleus (AGN), e.g., UXCLUMPY, CTORUS, and MYTORUS. They span a range of assumed torus geometries and morphologies-some posit smooth gas distributions, and others posit distributions of clouds. It is suspected that given the quality of currently available data, certain model parameters, such as coronal power law photon index and parameters determining the morphology of the AGN torus, may be poorly constrained due to model degeneracies. In this work, we test the reliability of these models in terms of recovery of parameters and the ability to discern the morphology of the torus using XMM-Newton and NuSTAR spectral data. We perform extensive simulations of X-ray spectra of Compton-thick AGNs under six X-ray spectral models of the torus. We use Bayesian methods to investigate degeneracy between model parameters, distinguish models and determine the dependence of the parameter constraints on the instruments used. For typical exposure times and fluxes for nearby Compton-thick AGN, we find that several parameters across the models used here cannot be well constrained, e.g., the distribution of clouds, the number of clouds in the radial direction, even when the applied model is correct. We also find that Bayesian evidence values can robustly distinguish between a correct and a wrong model only if there is sufficient energy coverage and only if the intrinsic flux of the object is above a particular value determined by the instrument combination and the model considered.△ Less"
"MinkLoc3D-SI: 3D LiDAR place recognition with sparse convolutions, spherical coordinates, and intensity","Authors:Kamil Żywanowski,Adam Banaszczyk,Michał R. Nowicki,Jacek Komorowski","Abstract:…M2DP, LiDAR IRIS) and deep learning-based solutions (e.g., PointNetVLAD, PCAN, LPDNet, DAGC, MinkLoc3D), which are often only evaluated on accumulated 2D scans from theOxfordRobotCar dataset. We introduce MinkLoc3D-SI, a sparse convolution-based solution that utilizes spherical coordinates of 3D points and processes the intensity of 3D LiDAR measurements,…▽ MoreThe 3D LiDAR place recognition aims to estimate a coarse localization in a previously seen environment based on a single scan from a rotating 3D LiDAR sensor. The existing solutions to this problem include hand-crafted point cloud descriptors (e.g., ScanContext, M2DP, LiDAR IRIS) and deep learning-based solutions (e.g., PointNetVLAD, PCAN, LPDNet, DAGC, MinkLoc3D), which are often only evaluated on accumulated 2D scans from theOxfordRobotCar dataset. We introduce MinkLoc3D-SI, a sparse convolution-based solution that utilizes spherical coordinates of 3D points and processes the intensity of 3D LiDAR measurements, improving the performance when a single 3D LiDAR scan is used. Our method integrates the improvements typical for hand-crafted descriptors (like ScanContext) with the most efficient 3D sparse convolutions (MinkLoc3D). Our experiments show improved results on single scans from 3D LiDARs (USyd Campus dataset) and great generalization ability (KITTI dataset). Using intensity information on accumulated 2D scans (RobotCar Intensity dataset) improves the performance, even though spherical representation doesn't produce a noticeable improvement. As a result, MinkLoc3D-SI is suited for single scans obtained from a 3D LiDAR, making it applicable in autonomous vehicles.△ Less"
Learning Token-based Representation for Image Retrieval,"Authors:Hui Wu,Min Wang,Wengang Zhou,Yang Hu,Houqiang Li","Abstract:…is trained end-to-end with image-level labels. Extensive experiments are conducted to evaluate our approach, which outperforms the state-of-the-art methods on the RevisitedOxfordand Paris datasets.▽ MoreIn image retrieval, deep local features learned in a data-driven manner have been demonstrated effective to improve retrieval performance. To realize efficient retrieval on large image database, some approaches quantize deep local features with a large codebook and match images with aggregated match kernel. However, the complexity of these approaches is non-trivial with large memory footprint, which limits their capability to jointly perform feature learning and aggregation. To generate compact global representations while maintaining regional matching capability, we propose a unified framework to jointly learn local feature representation and aggregation. In our framework, we first extract deep local features using CNNs. Then, we design a tokenizer module to aggregate them into a few visual tokens, each corresponding to a specific visual pattern. This helps to remove background noise, and capture more discriminative regions in the image. Next, a refinement block is introduced to enhance the visual tokens with self-attention and cross-attention. Finally, different visual tokens are concatenated to generate a compact global representation. The whole framework is trained end-to-end with image-level labels. Extensive experiments are conducted to evaluate our approach, which outperforms the state-of-the-art methods on the RevisitedOxfordand Paris datasets.△ Less"
Is the molecular KS relationship universal down to low metallicities?,"Authors:David J. Whitworth,Rowan J. Smith,Robin Tress,Scott T. Kay,Simon C. O. Glover,Mattia C. Sormani,Ralf S. Klessen","Abstract:In recent years it has been speculated that in extreme low metallicity galactic environments, stars form in regions that lack H2. In this paper we investigate how changing the metallicity and UV-field strength of a galaxy affects the star formation within, and the molecular gas Kennicutt-Schmidt relation. Using extremely high resolution arepo simulations of isolated dwarf galaxies, we independentl…▽ MoreIn recent years it has been speculated that in extreme low metallicity galactic environments, stars form in regions that lack H2. In this paper we investigate how changing the metallicity and UV-field strength of a galaxy affects the star formation within, and the molecular gas Kennicutt-Schmidt relation. Using extremely high resolution arepo simulations of isolated dwarf galaxies, we independently vary the metallicity and UV-field to between 1% and 10% solar neighbourhood values. We include a non-equilibrium, time-dependant chemical network to model the molecular composition of the ISM, and include the effects of gas shielding from an ambient UV field. Crucially our simulations directly model the gravitational collapse of gas into star-forming clumps and cores and their subsequent accretion using sink particles. In this first publication we find that reducing the metallicity and UV-field by a factor of 10 has no effect on star formation, and minimal effect on the cold, dense star forming gas. The cold gas depletion times are almost an order of magnitude longer than the molecular gas depletion time due to the presence of star formation in HI dominated cold gas. We study the H2 Kennicutt-Schmidt relationship that arises naturally within the simulations and find a near linear power law index of N = 1.09 +/- 0.014 in our fiducial 10% solar metallicity model. As the metallicity and UV-field are reduced this becomes moderately steeper, with a slope of N = 1.24 +/- 0.022 for our 1% solar metallicity and 1% solar UV field model.△ Less"
A High-Resolution Investigation of the Multi-Phase ISM in a Galaxy during the First Two Billion Years,"Authors:S. Dye,S. A. Eales,H. L. Gomez,G. C. Jones,M. W. L. Smith,E. Borsato,A. Moss,L. Dunne,J. Maresca,A. Amvrosiadis,M. Negrello,L. Marchetti,E. M. Corsini,R. J. Ivison,G. J. Bendo,T. Bakx,A. Cooray,P. Cox,H. Dannerbauer,S. Serjeant,D. Riechers,P. Temi,C. Vlahakis","Abstract:We have carried out the first spatially-resolved investigation of the multi-phase interstellar medium (ISM) at high redshift, using the z=4.24 strongly-lensed sub-millimetre galaxy H-ATLASJ142413.9+022303 (ID141). We present high-resolution (down to ~350 pc) ALMA observations in dust continuum emission and in the CO(7-6), H_2O (2_{1,1} - 2_{0,2}), CI(1-0) and CI(2-1) lines, the latter two allowing…▽ MoreWe have carried out the first spatially-resolved investigation of the multi-phase interstellar medium (ISM) at high redshift, using the z=4.24 strongly-lensed sub-millimetre galaxy H-ATLASJ142413.9+022303 (ID141). We present high-resolution (down to ~350 pc) ALMA observations in dust continuum emission and in the CO(7-6), H_2O (2_{1,1} - 2_{0,2}), CI(1-0) and CI(2-1) lines, the latter two allowing us to spatially resolve the cool phase of the ISM for the first time. Our modelling of the kinematics reveals that the system appears to be dominated by a rotationally-supported gas disk with evidence of a nearby perturber. We find that the CI(1-0) line has a very different distribution to the other lines, showing the existence of a reservoir of cool gas that might have been missed in studies of other galaxies. We have estimated the mass of the ISM using four different tracers, always obtaining an estimate in the range (3.2-3.8) x 10^{11} M_sol, significantly higher than our dynamical mass estimate of (0.8-1.3) x 10^{11} M_sol. We suggest that this conflict and other similar conflicts reported in the literature is because the gas-to-tracer ratios are ~4 times lower than the Galactic values used to calibrate the ISM in high-redshift galaxies. We demonstrate that this could result from a top-heavy initial mass function and strong chemical evolution. Using a variety of quantitative indicators, we show that, extreme though it is at z=4.24, ID141 will likely join the population of quiescent galaxies that appears in the Universe at z~3.△ Less"
Eager Functions as Processes,"Authors:Adrien Durier,Daniel Hirschkoff,Davide Sangiorgi","Abstract:We study Milner's encoding of the call-by-value $λ$-calculus into the $π$-calculus. We show that, by tuning the encoding to two subcalculi of the $π$-calculus (Internal $π$ and Asynchronous Local $π$), the equivalence on $λ$-terms induced by the encoding coincides with Lassen's eager normal-form bisimilarity, extended to handle $η$-equality. As behavioural equivalence in the $π$-calculus we consid…▽ MoreWe study Milner's encoding of the call-by-value $λ$-calculus into the $π$-calculus. We show that, by tuning the encoding to two subcalculi of the $π$-calculus (Internal $π$ and Asynchronous Local $π$), the equivalence on $λ$-terms induced by the encoding coincides with Lassen's eager normal-form bisimilarity, extended to handle $η$-equality. As behavioural equivalence in the $π$-calculus we consider contextual equivalence and barbed congruence. We also extend the results to preorders. A crucial technical ingredient in the proofs is the recently-introduced technique of unique solutions of equations, further developed in this paper. In this respect, the paper also intends to be an extended case study on the applicability and expressiveness of the technique.△ Less"
Joint modeling of geometric features of longitudinal process and discrete survival time measured on nested timescales: an application to fecundity studies,"Authors:Abhisek Saha,Ling Ma,Animikh Biswas,Rajeshwari Sundaram","Abstract:…on their available longitudinal measurements. Our proposed model and approach is illustrated through simulation and analysis of Stress and Time-to-Pregnancy, a component ofOxfordConception Study. A joint modeling approach was used to assess whether the cycle-specific geometric features of the lutenizing hormone measurements, such as its peak or its curvatu…▽ MoreIn biomedical studies, longitudinal processes are collected till time-to-event, sometimes on nested timescales (example, days within months). Most of the literature in joint modeling of longitudinal and time-to-event data has focused on modeling the mean or dispersion of the longitudinal process with the hazard for time-to-event. However, based on the motivating studies, it may be of interest to investigate how the cycle-level {\it geometric features} (such as the curvature, location and height of a peak), of a cyclical longitudinal process is associated with the time-to-event being studied. We propose a shared parameter joint model for a cyclical longitudinal process and a discrete survival time, measured on nested timescales, where the cycle-varying geometric feature is modeled through a linear mixed effects model and a proportional hazards model for the discrete survival time. The proposed approach allows for prediction of survival probabilities for future subjects based on their available longitudinal measurements. Our proposed model and approach is illustrated through simulation and analysis of Stress and Time-to-Pregnancy, a component ofOxfordConception Study. A joint modeling approach was used to assess whether the cycle-specific geometric features of the lutenizing hormone measurements, such as its peak or its curvature, are associated with time-to-pregnancy (TTP).△ Less"
MUSE spectroscopy of planetary nebulae with high abundance discrepancies,"Authors:Jorge García-Rojas,Christophe Morisset,David Jones,Roger Wesson,Henri M. J. Boffin,Hektor Monteiro,Romano L. M. Corradi,Pablo Rodríguez-Gil","Abstract:We present MUSE deep integral-field unit spectroscopy of three planetary nebulae(PNe) with high abundance discrepancy factors (ADF > 20): NGC 6778, M 1-42 and Hf 2-2. We have constructed flux maps for more than 40 emission lines, and use them to build extinction, electron temperature (T$_e$), electron density (n$_e$), and ionic abundances maps of a number of ionic species. The effects of the contr…▽ MoreWe present MUSE deep integral-field unit spectroscopy of three planetary nebulae(PNe) with high abundance discrepancy factors (ADF > 20): NGC 6778, M 1-42 and Hf 2-2. We have constructed flux maps for more than 40 emission lines, and use them to build extinction, electron temperature (T$_e$), electron density (n$_e$), and ionic abundances maps of a number of ionic species. The effects of the contribution of recombination to the auroral [N II] and [O II] lines on T$_e$ and the abundance maps of low-ionization species are evaluated using recombination diagnostics. As a result, low T$_e$ values and a downward gradient of T$_e$ are found toward the inner zones of each PN. Spatially, this nearly coincides with the increase of abundances of heavy elements measured using recombination lines in the inner regions of PNe, and strongly supports the presence of two distinct gas phases: a cold and metal-rich and a warm one with ""normal"" metal content. We have simultaneously constructed, for the first time, the ADF maps of O$^+$ and O$^{2+}$ and found that they centrally peak for all three PNe under study. We show that the main issue when trying to compute realistic abundances from either ORLs or CELs is to estimate the relative contribution of each gas component to the H I emission, and we present a method to evaluate it. It is also found that, for the studied high-ADF PNe, the amount of oxygen in the cold and warm regions is of the same order.△ Less"
Introduction to SPDEs from Probability and PDE,Authors:Avi Mayorcas,"Abstract:Lecture notes accompanying a 6 hour mini-course on SPDE given at the University ofOxfordin April/May 2021. The main focus of these notes is on an exposition of the variational method for monotone and coercive SPDE. A recap of the necessary functional analysis, operator theory and stochastic analysis on Hilbert spaces is included, with additional references…▽ MoreLecture notes accompanying a 6 hour mini-course on SPDE given at the University ofOxfordin April/May 2021. The main focus of these notes is on an exposition of the variational method for monotone and coercive SPDE. A recap of the necessary functional analysis, operator theory and stochastic analysis on Hilbert spaces is included, with additional references. The final chapter contains a discussion of the pathwise approach.△ Less"
Sharpness-aware Quantization for Deep Neural Networks,"Authors:Jing Liu,Jianfei Cai,Bohan Zhuang","Abstract:…the default optimizer (e.g., SGD or AdamW). Extensive experiments on both convolutional neural networks and Transformers across various datasets (i.e., ImageNet, CIFAR-10/100,OxfordFlowers-102,Oxford-IIIT Pets) show that SAQ improves the generalization performance of the quantized models, yielding the SOTA results i…▽ MoreNetwork quantization is a dominant paradigm of model compression. However, the abrupt changes in quantized weights during training often lead to severe loss fluctuations and result in a sharp loss landscape, making the gradients unstable and thus degrading the performance. Recently, Sharpness-Aware Minimization (SAM) has been proposed to smooth the loss landscape and improve the generalization performance of the models. Nevertheless, directly applying SAM to the quantized models can lead to perturbation mismatch or diminishment issues, resulting in suboptimal performance. In this paper, we propose a novel method, dubbed Sharpness-Aware Quantization (SAQ), to explore the effect of SAM in model compression, particularly quantization for the first time. Specifically, we first provide a unified view of quantization and SAM by treating them as introducing quantization noises and adversarial perturbations to the model weights, respectively. According to whether the noise and perturbation terms depend on each other, SAQ can be formulated into three cases, which are analyzed and compared comprehensively. Furthermore, by introducing an efficient training strategy, SAQ only incurs a little additional training overhead compared with the default optimizer (e.g., SGD or AdamW). Extensive experiments on both convolutional neural networks and Transformers across various datasets (i.e., ImageNet, CIFAR-10/100,OxfordFlowers-102,Oxford-IIIT Pets) show that SAQ improves the generalization performance of the quantized models, yielding the SOTA results in uniform quantization. For example, on ImageNet, SAQ outperforms AdamW by 1.2% on the Top-1 accuracy for 4-bit ViT-B/16. Our 4-bit ResNet-50 surpasses the previous SOTA method by 0.9% on the Top-1 accuracy.△ Less"
Kontsevich's deformation quantization: from Dirac to multiple zeta values,Authors:Oisin Kim,"Abstract:…. The primary aim of this essay, largely drawn from the author's MMath dissertation atOxford, is to present and explain Kontsevich's results. Starting with the motivation, we discuss how the problem is solved by situating it in a richer mathematical structure, performing a few original calculations along the way. We hope to communicate a sense of th…▽ MoreOne way of reconciling classical and quantum mechanics is deformation quantization, which involves deforming the commutative algebra of functions on a Poisson manifold to a non-commutative, associative algebra, reminiscent of the space of quantum observables. This depends on a formal parameter $\hbar$, so that the original pointwise product is recovered when $\hbar=0$. In 1997 Kontsevich showed that a deformation quantization exists for every Poisson manifold. He furthermore gave a simple, combinatorial formula for producing a quantization of any Poisson structure on $\mathbb{R}^n$. The primary aim of this essay, largely drawn from the author's MMath dissertation atOxford, is to present and explain Kontsevich's results. Starting with the motivation, we discuss how the problem is solved by situating it in a richer mathematical structure, performing a few original calculations along the way. We hope to communicate a sense of the strange links between this subject and seemingly distant areas of mathematics, and also to describe some of the contemporary research in the field. To these ends, we consider a recent paper, arXiv:1812.11649 [math.QA], which connects deformation quantization to multiple zeta values.△ Less"
Digital Audio Processing Tools for Music Corpus Studies,Authors:Johanna Devaney,"Abstract:Digital audio processing tools offer music researchers the opportunity to examine both non-notated music and music as performance. This chapter summarises the types of information that can be extracted from audio as well as currently available audio tools for music corpus studies. The survey of extraction methods includes both a primer on signal processing and background theory on audio feature ex…▽ MoreDigital audio processing tools offer music researchers the opportunity to examine both non-notated music and music as performance. This chapter summarises the types of information that can be extracted from audio as well as currently available audio tools for music corpus studies. The survey of extraction methods includes both a primer on signal processing and background theory on audio feature extraction. The survey of audio tools focuses on widely used tools, including both those with a graphical user interface, namely Audacity and Sonic Visualiser, and code-based tools written in the C/C++, Java, MATLAB, and Python computer programming languages.△ Less"
BBC-OxfordBritish Sign Language Dataset,"Authors:Samuel Albanie,Gül Varol,Liliane Momeni,Hannah Bull,Triantafyllos Afouras,Himel Chowdhury,Neil Fox,Bencie Woll,Rob Cooper,Andrew McParland,Andrew Zisserman","Abstract:In this work, we introduce the BBC-OxfordBritish Sign Language (BOBSL) dataset, a large-scale video collection of British Sign Language (BSL). BOBSL is an extended and publicly released dataset based on the BSL-1K dataset introduced in previous work. We describe the motivation for the dataset, together with statistics and available annotations. We conduct e…▽ MoreIn this work, we introduce the BBC-OxfordBritish Sign Language (BOBSL) dataset, a large-scale video collection of British Sign Language (BSL). BOBSL is an extended and publicly released dataset based on the BSL-1K dataset introduced in previous work. We describe the motivation for the dataset, together with statistics and available annotations. We conduct experiments to provide baselines for the tasks of sign recognition, sign language alignment, and sign language translation. Finally, we describe several strengths and limitations of the data from the perspectives of machine learning and linguistics, note sources of bias present in the dataset, and discuss potential applications of BOBSL in the context of sign language technology. The dataset is available at https://www.robots.ox.ac.uk/~vgg/data/bobsl/.△ Less"
Transfinite game values in infinite draughts,"Authors:Joel David Hamkins,Davide Leonessi","Abstract:Infinite draughts, or checkers, is played just like the finite game, but on an infinite checkerboard extending without bound in all four directions. We prove that every countable ordinal arises as the game value of a position in infinite draughts. Thus, there are positions from which Red has a winning strategy enabling her to win always in finitely many moves, but the length of play can be complet…▽ MoreInfinite draughts, or checkers, is played just like the finite game, but on an infinite checkerboard extending without bound in all four directions. We prove that every countable ordinal arises as the game value of a position in infinite draughts. Thus, there are positions from which Red has a winning strategy enabling her to win always in finitely many moves, but the length of play can be completely controlled by Black in a manner as though counting down from a given countable ordinal.△ Less"
Rotational excitation of H$_3$O$^+$ cations by para-H$_2$: improved collisional data at low temperatures,"Authors:S. Demes,François Lique,Alexandre Faure,Floris van der Tak,Claire Rist,Pierre Hily-Blant","Abstract:The hydronium cation plays a crucial role in interstellar oxygen and water chemistry. While its spectroscopy was extensively investigated earlier, the collisional excitation of H$_3$O$^+$ is not well studied yet. In this work we present state-to-state collisional data for rotational de-excitation of both ortho- and para-H$_3$O$^+$ due to para-H$_2$ impact. The cross sections are calculated within…▽ MoreThe hydronium cation plays a crucial role in interstellar oxygen and water chemistry. While its spectroscopy was extensively investigated earlier, the collisional excitation of H$_3$O$^+$ is not well studied yet. In this work we present state-to-state collisional data for rotational de-excitation of both ortho- and para-H$_3$O$^+$ due to para-H$_2$ impact. The cross sections are calculated within the close-coupling formalism using our recent, highly accurate rigid-rotor potential energy surface for this collision system. The corresponding thermal rate coefficients are calculated up to 100 K. For para-H$_3$O$^+$ the lowest 20 rotation-inversion states were considered in the calculations, while for ortho-H$_3$O$^+$ the lowest 11 states are involved (up to $j\leq5$), so all levels with rotational energy below 420 K (292 cm$^{-1}$) are studied. In order to analyse the impact of the new collisional rates on the excitation of H$_3$O$^+$ in astrophysical environments radiative transfer calculations are also provided. The most relevant emission lines from an astrophysical point of view are studied, taking into account the transitions at 307, 365, 389 and 396 GHz. We show that our new collisional data have a non-negligible impact (from a few percents up to about a factor of 3) on the brightness and excitation temperatures of H$_3$O$^+$, justifying the revision of the physical conditions in the appropriate astrophysical observations. The calculated rate coefficients allow one to recalculate the column density of hydronium in interstellar clouds, which can lead to a better understanding of interstellar water and oxygen chemistry.△ Less"
Exclusive $B \to X_u \ell ν_\ell$ Decays with Hadronic Full-event-interpretation Tagging in 62.8 fb$^{-1}$ of Belle II Data,"Authors:Belle II Collaboration,F. Abudinén,I. Adachi,R. Adak,K. Adamczyk,L. Aggarwal,P. Ahlburg,H. Ahmed,J. K. Ahn,H. Aihara,N. Akopov,A. Aloisio,F. Ameli,L. Andricek,N. Anh Ky,D. M. Asner,H. Atmacan,V. Aulchenko,T. Aushev,V. Aushev,T. Aziz,V. Babu,S. Bacher,H. Bae,S. Baehr, et al. (543 additional authors not shown)","Abstract:We present a reconstruction in early data of the semileptonic decay $B^+ \to π^0 \ell^+ ν_\ell$, and first results of a reconstruction of the decays $B^+ \to ρ^0 \ell^+ ν_\ell$ and $B^0 \to ρ^- \ell^+ ν_\ell$ in a sample corresponding to 62.8 fb$^{-1}$ of Belle II data using hadronic $B$-tagging via the full-event-interpretation algorithm. We determine the total branching fractions via fits to the…▽ MoreWe present a reconstruction in early data of the semileptonic decay $B^+ \to π^0 \ell^+ ν_\ell$, and first results of a reconstruction of the decays $B^+ \to ρ^0 \ell^+ ν_\ell$ and $B^0 \to ρ^- \ell^+ ν_\ell$ in a sample corresponding to 62.8 fb$^{-1}$ of Belle II data using hadronic $B$-tagging via the full-event-interpretation algorithm. We determine the total branching fractions via fits to the distribution of the square of the missing mass, and find $\mathcal{B}(B^+ \to π^0 \ell^+ ν_\ell)$ = (8.29 $\pm$ 1.99(stat) $\pm$ 0.46(syst)) $\times 10^{-5}$. We obtain $95\%$ CL upper limits on the branching fractions with $\mathcal{B}(B^0 \to ρ^- \ell^+ ν_\ell) < 3.37 \times 10^{-4}$ and $\mathcal{B}(B^+ \to ρ^0 \ell^+ ν_\ell) < 19.7 \times 10^{-5}$. We also obtain an updated branching fraction for the $B^0 \to π^- \ell^+ ν_\ell$ decay, $\mathcal{B}(B^0 \to π^- \ell^+ ν_\ell)$ = (1.47 $\pm$ 0.29(stat) $\pm$ 0.05(syst)) $\times 10^{-4}$, based on the sum of the partial branching fractions in three bins of the squared momentum transfer to the leptonic system.△ Less"
Multimotion Visual Odometry (MVO),"Authors:Kevin M. Judd,Jonathan D. Gammell","Abstract:…motion priors to extrapolate motions through temporary occlusions and identify the reappearance of motions through motion closure. Evaluations on real-world data from theOxfordMultimotion Dataset (OMD) and the KITTI Vision Benchmark Suite demonstrate that MVO achieves good estimation accuracy compared to similar approaches and is applicable to a variety of…▽ MoreVisual motion estimation is a well-studied challenge in autonomous navigation. Recent work has focused on addressing multimotion estimation in highly dynamic environments. These environments not only comprise multiple, complex motions but also tend to exhibit significant occlusion.
  Estimating third-party motions simultaneously with the sensor egomotion is difficult because an object's observed motion consists of both its true motion and the sensor motion. Most previous works in multimotion estimation simplify this problem by relying on appearance-based object detection or application-specific motion constraints. These approaches are effective in specific applications and environments but do not generalize well to the full multimotion estimation problem (MEP).
  This paper presents Multimotion Visual Odometry (MVO), a multimotion estimation pipeline that estimates the full SE(3) trajectory of every motion in the scene, including the sensor egomotion, without relying on appearance-based information. MVO extends the traditional visual odometry (VO) pipeline with multimotion segmentation and tracking techniques. It uses physically founded motion priors to extrapolate motions through temporary occlusions and identify the reappearance of motions through motion closure. Evaluations on real-world data from theOxfordMultimotion Dataset (OMD) and the KITTI Vision Benchmark Suite demonstrate that MVO achieves good estimation accuracy compared to similar approaches and is applicable to a variety of multimotion estimation challenges.△ Less"
Validating Gaussian Process Models with Simulation-Based Calibration,"Authors:John Mcleod,Fergus Simpson","Abstract:Gaussian process priors are a popular choice for Bayesian analysis of regression problems. However, the implementation of these models can be complex, and ensuring that the implementation is correct can be challenging. In this paper we introduce Gaussian process simulation-based calibration, a procedure for validating the implementation of Gaussian process models and demonstrate the efficacy of th…▽ MoreGaussian process priors are a popular choice for Bayesian analysis of regression problems. However, the implementation of these models can be complex, and ensuring that the implementation is correct can be challenging. In this paper we introduce Gaussian process simulation-based calibration, a procedure for validating the implementation of Gaussian process models and demonstrate the efficacy of this procedure in identifying a bug in existing code. We also present a novel application of this procedure to identify when marginalisation of the model hyperparameters is necessary.△ Less"
Normative Epistemology for Lethal Autonomous Weapons Systems,Authors:Susannah Kate Devitt,"Abstract:The rise of human-information systems, cybernetic systems, and increasingly autonomous systems requires the application of epistemic frameworks to machines and human-machine teams. This chapter discusses higher-order design principles to guide the design, evaluation, deployment, and iteration of Lethal Autonomous Weapons Systems (LAWS) based on epistemic models. Epistemology is the study of knowle…▽ MoreThe rise of human-information systems, cybernetic systems, and increasingly autonomous systems requires the application of epistemic frameworks to machines and human-machine teams. This chapter discusses higher-order design principles to guide the design, evaluation, deployment, and iteration of Lethal Autonomous Weapons Systems (LAWS) based on epistemic models. Epistemology is the study of knowledge. Epistemic models consider the role of accuracy, likelihoods, beliefs, competencies, capabilities, context, and luck in the justification of actions and the attribution of knowledge. The aim is not to provide ethical justification for or against LAWS, but to illustrate how epistemological frameworks can be used in conjunction with moral apparatus to guide the design and deployment of future systems. The models discussed in this chapter aim to make Article 36 reviews of LAWS systematic, expedient, and evaluable. A Bayesian virtue epistemology is proposed to enable justified actions under uncertainty that meet the requirements of the Laws of Armed Conflict and International Humanitarian Law. Epistemic concepts can provide some of the apparatus to meet explainability and transparency requirements in the development, evaluation, deployment, and review of ethical AI.△ Less"
The Quantum Revolution in Philosophy (Book Review),Authors:Eddy Keming Chen,"Abstract:In this thought-provoking book, Richard Healey proposes a new interpretation of quantum theory inspired by pragmatist philosophy. Healey puts forward the interpretation as an alternative to realist quantum theories on the one hand such as Bohmian mechanics, spontaneous collapse theories, and many-worlds interpretations, which are different proposals for describing what the quantum world is like an…▽ MoreIn this thought-provoking book, Richard Healey proposes a new interpretation of quantum theory inspired by pragmatist philosophy. Healey puts forward the interpretation as an alternative to realist quantum theories on the one hand such as Bohmian mechanics, spontaneous collapse theories, and many-worlds interpretations, which are different proposals for describing what the quantum world is like and what the basic laws of physics are, and non-realist interpretations on the other hand such as quantum Bayesianism, which proposes to understand quantum theory as describing agents' subjective epistemic states. The central idea of Healey's proposal is to understand quantum theory as providing not a description of the physical world but a set of authoritative and objectively correct prescriptions about how agents should act. The book provides a detailed development and defense of that idea, and it contains interesting discussions about a wide range of philosophical issues such as representation, probability, explanation, causation, objectivity, meaning, and fundamentality. Healey's project is at the intersection of physics and philosophy. The book is divided into two parts. Part I of the book discusses the foundational questions in quantum theory from the perspective of the prescriptive interpretation. In Part II, Healey discusses the philosophical implications of the view. Both parts are written in a way that is largely accessible to non-specialists. In this brief book review, I will focus on two questions: (1) How does Healey's idea work? (2) What reasons are there to believe in it?△ Less"
The Cosmic Void,Authors:Eddy Keming Chen,"Abstract:What exists at the fundamental level of reality? On the standard picture, the fundamental reality contains (among other things) fundamental matter, such as particles, fields, or even the quantum state. Non-fundamental facts are explained by facts about fundamental matter, at least in part. In this paper, I introduce a non-standard picture called the ""cosmic void"" in which the universe is devoid of…▽ MoreWhat exists at the fundamental level of reality? On the standard picture, the fundamental reality contains (among other things) fundamental matter, such as particles, fields, or even the quantum state. Non-fundamental facts are explained by facts about fundamental matter, at least in part. In this paper, I introduce a non-standard picture called the ""cosmic void"" in which the universe is devoid of any fundamental material ontology. Facts about tables and chairs are recovered from a special kind of laws that satisfy strong determinism. All non-fundamental facts are completely explained by nomic facts. I discuss a concrete example of this picture in a strongly deterministic version of the many-worlds theory of quantum mechanics. I discuss some philosophical and scientific challenges to this view, as well as some connections to ontological nihilism.△ Less"
"Predicting Indian Supreme Court Judgments, Decisions, Or Appeals","Authors:Sugam Sharma,Ritu Shandilya,Swadesh Sharma","Abstract:Legal predictive models are of enormous interest and value to legal community. The stakeholders, specially, the judges and attorneys can take the best advantages of these models to predict the case outcomes to further augment their future course of actions, for example speeding up the decision making, support the arguments, strengthening the defense, etc. However, accurately predicting the legal d…▽ MoreLegal predictive models are of enormous interest and value to legal community. The stakeholders, specially, the judges and attorneys can take the best advantages of these models to predict the case outcomes to further augment their future course of actions, for example speeding up the decision making, support the arguments, strengthening the defense, etc. However, accurately predicting the legal decisions and case outcomes is an arduous process, which involves several complex steps -- finding suitable bulk case documents, data extracting, cleansing and engineering, etc. Additionally, the legal complexity further adds to its intricacies. In this paper, we introduce our newly developed ML-enabled legal prediction model and its operational prototype, eLegPredict; which successfully predicts the Indian supreme court decisions. The eLegPredict is trained and tested over 3072 supreme court cases and has achieved 76% accuracy (F1-score). The eLegPredict is equipped with a mechanism to aid end users, where as soon as a document with new case description is dropped into a designated directory, the system quickly reads through its content and generates prediction. To our best understanding, eLegPredict is the first legal prediction model to predict Indian supreme court decisions.△ Less"
Measurements of the branching fractions for $B \to K^{*}γ$ decays at Belle II,"Authors:F. Abudinén,I. Adachi,R. Adak,K. Adamczyk,L. Aggarwal,P. Ahlburg,H. Ahmed,J. K. Ahn,H. Aihara,N. Akopov,A. Aloisio,F. Ameli,L. Andricek,N. Anh Ky,D. M. Asner,H. Atmacan,V. Aulchenko,T. Aushev,V. Aushev,T. Aziz,V. Babu,S. Bacher,H. Bae,S. Baehr,S. Bahinipati, et al. (543 additional authors not shown)","Abstract:This paper reports a study of $B \to K^{*}γ$ decays using $62.8\pm 0.6$ fb$^{-1}$ of data collected during 2019--2020 by the Belle II experiment at the SuperKEKB $e^{+}e^{-}$ asymmetric-energy collider, corresponding to $(68.2 \pm 0.8) \times 10^6$ $B\overline{B}$ events. We find $454 \pm 28$, $50 \pm 10$, $169 \pm 18$, and $160 \pm 17$ signal events in the decay modes…▽ MoreThis paper reports a study of $B \to K^{*}γ$ decays using $62.8\pm 0.6$ fb$^{-1}$ of data collected during 2019--2020 by the Belle II experiment at the SuperKEKB $e^{+}e^{-}$ asymmetric-energy collider, corresponding to $(68.2 \pm 0.8) \times 10^6$ $B\overline{B}$ events. We find $454 \pm 28$, $50 \pm 10$, $169 \pm 18$, and $160 \pm 17$ signal events in the decay modes $B^{0} \to K^{*0}[K^{+}π^{-}]γ$, $B^{0} \to K^{*0}[K^0_{\rm S}π^{0}]γ$, $B^{+} \to K^{*+}[K^{+}π^{0}]γ$, and $B^{+} \to K^{*+}[K^{+}π^{0}]γ$, respectively. The uncertainties quoted for the signal yield are statistical only. We report the branching fractions of these decays: $$\mathcal{B} [B^{0} \to K^{*0}[K^{+}π^{-}]γ] = (4.5 \pm 0.3 \pm 0.2) \times 10^{-5}, $$ $$\mathcal{B} [B^{0} \to K^{*0}[K^0_{\rm S}π^{0}]γ] = (4.4 \pm 0.9 \pm 0.6) \times 10^{-5},$$ $$\mathcal{B} [B^{+} \to K^{*+}[K^{+}π^{0}]γ] = (5.0 \pm 0.5 \pm 0.4)\times 10^{-5},\text{ and}$$ $$\mathcal{B} [B^{+} \to K^{*+}[K^0_{\rm S}π^{+}]γ] = (5.4 \pm 0.6 \pm 0.4) \times 10^{-5},$$ where the first uncertainty is statistical, and the second is systematic. The results are consistent with world-average values.△ Less"
A Comprehensive Study on Torchvision Pre-trained Models for Fine-grained Inter-species Classification,"Authors:Feras Albardi,H M Dipu Kabir,Md Mahbub Islam Bhuiyan,Parham M. Kebria,Abbas Khosravi,Saeid Nahavandi","Abstract:…a guideline for the selection of a good model. We investigate Torchvision pre-trained models on four different data sets: 10 Monkey Species, 225 Bird Species, Fruits 360, andOxford102 Flowers. These data sets have images of different resolutions, class numbers, and different achievable accuracies. We also apply their usual fully-connected layer and the Spi…▽ MoreThis study aims to explore different pre-trained models offered in the Torchvision package which is available in the PyTorch library. And investigate their effectiveness on fine-grained images classification. Transfer Learning is an effective method of achieving extremely good performance with insufficient training data. In many real-world situations, people cannot collect sufficient data required to train a deep neural network model efficiently. Transfer Learning models are pre-trained on a large data set, and can bring a good performance on smaller datasets with significantly lower training time. Torchvision package offers us many models to apply the Transfer Learning on smaller datasets. Therefore, researchers may need a guideline for the selection of a good model. We investigate Torchvision pre-trained models on four different data sets: 10 Monkey Species, 225 Bird Species, Fruits 360, andOxford102 Flowers. These data sets have images of different resolutions, class numbers, and different achievable accuracies. We also apply their usual fully-connected layer and the Spinal fully-connected layer to investigate the effectiveness of SpinalNet. The Spinal fully-connected layer brings better performance in most situations. We apply the same augmentation for different models for the same data set for a fair comparison. This paper may help future Computer Vision researchers in choosing a proper Transfer Learning model.△ Less"
Fine-Grained Adversarial Semi-supervised Learning,"Authors:Daniele Mugnai,Federico Pernici,Francesco Turchini,Alberto Del Bimbo","Abstract:…demonstrate the effectiveness of the combined use by conducting experiments on six state-of-the-art fine-grained datasets, which include Aircrafts, Stanford Cars, CUB-200-2011,OxfordFlowers, Stanford Dogs, and the recent Semi-Supervised iNaturalist-Aves. Experimental results clearly show that our proposed method has better performance than the only previou…▽ MoreIn this paper we exploit Semi-Supervised Learning (SSL) to increase the amount of training data to improve the performance of Fine-Grained Visual Categorization (FGVC). This problem has not been investigated in the past in spite of prohibitive annotation costs that FGVC requires. Our approach leverages unlabeled data with an adversarial optimization strategy in which the internal features representation is obtained with a second-order pooling model. This combination allows to back-propagate the information of the parts, represented by second-order pooling, onto unlabeled data in an adversarial training setting. We demonstrate the effectiveness of the combined use by conducting experiments on six state-of-the-art fine-grained datasets, which include Aircrafts, Stanford Cars, CUB-200-2011,OxfordFlowers, Stanford Dogs, and the recent Semi-Supervised iNaturalist-Aves. Experimental results clearly show that our proposed method has better performance than the only previous approach that examined this problem; it also obtained higher classification accuracy with respect to the supervised learning methods with which we compared.△ Less"
EchoVPR: Echo State Networks for Visual Place Recognition,"Authors:Anil Ozdemir,Mark Scerri,Andrew B. Barron,Andrew Philippides,Michael Mangan,Eleni Vasilaki,Luca Manneschi","Abstract:…neural networks led to a dramatic boost in performance in comparison to non-recurrent networks in five out of six standard benchmarks (GardensPoint, SPEDTest, ESSEX3IN1,OxfordRobotCar, and Nordland), demonstrating that ESNs are able to capture the temporal structure inherent in VPR problems. Moreover, we show that models that include ESNs can outperform cl…▽ MoreRecognising previously visited locations is an important, but unsolved, task in autonomous navigation. Current visual place recognition (VPR) benchmarks typically challenge models to recover the position of a query image (or images) from sequential datasets that include both spatial and temporal components. Recently, Echo State Network (ESN) varieties have proven particularly powerful at solving machine learning tasks that require spatio-temporal modelling. These networks are simple, yet powerful neural architectures that--exhibiting memory over multiple time-scales and non-linear high-dimensional representations--can discover temporal relations in the data while still maintaining linearity in the learning time. In this paper, we present a series of ESNs and analyse their applicability to the VPR problem. We report that the addition of ESNs to pre-processed convolutional neural networks led to a dramatic boost in performance in comparison to non-recurrent networks in five out of six standard benchmarks (GardensPoint, SPEDTest, ESSEX3IN1,OxfordRobotCar, and Nordland), demonstrating that ESNs are able to capture the temporal structure inherent in VPR problems. Moreover, we show that models that include ESNs can outperform class-leading VPR models which also exploit the sequential dynamics of the data. Finally, our results demonstrate that ESNs improve generalisation abilities, robustness, and accuracy further supporting their suitability to VPR applications.△ Less"
Large-Scale Topological Radar Localization Using Learned Descriptors,"Authors:Jacek Komorowski,Monika Wysoczanska,Tomasz Trzcinski","Abstract:…from a radar scan image. The performance and generalization ability of the proposed method is experimentally evaluated on two large scale driving datasets: MulRan andOxfordRadar RobotCar. Additionally, we present a comparative evaluation of radar-based and LiDAR-based localization using learned global descriptors. Our code and trained models are publicly a…▽ MoreIn this work, we propose a method for large-scale topological localization based on radar scan images using learned descriptors. We present a simple yet efficient deep network architecture to compute a rotationally invariant discriminative global descriptor from a radar scan image. The performance and generalization ability of the proposed method is experimentally evaluated on two large scale driving datasets: MulRan andOxfordRadar RobotCar. Additionally, we present a comparative evaluation of radar-based and LiDAR-based localization using learned global descriptors. Our code and trained models are publicly available on the project website.△ Less"
Seed Classification using Synthetic Image Datasets Generated from Low-Altitude UAV Imagery,"Authors:Venkat Margapuri,Niketa Penumajji,Mitchell Neilsen","Abstract:…AR Drone 2.0. Besides, the article proposes a seed classification framework as a proof-of-concept using the convolutional neural networks of Microsoft's ResNet-100,Oxford'sVGG-16, and VGG-19. To enhance the classification accuracy of the framework, an ensemble model is developed resulting in an overall accuracy of 94.6%.▽ MorePlant breeding programs extensively monitor the evolution of seed kernels for seed certification, wherein lies the need to appropriately label the seed kernels by type and quality. However, the breeding environments are large where the monitoring of seed kernels can be challenging due to the minuscule size of seed kernels. The use of unmanned aerial vehicles aids in seed monitoring and labeling since they can capture images at low altitudes whilst being able to access even the remotest areas in the environment. A key bottleneck in the labeling of seeds using UAV imagery is drone altitude i.e. the classification accuracy decreases as the altitude increases due to lower image detail. Convolutional neural networks are a great tool for multi-class image classification when there is a training dataset that closely represents the different scenarios that the network might encounter during evaluation. The article addresses the challenge of training data creation using Domain Randomization wherein synthetic image datasets are generated from a meager sample of seeds captured by the bottom camera of an autonomously driven Parrot AR Drone 2.0. Besides, the article proposes a seed classification framework as a proof-of-concept using the convolutional neural networks of Microsoft's ResNet-100,Oxford'sVGG-16, and VGG-19. To enhance the classification accuracy of the framework, an ensemble model is developed resulting in an overall accuracy of 94.6%.△ Less"
Data-Driven Electron Microscopy: Electron Diffraction Imaging of Materials Structural Properties,"Authors:Jian-Min Zuo,Renliang Yuan,Yu-Tsun Shao,Haw-Wen Hsiao,Saran Pidaparthy,Yang Hu,Qun Yang,Jiong Zhang","Abstract:Transmission electron diffraction is a powerful and versatile structural probe for the characterization of a broad range of materials, from nanocrystalline thin films to single crystals. With recent developments in fast electron detectors and efficient computer algorithms, it now becomes possible to collect unprecedently large datasets of diffraction patterns (DPs) and process DPs to extract cryst…▽ MoreTransmission electron diffraction is a powerful and versatile structural probe for the characterization of a broad range of materials, from nanocrystalline thin films to single crystals. With recent developments in fast electron detectors and efficient computer algorithms, it now becomes possible to collect unprecedently large datasets of diffraction patterns (DPs) and process DPs to extract crystallographic information to form images or tomograms based on crystal structural properties, giving rise to data-driven electron microscopy. Critical to this kind of imaging is the type of crystallographic information being collected, which can be achieved with a judicious choice of electron diffraction techniques, and the efficiency and accuracy of DP processing, which requires the development of new algorithms. Here, we review recent progress made in data collection, new algorithms, and automated electron DP analysis. These progresses will be highlighted using application examples in materials research. Future opportunities based on smart sampling and machine learning are also discussed.△ Less"
$B$-flavor tagging at Belle II,"Authors:F. Abudinén,N. Akopov,A. Aloisio,V. Babu,Sw. Banerjee,M. Bauer,J. V. Bennett,F. U. Bernlochner,M. Bessner,S. Bettarini,T. Bilka,S. Bilokin,D. Biswas,D. Bodrov,J. Borah,M. Bračko,P. Branchini,A. Budano,M. Campajola,G. Casarosa,C. Cecchi,R. Cheaib,V. Chekelian,C. Chen,Y. Q. Chen, et al. (96 additional authors not shown)","Abstract:We report on new flavor tagging algorithms developed to determine the quark-flavor content of bottom ($B$) mesons at Belle II. The algorithms provide essential inputs for measurements of quark-flavor mixing and charge-parity violation. We validate and evaluate the performance of the algorithms using hadronic $B$ decays with flavor-specific final states reconstructed in a data set corresponding to…▽ MoreWe report on new flavor tagging algorithms developed to determine the quark-flavor content of bottom ($B$) mesons at Belle II. The algorithms provide essential inputs for measurements of quark-flavor mixing and charge-parity violation. We validate and evaluate the performance of the algorithms using hadronic $B$ decays with flavor-specific final states reconstructed in a data set corresponding to an integrated luminosity of $62.8$ fb$^{-1}$, collected at the $Υ$(4$S$) resonance with the Belle II detector at the SuperKEKB collider. We measure the total effective tagging efficiency to be $\varepsilon_{\rm eff} = \big(30.0 \pm 1.2(\text{stat}) \pm 0.4(\text{syst})\big)\%$ for a category-based algorithm and $\varepsilon_{\rm eff} = \big(28.8 \pm 1.2(\text{stat}) \pm 0.4(\text{syst})\big)\%$ for a deep-learning-based algorithm.△ Less"
The Explanatory Gap in Algorithmic News Curation,Authors:Hendrik Heuer,"Abstract:Considering the large amount of available content, social media platforms increasingly employ machine learning (ML) systems to curate news. This paper examines how well different explanations help expert users understand why certain news stories are recommended to them. The expert users were journalists, who are trained to judge the relevance of news. Surprisingly, none of the explanations are per…▽ MoreConsidering the large amount of available content, social media platforms increasingly employ machine learning (ML) systems to curate news. This paper examines how well different explanations help expert users understand why certain news stories are recommended to them. The expert users were journalists, who are trained to judge the relevance of news. Surprisingly, none of the explanations are perceived as helpful. Our investigation provides a first indication of a gap between what is available to explain ML-based curation systems and what users need to understand such systems. We call this the Explanatory Gap in Machine Learning-based Curation Systems.△ Less"
Angular analysis of $B^+ \to ρ^+ρ^0$ decays reconstructed in 2019-2020 Belle II data,"Authors:Belle II collaboration,F. Abudinén,I. Adachi,R. Adak,K. Adamczyk,P. Ahlburg,J. K. Ahn,H. Aihara,N. Akopov,A. Aloisio,F. Ameli,L. Andricek,N. Anh Ky,D. M. Asner,H. Atmacan,V. Aulchenko,T. Aushev,V. Aushev,T. Aziz,V. Babu,S. Bacher,S. Baehr,S. Bahinipati,A. M. Bakich,P. Bambade, et al. (527 additional authors not shown)","Abstract:We report on the first Belle II measurement of the branching fraction ($\mathcal{B}$) and longitudinal polarization fraction ($f_L$) of $B^+\to ρ^+ρ^0$ decays. We reconstruct $B^+\to ρ^+(\to π^+π^0(\to γγ))ρ^0(\to π^+π^-)$ decays in a sample of SuperKEKB electron-positron collisions collected by the Belle II experiment in 2019 and 2020 at the $Υ$(4S) resonance and corresponding to $62.8$ fb…▽ MoreWe report on the first Belle II measurement of the branching fraction ($\mathcal{B}$) and longitudinal polarization fraction ($f_L$) of $B^+\to ρ^+ρ^0$ decays. We reconstruct $B^+\to ρ^+(\to π^+π^0(\to γγ))ρ^0(\to π^+π^-)$ decays in a sample of SuperKEKB electron-positron collisions collected by the Belle II experiment in 2019 and 2020 at the $Υ$(4S) resonance and corresponding to $62.8$ fb$^{-1}$ of integrated luminosity. We fit the distributions of the difference between expected and observed $B$ candidate energy, continuum-suppression variable, dipion masses, and angular distributions of the resulting samples, to determine a signal yield of $104\pm16$ events. The signal yields are corrected for efficiencies determined from simulation and control data samples to obtain $\mathcal{B}(B^+ \to ρ^+ρ^0) = [20.6 \pm 3.2(\rm stat) \pm 4.0(\rm syst)]\times 10^{-6}$, and $f_L(B^+ \to ρ^+ρ^0) = 0.936 ^{+0.049}_{-0.041}(\rm stat)\pm 0.021(\rm syst)$. This first Belle II $B^+ \to ρ^+ρ^0$ angular analysis yields results compatible with previous determinations, and indicates Belle II performance superior to early Belle results.△ Less"
Refined ultralight scalar dark matter searches with compact atom gradiometers,"Authors:Leonardo Badurina,Diego Blas,Christopher McCabe","Abstract:…Using these results, we refine the sensitivity estimates in the limit where shot noise dominates for AION-10, a compact 10 m gradiometer that will be operated inOxford, and discuss optimal experimental parameters that enhance the reach of searches for linearly-coupled scalar ULDM. After comparing the reach of devices operating in broadband and resonant mod…▽ MoreAtom interferometry is a powerful experimental technique that can be employed to search for the oscillation of atomic transition energies induced by ultralight scalar dark matter (ULDM). Previous studies have focused on the sensitivity to ULDM of km-length atom gradiometers, where atom interferometers are located at the ends of very long baselines. In this work, we generalize the treatment of the time-dependent signal induced by a linearly-coupled scalar ULDM candidate for vertical atom gradiometers of any length and find correction factors that especially impact the ULDM signal in short-baseline gradiometer configurations. Using these results, we refine the sensitivity estimates in the limit where shot noise dominates for AION-10, a compact 10 m gradiometer that will be operated inOxford, and discuss optimal experimental parameters that enhance the reach of searches for linearly-coupled scalar ULDM. After comparing the reach of devices operating in broadband and resonant modes, we show that well-designed compact atom gradiometers are able to explore regions of dark matter parameter space that are not yet constrained.△ Less"
Measurements of branching fractions and CP-violating charge asymmetries in multibody charmless $B$ decays reconstructed in 2019-2020 Belle II data,"Authors:Belle II collaboration,F. Abudinén,I. Adachi,R. Adak,K. Adamczyk,P. Ahlburg,J. K. Ahn,H. Aihara,N. Akopov,A. Aloisio,F. Ameli,L. Andricek,N. Anh Ky,D. M. Asner,H. Atmacan,V. Aulchenko,T. Aushev,V. Aushev,T. Aziz,V. Babu,S. Bacher,S. Baehr,S. Bahinipati,A. M. Bakich,P. Bambade, et al. (527 additional authors not shown)","Abstract:We report on measurements of branching fractions ($\mathcal{B}$) and CP-violating charge asymmetries ($\mathcal{A}_{\rm CP}$) of multibody charmless $B$ decays reconstructed by the Belle II experiment at the SuperKEKB electron-positron collider. We use a sample of collisions collected in 2019 and 2020 at the $Υ(4S)$ resonance and corresponding to $62.8$ fb$^{-1}$ of integrated luminosity. We use s…▽ MoreWe report on measurements of branching fractions ($\mathcal{B}$) and CP-violating charge asymmetries ($\mathcal{A}_{\rm CP}$) of multibody charmless $B$ decays reconstructed by the Belle II experiment at the SuperKEKB electron-positron collider. We use a sample of collisions collected in 2019 and 2020 at the $Υ(4S)$ resonance and corresponding to $62.8$ fb$^{-1}$ of integrated luminosity. We use simulation to determine optimized event selections. The $ΔE$ and $M_{\rm bc}$ distributions of the resulting samples are fit to determine signal yields of approximately 690, 840, and 380 decays for the channels $B^+ \to K^+K^-K^+$, $B^+ \to K^+π^-π^+$, and $B^0 \to K^+π^-π^0$, respectively. These yields are corrected for efficiencies determined from simulation and control data samples to obtain $\mathcal{B}(B^+ \to K^+K^-K^+) = [35.8 \pm 1.6(\rm stat) \pm 1.4 (\rm syst)]\times 10^{-6}$, $\mathcal{B}(B^+ \to K^+π^-π^+) = [67.0 \pm 3.3 (\rm stat)\pm 2.3 (\rm syst)]\times 10^{-6}$, $\mathcal{B}(B^0 \to K^+π^-π^0) = [38.1 \pm 3.5 (\rm stat)\pm 3.9 (\rm syst)]\times 10^{-6}$, $\mathcal{A}_{\rm CP}(B^+ \to K^+K^-K^+) = -0.103 \pm 0.042(\rm stat) \pm 0.020 (\rm syst)$, $\mathcal{A}_{\rm CP}(B^+ \to K^+π^-π^+) = -0.010 \pm 0.050 (\rm stat)\pm 0.021(\rm syst)$, and $\mathcal{A}_{\rm CP}(B^0 \to K^+π^-π^0) = 0.207 \pm 0.088 (\rm stat)\pm 0.011(\rm syst)$. Results are consistent with previous measurements and demonstrate detector performance comparable with the best Belle results.△ Less"
Periodic activity from fast radio burst FRB180916 explained in the frameof the orbiting asteroid model,"Authors:Guillaume Voisin,Fabrice Mottez,Philippe Zarka","Abstract:Observation of fast radio bursts (FRBs) are rising very quickly with the advent of specialised instruments and surveys, and it has recently been shown that some of them repeat quasi-periodically. In particular, evidence of a $P=16.35$ day period has been reported for FRB 180916.J0158+65. We seek an explanation within the frame of our orbiting asteroid model, whereby FRBs are produced in the plasma…▽ MoreObservation of fast radio bursts (FRBs) are rising very quickly with the advent of specialised instruments and surveys, and it has recently been shown that some of them repeat quasi-periodically. In particular, evidence of a $P=16.35$ day period has been reported for FRB 180916.J0158+65. We seek an explanation within the frame of our orbiting asteroid model, whereby FRBs are produced in the plasma wake of asteroids immersed in the wind of a pulsar or a magnetar. We used the data reported by the CHIME/FRB collaboration in order to infer the orbital characteristics of asteroid swarms, and performed parametric studies to explore the possible characteristics of the pulsar, its wind, and of the asteroids, under the constraint that the latter remain dynamically and thermally stable. We found a plausible configuration in which a young pulsar is orbited by a main $\sim 10^{-3}M_\odot$ companion with a period $3P = 49$d, three times longer than the apparent periodicity $P$. Asteroids responsible for FRBs are located in three dynamical swarms near the L3, L4 and L5 Lagrange points, in a 2:3 orbital resonance akin to the Hildas class of asteroids in the Solar system. In addition, asteroids could be present in the Trojan swarms at the L4 and L5 Lagrange points. Together these swarms form a carousel that explains the apparent $P$ periodicity and dispersion. We estimated that the presence of at least a few thousand asteroids, of size $\sim20$km, is necessary to produce the observed burst rate. We show how radius-to-frequency mapping in the wind and small perturbations by turbulence can suffice to explain downward-drifting sub-pulses, micro-structures, and narrow spectral occupancy.△ Less"
Measurement-Based Quantum Computation,Authors:Tzu-Chieh Wei,"Abstract:Measurement-based quantum computation is a framework of quantum computation, where entanglement is used as a resource and local measurements on qubits are used to drive the computation. It originates from the one-way quantum computer of Raussendorf and Briegel, who introduced the so-called cluster state as the underlying entangled resource state and showed that any quantum circuit could be execute…▽ MoreMeasurement-based quantum computation is a framework of quantum computation, where entanglement is used as a resource and local measurements on qubits are used to drive the computation. It originates from the one-way quantum computer of Raussendorf and Briegel, who introduced the so-called cluster state as the underlying entangled resource state and showed that any quantum circuit could be executed by performing only local measurement on individual qubits. The randomness in the measurement outcomes can be dealt with by adapting future measurement axes so that computation is deterministic. Subsequent works have expanded the discussions of the measurement-based quantum computation to various subjects, including the quantification of entanglement for such a measurement-based scheme, the search for other resource states beyond cluster states and computational phases of matter. In addition, the measurement-based framework also provides useful connections to the emergence of time ordering, computational complexity and classical spin models, blind quantum computation, etc. and has given an alternative, resource-efficient approach to implement the original linear-optic quantum computation of Knill, Laflamme and Milburn. Cluster states and a few other resource states have been created experimentally in various physical systems and the measurement-based approach offers a potential alternative to the standard circuit approach to realize a practical quantum computer.△ Less"
The Relational Interpretation of Quantum Physics,Authors:Carlo Rovelli,"Abstract:The relational interpretation (or RQM, for Relational Quantum Mechanics) solves the measurement problem by considering an ontology of sparse relative events, or ""facts"". Facts are realized in interactions between any two physical systems and are relative to these systems. RQM's technical core is the realisation that quantum transition amplitudes determine physical probabilities only when their arg…▽ MoreThe relational interpretation (or RQM, for Relational Quantum Mechanics) solves the measurement problem by considering an ontology of sparse relative events, or ""facts"". Facts are realized in interactions between any two physical systems and are relative to these systems. RQM's technical core is the realisation that quantum transition amplitudes determine physical probabilities only when their arguments are facts relative to the same system. The relativity of facts can be neglected in the approximation where decoherence hides interference, thus making facts approximately stable.△ Less"
Spiking Neural Networks for Visual Place Recognition via Weighted Neuronal Assignments,"Authors:Somayeh Hussaini,Michael Milford,Tobias Fischer","Abstract:…neurons and down-weighting ""ambiguous"" neurons that respond to multiple different reference places. In a range of experiments on the challenging Nordland,OxfordRobotCar, SPEDTest, Synthia, and St Lucia datasets, we show that our SNN achieves comparable VPR performance to state-of-the-art and classical techniques, and degrades gracefully in performa…▽ MoreSpiking neural networks (SNNs) offer both compelling potential advantages, including energy efficiency and low latencies and challenges including the non-differentiable nature of event spikes. Much of the initial research in this area has converted deep neural networks to equivalent SNNs, but this conversion approach potentially negates some of the advantages of SNN-based approaches developed from scratch. One promising area for high-performance SNNs is template matching and image recognition. This research introduces the first high-performance SNN for the Visual Place Recognition (VPR) task: given a query image, the SNN has to find the closest match out of a list of reference images. At the core of this new system is a novel assignment scheme that implements a form of ambiguity-informed salience, by up-weighting single-place-encoding neurons and down-weighting ""ambiguous"" neurons that respond to multiple different reference places. In a range of experiments on the challenging Nordland,OxfordRobotCar, SPEDTest, Synthia, and St Lucia datasets, we show that our SNN achieves comparable VPR performance to state-of-the-art and classical techniques, and degrades gracefully in performance with an increasing number of reference places. Our results provide a significant milestone towards SNNs that can provide robust, energy-efficient, and low latency robot localization.△ Less"
Evolution of axial perturbations in space-time of a non-rotating uncharged primordial black hole,"Authors:Arnab Sarkar,Sabiruddin Molla,K. Rajesh Nayak","Abstract:…the procedure of derivation of S. Chandrasekhar, for deriving the Regge-Wheeler equation for Schwarzschild metric [S. Chandrasekhar, The Mathematical Theory of Black holes ;OxfordUniversity Press (1983)] ; but it has some distinct differences with that due to the complexity and time-dependency of the generalized McVittie metric. We show that after applying…▽ MoreWe derive the equation governing the axial-perturbations in the space-time of a non-rotating uncharged primordial black hole (PBH), produced in early Universe, whose metric is taken as the generalized McVittie metric. The generalized McVittie metric is a cosmological black hole metric, proposed by V. Faraoni and A. Jacques in 2007 [Phys. Rev. D 76, 063510 (2007)]. This describes the space-time of a Schwarzschild black hole embedded in FLRW-Universe, while allowing its mass-change. Our derivation has basic similarities with the procedure of derivation of S. Chandrasekhar, for deriving the Regge-Wheeler equation for Schwarzschild metric [S. Chandrasekhar, The Mathematical Theory of Black holes ;OxfordUniversity Press (1983)] ; but it has some distinct differences with that due to the complexity and time-dependency of the generalized McVittie metric. We show that after applying some approximations which are very well valid in the early radiation-dominated Universe, the overall equation governing the axial perturbations can be separated into radial and angular parts, among which the radial part is the intended one, as the angular part is identical to the case of Schwarzschild metric as expected. We identify the potential from the Schrödinger-like format of the equation and draw some physical interpretation from it.△ Less"
Information Symmetry Matters: A Modal-Alternating Propagation Network for Few-Shot Learning,"Authors:Zhong Ji,Zhishen Hou,Xiyao Liu,Yanwei Pang,Jungong Han","Abstract:…the propagated information is more beneficial. Extensive experimental results on three semantic-labeled datasets, i.e., Caltech-UCSD-Birds 200-2011, SUN Attribute Database, andOxford102 Flower, have demonstrated that our proposed method achieves promising performance and outperforms the state-of-the-art approaches, which indicates the necessity of informat…▽ MoreSemantic information provides intra-class consistency and inter-class discriminability beyond visual concepts, which has been employed in Few-Shot Learning (FSL) to achieve further gains. However, semantic information is only available for labeled samples but absent for unlabeled samples, in which the embeddings are rectified unilaterally by guiding the few labeled samples with semantics. Therefore, it is inevitable to bring a cross-modal bias between semantic-guided samples and nonsemantic-guided samples, which results in an information asymmetry problem. To address this problem, we propose a Modal-Alternating Propagation Network (MAP-Net) to supplement the absent semantic information of unlabeled samples, which builds information symmetry among all samples in both visual and semantic modalities. Specifically, the MAP-Net transfers the neighbor information by the graph propagation to generate the pseudo-semantics for unlabeled samples guided by the completed visual relationships and rectify the feature embeddings. In addition, due to the large discrepancy between visual and semantic modalities, we design a Relation Guidance (RG) strategy to guide the visual relation vectors via semantics so that the propagated information is more beneficial. Extensive experimental results on three semantic-labeled datasets, i.e., Caltech-UCSD-Birds 200-2011, SUN Attribute Database, andOxford102 Flower, have demonstrated that our proposed method achieves promising performance and outperforms the state-of-the-art approaches, which indicates the necessity of information symmetry.△ Less"
Towards Fine-grained Image Classification with Generative Adversarial Networks and Facial Landmark Detection,"Authors:Mahdi Darvish,Mahsa Pouramini,Hamid Bahador","Abstract:…performance, they need an extensive volume of input data. To encounter this problem, we made the best use of GAN-based data augmentation to generate extra dataset instances.Oxford-IIIT Pets was our dataset of choice for this experiment. It consists of 37 breeds of cats and dogs with variations in scale, poses, and lighting, which intensifies the difficulty…▽ MoreFine-grained classification remains a challenging task because distinguishing categories needs learning complex and local differences. Diversity in the pose, scale, and position of objects in an image makes the problem even more difficult. Although the recent Vision Transformer models achieve high performance, they need an extensive volume of input data. To encounter this problem, we made the best use of GAN-based data augmentation to generate extra dataset instances.Oxford-IIIT Pets was our dataset of choice for this experiment. It consists of 37 breeds of cats and dogs with variations in scale, poses, and lighting, which intensifies the difficulty of the classification task. Furthermore, we enhanced the performance of the recent Generative Adversarial Network (GAN), StyleGAN2-ADA model to generate more realistic images while preventing overfitting to the training set. We did this by training a customized version of MobileNetV2 to predict animal facial landmarks; then, we cropped images accordingly. Lastly, we combined the synthetic images with the original dataset and compared our proposed method with standard GANs augmentation and no augmentation with different subsets of training data. We validated our work by evaluating the accuracy of fine-grained image classification on the recent Vision Transformer (ViT) Model.△ Less"
Memory capacity of neural network models,Authors:Stefano Fusi,"Abstract:Memory is a complex phenomenon that involves several distinct mechanisms. These mechanisms operate at different spatial and temporal levels. This chapter focuses on the theoretical framework and the mathematical models that have been developed to understand how these mechanisms are orchestrated to store, preserve and retrieve a large number of memories. In particular, this chapter reviews the theo…▽ MoreMemory is a complex phenomenon that involves several distinct mechanisms. These mechanisms operate at different spatial and temporal levels. This chapter focuses on the theoretical framework and the mathematical models that have been developed to understand how these mechanisms are orchestrated to store, preserve and retrieve a large number of memories. In particular, this chapter reviews the theoretical studies on memory capacity, in which the investigators estimated how the number of storable memories scales with the number of neurons and synapses in the neural circuitry. The memory capacity depends on the complexity of the synapses, the sparseness of the representations, the spatial and temporal correlations between memories and the specific way memories are retrieved. Complexity is important when the synapses can only be modified with a limited precision, as in the case of biological synapses, and sparseness can greatly increase memory capacity and be particularly beneficial when memories are structured (correlated to each other). The theoretical tools discussed by this chapter can be harnessed to identify the important computational principles that underlie memory storage, preservation and retrieval and provide guidance in designing and interpreting memory experiments.△ Less"
InOxfordHandbook on AI Governance: The Role of Workers in AI Ethics and Governance,"Authors:Nataliya Nedzhvetskaya,JS Tan","Abstract:While the role of states, corporations, and international organizations in AI governance has been extensively theorized, the role of workers has received comparatively little attention. This chapter looks at the role that workers play in identifying and mitigating harms from AI technologies. Harms are the causally assessed impacts of technologies. They arise despite technical reliability and are n…▽ MoreWhile the role of states, corporations, and international organizations in AI governance has been extensively theorized, the role of workers has received comparatively little attention. This chapter looks at the role that workers play in identifying and mitigating harms from AI technologies. Harms are the causally assessed impacts of technologies. They arise despite technical reliability and are not a result of technical negligence but rather of normative uncertainty around questions of safety and fairness in complex social systems. There is high consensus in the AI ethics community on the benefits of reducing harms but less consensus on mechanisms for determining or addressing harms. This lack of consensus has resulted in a number of collective actions by workers protesting how harms are identified and addressed in their workplace. We theorize the role of workers within AI governance and construct a model of harm reporting processes in AI workplaces. The harm reporting process involves three steps, identification, the governance decision, and the response. Workers draw upon three types of claims to argue for jurisdiction over questions of AI governance, subjection, control over the product of labor, and proximate knowledge of systems. Examining the past decade of AI related worker activism allows us to understand how different types of workers are positioned within a workplace that produces AI systems, how their position informs their claims, and the place of collective action in staking their claims. This chapter argues that workers occupy a unique role in identifying and mitigating harms caused by AI systems.△ Less"
Self-supervised Monocular Depth Estimation for All Day Images using Domain Separation,"Authors:Lina Liu,Xibin Song,Mengmeng Wang,Yong Liu,Liangjun Zhang","Abstract:…and depth maps effectively. Experimental results demonstrate that our approach achieves state-of-the-art depth estimation results for all-day images on the challengingOxfordRobotCar dataset, proving the superiority of our proposed approach.▽ MoreRemarkable results have been achieved by DCNN based self-supervised depth estimation approaches. However, most of these approaches can only handle either day-time or night-time images, while their performance degrades for all-day images due to large domain shift and the variation of illumination between day and night images. To relieve these limitations, we propose a domain-separated network for self-supervised depth estimation of all-day images. Specifically, to relieve the negative influence of disturbing terms (illumination, etc.), we partition the information of day and night image pairs into two complementary sub-spaces: private and invariant domains, where the former contains the unique information (illumination, etc.) of day and night images and the latter contains essential shared information (texture, etc.). Meanwhile, to guarantee that the day and night images contain the same information, the domain-separated network takes the day-time images and corresponding night-time images (generated by GAN) as input, and the private and invariant feature extractors are learned by orthogonality and similarity loss, where the domain gap can be alleviated, thus better depth maps can be expected. Meanwhile, the reconstruction and photometric losses are utilized to estimate complementary information and depth maps effectively. Experimental results demonstrate that our approach achieves state-of-the-art depth estimation results for all-day images on the challengingOxfordRobotCar dataset, proving the superiority of our proposed approach.△ Less"
Identifying and Exploiting Structures for Reliable Deep Learning,Authors:Amartya Sanyal,"Abstract:Deep learning research has recently witnessed an impressively fast-paced progress in a wide range of tasks including computer vision, natural language processing, and reinforcement learning. The extraordinary performance of these systems often gives the impression that they can be used to revolutionise our lives for the better. However, as recent works point out, these systems suffer from several…▽ MoreDeep learning research has recently witnessed an impressively fast-paced progress in a wide range of tasks including computer vision, natural language processing, and reinforcement learning. The extraordinary performance of these systems often gives the impression that they can be used to revolutionise our lives for the better. However, as recent works point out, these systems suffer from several issues that make them unreliable for use in the real world, including vulnerability to adversarial attacks (Szegedy et al. [248]), tendency to memorise noise (Zhang et al. [292]), being over-confident on incorrect predictions (miscalibration) (Guo et al. [99]), and unsuitability for handling private data (Gilad-Bachrach et al. [88]). In this thesis, we look at each of these issues in detail, investigate their causes, and propose computationally cheap algorithms for mitigating them in practice. To do this, we identify structures in deep neural networks that can be exploited to mitigate the above causes of unreliability of deep learning algorithms.△ Less"
NIAPU: network-informed adaptive positive-unlabeled learning for disease gene identification,"Authors:Paola Stolfi,Andrea Mastropietro,Giuseppe Pasculli,Paolo Tieri,Davide Vergni","Abstract:Gene-disease associations are fundamental for understanding disease etiology and developing effective interventions and treatments. Identifying genes not yet associated with a disease due to a lack of studies is a challenging task in which prioritization based on prior knowledge is an important element. The computational search for new candidate disease genes may be eased by positive-unlabeled lea…▽ MoreGene-disease associations are fundamental for understanding disease etiology and developing effective interventions and treatments. Identifying genes not yet associated with a disease due to a lack of studies is a challenging task in which prioritization based on prior knowledge is an important element. The computational search for new candidate disease genes may be eased by positive-unlabeled learning, the machine learning setting in which only a subset of instances are labeled as positive while the rest of the data set is unlabeled. In this work, we propose a set of effective network-based features to be used in a novel Markov diffusion-based multi-class labeling strategy for putative disease gene discovery. The performances of the new labeling algorithm and the effectiveness of the proposed features have been tested on ten different disease data sets using three machine learning algorithms. The new features have been compared against classical topological and functional/ontological features and a set of network- and biological-derived features already used in gene discovery tasks. The predictive power of the integrated methodology in searching for new disease genes has been found to be competitive against state-of-the-art algorithms.△ Less"
Bandit Algorithms for Precision Medicine,"Authors:Yangyi Lu,Ziping Xu,Ambuj Tewari","Abstract:TheOxfordEnglish Dictionary defines precision medicine as ""medical care designed to optimize efficiency or therapeutic benefit for particular groups of patients, especially by using genetic or molecular profiling."" It is not an entirely new idea: physicians from ancient times have recognized that medical treatment needs to consider individual varia…▽ MoreTheOxfordEnglish Dictionary defines precision medicine as ""medical care designed to optimize efficiency or therapeutic benefit for particular groups of patients, especially by using genetic or molecular profiling."" It is not an entirely new idea: physicians from ancient times have recognized that medical treatment needs to consider individual variations in patient characteristics. However, the modern precision medicine movement has been enabled by a confluence of events: scientific advances in fields such as genetics and pharmacology, technological advances in mobile devices and wearable sensors, and methodological advances in computing and data sciences.
  This chapter is about bandit algorithms: an area of data science of special relevance to precision medicine. With their roots in the seminal work of Bellman, Robbins, Lai and others, bandit algorithms have come to occupy a central place in modern data science ( Lattimore and Szepesvari, 2020). Bandit algorithms can be used in any situation where treatment decisions need to be made to optimize some health outcome. Since precision medicine focuses on the use of patient characteristics to guide treatment, contextual bandit algorithms are especially useful since they are designed to take such information into account. The role of bandit algorithms in areas of precision medicine such as mobile health and digital phenotyping has been reviewed before (Tewari and Murphy, 2017; Rabbi et al., 2019). Since these reviews were published, bandit algorithms have continued to find uses in mobile health and several new topics have emerged in the research on bandit algorithms. This chapter is written for quantitative researchers in fields such as statistics, machine learning, and operations research who might be interested in knowing more about the algorithmic and mathematical details of bandit algorithms that have been used in mobile health.△ Less"
Precise measurement of the $D^0$ and $D^+$ lifetimes at Belle II,"Authors:Belle II collaboration,F. Abudinén,I. Adachi,K. Adamczyk,L. Aggarwal,H. Ahmed,H. Aihara,N. Akopov,A. Aloisio,N. Anh Ky,D. M. Asner,H. Atmacan,V. Aushev,V. Babu,S. Bacher,H. Bae,S. Baehr,S. Bahinipati,P. Bambade,Sw. Banerjee,S. Bansal,M. Barrett,J. Baudot,M. Bauer,A. Baur, et al. (363 additional authors not shown)","Abstract:We report a measurement of the $D^0$ and $D^+$ lifetimes using $D^0\to K^-π^+$ and $D^+\to K^-π^+π^+$ decays reconstructed in $e^+e^-\to c\bar{c}$ data recorded by the Belle II experiment at the SuperKEKB asymmetric-energy $e^+e^-$ collider. The data, collected at center-of-mass energies at or near the $Υ(4S)$ resonance, correspond to an integrated luminosity of $72\,{\rm fb}^{-1}$. The results,…▽ MoreWe report a measurement of the $D^0$ and $D^+$ lifetimes using $D^0\to K^-π^+$ and $D^+\to K^-π^+π^+$ decays reconstructed in $e^+e^-\to c\bar{c}$ data recorded by the Belle II experiment at the SuperKEKB asymmetric-energy $e^+e^-$ collider. The data, collected at center-of-mass energies at or near the $Υ(4S)$ resonance, correspond to an integrated luminosity of $72\,{\rm fb}^{-1}$. The results, $τ(D^0) = 410.5\pm1.1\,{\rm(stat)}\pm0.8\,{\rm(syst.)}\,{\rm fs}$ and $τ(D^+) = 1030.4\pm4.7\,{\rm(stat)}\pm3.1\,{\rm(syst.)}\,{\rm fs}$, are the most precise to date and are consistent with previous determinations.△ Less"
DOLG: Single-Stage Image Retrieval with Deep Orthogonal Fusion of Local and Global Features,"Authors:Min Yang,Dongliang He,Miao Fan,Baorong Shi,Xuetong Xue,Fu Li,Errui Ding,Jizhou Huang","Abstract:…labels. Extensive experimental results validate the effectiveness of our solution and show that our model achieves state-of-the-art image retrieval performances on RevisitedOxfordand Paris datasets.▽ MoreImage Retrieval is a fundamental task of obtaining images similar to the query one from a database. A common image retrieval practice is to firstly retrieve candidate images via similarity search using global image features and then re-rank the candidates by leveraging their local features. Previous learning-based studies mainly focus on either global or local image representation learning to tackle the retrieval task. In this paper, we abandon the two-stage paradigm and seek to design an effective single-stage solution by integrating local and global information inside images into compact image representations. Specifically, we propose a Deep Orthogonal Local and Global (DOLG) information fusion framework for end-to-end image retrieval. It attentively extracts representative local information with multi-atrous convolutions and self-attention at first. Components orthogonal to the global image representation are then extracted from the local information. At last, the orthogonal components are concatenated with the global representation as a complementary, and then aggregation is performed to generate the final representation. The whole framework is end-to-end differentiable and can be trained with image-level labels. Extensive experimental results validate the effectiveness of our solution and show that our model achieves state-of-the-art image retrieval performances on RevisitedOxfordand Paris datasets.△ Less"
A Bayesian inference and model selection algorithm with an optimisation scheme to infer the model noise power,"Authors:J. Lopez-Santiago,L. Martino,J. Miguez,M. A. Vazquez","Abstract:Model fitting is possibly the most extended problem in science. Classical approaches include the use of least-squares fitting procedures and maximum likelihood methods to estimate the value of the parameters in the model. However, in recent years, Bayesian inference tools have gained traction. Usually, Markov chain Monte Carlo methods are applied to inference problems, but they present some disadv…▽ MoreModel fitting is possibly the most extended problem in science. Classical approaches include the use of least-squares fitting procedures and maximum likelihood methods to estimate the value of the parameters in the model. However, in recent years, Bayesian inference tools have gained traction. Usually, Markov chain Monte Carlo methods are applied to inference problems, but they present some disadvantages, particularly when comparing different models fitted to the same dataset. Other Bayesian methods can deal with this issue in a natural and effective way. We have implemented an importance sampling algorithm adapted to Bayesian inference problems in which the power of the noise in the observations is not known a priori. The main advantage of importance sampling is that the model evidence can be derived directly from the so-called importance weights -- while MCMC methods demand considerable postprocessing. The use of our adaptive target, adaptive importance sampling (ATAIS) method is shown by inferring, on the one hand, the parameters of a simulated flaring event which includes a damped oscillation {and, on the other hand, real data from the Kepler mission. ATAIS includes a novel automatic adaptation of the target distribution. It automatically estimates the variance of the noise in the model. ATAIS admits parallelisation, which decreases the computational run-times notably. We compare our method against a nested sampling method within a model selection problem.△ Less"
Tutorials on Testing Neural Networks,"Authors:Nicolas Berthier,Youcheng Sun,Wei Huang,Yanghao Zhang,Wenjie Ruan,Xiaowei Huang","Abstract:…and security. This tutorial is based on a stream of research conducted since the summer of 2018 at a few UK universities, including the University of Liverpool, University ofOxford, Queen's University Belfast, University of Lancaster, University of Loughborough, and University of Exeter.
  The research aims to adapt software engineering methods, in part…▽ MoreDeep learning achieves remarkable performance on pattern recognition, but can be vulnerable to defects of some important properties such as robustness and security. This tutorial is based on a stream of research conducted since the summer of 2018 at a few UK universities, including the University of Liverpool, University ofOxford, Queen's University Belfast, University of Lancaster, University of Loughborough, and University of Exeter.
  The research aims to adapt software engineering methods, in particular software testing methods, to work with machine learning models. Software testing techniques have been successful in identifying software bugs, and helping software developers in validating the software they design and implement. It is for this reason that a few software testing techniques -- such as the MC/DC coverage metric -- have been mandated in industrial standards for safety critical systems, including the ISO26262 for automotive systems and the RTCA DO-178B/C for avionics systems. However, these techniques cannot be directly applied to machine learning models, because the latter are drastically different from traditional software, and their design follows a completely different development life-cycle.
  As the outcome of this thread of research, the team has developed a series of methods that adapt the software testing techniques to work with a few classes of machine learning models. The latter notably include convolutional neural networks, recurrent neural networks, and random forest. The tools developed from this research are now collected, and publicly released, in a GitHub repository: \url{https://github.com/TrustAI/DeepConcolic}, with the BSD 3-Clause licence.
  This tutorial is to go through the major functionalities of the tools with a few running examples, to exhibit how the developed techniques work, what the results are, and how to interpret them.△ Less"
Persistent homology method to detect block structures in weighted networks,Authors:Wooseok Jung,"Abstract:Unravelling the block structure of a network is critical for studying macroscopic features and community-level dynamics. The weighted stochastic block model (WSBM), a variation of the traditional stochastic block model, is designed for weighted networks, but it is not always optimal. We introduce a novel topological method to study the block structure of weighted networks by comparing their persis…▽ MoreUnravelling the block structure of a network is critical for studying macroscopic features and community-level dynamics. The weighted stochastic block model (WSBM), a variation of the traditional stochastic block model, is designed for weighted networks, but it is not always optimal. We introduce a novel topological method to study the block structure of weighted networks by comparing their persistence diagrams. We found persistence diagrams of networks with different block structures show distinct features, sufficient to distinguish. Moreover, the overall characteristics are preserved even with more stochastic examples or modified hyperparameters. Finally, when random graphs whose latent block structure is unknown are tested, results from persistence diagram analysis are consistent with their weighted stochastic block model. Although this topological method cannot completely replace the original WSBM method for some reasons, it is worth to be investigated further.△ Less"
Online Estimation of Diameter at Breast Height (DBH) of Forest Trees Using a Handheld LiDAR,"Authors:Alexander Proudman,Milad Ramezani,Maurice Fallon","Abstract:…7 cm accuracy for 90% of individual trees in a forest (Wytham Woods,Oxford)▽ MoreWhile mobile LiDAR sensors are increasingly used to scan in ecology and forestry applications, reconstruction and characterisation are typically carried out offline (to the best of our knowledge). Motivated by this, we present an online LiDAR system which can run on a handheld device to segment and track individual trees and identify them in a fixed coordinate system. Segments relating to each tree are accumulated over time, and tree models are completed as more scans are captured from different perspectives. Using this reconstruction we then fit a cylinder model to each tree trunk by solving a least-squares optimisation over the points to estimate the Diameter at Breast Height (DBH) of the trees. Experimental results demonstrate that our system can estimate DBH to within $\sim$7 cm accuracy for 90% of individual trees in a forest (Wytham Woods,Oxford)△ Less"
CRD-CGAN: Category-Consistent and Relativistic Constraints for Diverse Text-to-Image Generation,"Authors:Tao Hu,Chengjiang Long,Chunxia Xiao","Abstract:…loss. Finally, we introduce a category-consistent loss to alleviate the over-category issues between K synthetic images. We evaluate our approach using the Birds-200-2011,Oxford-102 flower and MSCOCO 2014 datasets, and the extensive experiments demonstrate superiority of the proposed method in comparison with state-of-the-art methods in terms of photorealis…▽ MoreGenerating photo-realistic images from a text description is a challenging problem in computer vision. Previous works have shown promising performance to generate synthetic images conditional on text by Generative Adversarial Networks (GANs). In this paper, we focus on the category-consistent and relativistic diverse constraints to optimize the diversity of synthetic images. Based on those constraints, a category-consistent and relativistic diverse conditional GAN (CRD-CGAN) is proposed to synthesize $K$ photo-realistic images simultaneously. We use the attention loss and diversity loss to improve the sensitivity of the GAN to word attention and noises. Then, we employ the relativistic conditional loss to estimate the probability of relatively real or fake for synthetic images, which can improve the performance of basic conditional loss. Finally, we introduce a category-consistent loss to alleviate the over-category issues between K synthetic images. We evaluate our approach using the Birds-200-2011,Oxford-102 flower and MSCOCO 2014 datasets, and the extensive experiments demonstrate superiority of the proposed method in comparison with state-of-the-art methods in terms of photorealistic and diversity of the generated synthetic images.△ Less"
TargetNet: Functional microRNA Target Prediction with Deep Neural Networks,"Authors:Seonwoo Min,Byunghan Lee,Sungroh Yoon","Abstract:Motivation: MicroRNAs (miRNAs) play pivotal roles in gene expression regulation by binding to target sites of messenger RNAs (mRNAs). While identifying functional targets of miRNAs is of utmost importance, their prediction remains a great challenge. Previous computational algorithms have major limitations. They use conservative candidate target site (CTS) selection criteria mainly focusing on cano…▽ MoreMotivation: MicroRNAs (miRNAs) play pivotal roles in gene expression regulation by binding to target sites of messenger RNAs (mRNAs). While identifying functional targets of miRNAs is of utmost importance, their prediction remains a great challenge. Previous computational algorithms have major limitations. They use conservative candidate target site (CTS) selection criteria mainly focusing on canonical site types, rely on laborious and time-consuming manual feature extraction, and do not fully capitalize on the information underlying miRNA-CTS interactions. Results: In this paper, we introduce TargetNet, a novel deep learning-based algorithm for functional miRNA target prediction. To address the limitations of previous approaches, TargetNet has three key components: (1) relaxed CTS selection criteria accommodating irregularities in the seed region, (2) a novel miRNA-CTS sequence encoding scheme incorporating extended seed region alignments, and (3) a deep residual network-based prediction model. The proposed model was trained with miRNA-CTS pair datasets and evaluated with miRNA-mRNA pair datasets. TargetNet advances the previous state-of-the-art algorithms used in functional miRNA target classification. Furthermore, it demonstrates great potential for distinguishing high-functional miRNA targets.△ Less"
Probabilistic Appearance-Invariant Topometric Localization with New Place Awareness,"Authors:Ming Xu,Tobias Fischer,Niko Sünderhauf,Michael Milford","Abstract:…traverses which feature significant route detours from the reference map to be successfully localized. We perform extensive evaluation on multiple query traverses from theOxfordRobotCar dataset exhibiting both significant appearance change and deviations from routes previously traversed. In particular, we evaluate performance on two practically relevant lo…▽ MoreProbabilistic state-estimation approaches offer a principled foundation for designing localization systems, because they naturally integrate sequences of imperfect motion and exteroceptive sensor data. Recently, probabilistic localization systems utilizing appearance-invariant visual place recognition (VPR) methods as the primary exteroceptive sensor have demonstrated state-of-the-art performance in the presence of substantial appearance change. However, existing systems 1) do not fully utilize odometry data within the motion models, and 2) are unable to handle route deviations, due to the assumption that query traverses exactly repeat the mapping traverse. To address these shortcomings, we present a new probabilistic topometric localization system which incorporates full 3-dof odometry into the motion model and furthermore, adds an ""off-map"" state within the state-estimation framework, allowing query traverses which feature significant route detours from the reference map to be successfully localized. We perform extensive evaluation on multiple query traverses from theOxfordRobotCar dataset exhibiting both significant appearance change and deviations from routes previously traversed. In particular, we evaluate performance on two practically relevant localization tasks: loop closure detection and global localization. Our approach achieves major performance improvements over both existing and improved state-of-the-art systems.△ Less"
SilGAN: Generating driving maneuvers for scenario-based software-in-the-loop testing,"Authors:Dhasarathy Parthasarathy,Anton Johansson","Abstract:Automotive software testing continues to rely largely upon expensive field tests to ensure quality because alternatives like simulation-based testing are relatively immature. As a step towards lowering reliance on field tests, we present SilGAN, a deep generative model that eases specification, stimulus generation, and automation of automotive software-in-the-loop testing. The model is trained usi…▽ MoreAutomotive software testing continues to rely largely upon expensive field tests to ensure quality because alternatives like simulation-based testing are relatively immature. As a step towards lowering reliance on field tests, we present SilGAN, a deep generative model that eases specification, stimulus generation, and automation of automotive software-in-the-loop testing. The model is trained using data recorded from vehicles in the field. Upon training, the model uses a concise specification for a driving scenario to generate realistic vehicle state transitions that can occur during such a scenario. Such authentic emulation of internal vehicle behavior can be used for rapid, systematic and inexpensive testing of vehicle control software. In addition, by presenting a targeted method for searching through the information learned by the model, we show how a test objective like code coverage can be automated. The data driven end-to-end testing pipeline that we present vastly expands the scope and credibility of automotive simulation-based testing. This reduces time to market while helping maintain required standards of quality.△ Less"
Measurement of the branching fraction for $B^{0} \rightarrow π^{0} π^{0}$ decays reconstructed in 2019-2020 Belle II data,"Authors:Belle II Collaboration,F. Abudinén,I. Adachi,R. Adak,K. Adamczyk,P. Ahlburg,J. K. Ahn,H. Aihara,N. Akopov,A. Aloisio,F. Ameli,L. Andricek,N. Anh Ky,D. M. Asner,H. Atmacan,V. Aulchenko,T. Aushev,V. Aushev,T. Aziz,V. Babu,S. Bacher,S. Baehr,S. Bahinipati,A. M. Bakich,P. Bambade, et al. (529 additional authors not shown)","Abstract:We report the first reconstruction of the $B^{0} \to π^{0} π^{0}$ decay mode at Belle II using samples of 2019 and 2020 data that correspond to 62.8 fb$^{-1}$ of integrated luminosity. We find $14.0^{+6.8}_{-5.6}$ signal decays, corresponding to a significance of 3.4 standard deviations and determine a branching ratio of…▽ MoreWe report the first reconstruction of the $B^{0} \to π^{0} π^{0}$ decay mode at Belle II using samples of 2019 and 2020 data that correspond to 62.8 fb$^{-1}$ of integrated luminosity. We find $14.0^{+6.8}_{-5.6}$ signal decays, corresponding to a significance of 3.4 standard deviations and determine a branching ratio of $\mathcal{B}(B^{0} \rightarrow π^{0} π^{0}) = [0.98^{+0.48}_{-0.39} \pm 0.27] \times 10^{-6}$. The results agree with previous determinations and contribute important information to an early assessment of detector performance and Belle II's potential for future determinations of $α/φ_2$ using $B \rightarrow ππ$ modes.△ Less"
Assessing the Lockdown Effects on Air Quality during COVID-19 Era,"Authors:Ioannis Kavouras,Eftychios Protopapadakis,Maria Kaselimia,Emmanuel Sardis,Nikolaos Doulamis","Abstract:…Lodz and Rome). Available data on pollutant factors were obtained using global satellite observations. The level of the employed prevention measures is employed using theOxfordCOVID-19 Government Response Tracker. The second part of the analysis employed a variety of machine learning tools, utilized for estimating the concentration of each pollutant, two…▽ MoreIn this work we investigate the short-term variations in air quality emissions, attributed to the prevention measures, applied in different cities, to mitigate the COVID-19 spread. In particular, we emphasize on the concentration effects regarding specific pollutant gases, such as carbon monoxide (CO), ozone (O3), nitrogen dioxide (NO2) and sulphur dioxide (SO2). The assessment of the impact of lockdown on air quality focused on four European Cities (Athens, Gladsaxe, Lodz and Rome). Available data on pollutant factors were obtained using global satellite observations. The level of the employed prevention measures is employed using theOxfordCOVID-19 Government Response Tracker. The second part of the analysis employed a variety of machine learning tools, utilized for estimating the concentration of each pollutant, two days ahead. The results showed that a weak to moderate correlation exists between the corresponding measures and the pollutant factors and that it is possible to create models which can predict the behaviour of the pollutant gases under daily human activities.△ Less"
Rediscovery of $B^0\to J\mskip 1mu / ψ\mskip 2mu K^0_{\scriptscriptstyle L}$ at Belle II,"Authors:Belle II Collaboration,F. Abudinén,I. Adachi,R. Adak,K. Adamczyk,P. Ahlburg,J. K. Ahn,H. Aihara,N. Akopov,A. Aloisio,F. Ameli,L. Andricek,N. Anh Ky,D. M. Asner,H. Atmacan,V. Aulchenko,T. Aushev,V. Aushev,T. Aziz,V. Babu,S. Bacher,S. Baehr,S. Bahinipati,A. M. Bakich,P. Bambade, et al. (523 additional authors not shown)","Abstract:We present preliminary results on the reconstruction of the $B^0\to J\mskip 1mu / ψ\mskip 2mu K^0_{\scriptscriptstyle L}$ decay, where $J\mskip 1mu / ψ\mskip 2mu\toμ^+μ^-$ or $e^+e^-$. Using a dataset corresponding to a luminosity of $62.8\pm0.6\mbox{fb}^{-1}$ collected by the Belle II experiment at the SuperKEKB asymmetric energy $e^+e^-$ collider, we measure a total of $267\pm21$ candidates with…▽ MoreWe present preliminary results on the reconstruction of the $B^0\to J\mskip 1mu / ψ\mskip 2mu K^0_{\scriptscriptstyle L}$ decay, where $J\mskip 1mu / ψ\mskip 2mu\toμ^+μ^-$ or $e^+e^-$. Using a dataset corresponding to a luminosity of $62.8\pm0.6\mbox{fb}^{-1}$ collected by the Belle II experiment at the SuperKEKB asymmetric energy $e^+e^-$ collider, we measure a total of $267\pm21$ candidates with $J\mskip 1mu / ψ\mskip 2mu\toμ^+μ^-$ and $226\pm20$ with with $J\mskip 1mu / ψ\mskip 2mu\to e^+e^-$. The quoted errors are statistical only.△ Less"
A Deep Learning Approach to Private Data Sharing of Medical Images Using Conditional GANs,"Authors:Hanxi Sun,Jason Plawinski,Sajanth Subramaniam,Amir Jamaludin,Timor Kadir,Aimee Readie,Gregory Ligozio,David Ohlssen,Mark Baillie,Thibaud Coroller","Abstract:…to anonymization is sharing a synthetic dataset that bears a behaviour similar to the real data but preserves privacy. As part of the collaboration between Novartis and theOxfordBig Data Institute, we generate a synthetic dataset based on COSENTYX (secukinumab) Ankylosing Spondylitis clinical study. We apply an Auxiliary Classifier GAN to generate syntheti…▽ MoreSharing data from clinical studies can facilitate innovative data-driven research and ultimately lead to better public health. However, sharing biomedical data can put sensitive personal information at risk. This is usually solved by anonymization, which is a slow and expensive process. An alternative to anonymization is sharing a synthetic dataset that bears a behaviour similar to the real data but preserves privacy. As part of the collaboration between Novartis and theOxfordBig Data Institute, we generate a synthetic dataset based on COSENTYX (secukinumab) Ankylosing Spondylitis clinical study. We apply an Auxiliary Classifier GAN to generate synthetic MRIs of vertebral units. The images are conditioned on the VU location (cervical, thoracic and lumbar). In this paper, we present a method for generating a synthetic dataset and conduct an in-depth analysis on its properties along three key metrics: image fidelity, sample diversity and dataset privacy.△ Less"
The Heraklion Extragalactic Catalogue (HECATE): a value-added galaxy catalogue for multi-messenger astrophysics,"Authors:Konstantinos Kovlakas,Andreas Zezas,Jeff J. Andrews,Antara Basu-Zych,Tassos Fragos,Ann Hornschemeier,Konstantinos Kouroumpatzakis,Bret Lehmer,Andrew Ptak","Abstract:We present the Heraklion Extragalactic Catalogue, or HECATE, an all-sky value-added galaxy catalogue, aiming to facilitate present and future multi-wavelength and multi-messenger studies in the local Universe. It contains 204,733 galaxies up to a redshift of 0.047 (D<200 Mpc), and it is >50% complete in terms of the B-band luminosity density at distances in the 0-170 Mpc range. By incorporating an…▽ MoreWe present the Heraklion Extragalactic Catalogue, or HECATE, an all-sky value-added galaxy catalogue, aiming to facilitate present and future multi-wavelength and multi-messenger studies in the local Universe. It contains 204,733 galaxies up to a redshift of 0.047 (D<200 Mpc), and it is >50% complete in terms of the B-band luminosity density at distances in the 0-170 Mpc range. By incorporating and homogenising data from astronomical databases and multi-wavelength surveys, the catalogue offers positions, sizes, distances, morphological classifications, star-formation rates, stellar masses, metallicities, and nuclear activity classifications. This wealth of information can enable a wide-range of applications, such as: (i) demographic studies of extragalactic sources, (ii) initial characterisation of transient events, and (iii) searches for electromagnetic counterparts of gravitational-wave events. The catalogue is publicly available to the community at a dedicated portal, which will also host future extensions in terms of the covered volume, and data products.△ Less"
TheOxfordRoad Boundaries Dataset,"Authors:Tarlan Suleymanov,Matthew Gadd,Daniele De Martini,Paul Newman","Abstract:In this paper we present theOxfordRoad Boundaries Dataset, designed for training and testing machine-learning-based road-boundary detection and inference approaches. We have hand-annotated two of the 10 km-long forays from the…▽ MoreIn this paper we present theOxfordRoad Boundaries Dataset, designed for training and testing machine-learning-based road-boundary detection and inference approaches. We have hand-annotated two of the 10 km-long forays from theOxfordRobotcar Dataset and generated from other forays several thousand further examples with semi-annotated road-boundary masks. To boost the number of training samples in this way, we used a vision-based localiser to project labels from the annotated datasets to other traversals at different times and weather conditions. As a result, we release 62605 labelled samples, of which 47639 samples are curated. Each of these samples contains both raw and classified masks for left and right lenses. Our data contains images from a diverse set of scenarios such as straight roads, parked cars, junctions, etc. Files for download and tools for manipulating the labelled data are available at:oxford-robotics-institute.github.io/road-boundaries-dataset△ Less"
Planetary Systems Around White Dwarfs,Authors:Dimitri Veras,"Abstract:White dwarf planetary science is a rapidly growing field of research featuring a diverse set of observations and theoretical explorations. Giant planets, minor planets, and debris discs have all been detected orbiting white dwarfs. The innards of broken-up minor planets are measured on an element-by-element basis, providing a unique probe of exoplanetary chemistry. Numerical simulations and analyt…▽ MoreWhite dwarf planetary science is a rapidly growing field of research featuring a diverse set of observations and theoretical explorations. Giant planets, minor planets, and debris discs have all been detected orbiting white dwarfs. The innards of broken-up minor planets are measured on an element-by-element basis, providing a unique probe of exoplanetary chemistry. Numerical simulations and analytical investigations trace the violent physical and dynamical history of these systems from au-scale distances to the immediate vicinity of the white dwarf, where minor planets are broken down into dust and gas and are accreted onto the white dwarf photosphere. Current and upcoming ground-based and space-based instruments are likely to further accelerate the pace of discoveries.△ Less"
"Measurements of branching fractions and direct CP asymmetries in $B^{0}\to K^{+} π^{-}$, $B^+ \to K_{\rm S}^0π^+$ and $B^0 \to π^+π^-$ using 2019 and 2020 data","Authors:Belle II Collaboration,F. Abudinén,I. Adachi,R. Adak,K. Adamczyk,P. Ahlburg,J. K. Ahn,H. Aihara,N. Akopov,A. Aloisio,F. Ameli,L. Andricek,N. Anh Ky,D. M. Asner,H. Atmacan,V. Aulchenko,T. Aushev,V. Aushev,T. Aziz,V. Babu,S. Bacher,S. Baehr,S. Bahinipati,A. M. Bakich,P. Bambade, et al. (527 additional authors not shown)","Abstract:We report updated measurements of branching fractions ($\mathcal{B}$) and CP-violating charge asymmetries ($\mathcal{A_{\rm CP}}$) for charmless $B$ decays at Belle II, which operates on or near the $Υ$(4S) resonance at the SuperKEKB asymmetric energy $e^{+}e^{-}$ collider. We use samples of 2019 and 2020 data corresponding to 62.8 fb$^{-1}$ of integrated luminosity. The samples are analysed using…▽ MoreWe report updated measurements of branching fractions ($\mathcal{B}$) and CP-violating charge asymmetries ($\mathcal{A_{\rm CP}}$) for charmless $B$ decays at Belle II, which operates on or near the $Υ$(4S) resonance at the SuperKEKB asymmetric energy $e^{+}e^{-}$ collider. We use samples of 2019 and 2020 data corresponding to 62.8 fb$^{-1}$ of integrated luminosity. The samples are analysed using two-dimensional fits in $ΔE$ and $M_{\it bc}$ to determine signal yields of approximately 568, 103, and 115 decays for the channels $B^0 \to K^+π^-$, $B^+ \to K_{\rm S}^0π^+$, and $B^0 \to π^+π^-$, respectively. Signal yields are corrected for efficiencies determined from simulation and control data samples to obtain branching fractions and CP-violating asymmetries for flavour-specific channels. The results are compatible with known determinations and contribute important information to an early assessment of Belle II detector performance.△ Less"
SBML2Modelica: integrating biochemical models within open-standard simulation ecosystems,"Authors:Filippo Maggioli,Toni Mancini,Enrico Tronci","Abstract:Motivation: SBML is the most widespread language for the definition of biochemical models. Although dozens of SBML simulators are available, there is a general lack of support to the integration of SBML models within open-standard general-purpose simulation ecosystems. This hinders co-simulation and integration of SBML models within larger model networks, in order to, e.g. enable in silico clinica…▽ MoreMotivation: SBML is the most widespread language for the definition of biochemical models. Although dozens of SBML simulators are available, there is a general lack of support to the integration of SBML models within open-standard general-purpose simulation ecosystems. This hinders co-simulation and integration of SBML models within larger model networks, in order to, e.g. enable in silico clinical trials of drugs, pharmacological protocols, or engineering artefacts such as biomedical devices against Virtual Physiological Human models. Modelica is one of the most popular existing open-standard general-purpose simulation languages, supported by many simulators. Modelica models are especially suited for the definition of complex networks of heterogeneous models from virtually all application domains. Models written in Modelica (and in 100+ other languages) can be readily exported into black-box Functional Mock-Up Units (FMUs), and seamlessly co-simulated and integrated into larger model networks within open-standard language-independent simulation ecosystems.
  Results: In order to enable SBML model integration within heterogeneous model networks, we present SBML2Modelica, a software system translating SBML models into well-structured, user-intelligible, easily modifiable Modelica models. SBML2Modelica is SBML Level 3 Version 2-compliant and succeeds on 96.47% of the SBML Test Suite Core (with a few rare, intricate and easily avoidable combinations of constructs unsupported and cleanly signalled to the user). Our experimental campaign on 613 models from the BioModels database (with up to 5438 variables) shows that the major open-source (general-purpose) Modelica and FMU simulators achieve performance comparable to state-of-the-art specialized SBML simulators.
  Availability and implementation: https://bitbucket.org/mclab/sbml2modelica△ Less"
The RapidXMM Upper Limit Server: X-ray aperture photometry of the XMM-Newton archival observations,"Authors:A. Ruiz,A. Georgakakis,S. Gerakakis,R. Saxton,P. Kretschmar,A. Akylas,I. Georgantopoulos","Abstract:This paper presents the construction of the RapidXMM database that is available through the XMM-Newton Science Archive and offers access to upper limits and aperture photometry across the field of view of the XMM-Newton Pointed and Slew Survey observations. The feature of RapidXMM is speed. It enables the fast retrieval of X-ray upper limits and photometry products in three energy bands (0.2-2, 2-…▽ MoreThis paper presents the construction of the RapidXMM database that is available through the XMM-Newton Science Archive and offers access to upper limits and aperture photometry across the field of view of the XMM-Newton Pointed and Slew Survey observations. The feature of RapidXMM is speed. It enables the fast retrieval of X-ray upper limits and photometry products in three energy bands (0.2-2, 2-12, 0.2-12 keV) for large numbers of input sky positions. This is accomplished using the Hierarchical Equal Area Iso Latitude pixelation of the sphere (HEALPix). The pre-calculated upper-limits and associated X-ray photometry products are reprojected into the HEALPix grid of cells before being ingested into the RapidXMM database. This results in tables of upper limits and aperture photometry within HEALPix cells of size ~3 arcsec (Pointed Observations) and ~6 arcsec (Slew Survey). The database tables are indexed by the unique integer number of the HEALPix cells. This reduces spatial nearest-neighbor queries by sky position to an integer-matching exercise and significantly accelerates the retrieval of results. We describe in detail the processing steps that lead from the science products available in the XMM-Newton archive to a database optimised for sky queries. We also present two simple show-case applications of RapidXMM for scientific studies: searching for variable X-ray sources, and stacking analysis of X-ray faint populations△ Less"
A data mining approach for improved interpretation of ERT inverted sections using the DBSCAN clustering algorithm,"Authors:Kawtar Sabor,Damien Jougnot,Roger Guerin,Barthélémy Steck,Jean-Marie Henault,Louis Apffel,Denis Vautrin","Abstract:SUMMARY Geophysical imaging using the inversion procedure is a powerful tool for the exploration of the Earth's subsurface. However, the interpretation of inverted images can sometimes be difficult, due to the inherent limitations of existing inversion algorithms, which produce smoothed sections. In order to improve and automate the processing and interpretation of inverted geophysical models, we…▽ MoreSUMMARY Geophysical imaging using the inversion procedure is a powerful tool for the exploration of the Earth's subsurface. However, the interpretation of inverted images can sometimes be difficult, due to the inherent limitations of existing inversion algorithms, which produce smoothed sections. In order to improve and automate the processing and interpretation of inverted geophysical models, we propose an approach inspired from data mining. We selected an algorithm known as DBSCAN (Density-Based Spatial Clustering of Applications with Noise) to perform clustering of inverted geophysical sections. The methodology relies on the automatic sorting and clustering of data. DBSCAN detects clusters in the inverted electrical resistivity values, with no prior knowledge of the number of clusters. This algorithm has the advantage of being defined by only two parameters: the neighbourhood of a point in the data space, and the minimum number of data points in this neighbourhood. We propose an objective procedure for the determination of these two parameters. The proof of concept described here is applied to simulated ERT (electrical resistivity tomography) sections, for the following three cases: two layers with a step, two layers with a rebound, and two layers with an anomaly embedded in the upper layer. To validate this approach, sensitivity studies were carried out on both of the above parameters, as well as to assess the influence of noise on the algorithm's performance. Finally, this methodology was tested on real field data. DBSCAN detects clusters in the inverted electrical resistivity models, and the former are then associated with various types of earth materials, thus allowing the structure of the prospected area to be determined. The proposed data-mining algorithm is shown to be effective, and to improve the interpretation of the inverted ERT sections. This new approach has considerable potential, as it can be applied to any geophysical data represented in the form of sections or maps.△ Less"
Measurement of the time-integrated mixing probability $χ_d$ with a semileptonic double-tagging strategy and $34.6 {\rm fb}^{-1}$ of Belle II collision data,"Authors:Belle II Collaboration,F. Abudinén,I. Adachi,R. Adak,K. Adamczyk,P. Ahlburg,J. K. Ahn,H. Aihara,N. Akopov,A. Aloisio,F. Ameli,L. Andricek,N. Anh Ky,D. M. Asner,H. Atmacan,V. Aulchenko,T. Aushev,V. Aushev,T. Aziz,V. Babu,S. Bacher,S. Baehr,S. Bahinipati,A. M. Bakich,P. Bambade, et al. (528 additional authors not shown)","Abstract:We present the first measurement of the time-integrated mixing probability $χ_d$ using Belle II data collected at a center-of-mass (CM) energy of 10.58 GeV, corresponding to the mass of the $Υ$(4S) resonance, with an integrated luminosity of $34.6 {\rm fb}^{-1}$ at the SuperKEKB $e^+ e^-$ collider. We reconstruct pairs of B mesons both of which decay to semileptonic final states. Using a novel met…▽ MoreWe present the first measurement of the time-integrated mixing probability $χ_d$ using Belle II data collected at a center-of-mass (CM) energy of 10.58 GeV, corresponding to the mass of the $Υ$(4S) resonance, with an integrated luminosity of $34.6 {\rm fb}^{-1}$ at the SuperKEKB $e^+ e^-$ collider. We reconstruct pairs of B mesons both of which decay to semileptonic final states. Using a novel methodology, we measure $χ_d = 0.187 \pm 0.010 \text{ (stat.)} \pm 0.019 \text{ (syst.)}$, which is compatible with existing indirect and direct determinations.△ Less"
Can Attention Enable MLPs To Catch Up With CNNs?,"Authors:Meng-Hao Guo,Zheng-Ning Liu,Tai-Jiang Mu,Dun Liang,Ralph R. Martin,Shi-Min Hu","Abstract:In the first week of May, 2021, researchers from four different institutions: Google, Tsinghua University,OxfordUniversity and Facebook, shared their latest work [16, 7, 12, 17] on arXiv.org almost at the same time, each proposing new learning architectures, consisting mainly of linear layers, claiming them to be comparable, or even superior to convolution…▽ MoreIn the first week of May, 2021, researchers from four different institutions: Google, Tsinghua University,OxfordUniversity and Facebook, shared their latest work [16, 7, 12, 17] on arXiv.org almost at the same time, each proposing new learning architectures, consisting mainly of linear layers, claiming them to be comparable, or even superior to convolutional-based models. This sparked immediate discussion and debate in both academic and industrial communities as to whether MLPs are sufficient, many thinking that learning architectures are returning to MLPs. Is this true? In this perspective, we give a brief history of learning architectures, including multilayer perceptrons (MLPs), convolutional neural networks (CNNs) and transformers. We then examine what the four newly proposed architectures have in common. Finally, we give our views on challenges and directions for new learning architectures, hoping to inspire future research.△ Less"
Radar Odometry Combining Probabilistic Estimation and Unsupervised Feature Learning,"Authors:Keenan Burnett,David J. Yoon,Angela P. Schoellig,Timothy D. Barfoot","Abstract:…for processing rich radar data, and a non-differentiable classic estimator for probabilistic inference. We provide extensive experimental results on both the publicly availableOxfordRadar RobotCar Dataset and an additional 100 km of driving collected in an urban setting. Our sliding-window implementation of radar odometry outperforms most hand-crafted meth…▽ MoreThis paper presents a radar odometry method that combines probabilistic trajectory estimation and deep learned features without needing groundtruth pose information. The feature network is trained unsupervised, using only the on-board radar data. With its theoretical foundation based on a data likelihood objective, our method leverages a deep network for processing rich radar data, and a non-differentiable classic estimator for probabilistic inference. We provide extensive experimental results on both the publicly availableOxfordRadar RobotCar Dataset and an additional 100 km of driving collected in an urban setting. Our sliding-window implementation of radar odometry outperforms most hand-crafted methods and approaches the current state of the art without requiring a groundtruth trajectory for training. We also demonstrate the effectiveness of radar odometry under adverse weather conditions. Code for this project can be found at: https://github.com/utiasASRL/hero_radar_odometry△ Less"
Identifying Brexit voting patterns in the British House of Commons: an analysis based on Bayesian mixture models with flexible concomitant covariate effects,"Authors:Marco Berrettini,Giuliano Galimberti,Saverio Ranciati,Thomas Brendan Murphy","Abstract:Brexit and its implications are an ongoing topic of interest since the Brexit referendum in 2016. In 2019 the House of commons held a number of ""indicative"" and ""meaningful"" votes as part of the Brexit approval process. The voting behaviour of members of the parliament in these votes is investigated to gain insight into the Brexit approval process. In particular, a mixture model with concomitant c…▽ MoreBrexit and its implications are an ongoing topic of interest since the Brexit referendum in 2016. In 2019 the House of commons held a number of ""indicative"" and ""meaningful"" votes as part of the Brexit approval process. The voting behaviour of members of the parliament in these votes is investigated to gain insight into the Brexit approval process. In particular, a mixture model with concomitant covariates is developed to identify groups of members of parliament who share similar voting behaviour while also considering characteristics of the members of parliament. The novelty of the method lies in the flexible structure used to model the effect of concomitant covariates on the component weights of the mixture, with the (potentially nonlinear) terms represented as a smooth function of the covariates. Results show this approach allows to quantify the effect of the age of members of parliament, as well as preferences and competitiveness in the constituencies they represent, on their position towards Brexit. This helps grouping the aforementioned politicians into homogeous clusters, whose composition departs sensibly from that of the parties.△ Less"
Role of Quarks in Nuclear Structure,Authors:Anthony W Thomas,"Abstract:The strong force that binds atomic nuclei is governed by the rules of Quantum Chromodynamics. Here we consider the suggestion the internal quark structure of a nucleon will adjust self-consistently to the local mean scalar field in a nuclear medium and that this may play a profound role in nuclear structure. We show that one can derive an energy density functional based on this idea, which success…▽ MoreThe strong force that binds atomic nuclei is governed by the rules of Quantum Chromodynamics. Here we consider the suggestion the internal quark structure of a nucleon will adjust self-consistently to the local mean scalar field in a nuclear medium and that this may play a profound role in nuclear structure. We show that one can derive an energy density functional based on this idea, which successfully describes the properties of atomic nuclei across the periodic table in terms of a small number of physically motivated parameters. Because this approach amounts to a new paradigm for nuclear theory, it is vital to find ways to test it experimentally and we review a number of the most promising possibilities.△ Less"
Dynamic Pooling Improves Nanopore Base Calling Accuracy,"Authors:Vladimír Boža,Peter Perešíni,Broňa Brejová,Tomáš Vinař","Abstract:…of dynamic pooling, we developed two base callers: Heron and Osprey. Heron improves the accuracy beyond the experimental high-accuracy base caller Bonito developed byOxfordNanopore. Osprey is a fast base caller that can compete in accuracy with Guppy high-accuracy mode, but does not require GPU acceleration and achieves a near real-time speed on common des…▽ MoreIn nanopore sequencing, electrical signal is measured as DNA molecules pass through the sequencing pores. Translating these signals into DNA bases (base calling) is a highly non-trivial task, and its quality has a large impact on the sequencing accuracy. The most successful nanopore base callers to date use convolutional neural networks (CNN) to accomplish the task.
  Convolutional layers in CNNs are typically composed of filters with constant window size, performing best in analysis of signals with uniform speed. However, the speed of nanopore sequencing varies greatly both within reads and between sequencing runs. Here, we present dynamic pooling, a novel neural network component, which addresses this problem by adaptively adjusting the pooling ratio. To demonstrate the usefulness of dynamic pooling, we developed two base callers: Heron and Osprey. Heron improves the accuracy beyond the experimental high-accuracy base caller Bonito developed byOxfordNanopore. Osprey is a fast base caller that can compete in accuracy with Guppy high-accuracy mode, but does not require GPU acceleration and achieves a near real-time speed on common desktop CPUs.
  Availability: https://github.com/fmfi-compbio/osprey, https://github.com/fmfi-compbio/heron
  Keywords: nanopore sequencing, base calling, convolutional neural networks, pooling△ Less"
The Atmosphere of Uranus,Authors:Leigh N. Fletcher,"Abstract:Uranus provides a unique laboratory to test our understanding of planetary atmospheres under extreme conditions. Multi-spectral observations from Voyager, ground-based observatories, and space telescopes have revealed a delicately banded atmosphere punctuated by storms, waves, and dark vortices, evolving slowly under the seasonal influence of Uranus' extreme axial tilt. Condensables like methane a…▽ MoreUranus provides a unique laboratory to test our understanding of planetary atmospheres under extreme conditions. Multi-spectral observations from Voyager, ground-based observatories, and space telescopes have revealed a delicately banded atmosphere punctuated by storms, waves, and dark vortices, evolving slowly under the seasonal influence of Uranus' extreme axial tilt. Condensables like methane and hydrogen sulphide play a crucial role in shaping circulation, clouds, and storm phenomena via latent heat release through condensation, strong equator-to-pole gradients suggestive of equatorial upwelling and polar subsidence, and through forming stabilising layers that may decouple different circulation and convective regimes as a function of depth. Weak vertical mixing and low atmospheric temperatures associated with Uranus' negligible internal heat means that stratospheric methane photochemistry occurs in a unique high-pressure regime, decoupled from the influx of external oxygen. The low homopause also allows for the formation of an extensive ionosphere. Finally, the atmosphere provides a window on the bulk composition of Uranus - the ice-to-rock ratio, supersolar elemental and isotopic enrichments inferred from remote sensing and future \textit{in situ} measurements - providing key insights into its formation and subsequent migration. This review reveals the state of our knowledge of the time-variable circulation, composition, meteorology, chemistry, and clouds on this enigmatic `Ice Giant,' summarising insights from more than three decades of observations, and highlighting key questions for the next generation of planetary missions. As a hydrogen-dominated, intermediate-sized, and chemically-enriched world, Uranus could be our closest and best example of atmospheric processes on a class of worlds that may dominate the census of planets beyond our own Solar System.△ Less"
Deep Spiking Convolutional Neural Network for Single Object Localization Based On Deep Continuous Local Learning,"Authors:Sami Barchid,José Mennesson,Chaabane Djéraba","Abstract:…object in a grayscale image. We propose a network based on DECOLLE, a spiking model that enables local surrogate gradient-based learning. The encouraging results reported onOxford-IIIT-Pet validates the exploitation of spiking neural networks with a supervised learning approach for more elaborate vision tasks in the future.▽ MoreWith the advent of neuromorphic hardware, spiking neural networks can be a good energy-efficient alternative to artificial neural networks. However, the use of spiking neural networks to perform computer vision tasks remains limited, mainly focusing on simple tasks such as digit recognition. It remains hard to deal with more complex tasks (e.g. segmentation, object detection) due to the small number of works on deep spiking neural networks for these tasks. The objective of this paper is to make the first step towards modern computer vision with supervised spiking neural networks. We propose a deep convolutional spiking neural network for the localization of a single object in a grayscale image. We propose a network based on DECOLLE, a spiking model that enables local surrogate gradient-based learning. The encouraging results reported onOxford-IIIT-Pet validates the exploitation of spiking neural networks with a supervised learning approach for more elaborate vision tasks in the future.△ Less"
Measurements of branching fractions and direct ${\it CP}$-violating asymmetries in $B^+ \to K^+ π^0~\mbox{and}~π^+ π^0$ decays using 2019 and 2020 Belle II data,"Authors:F. Abudinén,I. Adachi,R. Adak,K. Adamczyk,P. Ahlburg,J. K. Ahn,H. Aihara,N. Akopov,A. Aloisio,F. Ameli,L. Andricek,N. Anh Ky,D. M. Asner,H. Atmacan,V. Aulchenko,T. Aushev,V. Aushev,T. Aziz,V. Babu,S. Bacher,S. Baehr,S. Bahinipati,A. M. Bakich,P. Bambade,Sw. Banerjee, et al. (527 additional authors not shown)","Abstract:We report measurements of branching fractions ($\mathcal B$) and direct ${\it CP}$-violating asymmetries ($\mathcal A_{\it CP}$) for the decays $B^+\to K^+π^0$ and $B^+ \to π^+π^0$ reconstructed with the Belle II detector in a sample of asymmetric-energy electron-positron collisions at the $Υ(4S)$ resonance corresponding to 62.8 $\text{fb}^{-1}$ of integrated luminosity. The results are…▽ MoreWe report measurements of branching fractions ($\mathcal B$) and direct ${\it CP}$-violating asymmetries ($\mathcal A_{\it CP}$) for the decays $B^+\to K^+π^0$ and $B^+ \to π^+π^0$ reconstructed with the Belle II detector in a sample of asymmetric-energy electron-positron collisions at the $Υ(4S)$ resonance corresponding to 62.8 $\text{fb}^{-1}$ of integrated luminosity. The results are $\mathcal{B}(B^+ \to K^+π^0) = [11.9 ^{+1.1}_{-1.0} (\rm stat)\pm 1.6(\rm syst)]\times 10^{-6}$, $\mathcal{B}(B^+ \to π^+π^0) = [5.5 ^{+1.0}_{-0.9} (\rm stat)\pm 0.7(\rm syst)]\times 10^{-6}$, $\mathcal A_{\it CP}(B^+ \to K^+π^0) = -0.09 \pm 0.09 (\rm stat)\pm 0.03(\rm syst)$, and $\mathcal A_{\it CP}(B^+ \to π^+π^0) = -0.04 \pm 0.17 (\rm stat)\pm 0.06(\rm syst)$. The results are consistent with previous measurements and show a detector performance comparable with early Belle performance.△ Less"
Probabilistic Visual Place Recognition for Hierarchical Localization,"Authors:Ming Xu,Niko Sünderhauf,Michael Milford","Abstract:…of the coarse localization stage using our methods, whilst retaining state-of-the-art performance under severe appearance change. Using extensive experimentation on theOxfordRobotCar dataset, results show that our approach outperforms comparable state-of-the-art methods in terms of precision-recall performance for localizing image sequences. In addition, o…▽ MoreVisual localization techniques often comprise a hierarchical localization pipeline, with a visual place recognition module used as a coarse localizer to initialize a pose refinement stage. While improving the pose refinement step has been the focus of much recent research, most work on the coarse localization stage has focused on improvements like increased invariance to appearance change, without improving what can be loose error tolerances. In this letter, we propose two methods which adapt image retrieval techniques used for visual place recognition to the Bayesian state estimation formulation for localization. We demonstrate significant improvements to the localization accuracy of the coarse localization stage using our methods, whilst retaining state-of-the-art performance under severe appearance change. Using extensive experimentation on theOxfordRobotCar dataset, results show that our approach outperforms comparable state-of-the-art methods in terms of precision-recall performance for localizing image sequences. In addition, our proposed methods provides the flexibility to contextually scale localization latency in order to achieve these improvements. The improved initial localization estimate opens up the possibility of both improved overall localization performance and modified pose refinement techniques that leverage this improved spatial prior.△ Less"
Consciousness and the Collapse of the Wave Function,"Authors:David J. Chalmers,Kelvin J. McQueen","Abstract:Does consciousness collapse the quantum wave function? This idea was taken seriously by John von Neumann and Eugene Wigner but is now widely dismissed. We develop the idea by combining a mathematical theory of consciousness (integrated information theory) with an account of quantum collapse dynamics (continuous spontaneous localization). Simple versions of the theory are falsified by the quantum Z…▽ MoreDoes consciousness collapse the quantum wave function? This idea was taken seriously by John von Neumann and Eugene Wigner but is now widely dismissed. We develop the idea by combining a mathematical theory of consciousness (integrated information theory) with an account of quantum collapse dynamics (continuous spontaneous localization). Simple versions of the theory are falsified by the quantum Zeno effect, but more complex versions remain compatible with empirical evidence. In principle, versions of the theory can be tested by experiments with quantum computers. The upshot is not that consciousness-collapse interpretations are clearly correct, but that there is a research program here worth exploring.△ Less"
First search for direct $CP$-violating asymmetry in $B^0 \to K^0 π^0$ decays at Belle II,"Authors:Belle II Collaboration,F. Abudinén,I. Adachi,R. Adak,K. Adamczyk,P. Ahlburg,J. K. Ahn,H. Aihara,N. Akopov,A. Aloisio,F. Ameli,L. Andricek,N. Anh Ky,D. M. Asner,H. Atmacan,V. Aulchenko,T. Aushev,V. Aushev,T. Aziz,V. Babu,S. Bacher,S. Baehr,S. Bahinipati,A. M. Bakich,P. Bambade, et al. (529 additional authors not shown)","Abstract:We report on the first measurement of the direct $CP$-violating asymmetry ($\mathcal{A}$) in the charmless decay $B^0 \to K^0π^0$ at Belle II and an updated measurement of its branching fraction ($\mathcal{B}$). We use a sample of electron-positron collisions collected in 2019 and 2020 at the $Υ(4S)$ resonance and corresponding to $62.8$ $\text{fb}^{-1}$ of integrated luminosity. We reconstruct an…▽ MoreWe report on the first measurement of the direct $CP$-violating asymmetry ($\mathcal{A}$) in the charmless decay $B^0 \to K^0π^0$ at Belle II and an updated measurement of its branching fraction ($\mathcal{B}$). We use a sample of electron-positron collisions collected in 2019 and 2020 at the $Υ(4S)$ resonance and corresponding to $62.8$ $\text{fb}^{-1}$ of integrated luminosity. We reconstruct and select about $50$ $B^0 \to K_S^0 π^0$ candidates, and we measure $\mathcal{A}_{K^0π^0} = -0.40_{-0.44}^{+0.46} (\text{stat}) \pm 0.04 (\text{syst})$ and $\mathcal{B}(B^0 \to K^0 π^0) = [8.5_{-1.6}^{+1.7} (\text{stat}) \pm 1.2 (\text{syst})] \times 10^{-6}$. This is the first measurement of $CP$ violation in $B^0 \to K^0π^0$ decays reported by Belle II. The results agree with previous determinations and show a detector performance comparable with the best Belle results.△ Less"
Probability and Irreversibility in Modern Statistical Mechanics: Classical and Quantum,Authors:David Wallace,"Abstract:Through extended consideration of two wide classes of case studies -- dilute gases and linear systems -- I explore the ways in which assumptions of probability and irreversibility occur in contemporary statistical mechanics, where the latter is understood as primarily concerned with the derivation of quantitative higher-level equations of motion, and only derivatively with underpinning the equilib…▽ MoreThrough extended consideration of two wide classes of case studies -- dilute gases and linear systems -- I explore the ways in which assumptions of probability and irreversibility occur in contemporary statistical mechanics, where the latter is understood as primarily concerned with the derivation of quantitative higher-level equations of motion, and only derivatively with underpinning the equilibrium concept in thermodynamics. I argue that at least in this wide class of examples, (i) irreversibility is introduced through a reasonably well-defined initial-state condition which does not precisely map onto those in the extant philosophical literature; (ii) probability is explicitly required both in the foundations and in the predictions of the theory. I then consider the same examples, as well as the more general context, in the light of quantum mechanics, and demonstrate that while the analysis of irreversibility is largely unaffected by quantum considerations, the notion of statistical-mechanical probability is entirely reduced to quantum-mechanical probability.△ Less"
An Improved Discriminative Optimization for 3D Rigid Point Cloud Registration,"Authors:Jia Wang,Ping Wang,Biao Li,Ruigang Fu,Junzheng Wu","Abstract:…In addition, we reweighted the extended histogram according to the model points' distribution. We evaluated the proposed Improved DO on the Stanford Bunny andOxfordSensatUrban dataset, and compared it with six classical State-Of-The-Art point cloud registration algorithms. The experimental result demonstrates our algorithm achieves comparable performa…▽ MoreThe Discriminative Optimization (DO) algorithm has been proved much successful in 3D point cloud registration. In the original DO, the feature (descriptor) of two point cloud was defined as a histogram, and the element of histogram indicates the weights of scene points in ""front"" or ""back"" side of a model point. In this paper, we extended the histogram which indicate the sides from ""front-back"" to ""front-back"", ""up-down"", and ""clockwise-anticlockwise"". In addition, we reweighted the extended histogram according to the model points' distribution. We evaluated the proposed Improved DO on the Stanford Bunny andOxfordSensatUrban dataset, and compared it with six classical State-Of-The-Art point cloud registration algorithms. The experimental result demonstrates our algorithm achieves comparable performance in point registration accuracy and root-mean-sqart-error.△ Less"
Measurement of the branching fractions of $B\toη' K$ decays using 2019/2020 Belle II data,"Authors:Belle II Collaboration,F. Abudinén,I. Adachi,R. Adak,K. Adamczyk,P. Ahlburg,J. K. Ahn,H. Aihara,N. Akopov,A. Aloisio,F. Ameli,L. Andricek,N. Anh Ky,D. M. Asner,H. Atmacan,V. Aulchenko,T. Aushev,V. Aushev,T. Aziz,V. Babu,S. Bacher,S. Baehr,S. Bahinipati,A. M. Bakich,P. Bambade, et al. (523 additional authors not shown)","Abstract:This note describes the rediscovery of $B\toη' K$ decays in Belle II data, both in the charged and neutral final state: $B_0\toη' K_S$ and $B^\pm\toη' K^\pm$. The $η'$ is searched for in two decay modes: $η'\toηπ^+π^-$ with $η\toγγ$, and $η'\toργ$. The analysis uses data collected in 2019 and 2020 at the SuperKEKB asymmetric $e^+e^-$ collider, with an integrated luminosity of $62.8~fb^{-1}$, corre…▽ MoreThis note describes the rediscovery of $B\toη' K$ decays in Belle II data, both in the charged and neutral final state: $B_0\toη' K_S$ and $B^\pm\toη' K^\pm$. The $η'$ is searched for in two decay modes: $η'\toηπ^+π^-$ with $η\toγγ$, and $η'\toργ$. The analysis uses data collected in 2019 and 2020 at the SuperKEKB asymmetric $e^+e^-$ collider, with an integrated luminosity of $62.8~fb^{-1}$, corresponding to $68.2$ million of $B\bar{B}$ pairs produced. The signal yield is obtained via an unbinned maximum likelihood fit to signal sensitive variables, obtaining branching ratios:
  $$\mathcal{B}\left(B^\pm\toη'K^\pm\right) = \left(63.4~^{+3.4}_{-3.3}\,(stat)\,\pm3.2\,(syst)\,\right) \times10^{-6} $$
  $$\mathcal{B}\left(B_0\toη'K_S\right) = \left(59.9~^{+5.8}_{-5.5}\,(stat)\,\pm2.9\,(syst)\,\right) \times10^{-6} $$ which are consistent with world average.△ Less"
Hyperspectral Pigment Analysis of Cultural Heritage Artifacts Using the Opaque Form of Kubelka-Munk Theory,"Authors:Abu Md Niamul Taufique,David W. Messinger","Abstract:…a navigational map of the South China Sea likely created in the early seventeenth century. Hyperspectral data of the map was collected at the Bodleian Library, University ofOxford, and can be used to estimate the pigment diversity, and spatial distribution, within the map. This work seeks to assess the utility of analyzing the data in the K/S space from Ku…▽ MoreKubelka-Munk (K-M) theory has been successfully used to estimate pigment concentrations in the pigment mixtures of modern paintings in spectral imagery. In this study the single-constant K-M theory has been utilized for the classification of green pigments in the Selden Map of China, a navigational map of the South China Sea likely created in the early seventeenth century. Hyperspectral data of the map was collected at the Bodleian Library, University ofOxford, and can be used to estimate the pigment diversity, and spatial distribution, within the map. This work seeks to assess the utility of analyzing the data in the K/S space from Kubelka-Munk theory, as opposed to the traditional reflectance domain. We estimate the dimensionality of the data and extract endmembers in the reflectance domain. Then we perform linear unmixing to estimate abundances in the K/S space, and following Bai, et al. (2017), we perform a classification in the abundance space. Finally, due to the lack of ground truth labels, the classification accuracy was estimated by computing the mean spectrum of each class as the representative signature of that class, and calculating the root mean squared error with all the pixels in that class to create a spatial representation of the error. This highlights both the magnitude of, and any spatial pattern in, the errors, indicating if a particular pigment is not well modeled in this approach.△ Less"
Lip reading using external viseme decoding,"Authors:Javad Peymanfard,Mohammad Reza Mohammadi,Hossein Zeinali,Nasser Mozayani","Abstract:…viseme to character by using separate models. Our proposed method improves word error rate by 4\% compared to the normal sequence to sequence lip-reading model on the BBC-OxfordLip Reading Sentences 2 (LRS2) dataset.▽ MoreLip-reading is the operation of recognizing speech from lip movements. This is a difficult task because the movements of the lips when pronouncing the words are similar for some of them. Viseme is used to describe lip movements during a conversation. This paper aims to show how to use external text data (for viseme-to-character mapping) by dividing video-to-character into two stages, namely converting video to viseme, and then converting viseme to character by using separate models. Our proposed method improves word error rate by 4\% compared to the normal sequence to sequence lip-reading model on the BBC-OxfordLip Reading Sentences 2 (LRS2) dataset.△ Less"
Study of $B\to D^{(*)}h$ decays using $62.8~\mathrm{fb}^{-1}$ of Belle II data,"Authors:Belle II Collaboration,F. Abudinen,I. Adachi,R. Adak,K. Adamczyk,P. Ahlburg,J. K. Ahn,H. Aihara,N. Akopov,A. Aloisio,F. Ameli,L. Andricek,N. Anh Ky,D. M. Asner,H. Atmacan,V. Aulchenko,T. Aushev,V. Aushev,T. Aziz,V. Babu,S. Bacher,S. Baehr,S. Bahinipati,A. M. Bakich,P. Bambade, et al. (527 additional authors not shown)","Abstract:We report measurements related to hadronic $B$ decays to final states that contain charm mesons. The analyses are performed on a $62.8~\mathrm{fb}^{-1}$ data set collected by the Belle II experiment at a center-of-mass energy corresponding to the mass of the $Υ(4S)$ resonance. The measurements reported are for the decay modes $B^-\to D^0 h^-$, $B^{-}\to D^{*0}h^-$, $\bar{B}^{0}\to D^{+} h^{-}$ and…▽ MoreWe report measurements related to hadronic $B$ decays to final states that contain charm mesons. The analyses are performed on a $62.8~\mathrm{fb}^{-1}$ data set collected by the Belle II experiment at a center-of-mass energy corresponding to the mass of the $Υ(4S)$ resonance. The measurements reported are for the decay modes $B^-\to D^0 h^-$, $B^{-}\to D^{*0}h^-$, $\bar{B}^{0}\to D^{+} h^{-}$ and $\bar{B}^{0}\to D^{*+} h^{-}$, where $h=π$ or $K$. These modes are either signal or control channels for measurements related to the unitarity triangle angle $γ$ in direct or time-dependent $CP$-violation measurements. The reported observables are the ratios between the $B\to D^{(*)}K$ and $B\to D^{(*)}π$ decay rates, which are found to be in agreement with previous measurements.△ Less"
DeepI2P: Image-to-Point Cloud Registration via Deep Classification,"Authors:Jiaxin Li,Gim Hee Lee","Abstract:…the camera frustum. These labeled points are subsequently passed into a novel inverse camera projection solver to estimate the relative pose. Extensive experimental results onOxfordRobotcar and KITTI datasets demonstrate the feasibility of our approach. Our source code is available at https://github.com/lijx10/DeepI2P▽ MoreThis paper presents DeepI2P: a novel approach for cross-modality registration between an image and a point cloud. Given an image (e.g. from a rgb-camera) and a general point cloud (e.g. from a 3D Lidar scanner) captured at different locations in the same scene, our method estimates the relative rigid transformation between the coordinate frames of the camera and Lidar. Learning common feature descriptors to establish correspondences for the registration is inherently challenging due to the lack of appearance and geometric correlations across the two modalities. We circumvent the difficulty by converting the registration problem into a classification and inverse camera projection optimization problem. A classification neural network is designed to label whether the projection of each point in the point cloud is within or beyond the camera frustum. These labeled points are subsequently passed into a novel inverse camera projection solver to estimate the relative pose. Extensive experimental results onOxfordRobotcar and KITTI datasets demonstrate the feasibility of our approach. Our source code is available at https://github.com/lijx10/DeepI2P△ Less"
Dopamine Transporter SPECT Image Classification for Neurodegenerative Parkinsonism via Diffusion Maps and Machine Learning Classifiers,"Authors:Jun-En Ding,Chi-Hsiang Chu,Mong-Na Lo Huang,Chien-Ching Hsu","Abstract:Neurodegenerative parkinsonism can be assessed by dopamine transporter single photon emission computed tomography (DaT-SPECT). Although generating images is time consuming, these images can show interobserver variability and they have been visually interpreted by nuclear medicine physicians to date. Accordingly, this study aims to provide an automatic and robust method based on Diffusion Maps and…▽ MoreNeurodegenerative parkinsonism can be assessed by dopamine transporter single photon emission computed tomography (DaT-SPECT). Although generating images is time consuming, these images can show interobserver variability and they have been visually interpreted by nuclear medicine physicians to date. Accordingly, this study aims to provide an automatic and robust method based on Diffusion Maps and machine learning classifiers to classify the SPECT images into two types, namely Normal and Abnormal DaT-SPECT image groups. In the proposed method, the 3D images of N patients are mapped to an N by N pairwise distance matrix and are visualized in Diffusion Maps coordinates. The images of the training set are embedded into a low-dimensional space by using diffusion maps. Moreover, we use Nyström's out-of-sample extension, which embeds new sample points as the testing set in the reduced space. Testing samples in the embedded space are then classified into two types through the ensemble classifier with Linear Discriminant Analysis (LDA) and voting procedure through twenty-five-fold cross-validation results. The feasibility of the method is demonstrated via Parkinsonism Progression Markers Initiative (PPMI) dataset of 1097 subjects and a clinical cohort from Kaohsiung Chang Gung Memorial Hospital (KCGMH-TW) of 630 patients. We compare performances using Diffusion Maps with those of three alternative manifold methods for dimension reduction, namely Locally Linear Embedding (LLE), Isomorphic Mapping Algorithm (Isomap), and Kernel Principal Component Analysis (Kernel PCA). We also compare results using 2D and 3D CNN methods. The diffusion maps method has an average accuracy of 98% for the PPMI and 90% for the KCGMH-TW dataset with twenty-five fold cross-validation results. It outperforms the other three methods concerning the overall accuracy and the robustness in the training and testing samples.△ Less"
NDT-Transformer: Large-Scale 3D Point Cloud Localisation using the Normal Distribution Transform Representation,"Authors:Zhicheng Zhou,Cheng Zhao,Daniel Adolfsson,Songzhi Su,Yang Gao,Tom Duckett,Li Sun","Abstract:…recognition. Compared to the state-of-the-art methods, the proposed approach achieves an improvement of 7.52% on average top 1 recall and 2.73% on average top 1% recall on theOxfordRobotcar benchmark.▽ More3D point cloud-based place recognition is highly demanded by autonomous driving in GPS-challenged environments and serves as an essential component (i.e. loop-closure detection) in lidar-based SLAM systems. This paper proposes a novel approach, named NDT-Transformer, for realtime and large-scale place recognition using 3D point clouds. Specifically, a 3D Normal Distribution Transform (NDT) representation is employed to condense the raw, dense 3D point cloud as probabilistic distributions (NDT cells) to provide the geometrical shape description. Then a novel NDT-Transformer network learns a global descriptor from a set of 3D NDT cell representations. Benefiting from the NDT representation and NDT-Transformer network, the learned global descriptors are enriched with both geometrical and contextual information. Finally, descriptor retrieval is achieved using a query-database for place recognition. Compared to the state-of-the-art methods, the proposed approach achieves an improvement of 7.52% on average top 1 recall and 2.73% on average top 1% recall on theOxfordRobotcar benchmark.△ Less"
Instance-level Image Retrieval using Reranking Transformers,"Authors:Fuwen Tan,Jiangbo Yuan,Vicente Ordonez","Abstract:…and can be easily parallelized so that reranking a set of top matching results can be performed in a single forward-pass. We perform extensive experiments on the RevisitedOxfordand Paris datasets, and the Google Landmarks v2 dataset, showing that RRTs outperform previous reranking approaches while using much fewer local descriptors. Moreover, we demonstrat…▽ MoreInstance-level image retrieval is the task of searching in a large database for images that match an object in a query image. To address this task, systems usually rely on a retrieval step that uses global image descriptors, and a subsequent step that performs domain-specific refinements or reranking by leveraging operations such as geometric verification based on local features. In this work, we propose Reranking Transformers (RRTs) as a general model to incorporate both local and global features to rerank the matching images in a supervised fashion and thus replace the relatively expensive process of geometric verification. RRTs are lightweight and can be easily parallelized so that reranking a set of top matching results can be performed in a single forward-pass. We perform extensive experiments on the RevisitedOxfordand Paris datasets, and the Google Landmarks v2 dataset, showing that RRTs outperform previous reranking approaches while using much fewer local descriptors. Moreover, we demonstrate that, unlike existing approaches, RRTs can be optimized jointly with the feature extractor, which can lead to feature representations tailored to downstream tasks and further accuracy improvements. The code and trained models are publicly available at https://github.com/uvavision/RerankingTransformer.△ Less"
Flipping Linear Algebra using a MOOC platform,"Authors:Ana Moura Santos,Luis Costa","Abstract:Between 2017 and 2019, a standard Linear Algebra course from Instituto Superior Técnico, University of Lisbon, used virtual learning content, mainly videos and formative assessment, delivered at the institution's MOOC platform to support a flipped classroom strategy. This strategy has involved around 100 first-year Computer Science enrolled students each year and was implemented by a faculty membe…▽ MoreBetween 2017 and 2019, a standard Linear Algebra course from Instituto Superior Técnico, University of Lisbon, used virtual learning content, mainly videos and formative assessment, delivered at the institution's MOOC platform to support a flipped classroom strategy. This strategy has involved around 100 first-year Computer Science enrolled students each year and was implemented by a faculty member of the Department of Mathematics. The flipped classroom was pedagogically developed and underwent new evaluations each year, with positive impacts on the way students and teachers work during the semester, within the context of teaching/learning mathematics. We will present and discuss the improvements and the results of this three-year hybrid experiment, analysing data from students' responses to questionnaires, online course completion rates, and data collected from students' final grades in the Linear Algebra course. Based on the findings, we have reason to believe that similar flipped classroom practices are a good response to the necessary changes in higher education towards more student-centred practices, especially in the areas of STEM higher education.△ Less"
A new candidate pulsating ULX in NGC 7793,"Authors:Erwan Quintin,Natalie Webb,Andrés Gúrpide,Matteo Bachetti,Felix Fürst","Abstract:We report here the discovery of NGC 7793 ULX-4, a new transient ultraluminous X-ray source (ULX) in NGC 7793, a spiral galaxy already well known for harbouring several ULXs. This new source underwent an outburst in 2012, when it was detected by \textit{XMM-Newton} and the \textit{Swift} X-ray telescope. The outburst reached a peak luminosity of 3.4$\times 10^{39}$ erg\ s$^{-1}$ and lasted for abou…▽ MoreWe report here the discovery of NGC 7793 ULX-4, a new transient ultraluminous X-ray source (ULX) in NGC 7793, a spiral galaxy already well known for harbouring several ULXs. This new source underwent an outburst in 2012, when it was detected by \textit{XMM-Newton} and the \textit{Swift} X-ray telescope. The outburst reached a peak luminosity of 3.4$\times 10^{39}$ erg\ s$^{-1}$ and lasted for about 8 months, after which the source went below a luminosity of $10^{37}$ erg\ s$^{-1}$; previous \textit{Chandra} observations constrain the low-state luminosity below $\sim$ 2$\times 10^{36}$ erg\ s$^{-1}$, implying a variability of at least a factor 1000. We propose four possible  optical counterparts, found in archival HST observations of the galaxy. A pulsation in the \textit{XMM-Newton} signal was found at 2.52 Hz, with a significance of $\sim3.4\,σ$, and an associated spin-up of $\dot{f} = 3.5\times10^{-8}$ Hz.s$^{-1}$. NGC 7793 is therefore the first galaxy to host more than one pulsating ULX.△ Less"
RadarLoc: Learning to Relocalize in FMCW Radar,"Authors:Wei Wang,Pedro P. B. de Gusmo,Bo Yang,Andrew Markham,Niki Trigoni","Abstract:…to improve the localization performance by utilizing geometric constraints between radar scans. We validate our approach on the recently released challenging outdoor datasetOxfordRadar RobotCar. Comprehensive experiments demonstrate that the proposed method outperforms radar-based localization and deep camera relocalization methods by a significant margin.▽ MoreRelocalization is a fundamental task in the field of robotics and computer vision. There is considerable work in the field of deep camera relocalization, which directly estimates poses from raw images. However, learning-based methods have not yet been applied to the radar sensory data. In this work, we investigate how to exploit deep learning to predict global poses from Emerging Frequency-Modulated Continuous Wave (FMCW) radar scans. Specifically, we propose a novel end-to-end neural network with self-attention, termed RadarLoc, which is able to estimate 6-DoF global poses directly. We also propose to improve the localization performance by utilizing geometric constraints between radar scans. We validate our approach on the recently released challenging outdoor datasetOxfordRadar RobotCar. Comprehensive experiments demonstrate that the proposed method outperforms radar-based localization and deep camera relocalization methods by a significant margin.△ Less"
CoordiNet: uncertainty-aware pose regressor for reliable vehicle localization,"Authors:Arthur Moreau,Nathan Piasco,Dzmitry Tsishkou,Bogdan Stanciulescu,Arnaud de La Fortelle","Abstract:…convolutional architecture, named CoordiNet, designed to embed some of the scene geometry. Our framework outperforms comparable methods on the largest available benchmark, theOxfordRobotCar dataset, with an average error of 8 meters where previous best was 19 meters. We have also investigated the performance of our method on large scenes for real time (18…▽ MoreIn this paper, we investigate visual-based camera re-localization with neural networks for robotics and autonomous vehicles applications. Our solution is a CNN-based algorithm which predicts camera pose (3D translation and 3D rotation) directly from a single image. It also provides an uncertainty estimate of the pose. Pose and uncertainty are learned together with a single loss function and are fused at test time with an EKF. Furthermore, we propose a new fully convolutional architecture, named CoordiNet, designed to embed some of the scene geometry. Our framework outperforms comparable methods on the largest available benchmark, theOxfordRobotCar dataset, with an average error of 8 meters where previous best was 19 meters. We have also investigated the performance of our method on large scenes for real time (18 fps) vehicle localization. In this setup, structure-based methods require a large database, and we show that our proposal is a reliable alternative, achieving 29cm median error in a 1.9km loop in a busy urban area△ Less"
Towards an Understanding of Why and How ICT Projects Are Initiated: Analysis via Repertory Grid,"Authors:Htike Htike Wut Yi,Stephen G. MacDonell","Abstract:Contemporary business innovation relies increasingly on information and communications technology (ICT) solutions. As ICT initiatives are generally implemented via projects the management of ICT projects has come under increasing scrutiny. ICT projects continue to fail; as a result, while research in ICT project management has indeed increased, many challenges for research and practice remain. Man…▽ MoreContemporary business innovation relies increasingly on information and communications technology (ICT) solutions. As ICT initiatives are generally implemented via projects the management of ICT projects has come under increasing scrutiny. ICT projects continue to fail; as a result, while research in ICT project management has indeed increased, many challenges for research and practice remain. Many studies have addressed the execution and management of ICT projects and the many factors that might relate to project outcomes. Very few, however, have considered ICT project initiation and the crucial decisions made at that very early, pre-life cycle stage. The primary intent of this research is therefore to investigate ICT projects with a particular focus on their initiation. In doing so we wished to understand why ICT projects are started, and how they are moved from idea or proposal to supported reality. A combination of semi-structured interviews and the repertory grid data collection and analysis method was employed to investigate and validate the motivating factors that influence individual IT Managers' project initiation decisions and the methods they use to transition from idea to enacted project. Our results showed that there are indeed multiple underlying reasons for the decisions made at this early stage and that there are some especially common decision drivers. Some were expected, in the sense that they mapped to recommended best practice. For instance, most projects are motivated by a desire to achieve efficiencies or cost savings, and their potential tends to be assessed using cost benefit analysis. Other results were more surprising - competitor pressure was not a common driver for ICT project initiation in our analysis. Unsurprisingly, formal evaluation methods are more frequently used to assess project proposals when those projects are larger and higher profile. (Abridged)△ Less"
A Normal Distribution Transform-Based Radar Odometry Designed For Scanning and Automotive Radars,"Authors:Pou-Chun Kung,Chieh-Chih Wang,Wen-Chieh Lin","Abstract:…the pipeline consists of thresholding, probabilistic submap building, and an NDT-based radar scan matching. The proposed RO has been tested on two public radar datasets: theOxfordRadar RobotCar dataset and the nuScenes dataset, which provide scanning and automotive radar data respectively. The results show that our approach surpasses state-of-the-art RO us…▽ MoreExisting radar sensors can be classified into automotive and scanning radars. While most radar odometry (RO) methods are only designed for a specific type of radar, our RO method adapts to both scanning and automotive radars. Our RO is simple yet effective, where the pipeline consists of thresholding, probabilistic submap building, and an NDT-based radar scan matching. The proposed RO has been tested on two public radar datasets: theOxfordRadar RobotCar dataset and the nuScenes dataset, which provide scanning and automotive radar data respectively. The results show that our approach surpasses state-of-the-art RO using either automotive or scanning radar by reducing translational error by 51% and 30%, respectively, and rotational error by 17% and 29%, respectively. Besides, we show that our RO achieves centimeter-level accuracy as lidar odometry, and automotive and scanning RO have similar accuracy.△ Less"
Beam delivery systems for linac-based proton therapy,"Authors:Titus-Stefan Dascalu,Suzanne L. Sheehy","Abstract:This report presents a design of a gantry for proton therapy based on the concept of adiabatic transition. The use of fixed-field alternating gradient magnets allows a large momentum acceptance and supports fast energy modulation. The optical performance of the gantry has been analysed using a beam tracking code. Several optimisations of the lattice and transition sections have been investigated t…▽ MoreThis report presents a design of a gantry for proton therapy based on the concept of adiabatic transition. The use of fixed-field alternating gradient magnets allows a large momentum acceptance and supports fast energy modulation. The optical performance of the gantry has been analysed using a beam tracking code. Several optimisations of the lattice and transition sections have been investigated to reduce size and ensure applicability to pencil beam scanning. Matching of the full energy range results in an increase in the size of the gantry, but reduces the weight and cost significantly compared to those that pertain to facilities in operation.△ Less"
A Brief Historical Perspective on the Consistent Histories Interpretation of Quantum Mechanics,"Authors:Gustavo Rodrigues Rocha,Dean Rickles,Florian J. Boge","Abstract:It will be presented in this chapter a historical account of the consistent histories interpretation of quantum mechanics based on primary and secondary literature. Firstly, the formalism of the consistent histories approach will be outlined.
  Secondly, the works by Robert Griffiths and Roland Omnès will be discussed. Griffiths' seminal 1984 paper, the first physicist to have proposed a consisten…▽ MoreIt will be presented in this chapter a historical account of the consistent histories interpretation of quantum mechanics based on primary and secondary literature. Firstly, the formalism of the consistent histories approach will be outlined.
  Secondly, the works by Robert Griffiths and Roland Omnès will be discussed. Griffiths' seminal 1984 paper, the first physicist to have proposed a consistent-histories interpretation of quantum mechanics, followed by Omnès' 1990 paper, were instrumental to the consistent-histories model based on Boolean logic.
  Thirdly, Murray Gell-Mann and James Hartle's steps to their own version of consistent-histories approach, motivated by a cosmological perspective, will then be described and evaluated. Gell-Mann and Hartle understood that spontaneous decoherence could path the way to a concrete physical model to Griffiths' consistent histories.
  Moreover, the collective biography of these figures will be put in the context of the role played by the Santa Fe Institute, co-founded by Gell-Mann in 1984 in Santa Fe, New Mexico, where Hartle is also a member of the external faculty.△ Less"
Coverage based testing for V&V and Safety Assurance of Self-driving Autonomous Vehicles: A Systematic Literature Review,"Authors:Zaid Tahir,Rob Alexander","Abstract:Self-driving Autonomous Vehicles (SAVs) are gaining more interest each passing day by the industry as well as the general public. Tech and automobile companies are investing huge amounts of capital in research and development of SAVs to make sure they have a head start in the SAV market in the future. One of the major hurdles in the way of SAVs making it to the public roads is the lack of confiden…▽ MoreSelf-driving Autonomous Vehicles (SAVs) are gaining more interest each passing day by the industry as well as the general public. Tech and automobile companies are investing huge amounts of capital in research and development of SAVs to make sure they have a head start in the SAV market in the future. One of the major hurdles in the way of SAVs making it to the public roads is the lack of confidence of public in the safety aspect of SAVs. In order to assure safety and provide confidence to the public in the safety of SAVs, researchers around the world have used coverage-based testing for Verification and Validation (V&V) and safety assurance of SAVs. The objective of this paper is to investigate the coverage criteria proposed and coverage maximizing techniques used by researchers in the last decade up till now, to assure safety of SAVs. We conduct a Systematic Literature Review (SLR) for this investigation in our paper. We present a classification of existing research based on the coverage criteria used. Several research gaps and research directions are also provided in this SLR to enable further research in this domain. This paper provides a body of knowledge in the domain of safety assurance of SAVs. We believe the results of this SLR will be helpful in the progression of V&V and safety assurance of SAVs.△ Less"
Increased isolation mass for pebble accreting planetary cores in pressure maxima of protoplanetary discs,"Authors:Zsolt Sándor,Zsolt Regály","Abstract:The growth of a pebble accreting planetary core is stopped when reaching its \textit{isolation mass} that is due to a pressure maximum emerging at the outer edge of the gap opened in gas. This pressure maximum traps the inward drifting pebbles stopping the accretion of solids onto the core. On the other hand, a large amount of pebbles ($\sim 100M_\oplus$) should flow through the orbit of the core…▽ MoreThe growth of a pebble accreting planetary core is stopped when reaching its \textit{isolation mass} that is due to a pressure maximum emerging at the outer edge of the gap opened in gas. This pressure maximum traps the inward drifting pebbles stopping the accretion of solids onto the core. On the other hand, a large amount of pebbles ($\sim 100M_\oplus$) should flow through the orbit of the core until reaching its isolation mass. The efficiency of pebble accretion increases if the core grows in a dust trap of the protoplanetary disc. Dust traps are observed as ring-like structures by ALMA suggesting the existence of global pressure maxima in discs that can also act as planet migration traps. This work aims to reveal how large a planetary core can grow in such a pressure maximum by pebble accretion. In our hydrodynamic simulations, pebbles are treated as a pressureless fluid mutually coupled to the gas via drag force. Our results show that in a global pressure maximum the pebble isolation mass for a planetary core is significantly larger than in discs with power-law surface density profile. An increased isolation mass shortens the formation time of giant planets.△ Less"
FisheyeSuperPoint: Keypoint Detection and Description Network for Fisheye Images,"Authors:Anna Konrad,Ciarán Eising,Ganesh Sistu,John McDonald,Rudi Villing,Senthil Yogamani","Abstract:…the performance on the HPatches benchmark, and, by introducing a fisheye based evaluation method for detection repeatability and descriptor matching correctness, on theOxfordRobotCar dataset.▽ MoreKeypoint detection and description is a commonly used building block in computer vision systems particularly for robotics and autonomous driving. However, the majority of techniques to date have focused on standard cameras with little consideration given to fisheye cameras which are commonly used in urban driving and automated parking. In this paper, we propose a novel training and evaluation pipeline for fisheye images. We make use of SuperPoint as our baseline which is a self-supervised keypoint detector and descriptor that has achieved state-of-the-art results on homography estimation. We introduce a fisheye adaptation pipeline to enable training on undistorted fisheye images. We evaluate the performance on the HPatches benchmark, and, by introducing a fisheye based evaluation method for detection repeatability and descriptor matching correctness, on theOxfordRobotCar dataset.△ Less"
Refinement Type Directed Search for Meta-Interpretive-Learning of Higher-Order Logic Programs,Authors:Rolf Morel,"Abstract:The program synthesis problem within the Inductive Logic Programming (ILP) community has typically been seen as untyped. We consider the benefits of user provided types on background knowledge. Building on the Meta-Interpretive Learning (MIL) framework, we show that type checking is able to prune large parts of the hypothesis space of programs. The introduction of polymorphic type checking to the…▽ MoreThe program synthesis problem within the Inductive Logic Programming (ILP) community has typically been seen as untyped. We consider the benefits of user provided types on background knowledge. Building on the Meta-Interpretive Learning (MIL) framework, we show that type checking is able to prune large parts of the hypothesis space of programs. The introduction of polymorphic type checking to the MIL approach to logic program synthesis is validated by strong theoretical and experimental results, showing a cubic reduction in the size of the search space and synthesis time, in terms of the number of typed background predicates. Additionally we are able to infer polymorphic types of synthesized clauses and of entire programs. The other advancement is in developing an approach to leveraging refinement types in ILP. Here we show that further pruning of the search space can be achieved, though the SMT solving used for refinement type checking comes△ Less"
ROAD: The ROad event Awareness Dataset for Autonomous Driving,"Authors:Gurkirt Singh,Stephen Akrigg,Manuele Di Maio,Valentina Fontana,Reza Javanmard Alitappeh,Suman Saha,Kossar Jeddisaravi,Farzad Yousefi,Jacob Culley,Tom Nicholson,Jordan Omokeowa,Salman Khan,Stanislao Grazioso,Andrew Bradley,Giuseppe Di Gironimo,Fabio Cuzzolin","Abstract:…detect road events, defined as triplets composed by an active agent, the action(s) it performs and the corresponding scene locations. ROAD comprises videos originally from theOxfordRobotCar Dataset annotated with bounding boxes showing the location in the image plane of each road event. We benchmark various detection tasks, proposing as a baseline a new in…▽ MoreHumans drive in a holistic fashion which entails, in particular, understanding dynamic road events and their evolution. Injecting these capabilities in autonomous vehicles can thus take situational awareness and decision making closer to human-level performance. To this purpose, we introduce the ROad event Awareness Dataset (ROAD) for Autonomous Driving, to our knowledge the first of its kind. ROAD is designed to test an autonomous vehicle's ability to detect road events, defined as triplets composed by an active agent, the action(s) it performs and the corresponding scene locations. ROAD comprises videos originally from theOxfordRobotCar Dataset annotated with bounding boxes showing the location in the image plane of each road event. We benchmark various detection tasks, proposing as a baseline a new incremental algorithm for online road event awareness termed 3D-RetinaNet. We also report the performance on the ROAD tasks of Slowfast and YOLOv5 detectors, as well as that of the winners of the ICCV2021 ROAD challenge, which highlight the challenges faced by situation awareness in autonomous driving. ROAD is designed to allow scholars to investigate exciting tasks such as complex (road) activity detection, future event anticipation and continual learning. The dataset is available at https://github.com/gurkirt/road-dataset; the baseline can be found at https://github.com/gurkirt/3D-RetinaNet.△ Less"
Unsupervised Learning of Lidar Features for Use in a Probabilistic Trajectory Estimator,"Authors:David J. Yoon,Haowei Zhang,Mona Gridseth,Hugues Thomas,Timothy D. Barfoot","Abstract:…the full estimator with a deep network, and comparable to state-of-the-art ICP-based methods on the KITTI odometry dataset. We additionally show results on lidar data from theOxfordRobotCar dataset.▽ MoreWe present unsupervised parameter learning in a Gaussian variational inference setting that combines classic trajectory estimation for mobile robots with deep learning for rich sensor data, all under a single learning objective. The framework is an extension of an existing system identification method that optimizes for the observed data likelihood, which we improve with modern advances in batch trajectory estimation and deep learning. Though the framework is general to any form of parameter learning and sensor modality, we demonstrate application to feature and uncertainty learning with a deep network for 3D lidar odometry. Our framework learns from only the on-board lidar data, and does not require any form of groundtruth supervision. We demonstrate that our lidar odometry performs better than existing methods that learn the full estimator with a deep network, and comparable to state-of-the-art ICP-based methods on the KITTI odometry dataset. We additionally show results on lidar data from theOxfordRobotCar dataset.△ Less"
User interface for in-vehicle systems with on-wheel finger spreading gestures and head-up displays,"Authors:Sang Hun Lee,Se-One Yoon","Abstract:Interacting with an in-vehicle system through a central console is known to induce visual and biomechanical distractions, thereby delaying the danger recognition and response times of the driver and significantly increasing the risk of an accident. To address this problem, various hand gestures have been developed. Although such gestures can reduce visual demand, they are limited in number, lack p…▽ MoreInteracting with an in-vehicle system through a central console is known to induce visual and biomechanical distractions, thereby delaying the danger recognition and response times of the driver and significantly increasing the risk of an accident. To address this problem, various hand gestures have been developed. Although such gestures can reduce visual demand, they are limited in number, lack passive feedback, and can be vague and imprecise, difficult to understand and remember, and culture-bound. To overcome these limitations, we developed a novel on-wheel finger spreading gestural interface combined with a head-up display (HUD) allowing the user to choose a menu displayed in the HUD with a gesture. This interface displays audio and air conditioning functions on the central console of a HUD and enables their control using a specific number of fingers while keeping both hands on the steering wheel. We compared the effectiveness of the newly proposed hybrid interface against a traditional tactile interface for a central console using objective measurements and subjective evaluations regarding both the vehicle and driver behaviour. A total of 32 subjects were recruited to conduct experiments on a driving simulator equipped with the proposed interface under various scenarios. The results showed that the proposed interface was approximately 20% faster in emergency response than the traditional interface, whereas its performance in maintaining vehicle speed and lane was not significantly different from that of the traditional one.△ Less"
Gaia EDR3 view on Galactic globular clusters,"Authors:Eugene Vasiliev,Holger Baumgardt","Abstract:We use the data from Gaia Early Data Release 3 (EDR3) to study the kinematic properties of Milky Way globular clusters. We measure the mean parallaxes and proper motions (PM) for 170 clusters, determine the PM dispersion profiles for more than 100 clusters, uncover rotation signatures in more than 20 objects, and find evidence for radial or tangential PM anisotropy in a dozen richest clusters. At…▽ MoreWe use the data from Gaia Early Data Release 3 (EDR3) to study the kinematic properties of Milky Way globular clusters. We measure the mean parallaxes and proper motions (PM) for 170 clusters, determine the PM dispersion profiles for more than 100 clusters, uncover rotation signatures in more than 20 objects, and find evidence for radial or tangential PM anisotropy in a dozen richest clusters. At the same time, we use the selection of cluster members to explore the reliability and limitations of the Gaia catalogue itself. We find that the formal uncertainties on parallax and PM are underestimated by 10-20% in dense central regions even for stars that pass numerous quality filters. We explore the the spatial covariance function of systematic errors, and determine a lower limit on the uncertainty of average parallaxes and PM at the level 0.01 mas and 0.025 mas/yr, respectively. Finally, a comparison of mean parallaxes of clusters with distances from various literature sources suggests that the parallaxes (after applying the zero-point correction suggested by Lindegren et al. 2021) are overestimated by 0.01+-0.003 mas. Despite these caveats, the quality of Gaia astrometry has been significantly improved in EDR3 and provides valuable insights into the properties of star clusters.△ Less"
"On the Philosophical, Cognitive and Mathematical Foundations of Symbiotic Autonomous Systems (SAS)","Authors:Yingxu Wang,Fakhri Karray,Sam Kwong,Konstantinos N. Plataniotis,Henry Leung,Ming Hou,Edward Tunstel,Imre J. Rudas,Ljiljana Trajkovic,Okyay Kaynak,Janusz Kacprzyk,Mengchu Zhou,Michael H. Smith,Philip Chen,Shushma Patel","Abstract:Symbiotic Autonomous Systems (SAS) are advanced intelligent and cognitive systems exhibiting autonomous collective intelligence enabled by coherent symbiosis of human-machine interactions in hybrid societies. Basic research in the emerging field of SAS has triggered advanced general AI technologies functioning without human intervention or hybrid symbiotic systems synergizing humans and intelligen…▽ MoreSymbiotic Autonomous Systems (SAS) are advanced intelligent and cognitive systems exhibiting autonomous collective intelligence enabled by coherent symbiosis of human-machine interactions in hybrid societies. Basic research in the emerging field of SAS has triggered advanced general AI technologies functioning without human intervention or hybrid symbiotic systems synergizing humans and intelligent machines into coherent cognitive systems. This work presents a theoretical framework of SAS underpinned by the latest advances in intelligence, cognition, computer, and system sciences. SAS are characterized by the composition of autonomous and symbiotic systems that adopt bio-brain-social-inspired and heterogeneously synergized structures and autonomous behaviors. This paper explores their cognitive and mathematical foundations. The challenge to seamless human-machine interactions in a hybrid environment is addressed. SAS-based collective intelligence is explored in order to augment human capability by autonomous machine intelligence towards the next generation of general AI, autonomous computers, and trustworthy mission-critical intelligent systems. Emerging paradigms and engineering applications of SAS are elaborated via an autonomous knowledge learning system that symbiotically works between humans and cognitive robots.△ Less"
Dark sky tourism and sustainable development in Namibia,"Authors:Hannah Dalgleish,Getachew Mengistie,Michael Backes,Garret Cotter,Eli Kasai","Abstract:…the astronomy community, and yet, the country is not well recognised as a dark sky destination by tourists and travellers. Forged by a collaboration between the Universities ofOxfordand Namibia, together we are using astronomy as a means for capacity-building and sustainable socio-economic growth via educating tour guides and promoting dark sky tourism to…▽ MoreNamibia is world-renowned for its incredibly dark skies by the astronomy community, and yet, the country is not well recognised as a dark sky destination by tourists and travellers. Forged by a collaboration between the Universities ofOxfordand Namibia, together we are using astronomy as a means for capacity-building and sustainable socio-economic growth via educating tour guides and promoting dark sky tourism to relevant stakeholders.△ Less"
Model-based Prediction and Optimal Control of Pandemics by Non-pharmaceutical Interventions,Authors:Reza Sameni,"Abstract:…by epidemiological facts from COVID-19 studies, and partially trained by using machine learning techniques. The developed algorithm is applied on ground truth data from theOxfordCOVID-19 Government Response Tracker project, which has categorized and quantified the regional responses to the pandemic for more than 300 countries and regions worldwide, since J…▽ MoreA model-based signal processing framework is proposed for pandemic trend forecasting and control, by using non-pharmaceutical interventions (NPI) at regional and country levels worldwide. The control objective is to prescribe quantifiable NPI strategies at different levels of stringency, which balance between human factors (such as new cases and death rates) and cost of intervention per region/country. Due to infrastructural disparities and differences in priorities of regions and countries, strategists are given the flexibility to weight between different NPIs and to select the desired balance between the human factor and overall NPI cost.
  The proposed framework is based on a \textit{finite-horizon optimal control} (FHOC) formulation of the bi-objective problem and the FHOC is numerically solved by using an ad hoc \textit{extended Kalman filtering/smoothing} framework for optimal NPI estimation and pandemic trend forecasting. The algorithm enables strategists to select the desired balance between the human factor and NPI cost with a set of weights and parameters. The parameters of the model are partially selected by epidemiological facts from COVID-19 studies, and partially trained by using machine learning techniques. The developed algorithm is applied on ground truth data from theOxfordCOVID-19 Government Response Tracker project, which has categorized and quantified the regional responses to the pandemic for more than 300 countries and regions worldwide, since January 2020. The dataset was used for NPI-based prediction and prescription during the XPRIZE Pandemic Response Challenge.△ Less"
Visualizing hierarchies in scRNA-seq data using a density tree-biased autoencoder,"Authors:Quentin Garrido,Sebastian Damrich,Alexander Jäger,Dario Cerletti,Manfred Claassen,Laurent Najman,Fred Hamprecht","Abstract:Motivation: Single cell RNA sequencing (scRNA-seq) data makes studying the development of cells possible at unparalleled resolution. Given that many cellular differentiation processes are hierarchical, their scRNA-seq data is expected to be approximately tree-shaped in gene expression space. Inference and representation of this tree-structure in two dimensions is highly desirable for biological in…▽ MoreMotivation: Single cell RNA sequencing (scRNA-seq) data makes studying the development of cells possible at unparalleled resolution. Given that many cellular differentiation processes are hierarchical, their scRNA-seq data is expected to be approximately tree-shaped in gene expression space. Inference and representation of this tree-structure in two dimensions is highly desirable for biological interpretation and exploratory analysis.Results:Our two contributions are an approach for identifying a meaningful tree structure from high-dimensional scRNA-seq data, and a visualization method respecting the tree-structure. We extract the tree structure by means of a density based minimum spanning tree on a vector quantization of the data and show that it captures biological information well. We then introduce DTAE, a tree-biased autoencoder that emphasizes the tree structure of the data in low dimensional space. We compare to other dimension reduction methods and demonstrate the success of our method both qualitatively and quantitatively on real and toy data.Availability: Our implementation relying on PyTorch and Higra is available at https://github.com/hci-unihd/DTAE.△ Less"
essHi-C: Essential component analysis of Hi-C matrices,"Authors:Stefano Franzini,Marco Di Stefano,Cristian Micheletti","Abstract:Motivation: Hi-C matrices are cornerstones for qualitative and quantitative studies of genome folding, from its territorial organization to compartments and topological domains. The high dynamic range of genomic distances probed in Hi-C assays reflects in an inherent stochastic background of the interactions matrices, which inevitably convolve the features of interest with largely aspecific ones.…▽ MoreMotivation: Hi-C matrices are cornerstones for qualitative and quantitative studies of genome folding, from its territorial organization to compartments and topological domains. The high dynamic range of genomic distances probed in Hi-C assays reflects in an inherent stochastic background of the interactions matrices, which inevitably convolve the features of interest with largely aspecific ones. Results: Here we introduce a discuss essHi-C, a method to isolate the specific, or essential component of Hi-C matrices from the aspecific portion of the spectrum that is compatible with random matrices. Systematic comparisons show that essHi-C improves the clarity of the interaction patterns, enhances the robustness against sequencing depth, allows the unsupervised clustering of experiments in different cell lines and recovers the cell-cycle phasing of single-cells based on Hi-C data. Thus, essHi-C provides means for isolating significant biological and physical features from Hi-C matrices.△ Less"
Particle Swarm Optimization: Development of a General-Purpose Optimizer,"Authors:Mauro S. Innocente,Johann Sienz","Abstract:Traditional methods present a very restrictive range of applications, mainly limited by the features of the function to be optimized and of the constraint functions. In contrast, evolutionary algorithms present almost no restriction to the features of these functions, although the most appropriate constraint-handling technique is still an open question. The particle swarm optimization (PSO) method…▽ MoreTraditional methods present a very restrictive range of applications, mainly limited by the features of the function to be optimized and of the constraint functions. In contrast, evolutionary algorithms present almost no restriction to the features of these functions, although the most appropriate constraint-handling technique is still an open question. The particle swarm optimization (PSO) method is sometimes viewed as another evolutionary algorithm because of their many similarities, despite not being inspired by the same metaphor. Namely, they evolve a population of individuals taking into consideration previous experiences and using stochastic operators to introduce new responses. The advantages of evolutionary algorithms with respect to traditional methods have been greatly discussed in the literature for decades. While all such advantages are valid when comparing the PSO paradigm to traditional methods, its main advantages with respect to evolutionary algorithms consist of its noticeably lower computational cost and easier implementation. In fact, the plain version can be programmed in a few lines of code, involving no operator design and few parameters to be tuned. This paper deals with three important aspects of the method: the influence of the parameters' tuning on the behaviour of the system; the design of stopping criteria so that the reliability of the solution found can be somehow estimated and computational cost can be saved; and the development of appropriate techniques to handle constraints, given that the original method is designed for unconstrained optimization problems.△ Less"
Reply: Early-onset phenotype of bi-allelic GRN mutations,"Authors:Vincent Huin,Mathieu Barbier,Alexandra Durr,Isabelle Le Ber","Abstract:We would like to reply to Neuray et al. who report a series of five new patients from four unrelated families with bi-allelic mutations of GRN. Their work nicely completes the few existing reports of similar cases, and refers to our recent publication describing six homozygous GRN pathogenic variant carriers with divergent phenotypes and ages at onset (Huin et al., 2020). In summary, the Letter fr…▽ MoreWe would like to reply to Neuray et al. who report a series of five new patients from four unrelated families with bi-allelic mutations of GRN. Their work nicely completes the few existing reports of similar cases, and refers to our recent publication describing six homozygous GRN pathogenic variant carriers with divergent phenotypes and ages at onset (Huin et al., 2020). In summary, the Letter from Neuray et al., reports valuable findings that lead to better define CLN11 due to bi-allelic GRN pathogenic variants. Despite the small sample number that does not allow statistical analysis, the authors underlined the occurrence of cognitive deterioration and epilepsy. Further study of the CLN11 families with functional brain imaging and neuropsychological examinations may be highly informative for the understanding and the clinical characterization of this rare disease.△ Less"
Out of Nowhere: Introduction: The emergence of spacetime,"Authors:Nick Huggett,Christian Wuthrich","Abstract:…monograph ""Out of Nowhere: The Emergence of Spacetime in Quantum Theories of Gravity"", co-authored by Nick Huggett and Christian Wüthrich and under contract withOxfordUniversity Press. (More information at https://beyondspacetime.net/.) This chapter introduces the problem of emergence of spacetime in quantum gravity. It introduces the main philosop…▽ MoreThis is a chapter of the planned monograph ""Out of Nowhere: The Emergence of Spacetime in Quantum Theories of Gravity"", co-authored by Nick Huggett and Christian Wüthrich and under contract withOxfordUniversity Press. (More information at https://beyondspacetime.net/.) This chapter introduces the problem of emergence of spacetime in quantum gravity. It introduces the main philosophical challenge to spacetime emergence and sketches our preferred solution to it.△ Less"
OrigamiSet1.0: Two New Datasets for Origami Classification and Difficulty Estimation,"Authors:Daniel Ma,Gerald Friedland,Mario Michael Krell","Abstract:Origami is becoming more and more relevant to research. However, there is no public dataset yet available and there hasn't been any research on this topic in machine learning. We constructed an origami dataset using images from the multimedia commons and other databases. It consists of two subsets: one for classification of origami images and the other for difficulty estimation. We obtained 16000…▽ MoreOrigami is becoming more and more relevant to research. However, there is no public dataset yet available and there hasn't been any research on this topic in machine learning. We constructed an origami dataset using images from the multimedia commons and other databases. It consists of two subsets: one for classification of origami images and the other for difficulty estimation. We obtained 16000 images for classification (half origami, half other objects) and 1509 for difficulty estimation with $3$ different categories (easy: 764, intermediate: 427, complex: 318). The data can be downloaded at: https://github.com/multimedia-berkeley/OriSet. Finally, we provide machine learning baselines.△ Less"
Leveraging Structured Biological Knowledge for Counterfactual Inference: a Case Study of Viral Pathogenesis,"Authors:Jeremy Zucker,Kaushal Paneri,Sara Mohammad-Taheri,Somya Bhargava,Pallavi Kolambkar,Craig Bakker,Jeremy Teuton,Charles Tapley Hoyt,Kristie Oxford,Robert Ness,Olga Vitek","Abstract:Counterfactual inference is a useful tool for comparing outcomes of interventions on complex systems. It requires us to represent the system in form of a structural causal model, complete with a causal diagram, probabilistic assumptions on exogenous variables, and functional assignments. Specifying such models can be extremely difficult in practice. The process requires substantial domain expertis…▽ MoreCounterfactual inference is a useful tool for comparing outcomes of interventions on complex systems. It requires us to represent the system in form of a structural causal model, complete with a causal diagram, probabilistic assumptions on exogenous variables, and functional assignments. Specifying such models can be extremely difficult in practice. The process requires substantial domain expertise, and does not scale easily to large systems, multiple systems, or novel system modifications. At the same time, many application domains, such as molecular biology, are rich in structured causal knowledge that is qualitative in nature. This manuscript proposes a general approach for querying a causal biological knowledge graph, and converting the qualitative result into a quantitative structural causal model that can learn from data to answer the question. We demonstrate the feasibility, accuracy and versatility of this approach using two case studies in systems biology. The first demonstrates the appropriateness of the underlying assumptions and the accuracy of the results. The second demonstrates the versatility of the approach by querying a knowledge base for the molecular determinants of a severe acute respiratory syndrome coronavirus 2 (SARS-CoV-2)-induced cytokine storm, and performing counterfactual inference to estimate the causal effect of medical countermeasures for severely ill patients.△ Less"
Adaptive modelling of variably saturated seepage problems,"Authors:Ben Ashby,Cassiano Bortolozo,Alex Lukyanov,Tristan Pryer","Abstract:In this article we present a goal-oriented adaptive finite element method for a class of subsurface flow problems in porous media, which exhibit seepage faces. We focus on a representative case of the steady state flows governed by a nonlinear Darcy-Buckingham law with physical constraints on subsurface-atmosphere boundaries. This leads to the formulation of the problem as a variational inequality…▽ MoreIn this article we present a goal-oriented adaptive finite element method for a class of subsurface flow problems in porous media, which exhibit seepage faces. We focus on a representative case of the steady state flows governed by a nonlinear Darcy-Buckingham law with physical constraints on subsurface-atmosphere boundaries. This leads to the formulation of the problem as a variational inequality. The solutions to this problem are investigated using an adaptive finite element method based on a dual-weighted a posteriori error estimate, derived with the aim of reducing error in a specific target quantity. The quantity of interest is chosen as volumetric water flux across the seepage face, and therefore depends on an a priori unknown free boundary. We apply our method to challenging numerical examples as well as specific case studies, from which this research originates, illustrating the major difficulties that arise in practical situations. We summarise extensive numerical results that clearly demonstrate the designed method produces rapid error reduction measured against the number of degrees of freedom.△ Less"
Efficient 3D Point Cloud Feature Learning for Large-Scale Place Recognition,"Authors:Le Hui,Mingmei Cheng,Jin Xie,Jian Yang","Abstract:…to aggregate global descriptors. By distilling the knowledge from EPC-Net, EPC-Net-L can obtain discriminative global descriptors for retrieval. Extensive experiments on theOxforddataset and three in-house datasets demonstrate that our proposed method can achieve state-of-the-art performance with lower parameters, FLOPs, and runtime per frame.▽ MorePoint cloud based retrieval for place recognition is still a challenging problem due to drastic appearance and illumination changes of scenes in changing environments. Existing deep learning based global descriptors for the retrieval task usually consume a large amount of computation resources (e.g., memory), which may not be suitable for the cases of limited hardware resources. In this paper, we develop an efficient point cloud learning network (EPC-Net) to form a global descriptor for visual place recognition, which can obtain good performance and reduce computation memory and inference time. First, we propose a lightweight but effective neural network module, called ProxyConv, to aggregate the local geometric features of point clouds. We leverage the spatial adjacent matrix and proxy points to simplify the original edge convolution for lower memory consumption. Then, we design a lightweight grouped VLAD network (G-VLAD) to form global descriptors for retrieval. Compared with the original VLAD network, we propose a grouped fully connected (GFC) layer to decompose the high-dimensional vectors into a group of low-dimensional vectors, which can reduce the number of parameters of the network and maintain the discrimination of the feature vector. Finally, to further reduce the inference time, we develop a simple version of EPC-Net, called EPC-Net-L, which consists of two ProxyConv modules and one max pooling layer to aggregate global descriptors. By distilling the knowledge from EPC-Net, EPC-Net-L can obtain discriminative global descriptors for retrieval. Extensive experiments on theOxforddataset and three in-house datasets demonstrate that our proposed method can achieve state-of-the-art performance with lower parameters, FLOPs, and runtime per frame.△ Less"
Earthquake magnitude and location estimation from real time seismic waveforms with a transformer network,"Authors:Jannes Münchmeyer,Dino Bindi,Ulf Leser,Frederik Tilmann","Abstract:Precise real time estimates of earthquake magnitude and location are essential for early warning and rapid response. While recently multiple deep learning approaches for fast assessment of earthquakes have been proposed, they usually rely on either seismic records from a single station or from a fixed set of seismic stations. Here we introduce a new model for real-time magnitude and location estim…▽ MorePrecise real time estimates of earthquake magnitude and location are essential for early warning and rapid response. While recently multiple deep learning approaches for fast assessment of earthquakes have been proposed, they usually rely on either seismic records from a single station or from a fixed set of seismic stations. Here we introduce a new model for real-time magnitude and location estimation using the attention based transformer networks. Our approach incorporates waveforms from a dynamically varying set of stations and outperforms deep learning baselines in both magnitude and location estimation performance. Furthermore, it outperforms a classical magnitude estimation algorithm considerably and shows promising performance in comparison to a classical localization algorithm. In this work, we furthermore conduct a comprehensive study of the requirements on training data, the training procedures and the typical failure modes using three diverse and large scale data sets. Our analysis gives several key insights. First, we can precisely pinpoint the effect of large training data; for example, a four times larger training set reduces the required time for real time assessment by a factor of four. Second, the basic model systematically underestimates large magnitude events. This issue can be mitigated by incorporating events from other regions into the training through transfer learning. Third, location estimation is highly precise in areas with sufficient training data, but is strongly degraded for events outside the training distribution. Our analysis suggests that these characteristics are not only present for our model, but for most deep learning models for fast assessment published so far. They result from the black box modeling and their mitigation will likely require imposing physics derived constraints on the neural network.△ Less"
Incremental learning with online SVMs on LiDAR sensory data,"Authors:Le Dinh Van Khoa,Zhiyuan Chen","Abstract:The pipelines transmission system is one of the growing aspects, which has existed for a long time in the energy industry. The cost of in-pipe exploration for maintaining service always draws lots of attention in this industry. Normally exploration methods (e.g. Magnetic flux leakage and eddy current) will establish the sensors stationary for each pipe milestone or carry sensors to travel inside t…▽ MoreThe pipelines transmission system is one of the growing aspects, which has existed for a long time in the energy industry. The cost of in-pipe exploration for maintaining service always draws lots of attention in this industry. Normally exploration methods (e.g. Magnetic flux leakage and eddy current) will establish the sensors stationary for each pipe milestone or carry sensors to travel inside the pipe. It makes the maintenance process very difficult due to the massive amount of sensors. One of the solutions is to implement machine learning techniques for the analysis of sensory data. Although SVMs can resolve this issue with kernel trick, the problem is that computing the kernel depends on the data size too. It is because the process can be exaggerated quickly if the number of support vectors becomes really large. Particularly LiDAR spins with an extremely rapid rate and the flow of input data might eventually lead to massive expansion. In our proposed approach, each sample is learned in an instant way and the supported kernel is computed simultaneously. In this research, incremental learning approach with online support vector machines (SVMs) is presented, which aims to deal with LiDAR sensory data only.△ Less"
Tweeting for the Cause: Network analysis of UK petition sharing,"Authors:Peter Cihon,Taha Yasseri,Scott Hale,Helen Margetts","Abstract:Online government petitions represent a new data-rich mode of political participation. This work examines the thus far understudied dynamics of sharing petitions on social media in order to garner signatures and, ultimately, a government response. Using 20 months of Twitter data comprising over 1 million tweets linking to a petition, we perform analyses of networks constructed of petitions and sup…▽ MoreOnline government petitions represent a new data-rich mode of political participation. This work examines the thus far understudied dynamics of sharing petitions on social media in order to garner signatures and, ultimately, a government response. Using 20 months of Twitter data comprising over 1 million tweets linking to a petition, we perform analyses of networks constructed of petitions and supporters on Twitter, revealing implicit social dynamics therein. We find that Twitter users do not exclusively share petitions on one issue nor do they share exclusively popular petitions. Among the over 240,000 Twitter users, we find latent support groups, with the most central users primarily being politically active ""average"" individuals. Twitter as a platform for sharing government petitions, thus, appears to hold potential to foster the creation of and coordination among a new form of latent support interest groups online.△ Less"
A Review of Machine Learning Techniques for Applied Eye Fundus and Tongue Digital Image Processing with Diabetes Management System,"Authors:Wei Xiang Lim,Zhiyuan Chen,Amr Ahmed,Tissa Chandesa,Iman Liao","Abstract:Diabetes is a global epidemic and it is increasing at an alarming rate. The International Diabetes Federation (IDF) projected that the total number of people with diabetes globally may increase by 48%, from 425 million (year 2017) to 629 million (year 2045). Moreover, diabetes had caused millions of deaths and the number is increasing drastically. Therefore, this paper addresses the background of…▽ MoreDiabetes is a global epidemic and it is increasing at an alarming rate. The International Diabetes Federation (IDF) projected that the total number of people with diabetes globally may increase by 48%, from 425 million (year 2017) to 629 million (year 2045). Moreover, diabetes had caused millions of deaths and the number is increasing drastically. Therefore, this paper addresses the background of diabetes and its complications. In addition, this paper investigates innovative applications and past researches in the areas of diabetes management system with applied eye fundus and tongue digital images. Different types of existing applied eye fundus and tongue digital image processing with diabetes management systems in the market and state-of-the-art machine learning techniques from previous literature have been reviewed. The implication of this paper is to have an overview in diabetic research and what new machine learning techniques can be proposed in solving this global epidemic.△ Less"
Practical Auto-Calibration for Spatial Scene-Understanding from Crowdsourced Dashcamera Videos,"Authors:Hemang Chawla,Matti Jukola,Shabbir Marzban,Elahe Arani,Bahram Zonooz","Abstract:…Here, we propose a system for practical monocular onboard camera auto-calibration from crowdsourced videos. We show the effectiveness of our proposed system on the KITTI raw,OxfordRobotCar, and the crowdsourced D$^2$-City datasets in varying conditions. Finally, we demonstrate its application for accurate monocular dense depth and ego-motion estimation on…▽ MoreSpatial scene-understanding, including dense depth and ego-motion estimation, is an important problem in computer vision for autonomous vehicles and advanced driver assistance systems. Thus, it is beneficial to design perception modules that can utilize crowdsourced videos collected from arbitrary vehicular onboard or dashboard cameras. However, the intrinsic parameters corresponding to such cameras are often unknown or change over time. Typical manual calibration approaches require objects such as a chessboard or additional scene-specific information. On the other hand, automatic camera calibration does not have such requirements. Yet, the automatic calibration of dashboard cameras is challenging as forward and planar navigation results in critical motion sequences with reconstruction ambiguities. Structure reconstruction of complete visual-sequences that may contain tens of thousands of images is also computationally untenable. Here, we propose a system for practical monocular onboard camera auto-calibration from crowdsourced videos. We show the effectiveness of our proposed system on the KITTI raw,OxfordRobotCar, and the crowdsourced D$^2$-City datasets in varying conditions. Finally, we demonstrate its application for accurate monocular dense depth and ego-motion estimation on uncalibrated videos.△ Less"
Understanding Image Retrieval Re-Ranking: A Graph Neural Network Perspective,"Authors:Xuanmeng Zhang,Minyue Jiang,Zhedong Zheng,Xiao Tan,Errui Ding,Yi Yang","Abstract:…post-processing. Similarly, we observe that our method achieves comparable or even better retrieval results on the other four image retrieval benchmarks, i.e., VeRi-776,Oxford-5k, Paris-6k and University-1652, with limited time cost. Our code is publicly available.▽ MoreThe re-ranking approach leverages high-confidence retrieved samples to refine retrieval results, which have been widely adopted as a post-processing tool for image retrieval tasks. However, we notice one main flaw of re-ranking, i.e., high computational complexity, which leads to an unaffordable time cost for real-world applications. In this paper, we revisit re-ranking and demonstrate that re-ranking can be reformulated as a high-parallelism Graph Neural Network (GNN) function. In particular, we divide the conventional re-ranking process into two phases, i.e., retrieving high-quality gallery samples and updating features. We argue that the first phase equals building the k-nearest neighbor graph, while the second phase can be viewed as spreading the message within the graph. In practice, GNN only needs to concern vertices with the connected edges. Since the graph is sparse, we can efficiently update the vertex features. On the Market-1501 dataset, we accelerate the re-ranking processing from 89.2s to 9.4ms with one K40m GPU, facilitating the real-time post-processing. Similarly, we observe that our method achieves comparable or even better retrieval results on the other four image retrieval benchmarks, i.e., VeRi-776,Oxford-5k, Paris-6k and University-1652, with limited time cost. Our code is publicly available.△ Less"
Hard Problem and Free Will: an information-theoretical approach,"Authors:Giacomo Mauro D'Ariano,Federico Faggin","Abstract:We explore definite theoretical assertions about consciousness, starting from a non-reductive psycho-informational solution of David Chalmers's 'hard problem', based on the hypothesis that a fundamental property of 'information' is its experience by the supporting 'system'. The kind of information involved in consciousness needs to be quantum for multiple reasons, including its intrinsic privacy a…▽ MoreWe explore definite theoretical assertions about consciousness, starting from a non-reductive psycho-informational solution of David Chalmers's 'hard problem', based on the hypothesis that a fundamental property of 'information' is its experience by the supporting 'system'. The kind of information involved in consciousness needs to be quantum for multiple reasons, including its intrinsic privacy and its power of building up thoughts by entangling qualia states. As a result we reach a quantum-information-based panpsychism, with classical physics supervening on quantum physics, quantum physics supervening on quantum information, and quantum information supervening on consciousness. We then argue that the internally experienced quantum state, since it corresponds to a definite experience-not to a random choice-must be pure, and we call it ontic, in contrast with the state predictable from the outside (i.e. the state describing the knowledge of the experience from the point of view of an external observer) which we call epistemic and is generally mixed. Purity of the ontic state requires an evolution that is purity preserving, namely a so-called 'atomic' quantum operation. The latter is generally probabilistic, and its particular outcome is interpreted as the free will, which is unpredictable even in principle since quantum probability cannot be interpreted as lack of knowledge. The same purity of state and evolution allows solving the 'combination problem' of panpsychism. Quantum state evolution accounts for a short-term buffer of experience and contains itself quantum-to-classical and classical-to-quantum information transfers. Long term memory, on the other hand, is classical, and needs memorization and recall processes that are quantum-to-classical and classical-to-quantum, respectively...△ Less"
A Registration-aided Domain Adaptation Network for 3D Point Cloud Based Place Recognition,"Authors:Zhijian Qiao,Hanjiang Hu,Weiang Shi,Siyuan Chen,Zhe Liu,Hesheng Wang","Abstract:…to the real-world domain by aligning the global features. Our results outperform state-of-the-art 3D place recognition baselines or achieve comparable on the real-worldOxfordRobotCar dataset with the visualization of registration on the virtual dataset.▽ MoreIn the field of large-scale SLAM for autonomous driving and mobile robotics, 3D point cloud based place recognition has aroused significant research interest due to its robustness to changing environments with drastic daytime and weather variance. However, it is time-consuming and effort-costly to obtain high-quality point cloud data for place recognition model training and ground truth for registration in the real world. To this end, a novel registration-aided 3D domain adaptation network for point cloud based place recognition is proposed. A structure-aware registration network is introduced to help to learn features with geometric information and a 6-DoFs pose between two point clouds with partial overlap can be estimated. The model is trained through a synthetic virtual LiDAR dataset through GTA-V with diverse weather and daytime conditions and domain adaptation is implemented to the real-world domain by aligning the global features. Our results outperform state-of-the-art 3D place recognition baselines or achieve comparable on the real-worldOxfordRobotCar dataset with the visualization of registration on the virtual dataset.△ Less"
Essay Review of Tanya and Jeffrey Bub's Totally Random: Why Nobody Understands Quantum Mechanics: A Serious Comic on Entanglement,"Authors:Michael E. Cuffaro,Emerson P. Doyle","Abstract:This is an extended essay review of Tanya and Jeffrey Bub's Totally Random: Why Nobody Understands Quantum Mechanics: A serious comic on entanglement. Princeton andOxford: Princeton University Press (2018), ISBN: 9780691176956, 272 pp., 7x10 in., 254 b/w illus., £18.99 / $22.95 (paperback). We review the philosophical aspects of the book, provide sugges…▽ MoreThis is an extended essay review of Tanya and Jeffrey Bub's Totally Random: Why Nobody Understands Quantum Mechanics: A serious comic on entanglement. Princeton andOxford: Princeton University Press (2018), ISBN: 9780691176956, 272 pp., 7x10 in., 254 b/w illus., £18.99 / $22.95 (paperback). We review the philosophical aspects of the book, provide suggestions for instructors on how to use the book in a class setting, and evaluate the authors' artistic choices in the context of comics theory.△ Less"
The Foundations of Quantum Mechanics in Post-War Italy's Cultural Context,Authors:Flavio Del Santo,"Abstract:After World War II, a hyper-pragmatic paradigm was established in physics in most of the western countries, within which foundations of quantum mechanics were vastly dismissed as pointless speculations. In this paper, we show that in Italy, however, the interest toward quantum foundations was revived at the turn of the 1960s, mainly thanks to the initiative of Franco Selleri, who started criticisi…▽ MoreAfter World War II, a hyper-pragmatic paradigm was established in physics in most of the western countries, within which foundations of quantum mechanics were vastly dismissed as pointless speculations. In this paper, we show that in Italy, however, the interest toward quantum foundations was revived at the turn of the 1960s, mainly thanks to the initiative of Franco Selleri, who started criticising the contents and the practice of modern physics (in the context of capitalistic society), and thought that the solution was to be sought in a rethinking of the foundations of the discipline. In 1969, supported by Luis de Broglie himself, Selleri wrote a paper reviving the idea of hidden variables and he successfully proposed to the Italian Physical Society to devote the ""Varenna School"" of 1970 to quantum foundations. This school's historical pivotal importance is twofold: it gathered some of the most preeminent international physicists working on the foundations of quantum theory; and it provided a first platform for young physicists to express their dissatisfaction towards ""scientism"". In fact, Selleri's highly politicised views found the favour of a critical mass of young, left-wing physicists, who made of quantum foundations their main topic of research in the 1970s. Although these physicists understood very early the central importance of Bell's theorem, their (ideological) aim was to demonstrate that quantum theory could have limits of validity. Such a research program turned out to be unsuccessful, yet the Italian endeavour was worldwide one of the first and most significant revivals of the interest towards quantum foundations.△ Less"
MonoRec: Semi-Supervised Dense Reconstruction in Dynamic Environments from a Single Moving Camera,"Authors:Felix Wimbauer,Nan Yang,Lukas von Stumberg,Niclas Zeller,Daniel Cremers","Abstract:…performance compared to both multi-view and single-view methods. With the model trained on KITTI, we further demonstrate that MonoRec is able to generalize well to both theOxfordRobotCar dataset and the more challenging TUM-Mono dataset recorded by a handheld camera. Code and related materials will be available at https://vision.in.tum.de/research/monorec.▽ MoreIn this paper, we propose MonoRec, a semi-supervised monocular dense reconstruction architecture that predicts depth maps from a single moving camera in dynamic environments. MonoRec is based on a multi-view stereo setting which encodes the information of multiple consecutive images in a cost volume. To deal with dynamic objects in the scene, we introduce a MaskModule that predicts moving object masks by leveraging the photometric inconsistencies encoded in the cost volumes. Unlike other multi-view stereo methods, MonoRec is able to reconstruct both static and moving objects by leveraging the predicted masks. Furthermore, we present a novel multi-stage training scheme with a semi-supervised loss formulation that does not require LiDAR depth values. We carefully evaluate MonoRec on the KITTI dataset and show that it achieves state-of-the-art performance compared to both multi-view and single-view methods. With the model trained on KITTI, we further demonstrate that MonoRec is able to generalize well to both theOxfordRobotCar dataset and the more challenging TUM-Mono dataset recorded by a handheld camera. Code and related materials will be available at https://vision.in.tum.de/research/monorec.△ Less"
CORAL: Colored structural representation for bi-modal place recognition,"Authors:Yiyuan Pan,Xuecheng Xu,Weijie Li,Yunxiang Cui,Yue Wang,Rong Xiong","Abstract:…and visual features in the consistent bird-eye view frame, yielding a semantic representation, namely CORAL. And the whole network is called CORAL-VLAD. Comparisons on theOxfordRobotCar show that CORAL-VLAD has superior performance against other state-of-the-art methods. We also demonstrate that our network can be generalized to other scenes and sensor con…▽ MorePlace recognition is indispensable for a drift-free localization system. Due to the variations of the environment, place recognition using single-modality has limitations. In this paper, we propose a bi-modal place recognition method, which can extract a compound global descriptor from the two modalities, vision and LiDAR. Specifically, we first build the elevation image generated from 3D points as a structural representation. Then, we derive the correspondences between 3D points and image pixels that are further used in merging the pixel-wise visual features into the elevation map grids. In this way, we fuse the structural features and visual features in the consistent bird-eye view frame, yielding a semantic representation, namely CORAL. And the whole network is called CORAL-VLAD. Comparisons on theOxfordRobotCar show that CORAL-VLAD has superior performance against other state-of-the-art methods. We also demonstrate that our network can be generalized to other scenes and sensor configurations on cross-city datasets.△ Less"
TMEM240 mutations cause spinocerebellar ataxia 21 with mental retardation and severe cognitive impairment,"Authors:Jérôme Delplanque,David Devos,Vincent Huin,Alexandre Genet,Olivier Sand,Caroline Moreau,Cyril Goizet,Perrine Charles,Mathieu Anheim,Marie Lorraine Monin,Luc Buée,Alain Destée,Guillaume Grolez,Christine Delmaire,Kathy Dujardin,Delphine Dellacherie,Alexis Brice,Giovanni Stevanin,Isabelle Strubi-Vuillaume,Alexandra Durr,Bernard Sablonnière","Abstract:Autosomal dominant cerebellar ataxia corresponds to a clinically and genetically heterogeneous group of neurodegenerative disorders that primarily affect the cerebellum. Here, we report the identification of the causative gene in spinocerebellar ataxia 21, an autosomal-dominant disorder previously mapped to chromosome 7p21.3-p15.1. This ataxia was firstly characterized in a large French family wit…▽ MoreAutosomal dominant cerebellar ataxia corresponds to a clinically and genetically heterogeneous group of neurodegenerative disorders that primarily affect the cerebellum. Here, we report the identification of the causative gene in spinocerebellar ataxia 21, an autosomal-dominant disorder previously mapped to chromosome 7p21.3-p15.1. This ataxia was firstly characterized in a large French family with slowly progressive cerebellar ataxia, accompanied by severe cognitive impairment and mental retardation in two young children. Following the recruitment of 12 additional young family members, linkage analysis enabled us to definitively map the disease locus to chromosome 1p36.33-p36.32. The causative mutation, (c.509C4T/p.P170L) in the transmembrane protein gene TMEM240, was identified by whole exome sequencing and then was confirmed by Sanger sequencing and co-segregation analyses. Index cases from 368 French families with autosomal-dominant cerebellar ataxia were also screened for mutations. In seven cases, we identified a range of missense mutations (c.509C4T/p.P170L, c.239C4T/p.T80M, c.346C4T/p.R116C, c.445G4A/p.E149K, c.511C4T/p.R171W), and a stop mutation (c.489C4G/p.Y163*) in the same gene. TMEM240 is a small, strongly conserved transmembrane protein of unknown function present in cerebellum and brain. Spinocerebellar ataxia 21 may be a particular early-onset disease associated with severe cognitive impairment.△ Less"
Homozygous GRN mutations: unexpected phenotypes and new insights into pathological and molecular mechanisms,"Authors:Vincent Huin,Mathieu Barbier,Armand Bottani,Johannes Lobrinus,Fabienne Clot,Foudil Lamari,Laureen Chat,Benoît Rucheton,Frédérique Fluchère,Stéphane Auvin,Peter Myers,Antoinette Gelot,Agnès Camuzat,Catherine Caillaud,Ludmila Jornéa,Sylvie Forlani,Dario Saracino,Charles Duyckaerts,Alexis Brice,Alexandra Durr,Isabelle Le Ber","Abstract:Homozygous mutations in the progranulin gene (GRN) are associated with neuronal ceroid lipofuscinosis 11 (CLN11), a rare lysosomal-storage disorder characterized by cerebellar ataxia, seizures, retinitis pigmentosa, and cognitive disorders, usually beginning between 13 and 25 years of age. This is a rare condition, previously reported in only four families. In contrast, heterozygous GRN mutations…▽ MoreHomozygous mutations in the progranulin gene (GRN) are associated with neuronal ceroid lipofuscinosis 11 (CLN11), a rare lysosomal-storage disorder characterized by cerebellar ataxia, seizures, retinitis pigmentosa, and cognitive disorders, usually beginning between 13 and 25 years of age. This is a rare condition, previously reported in only four families. In contrast, heterozygous GRN mutations are a major cause of frontotemporal dementia associated with neuronal cytoplasmic TDP-43 inclusions. We identified homozygous GRN mutations in six new patients. The phenotypic spectrum is much broader than previously reported, with two remarkably distinct presentations, depending on the age of onset. A childhood/juvenile form is characterized by classical CLN11 symptoms at an early age at onset. Unexpectedly, other homozygous patients presented a distinct delayed phenotype of frontotemporal dementia and parkinsonism after 50 years; none had epilepsy or cerebellar ataxia. Another major finding of this study is that all GRN mutations may not have the same impact on progranulin protein synthesis. A hypomorphic effect of some mutations is supported by the presence of residual levels of plasma progranulin and low levels of normal transcript detected in one case with a homozygous splice-site mutation and late onset frontotemporal dementia. This is a new critical finding that must be considered in therapeutic trials based on replacement strategies. The first neuropathological study in a homozygous carrier provides new insights into the pathological mechanisms of the disease. Hallmarks of neuronal ceroid lipofuscinosis were present. The absence of TDP-43 cytoplasmic inclusions markedly differs from observations of heterozygous mutations, suggesting a pathological shift between lysosomal and TDP-43 pathologies depending on the mono or bi-allelic status. An intriguing observation was the loss of normal TDP-43 staining in the nucleus of some neurons, which could be the first stage of the TDP-43 pathological process preceding the formation of typical cytoplasmic inclusions. Finally, this study has important implications for genetic counselling and molecular diagnosis. Semi-dominant inheritance of GRN mutations implies that specific genetic counseling should be delivered to children and parents of CLN11 patients, as they are heterozygous carriers with a high risk of developing dementia. More broadly, this study illustrates the fact that genetic variants can lead to different phenotypes according to their mono- or bi-allelic state, which is a challenge for genetic diagnosis.△ Less"
Tackling Loopholes in Experimental Tests of Bell's Inequality,Authors:David I. Kaiser,"Abstract:Bell's inequality sets a strict threshold for how strongly correlated the outcomes of measurements on two or more particles can be, if the outcomes of each measurement are independent of actions undertaken at arbitrarily distant locations. Quantum mechanics, on the other hand, predicts that measurements on particles in entangled states can be more strongly correlated than Bell's inequality would a…▽ MoreBell's inequality sets a strict threshold for how strongly correlated the outcomes of measurements on two or more particles can be, if the outcomes of each measurement are independent of actions undertaken at arbitrarily distant locations. Quantum mechanics, on the other hand, predicts that measurements on particles in entangled states can be more strongly correlated than Bell's inequality would allow. Whereas experimental tests conducted over the past half-century have consistently measured violations of Bell's inequality---consistent with the predictions of quantum mechanics---the experiments have been subject to one or more ""loopholes,"" by means of which certain alternatives to quantum theory could remain consistent with the experimental results. This chapter reviews three of the most significant loopholes, often dubbed the ""locality,"" ""fair-sampling,"" and ""freedom-of-choice"" loopholes, and describes how recent experiments have addressed them.△ Less"
Multimodal Prototypical Networks for Few-shot Learning,"Authors:Frederik Pahde,Mihai Puscas,Tassilo Klein,Moin Nabi","Abstract:…that in such cases nearest neighbor classification is a viable approach and outperform state-of-the-art single-modal and multimodal few-shot learning methods on the CUB-200 andOxford-102 datasets.▽ MoreAlthough providing exceptional results for many computer vision tasks, state-of-the-art deep learning algorithms catastrophically struggle in low data scenarios. However, if data in additional modalities exist (e.g. text) this can compensate for the lack of data and improve the classification results. To overcome this data scarcity, we design a cross-modal feature generation framework capable of enriching the low populated embedding space in few-shot scenarios, leveraging data from the auxiliary modality. Specifically, we train a generative model that maps text data into the visual feature space to obtain more reliable prototypes. This allows to exploit data from additional modalities (e.g. text) during training while the ultimate task at test time remains classification with exclusively visual data. We show that in such cases nearest neighbor classification is a viable approach and outperform state-of-the-art single-modal and multimodal few-shot learning methods on the CUB-200 andOxford-102 datasets.△ Less"
DeepSeqSLAM: A Trainable CNN+RNN for Joint Global Description and Sequence-based Place Recognition,"Authors:Marvin Chancán,Michael Milford","Abstract:…learning visual and positional representations from a single monocular image sequence of a route. We demonstrate our approach on two large benchmark datasets, Nordland andOxfordRobotCar - recorded over 728 km and 10 km routes, respectively, each during 1 year with multiple seasons, weather, and lighting conditions. On Nordland, we compare our method to two…▽ MoreSequence-based place recognition methods for all-weather navigation are well-known for producing state-of-the-art results under challenging day-night or summer-winter transitions. These systems, however, rely on complex handcrafted heuristics for sequential matching - which are applied on top of a pre-computed pairwise similarity matrix between reference and query image sequences of a single route - to further reduce false-positive rates compared to single-frame retrieval methods. As a result, performing multi-frame place recognition can be extremely slow for deployment on autonomous vehicles or evaluation on large datasets, and fail when using relatively short parameter values such as a sequence length of 2 frames. In this paper, we propose DeepSeqSLAM: a trainable CNN+RNN architecture for jointly learning visual and positional representations from a single monocular image sequence of a route. We demonstrate our approach on two large benchmark datasets, Nordland andOxfordRobotCar - recorded over 728 km and 10 km routes, respectively, each during 1 year with multiple seasons, weather, and lighting conditions. On Nordland, we compare our method to two state-of-the-art sequence-based methods across the entire route under summer-winter changes using a sequence length of 2 and show that our approach can get over 72% AUC compared to 27% AUC for Delta Descriptors and 2% AUC for SeqSLAM; while drastically reducing the deployment time from around 1 hour to 1 minute against both. The framework code and video are available at https://mchancan.github.io/deepseqslam△ Less"
Multiplicity and Diversity: Analyzing the Optimal Solution Space of the Correlation Clustering Problem on Complete Signed Graphs,"Authors:Nejat Arinik,Rosa Figueiredo,Vincent Labatut","Abstract:In order to study real-world systems, many applied works model them through signed graphs, i.e. graphs whose edges are labeled as either positive or negative. Such a graph is considered as structurally balanced when it can be partitioned into a number of modules, such that positive (resp. negative) edges are located inside (resp. in-between) the modules. When it is not the case, authors look for t…▽ MoreIn order to study real-world systems, many applied works model them through signed graphs, i.e. graphs whose edges are labeled as either positive or negative. Such a graph is considered as structurally balanced when it can be partitioned into a number of modules, such that positive (resp. negative) edges are located inside (resp. in-between) the modules. When it is not the case, authors look for the closest partition to such balance, a problem called Correlation Clustering (CC). Due to the complexity of the CC problem, the standard approach is to find a single optimal partition and stick to it, even if other optimal or high scoring solutions possibly exist. In this work, we study the space of optimal solutions of the CC problem, on a collection of synthetic complete graphs. We show empirically that under certain conditions, there can be many optimal partitions of a signed graph. Some of these are very different and thus provide distinct perspectives on the system, as illustrated on a small real-world graph. This is an important result, as it implies that one may have to find several, if not all, optimal solutions of the CC problem, in order to properly study the considered system.△ Less"
Newtonian Fractional-Dimension Gravity and Rotationally Supported Galaxies,Authors:Gabriele U. Varieschi,"Abstract:We continue our analysis of Newtonian Fractional-Dimension Gravity, an extension of the standard laws of Newtonian gravity to lower dimensional spaces including those with fractional (i.e., non-integer) dimension. We apply our model to three rotationally supported galaxies: NGC 7814 (Bulge-Dominated Spiral), NGC 6503 (Disk-Dominated Spiral), and NGC 3741 (Gas-Dominated Dwarf).
  As was done in the…▽ MoreWe continue our analysis of Newtonian Fractional-Dimension Gravity, an extension of the standard laws of Newtonian gravity to lower dimensional spaces including those with fractional (i.e., non-integer) dimension. We apply our model to three rotationally supported galaxies: NGC 7814 (Bulge-Dominated Spiral), NGC 6503 (Disk-Dominated Spiral), and NGC 3741 (Gas-Dominated Dwarf).
  As was done in the general cases of spherically-symmetric and axially-symmetric structures, which were studied in previous work on the subject, we examine a possible connection between our model and Modified Newtonian Dynamics, a leading alternative gravity model which explains the observed properties of these galaxies without requiring the Dark Matter hypothesis.
  In our model, the MOND acceleration constant $a_{0} \simeq 1.2 \times 10^{-10}\mbox{m}\thinspace \mbox{s}^{ -2}$ can be related to a natural scale length $l_{0}$, namely $a_{0} \approx GM/l_{0}^{2}$ for a galaxy of mass $M$. Also, the empirical Radial Acceleration Relation, connecting the observed radial acceleration $g_{obs}$ with the baryonic one $g_{bar}$, can be explained in terms of a variable local dimension $D$. As an example of this methodology, we provide detailed rotation curve fits for the three galaxies mentioned above.△ Less"
Contextuality: At the Borders of Paradox,Authors:Samson Abramsky,"Abstract:Contextuality is a key feature of quantum mechanics. We present the sheaf-theoretic approach to contextuality introduced by Abramsky and Brandenburger, and show how it covers a range of logical and physical phenomena ""at the borders of paradox"".Contextuality is a key feature of quantum mechanics. We present the sheaf-theoretic approach to contextuality introduced by Abramsky and Brandenburger, and show how it covers a range of logical and physical phenomena ""at the borders of paradox"".△ Less"
Do We Need to Compensate for Motion Distortion and Doppler Effects in Spinning Radar Navigation?,"Authors:Keenan Burnett,Angela P. Schoellig,Timothy D. Barfoot","Abstract:…which may be significant in the self-driving car domain where speeds can be high. In this work, we demonstrate the effect of these distortions on radar odometry using theOxfordRadar RobotCar Dataset and metric localization using our own data-taking platform. We revisit a lightweight estimator that can recover the motion between a pair of radar scans while…▽ MoreIn order to tackle the challenge of unfavorable weather conditions such as rain and snow, radar is being revisited as a parallel sensing modality to vision and lidar. Recent works have made tremendous progress in applying spinning radar to odometry and place recognition. However, these works have so far ignored the impact of motion distortion and Doppler effects on spinning-radar-based navigation, which may be significant in the self-driving car domain where speeds can be high. In this work, we demonstrate the effect of these distortions on radar odometry using theOxfordRadar RobotCar Dataset and metric localization using our own data-taking platform. We revisit a lightweight estimator that can recover the motion between a pair of radar scans while accounting for both effects. Our conclusion is that both motion distortion and the Doppler effect are significant in different aspects of spinning radar navigation, with the former more prominent than the latter. Code for this project can be found at: https://github.com/keenan-burnett/yeti_radar_odometry△ Less"
Event-VPR: End-to-End Weakly Supervised Network Architecture for Event-based Visual Place Recognition,"Authors:Delei Kong,Zheng Fang,Haojia Li,Kuanxu Hou,Sonya Coleman,Dermot Kerr","Abstract:…of the proposed algorithm, we compare the proposed method with classical VPR methods on the event-based driving datasets (MVSEC, DDD17) and the synthetic datasets (OxfordRobotCar). Experimental results show that the proposed method can achieve much better performance in challenging scenarios. To our knowledge, this is the first end-to-end event-based VPR me…▽ MoreTraditional visual place recognition (VPR) methods generally use frame-based cameras, which is easy to fail due to dramatic illumination changes or fast motions. In this paper, we propose an end-to-end visual place recognition network for event cameras, which can achieve good place recognition performance in challenging environments. The key idea of the proposed algorithm is firstly to characterize the event streams with the EST voxel grid, then extract features using a convolution network, and finally aggregate features using an improved VLAD network to realize end-to-end visual place recognition using event streams. To verify the effectiveness of the proposed algorithm, we compare the proposed method with classical VPR methods on the event-based driving datasets (MVSEC, DDD17) and the synthetic datasets (OxfordRobotCar). Experimental results show that the proposed method can achieve much better performance in challenging scenarios. To our knowledge, this is the first end-to-end event-based VPR method. The accompanying source code is available at https://github.com/kongdelei/Event-VPR.△ Less"
Learning to Reconstruct and Segment 3D Objects,Authors:Bo Yang,"Abstract:To endow machines with the ability to perceive the real-world in a three dimensional representation as we do as humans is a fundamental and long-standing topic in Artificial Intelligence. Given different types of visual inputs such as images or point clouds acquired by 2D/3D sensors, one important goal is to understand the geometric structure and semantics of the 3D environment. Traditional approa…▽ MoreTo endow machines with the ability to perceive the real-world in a three dimensional representation as we do as humans is a fundamental and long-standing topic in Artificial Intelligence. Given different types of visual inputs such as images or point clouds acquired by 2D/3D sensors, one important goal is to understand the geometric structure and semantics of the 3D environment. Traditional approaches usually leverage hand-crafted features to estimate the shape and semantics of objects or scenes. However, they are difficult to generalize to novel objects and scenarios, and struggle to overcome critical issues caused by visual occlusions. By contrast, we aim to understand scenes and the objects within them by learning general and robust representations using deep neural networks, trained on large-scale real-world 3D data. To achieve these aims, this thesis makes three core contributions from object-level 3D shape estimation from single or multiple views to scene-level semantic understanding.△ Less"
"Body models in humans, animals, and robots: mechanisms and plasticity",Authors:Matej Hoffmann,"Abstract:Humans and animals excel in combining information from multiple sensory modalities, controlling their complex bodies, adapting to growth, failures, or using tools. These capabilities are also highly desirable in robots. They are displayed by machines to some extent - yet, as is so often the case, the artificial creatures are lagging behind. The key foundation is an internal representation of the b…▽ MoreHumans and animals excel in combining information from multiple sensory modalities, controlling their complex bodies, adapting to growth, failures, or using tools. These capabilities are also highly desirable in robots. They are displayed by machines to some extent - yet, as is so often the case, the artificial creatures are lagging behind. The key foundation is an internal representation of the body that the agent - human, animal, or robot - has developed. In the biological realm, evidence has been accumulated by diverse disciplines giving rise to the concepts of body image, body schema, and others. In robotics, a model of the robot is an indispensable component that enables to control the machine. In this article I compare the character of body representations in biology with their robotic counterparts and relate that to the differences in performance that we observe. I put forth a number of axes regarding the nature of such body models: fixed vs. plastic, amodal vs. modal, explicit vs. implicit, serial vs. parallel, modular vs. holistic, and centralized vs. distributed. An interesting trend emerges: on many of the axes, there is a sequence from robot body models, over body image, body schema, to the body representation in lower animals like the octopus. In some sense, robots have a lot in common with Ian Waterman - ""the man who lost his body"" - in that they rely on an explicit, veridical body model (body image taken to the extreme) and lack any implicit, multimodal representation (like the body schema) of their bodies. I will then detail how robots can inform the biological sciences dealing with body representations and finally, I will study which of the features of the ""body in the brain"" should be transferred to robots, giving rise to more adaptive and resilient, self-calibrating machines.△ Less"
LM-Reloc: Levenberg-Marquardt Based Direct Visual Relocalization,"Authors:Lukas von Stumberg,Patrick Wenzel,Nan Yang,Daniel Cremers","Abstract:…image baselines, we propose a pose estimation network, CorrPoseNet, which regresses the relative pose to bootstrap the direct image alignment. Evaluations on the CARLA andOxfordRobotCar relocalization tracking benchmark show that our approach delivers more accurate results than previous state-of-the-art methods while being comparable in terms of robustness…▽ MoreWe present LM-Reloc -- a novel approach for visual relocalization based on direct image alignment. In contrast to prior works that tackle the problem with a feature-based formulation, the proposed method does not rely on feature matching and RANSAC. Hence, the method can utilize not only corners but any region of the image with gradients. In particular, we propose a loss formulation inspired by the classical Levenberg-Marquardt algorithm to train LM-Net. The learned features significantly improve the robustness of direct image alignment, especially for relocalization across different conditions. To further improve the robustness of LM-Net against large image baselines, we propose a pose estimation network, CorrPoseNet, which regresses the relative pose to bootstrap the direct image alignment. Evaluations on the CARLA andOxfordRobotCar relocalization tracking benchmark show that our approach delivers more accurate results than previous state-of-the-art methods while being comparable in terms of robustness.△ Less"
Graphical Calculi and their Conjecture Synthesis,Authors:Hector Miller-Bakewell,"Abstract:Categorical Quantum Mechanics, and graphical calculi in particular, has proven to be an intuitive and powerful way to reason about quantum computing. This work continues the exploration of graphical calculi, inside and outside of the quantum computing setting, by investigating the algebraic structures with which we label diagrams. The initial aim for this was Conjecture Synthesis; the algorithmic…▽ MoreCategorical Quantum Mechanics, and graphical calculi in particular, has proven to be an intuitive and powerful way to reason about quantum computing. This work continues the exploration of graphical calculi, inside and outside of the quantum computing setting, by investigating the algebraic structures with which we label diagrams. The initial aim for this was Conjecture Synthesis; the algorithmic process of creating theorems. To this process we introduce a generalisation step, which itself requires the ability to infer and then verify parameterised families of theorems. This thesis introduces such inference and verification frameworks, in doing so forging novel links between graphical calculi and fields such as Algebraic Geometry and Galois Theory. These frameworks inspired further research into the design of graphical calculi, and we introduce two important new calculi here. First is the calculus RING, which is initial among ring-based qubit graphical calculi, and in turn inspired the introduction and classification of phase homomorphism pairs also presented here. The second is the calculus ZQ, an edge-decorated calculus which naturally expresses arbitrary qubit rotations, eliminating the need for non-linear rules such as (EU) of ZX. It is expected that these results will be of use to those creating optimisation schemes and intermediate representations for quantum computing, to those creating new graphical calculi, and for those performing conjecture synthesis.△ Less"
Automotive Radar Data Acquisition using Object Detection,"Authors:Madhumitha Sakthi,Ahmed Tewfik","Abstract:…matrix for radar acquisition which is hardware-efficient and show its performance is similar to Gaussian and Binary Permuted Block Diagonal matrix. Our experiments on theOxfordradar dataset show an effective reconstruction of objects of interest with 10% sampling rate. Finally, we develop a transformer-based 2D object detection network using the NuScenes r…▽ MoreThe growing urban complexity demands an efficient algorithm to acquire and process various sensor information from autonomous vehicles. In this paper, we introduce an algorithm to utilize object detection results from the image to adaptively sample and acquire radar data using Compressed Sensing (CS). This novel algorithm is motivated by the hypothesis that with a limited sampling budget, allocating more sampling budget to areas with the object as opposed to a uniform sampling ultimately improves relevant object detection performance. We improve detection performance by dynamically allocating a lower sampling rate to objects such as buses than pedestrians leading to better reconstruction than baseline across areas with objects of interest. We automate the sampling rate allocation using linear programming and show significant time savings while reducing the radar block size by a factor of 2. We also analyze a Binary Permuted Diagonal measurement matrix for radar acquisition which is hardware-efficient and show its performance is similar to Gaussian and Binary Permuted Block Diagonal matrix. Our experiments on theOxfordradar dataset show an effective reconstruction of objects of interest with 10% sampling rate. Finally, we develop a transformer-based 2D object detection network using the NuScenes radar and image data.△ Less"
Explaining Deep Neural Networks,Authors:Oana-Maria Camburu,"Abstract:Deep neural networks are becoming more and more popular due to their revolutionary success in diverse areas, such as computer vision, natural language processing, and speech recognition. However, the decision-making processes of these models are generally not interpretable to users. In various domains, such as healthcare, finance, or law, it is critical to know the reasons behind a decision made b…▽ MoreDeep neural networks are becoming more and more popular due to their revolutionary success in diverse areas, such as computer vision, natural language processing, and speech recognition. However, the decision-making processes of these models are generally not interpretable to users. In various domains, such as healthcare, finance, or law, it is critical to know the reasons behind a decision made by an artificial intelligence system. Therefore, several directions for explaining neural models have recently been explored. In this thesis, I investigate two major directions for explaining deep neural networks. The first direction consists of feature-based post-hoc explanatory methods, that is, methods that aim to explain an already trained and fixed model (post-hoc), and that provide explanations in terms of input features, such as tokens for text and superpixels for images (feature-based). The second direction consists of self-explanatory neural models that generate natural language explanations, that is, models that have a built-in module that generates explanations for the predictions of the model.△ Less"
Unsupervised Monocular Depth Estimation for Night-time Images using Adversarial Domain Feature Adaptation,"Authors:Madhu Vankadari,Sourav Garg,Anima Majumder,Swagat Kumar,Ardhendu Behera","Abstract:…Hence, the resulting method is termed as ""Adversarial Domain Feature Adaptation (ADFA)"" and its efficacy is demonstrated through experimentation on the challengingOxfordnight driving dataset. Also, The modular encoder-decoder architecture for the proposed ADFA method allows us to use the encoder module as a feature extractor which can be used in m…▽ MoreIn this paper, we look into the problem of estimating per-pixel depth maps from unconstrained RGB monocular night-time images which is a difficult task that has not been addressed adequately in the literature. The state-of-the-art day-time depth estimation methods fail miserably when tested with night-time images due to a large domain shift between them. The usual photo metric losses used for training these networks may not work for night-time images due to the absence of uniform lighting which is commonly present in day-time images, making it a difficult problem to solve. We propose to solve this problem by posing it as a domain adaptation problem where a network trained with day-time images is adapted to work for night-time images. Specifically, an encoder is trained to generate features from night-time images that are indistinguishable from those obtained from day-time images by using a PatchGAN-based adversarial discriminative learning method. Unlike the existing methods that directly adapt depth prediction (network output), we propose to adapt feature maps obtained from the encoder network so that a pre-trained day-time depth decoder can be directly used for predicting depth from these adapted features. Hence, the resulting method is termed as ""Adversarial Domain Feature Adaptation (ADFA)"" and its efficacy is demonstrated through experimentation on the challengingOxfordnight driving dataset. Also, The modular encoder-decoder architecture for the proposed ADFA method allows us to use the encoder module as a feature extractor which can be used in many other applications. One such application is demonstrated where the features obtained from our adapted encoder network are shown to outperform other state-of-the-art methods in a visual place recognition problem, thereby, further establishing the usefulness and effectiveness of the proposed approach.△ Less"
DASGIL: Domain Adaptation for Semantic and Geometric-aware Image-based Localization,"Authors:Hanjiang Hu,Zhijian Qiao,Ming Cheng,Zhe Liu,Hesheng Wang","Abstract:…to achieve the domain adaptation from synthetic virtual KITTI dataset to real-world KITTI dataset. The proposed approach is validated on the Extended CMU-Seasons dataset andOxfordRobotCar dataset through a series of crucial comparison experiments, where our performance outperforms state-of-the-art baselines for retrieval-based localization and large-scale…▽ MoreLong-Term visual localization under changing environments is a challenging problem in autonomous driving and mobile robotics due to season, illumination variance, etc. Image retrieval for localization is an efficient and effective solution to the problem. In this paper, we propose a novel multi-task architecture to fuse the geometric and semantic information into the multi-scale latent embedding representation for visual place recognition. To use the high-quality ground truths without any human effort, the effective multi-scale feature discriminator is proposed for adversarial training to achieve the domain adaptation from synthetic virtual KITTI dataset to real-world KITTI dataset. The proposed approach is validated on the Extended CMU-Seasons dataset andOxfordRobotCar dataset through a series of crucial comparison experiments, where our performance outperforms state-of-the-art baselines for retrieval-based localization and large-scale place recognition under the challenging environment.△ Less"
Dosimetry and calorimetry performance of a scientific CMOS camera for environmental monitoring,"Authors:Alexis Aguilar-Arevalo,Xavier Bertou,Carles Canet,Miguel Angel Cruz-Perez,Alexander Deisting,Adriana Dias,Juan Carlos D'Olivo,Francisco Favela-Perez,Estela A. Garces,Adiv Gonzalez Munoz,Jaime Octavio Guerra-Pulido,Javier Mancera-Alejandrez,Daniel Jose Marin-Lambarri,Mauricio Martinez Montero,Jocelyn Monroe,Sean Paling,Simon J. M. Peeters,Paul Scovell,Cenk Turkoglu,Eric Vazquez-Jauregui,Joseph Walding","Abstract:…-emissions with energies ranging from 10 keV to several 100 keV when they decay; this range is detectable in silicon sensors. In this paper we test a CMOS camera (OxfordInstruments Neo 5.5) for its general performance as a detector of x-rays and low energy $γ$-rays and assess its sensitivity relative to the World Health Organization upper limit on lead in d…▽ MoreThis paper explores the prospect of CMOS devices to assay lead in drinking water, using calorimetry. Lead occurs together with traces of radioisotopes, e.g. Lead-210, producing $γ$-emissions with energies ranging from 10 keV to several 100 keV when they decay; this range is detectable in silicon sensors. In this paper we test a CMOS camera (OxfordInstruments Neo 5.5) for its general performance as a detector of x-rays and low energy $γ$-rays and assess its sensitivity relative to the World Health Organization upper limit on lead in drinking water. Energies from 6 keV to 60 keV are examined. The CMOS camera has a linear energy response over this range and its energy resolution is for the most part slightly better than 2 %. The Neo sCMOS is not sensitive to x-rays with energies below $\sim\!\!10 keV$. The smallest detectable rate is 40$\pm$3 mHz, corresponding to an incident activity on the chip of 7$\pm$4 Bq. The estimation of the incident activity sensitivity from the detected activity relies on geometric acceptance and the measured efficiency vs. energy. We report the efficiency measurement, which is 0.08$\pm$0.02 % (0.0011$\pm$0.0002 %) at 26.3 keV (59.5 keV). Taking calorimetric information into account we measure a minimal detectable rate of 4$\pm$1 mHz (1.5$\pm$0.1 mHz) for 26.3 keV (59.5 keV) $γ$-rays, which corresponds to an incident activity of 1.0$\pm$0.6 Bq (57$\pm$33 Bq). Toy Monte Carlo and Geant4 simulations agree with these results. These results show this CMOS sensor is well-suited as a $γ$- and x-ray detector with sensitivity at the few to 100 ppb level for Lead-210 in a sample.△ Less"
Measurements of branching fractions and CP-violating charge asymmetries in charmless $B$ decays reconstructed in 2019--2020 Belle~II data,"Authors:Belle II Collaboration,F. Abudinén,I. Adachi,R. Adak,K. Adamczyk,P. Ahlburg,J. K. Ahn,H. Aihara,N. Akopov,A. Aloisio,F. Ameli,L. Andricek,N. Anh Ky,D. M. Asner,H. Atmacan,V. Aulchenko,T. Aushev,V. Aushev,T. Aziz,V. Babu,S. Bacher,S. Baehr,S. Bahinipati,A. M. Bakich,P. Bambade, et al. (522 additional authors not shown)","Abstract:We report on first measurements of branching fractions~($\mathcal{B}$) and CP-violating charge asymmetries~($\mathcal{A}$) in charmless $B$ decays at Belle~II. We use a sample of electron-positron collisions collected in 2019 and 2020 at the $Υ(4S)$ resonance and corresponding to $34.6$\,fb$^{-1}$ of integrated luminosity. We use simulation to determine optimized event selections. The $ΔE$ distrib…▽ MoreWe report on first measurements of branching fractions~($\mathcal{B}$) and CP-violating charge asymmetries~($\mathcal{A}$) in charmless $B$ decays at Belle~II. We use a sample of electron-positron collisions collected in 2019 and 2020 at the $Υ(4S)$ resonance and corresponding to $34.6$\,fb$^{-1}$ of integrated luminosity. We use simulation to determine optimized event selections. The $ΔE$ distributions of the resulting samples, restricted in $M_{\rm bc}$, are fit to determine signal yields. Signal yields are corrected for efficiencies determined from simulation and control data samples to obtain branching fractions and CP-violating asymmetries for flavour-specific channels. These are the first measurements in charmless decays reported by Belle~II. Results are compatible with known determinations and show detector performance comparable with the best Belle results offering a reliable basis to assess projections for future reach.△ Less"
"International Trade Finance from the Origins to the Present: Market Structures, Regulation and Governance","Authors:Olivier Accominotti,Stefano Ugolini","Abstract:This chapter presents a history of international trade finance - the oldest domain of international finance - from its emergence in the Middle Ages up to today. We describe how the structure and governance of the global trade finance market changed over time and how trade credit instruments evolved. Trade finance products initially consisted of idiosyncratic assets issued by local merchants and ba…▽ MoreThis chapter presents a history of international trade finance - the oldest domain of international finance - from its emergence in the Middle Ages up to today. We describe how the structure and governance of the global trade finance market changed over time and how trade credit instruments evolved. Trade finance products initially consisted of idiosyncratic assets issued by local merchants and bankers. The financing of international trade then became increasingly centralized and credit instruments were standardized through the diffusion of the local standards of consecutive leading trading centres (Antwerp, Amsterdam, London). This process of market centralization/product standardization culminated in the nineteenth century when London became the global centre for international trade finance and the sterling bill of exchange emerged as the most widely used trade finance instrument. The structure of the trade finance market then evolved considerably following the First World War and disintegrated during the interwar de-globalization and Bretton Woods period. The reconstruction of global trade finance in the post-1970 period gave way to the decentralized market structure that prevails nowadays.△ Less"
Adaptive Convolution Kernel for Artificial Neural Networks,"Authors:F. Boray Tek,İlker Çam,Deniz Karlı","Abstract:…in the Wild'' showed that the adaptive kernels can provide statistically significant improvements on ordinary convolution kernels. A segmentation experiment in theOxford-Pets dataset demonstrated that replacing a single ordinary convolution layer in a U-shaped network with a single 7$\times$7 adaptive layer can improve its learning performance and a…▽ MoreMany deep neural networks are built by using stacked convolutional layers of fixed and single size (often 3$\times$3) kernels. This paper describes a method for training the size of convolutional kernels to provide varying size kernels in a single layer. The method utilizes a differentiable, and therefore backpropagation-trainable Gaussian envelope which can grow or shrink in a base grid. Our experiments compared the proposed adaptive layers to ordinary convolution layers in a simple two-layer network, a deeper residual network, and a U-Net architecture. The results in the popular image classification datasets such as MNIST, MNIST-CLUTTERED, CIFAR-10, Fashion, and ``Faces in the Wild'' showed that the adaptive kernels can provide statistically significant improvements on ordinary convolution kernels. A segmentation experiment in theOxford-Pets dataset demonstrated that replacing a single ordinary convolution layer in a U-shaped network with a single 7$\times$7 adaptive layer can improve its learning performance and ability to generalize.△ Less"
The transformer earthquake alerting model: A new versatile approach to earthquake early warning,"Authors:Jannes Münchmeyer,Dino Bindi,Ulf Leser,Frederik Tilmann","Abstract:Earthquakes are major hazards to humans, buildings and infrastructure. Early warning methods aim to provide advance notice of incoming strong shaking to enable preventive action and mitigate seismic risk. Their usefulness depends on accuracy, the relation between true, missed and false alerts, and timeliness, the time between a warning and the arrival of strong shaking. Current approaches suffer f…▽ MoreEarthquakes are major hazards to humans, buildings and infrastructure. Early warning methods aim to provide advance notice of incoming strong shaking to enable preventive action and mitigate seismic risk. Their usefulness depends on accuracy, the relation between true, missed and false alerts, and timeliness, the time between a warning and the arrival of strong shaking. Current approaches suffer from apparent aleatoric uncertainties due to simplified modelling or short warning times. Here we propose a novel early warning method, the deep-learning based transformer earthquake alerting model (TEAM), to mitigate these limitations. TEAM analyzes raw, strong motion waveforms of an arbitrary number of stations at arbitrary locations in real-time, making it easily adaptable to changing seismic networks and warning targets. We evaluate TEAM on two regions with high seismic hazard, Japan and Italy, that are complementary in their seismicity. On both datasets TEAM outperforms existing early warning methods considerably, offering accurate and timely warnings. Using domain adaptation, TEAM even provides reliable alerts for events larger than any in the training data, a property of highest importance as records from very large events are rare in many regions.△ Less"
"Data mining and analysis of scientific research data records on Covid 19 mortality, immunity, and vaccine development in the first wave of the Covid 19 pandemic","Authors:Petar Radanliev,David De Roure,Rob Walton","Abstract:…and vaccine, we discovered that although the USA is leading in volume of scientific research on Covid 19 vaccine, the leading 3 research institutions (Fudan, Melbourne,Oxford) are not based in the USA. Hence, it is difficult to predict which country would be first to produce a Covid 19 vaccine.▽ MoreIn this study, we investigate the scientific research response from the early stages of the pandemic, and we review key findings on how the early warning systems developed in previous epidemics responded to contain the virus. The data records are analysed with commutable statistical methods, including R Studio, Bibliometrix package, and the Web of Science data mining tool. We identified few different clusters, containing references to exercise, inflammation, smoking, obesity and many additional factors. From the analysis on Covid-19 and vaccine, we discovered that although the USA is leading in volume of scientific research on Covid 19 vaccine, the leading 3 research institutions (Fudan, Melbourne,Oxford) are not based in the USA. Hence, it is difficult to predict which country would be first to produce a Covid 19 vaccine.△ Less"
Measurement of Hadronic Mass Moments $\langle M_X^n \rangle $ in $B \rightarrow X_c \ell ν$ Decays at Belle II,"Authors:Belle II Collaboration,F. Abudinén,I. Adachi,R. Adak,K. Adamczyk,P. Ahlburg,J. K. Ahn,H. Aihara,N. Akopov,A. Aloisio,F. Ameli,L. Andricek,N. Anh Ky,D. M. Asner,H. Atmacan,V. Aulchenko,T. Aushev,V. Aushev,T. Aziz,V. Babu,S. Bacher,S. Baehr,S. Bahinipati,A. M. Bakich,P. Bambade, et al. (518 additional authors not shown)","Abstract:We present measurements of the first six hadronic mass moments in semileptonic $B \rightarrow X_c \ell ν$ decays. The hadronic mass moments, together with other observables of inclusive $B$ decays, can be used to determine the CKM matrix element $|{V_{cb}}|$ and mass of the $b$-quark $m_b$ in the context of Heavy Quark Expansions of QCD. The Belle~II data recorded at the $Υ(4S)$ resonance in 2019…▽ MoreWe present measurements of the first six hadronic mass moments in semileptonic $B \rightarrow X_c \ell ν$ decays. The hadronic mass moments, together with other observables of inclusive $B$ decays, can be used to determine the CKM matrix element $|{V_{cb}}|$ and mass of the $b$-quark $m_b$ in the context of Heavy Quark Expansions of QCD. The Belle~II data recorded at the $Υ(4S)$ resonance in 2019 and 2020 (March-July), corresponding to an integrated luminosity of $34.6\;\mathrm{fb}^{-1}$, is used for this measurement. The decay $Υ(4S) \rightarrow B \overline{B}$ is reconstructed by applying the hadronic tagging algorithm provided by the Full Event Interpretation to fully reconstruct one $B$ meson. The second $B$ meson is reconstructed inclusively by selecting a high-momentum lepton. The $X_c$ system is identified by the remaining reconstructed tracks and clusters in the electromagnetic calorimeter. We report preliminary results for the hadronic mass moments $\langle M_X^n \rangle $ with $n=1,\dots,6$, measured as a function of a lower cut on the lepton momentum in the signal $B$ rest frame.△ Less"
Out of Nowhere: The emergence of spacetime from causal sets,"Authors:Christian Wüthrich,Nick Huggett","Abstract:…monograph ""Out of Nowhere: The Emergence of Spacetime in Quantum Theories of Gravity"", co-authored by Nick Huggett and Christian Wüthrich and under contract withOxfordUniversity Press. (More information at www.beyondspacetime.net.) This chapter sketches how spacetime emerges in causal set theory and demonstrates how this question is deeply entangle…▽ MoreThis is a chapter of the planned monograph ""Out of Nowhere: The Emergence of Spacetime in Quantum Theories of Gravity"", co-authored by Nick Huggett and Christian Wüthrich and under contract withOxfordUniversity Press. (More information at www.beyondspacetime.net.) This chapter sketches how spacetime emerges in causal set theory and demonstrates how this question is deeply entangled with genuinely philosophical concerns.△ Less"
Online Spatiotemporal Action Detection and Prediction via Causal Representations,Authors:Gurkirt Singh,"Abstract:In this thesis, we focus on video action understanding problems from an online and real-time processing point of view. We start with the conversion of the traditional offline spatiotemporal action detection pipeline into an online spatiotemporal action tube detection system. An action tube is a set of bounding connected over time, which bounds an action instance in space and time. Next, we explore…▽ MoreIn this thesis, we focus on video action understanding problems from an online and real-time processing point of view. We start with the conversion of the traditional offline spatiotemporal action detection pipeline into an online spatiotemporal action tube detection system. An action tube is a set of bounding connected over time, which bounds an action instance in space and time. Next, we explore the future prediction capabilities of such detection methods by extending an existing action tube into the future by regression. Later, we seek to establish that online/causal representations can achieve similar performance to that of offline three dimensional (3D) convolutional neural networks (CNNs) on various tasks, including action recognition, temporal action segmentation and early prediction.△ Less"
Learning Condition Invariant Features for Retrieval-Based Localization from 1M Images,"Authors:Janine Thoma,Danda Pani Paudel,Ajad Chhatkuli,Luc Van Gool","Abstract:…between different loss functions on large datasets. In this paper, we train and evaluate several localization methods on three different benchmark datasets, includingOxfordRobotCar with over one million images. This large scale evaluation yields valuable insights into the generalizability and performance of retrieval-based localization. Based on our findin…▽ MoreImage features for retrieval-based localization must be invariant to dynamic objects (e.g. cars) as well as seasonal and daytime changes. Such invariances are, up to some extent, learnable with existing methods using triplet-like losses, given a large number of diverse training images. However, due to the high algorithmic training complexity, there exists insufficient comparison between different loss functions on large datasets. In this paper, we train and evaluate several localization methods on three different benchmark datasets, includingOxfordRobotCar with over one million images. This large scale evaluation yields valuable insights into the generalizability and performance of retrieval-based localization. Based on our findings, we develop a novel method for learning more accurate and better generalizing localization features. It consists of two main contributions: (i) a feature volume-based loss function, and (ii) hard positive and pairwise negative mining. On the challengingOxfordRobotCar night condition, our method outperforms the well-known triplet loss by 24.4% in localization accuracy within 5m.△ Less"
Disappearing errors in a conversion model,Authors:David P. Fan,"Abstract:The same basic differential equation model has been adapted for time-dependent conversions of members of a population among different states. The conversion model has been applied in different contexts such as epidemiological infections, the Bass model for the diffusion of innovations, and the ideodynamic model for public opinion. For example, the ideodynamic version of the model predicts changes…▽ MoreThe same basic differential equation model has been adapted for time-dependent conversions of members of a population among different states. The conversion model has been applied in different contexts such as epidemiological infections, the Bass model for the diffusion of innovations, and the ideodynamic model for public opinion. For example, the ideodynamic version of the model predicts changes in public opinions in response to persuasive messages extending back to an indefinite past. All messages are measured with error, and this chapter discusses how errors in message measurements disappear with time so that predicted opinion values gradually become unaffected by past measurement errors. Prediction uncertainty is discussed using formal statistics, sensitivity analysis and bootstrap variance calculations. This chapter presents ideodynamic predictions for opinion time series about the Toyota car manufacturer calculated from daily Twitter scores over two and half years. During this time, there was a sudden onslaught of bad news for Toyota, and the model could accurately predict the accompanying drop in favorable Toyota opinion and rise in unfavorable opinion.△ Less"
DeepSOCIAL: Social Distancing Monitoring and Infection Risk Assessment in COVID-19 Pandemic,"Authors:Mahdi Rezaei,Mohsen Azarmi","Abstract:…comprehensive datasets by the time of the research the Microsoft Common Objects in Context (MS COCO) and Google Open Image datasets. The system has been evaluated against theOxfordTown Centre dataset with superior performance compared to three state-of-the-art methods. The evaluation has been conducted in challenging conditions, including occlusion, partia…▽ MoreSocial distancing is a recommended solution by the World Health Organisation (WHO) to minimise the spread of COVID-19 in public places. The majority of governments and national health authorities have set the 2-meter physical distancing as a mandatory safety measure in shopping centres, schools and other covered areas. In this research, we develop a hybrid Computer Vision and YOLOv4-based Deep Neural Network model for automated people detection in the crowd in indoor and outdoor environments using common CCTV security cameras. The proposed DNN model in combination with an adapted inverse perspective mapping (IPM) technique and SORT tracking algorithm leads to a robust people detection and social distancing monitoring. The model has been trained against two most comprehensive datasets by the time of the research the Microsoft Common Objects in Context (MS COCO) and Google Open Image datasets. The system has been evaluated against theOxfordTown Centre dataset with superior performance compared to three state-of-the-art methods. The evaluation has been conducted in challenging conditions, including occlusion, partial visibility, and under lighting variations with the mean average precision of 99.8% and the real-time speed of 24.1 fps. We also provide an online infection risk assessment scheme by statistical analysis of the Spatio-temporal data from people's moving trajectories and the rate of social distancing violations. The developed model is a generic and accurate people detection and tracking solution that can be applied in many other fields such as autonomous vehicles, human action recognition, anomaly detection, sports, crowd analysis, or any other research areas where the human detection is in the centre of attention.△ Less"
Measurement of the semileptonic $\bar{B}^0 \to D^{*+} \ell^{-} ν_{\ell}$ branching fraction with fully reconstructed $B$ meson decays and 34.6 fb$^{-1}$of Belle II data,"Authors:Belle II Collaboration,F. Abudinen,I. Adachi,R. Adak,K. Adamczyk,P. Ahlburg,J. K. Ahn,H. Aihara,N. Akopov,A. Aloisio,F. Ameli,K. Amirie,L. Andricek,N. Anh Ky,D. M. Asner,H. Atmacan,V. Aulchenko,T. Aushev,V. Aushev,T. Aziz,V. Babu,S. Bacher,S. Baehr,S. Bahinipati,A. M. Bakich, et al. (518 additional authors not shown)","Abstract:We present a first measurement of the $\bar{B^{0}} \rightarrow D^{*+} \ell^{-} ν_{\ell}$ branching fraction using fully reconstructed $B$ meson decays employing the Full Event Interpretation algorithm. Collision events corresponding to an integrated luminosity of \lumi are analyzed, which were recorded by the Belle~II detector operated at the SuperKEKB accelerator complex. We measure…▽ MoreWe present a first measurement of the $\bar{B^{0}} \rightarrow D^{*+} \ell^{-} ν_{\ell}$ branching fraction using fully reconstructed $B$ meson decays employing the Full Event Interpretation algorithm. Collision events corresponding to an integrated luminosity of \lumi are analyzed, which were recorded by the Belle~II detector operated at the SuperKEKB accelerator complex. We measure$\cal{B}(\bar{B^{0}} \rightarrow D^{*+} \ell^{-} ν_{\ell}) =4.51 \pm 0.41_{stat}\pm0.27_{syst} \pm0.45_{π_s}$, with the first and second error denoting the statistical and systematic uncertainty, respectively, and the third dominant uncertainty is from the slow pion reconstruction efficiency.△ Less"
Automatic sleep stage classification with deep residual networks in a mixed-cohort setting,"Authors:Alexander Neergaard Olesen,Poul Jennum,Emmanuel Mignot,Helge B D Sorensen","Abstract:Study Objectives: Sleep stage scoring is performed manually by sleep experts and is prone to subjective interpretation of scoring rules with low intra- and interscorer reliability. Many automatic systems rely on few small-scale databases for developing models, and generalizability to new datasets is thus unknown. We investigated a novel deep neural network to assess the generalizability of several…▽ MoreStudy Objectives: Sleep stage scoring is performed manually by sleep experts and is prone to subjective interpretation of scoring rules with low intra- and interscorer reliability. Many automatic systems rely on few small-scale databases for developing models, and generalizability to new datasets is thus unknown. We investigated a novel deep neural network to assess the generalizability of several large-scale cohorts.
  Methods: A deep neural network model was developed using 15684 polysomnography studies from five different cohorts. We applied four different scenarios: 1) impact of varying time-scales in the model; 2) performance of a single cohort on other cohorts of smaller, greater or equal size relative to the performance of other cohorts on a single cohort; 3) varying the fraction of mixed-cohort training data compared to using single-origin data; and 4) comparing models trained on combinations of data from 2, 3, and 4 cohorts.
  Results: Overall classification accuracy improved with increasing fractions of training data (0.25$\%$: 0.782 $\pm$ 0.097, 95$\%$ CI [0.777-0.787]; 100$\%$: 0.869 $\pm$ 0.064, 95$\%$ CI [0.864-0.872]), and with increasing number of data sources (2: 0.788 $\pm$ 0.102, 95$\%$ CI [0.787-0.790]; 3: 0.808 $\pm$ 0.092, 95$\%$ CI [0.807-0.810]; 4: 0.821 $\pm$ 0.085, 95$\%$ CI [0.819-0.823]). Different cohorts show varying levels of generalization to other cohorts.
  Conclusions: Automatic sleep stage scoring systems based on deep learning algorithms should consider as much data as possible from as many sources available to ensure proper generalization. Public datasets for benchmarking should be made available for future research.△ Less"
Exclusive $B^0 \to π^- \ell^+ ν_\ell$ Decays with Hadronic Full Event Interpretation Tagging in 34.6 fb$^{-1}$ of Belle II Data,"Authors:Belle II Collaboration,F. Abudinén,I. Adachi,R. Adak,K. Adamczyk,P. Ahlburg,J. K. Ahn,H. Aihara,N. Akopov,A. Aloisio,F. Ameli,L. Andricek,N. Anh Ky,D. M. Asner,H. Atmacan,V. Aulchenko,T. Aushev,V. Aushev,T. Aziz,V. Babu,S. Bacher,S. Baehr,S. Bahinipati,A. M. Bakich,P. Bambade, et al. (519 additional authors not shown)","Abstract:We present the results of the re-discovery of the decay $B^0 \to π^- \ell^+ ν_\ell$ in 34.6 fb$^{-1}$ of Belle II data using hadronic $B$-tagging via the Full Event Interpretation algorithm. We observe 21 signal events on a background of 155 in a fit to the distribution of the square of the missing mass, $M_{\mathrm{miss}}^2$, with a significance of 5.69$σ$, and determine a total branching fractio…▽ MoreWe present the results of the re-discovery of the decay $B^0 \to π^- \ell^+ ν_\ell$ in 34.6 fb$^{-1}$ of Belle II data using hadronic $B$-tagging via the Full Event Interpretation algorithm. We observe 21 signal events on a background of 155 in a fit to the distribution of the square of the missing mass, $M_{\mathrm{miss}}^2$, with a significance of 5.69$σ$, and determine a total branching fraction of (1.58 $\pm$ 0.43$_{\mathrm{stat}}$ $\pm$ 0.07$_{\mathrm{sys}}$) $\times 10^{-4}$.△ Less"
"COVID-19 Pandemic Severity, Lockdown Regimes, and People Mobility: Early Evidence from 88 Countries","Authors:Md. Mokhlesur Rahman,Jean-Claude Thill,Kamal Chandra Paul","Abstract:…socioeconomic and demographic characteristics of people, lockdown measures, and coronavirus pandemic were collected from multiple sources (e.g., Google, UNDP, UN, BBC,OxfordUniversity, Worldometer). A Structural Equation Modeling (SEM) technique is used to investigate the direct and indirect effects of independent variables on dependent variables consider…▽ MoreThis study empirically investigates the complex interplay between the severity of the coronavirus pandemic, mobility changes in retail and recreation, transit stations, workplaces, and residential areas, and lockdown measures in 88 countries of the word. To conduct the study, data on mobility patterns, socioeconomic and demographic characteristics of people, lockdown measures, and coronavirus pandemic were collected from multiple sources (e.g., Google, UNDP, UN, BBC,OxfordUniversity, Worldometer). A Structural Equation Modeling (SEM) technique is used to investigate the direct and indirect effects of independent variables on dependent variables considering the intervening effects of mediators. Results show that lockdown measures have significant effects to encourage people to maintain social distancing. However, pandemic severity and socioeconomic and institutional factors have limited effects to sustain social distancing practice. The results also explain that socioeconomic and institutional factors of urbanity and modernity have significant effects on pandemic severity. Countries with a higher number of elderly people, employment in the service sector, and higher globalization trend are the worst victims of the coronavirus pandemic (e.g., USA, UK, Italy, and Spain). Social distancing measures are reasonably effective at tempering the severity of the pandemic.△ Less"
LPOP: Challenges and Advances in Logic and Practice of Programming,"Authors:David S. Warren,Yanhong A. Liu","Abstract:This article describes the work presented at the first Logic and Practice of Programming (LPOP) Workshop, which was held inOxford, UK, on July 18, 2018, in conjunction with the Federated Logic Conference (FLoC) 2018. Its focus is challenges and advances in logic and practice of programming. The workshop was organized around a challenge problem that specifie…▽ MoreThis article describes the work presented at the first Logic and Practice of Programming (LPOP) Workshop, which was held inOxford, UK, on July 18, 2018, in conjunction with the Federated Logic Conference (FLoC) 2018. Its focus is challenges and advances in logic and practice of programming. The workshop was organized around a challenge problem that specifies issues in role-based access control (RBAC), with many participants proposing combined imperative and declarative solutions expressed in the languages of their choice.△ Less"
Studies of the semileptonic $\bar B^0\to D^{*+}\ell^-\barν_\ell$ and $B^-\to D^{0}\ell^-\barν_\ell$ decay processes with 34.6 fb$^{-1}$ of Belle II data,"Authors:Belle II Collaboration,F. Abudinén,I. Adachi,R. Adak,K. Adamczyk,P. Ahlburg,J. K. Ahn,H. Aihara,N. Akopov,A. Aloisio,F. Ameli,L. Andricek,N. Anh Ky,D. M. Asner,H. Atmacan,V. Aulchenko,T. Aushev,V. Aushev,T. Aziz,V. Babu,S. Bacher,S. Baehr,S. Bahinipati,A. M. Bakich,P. Bambade, et al. (520 additional authors not shown)","Abstract:We report measurements of the $\bar{B}^0 \to D^{*+} \ell^{-} \barν_l$ and $B^- \to D^{0} \ell^{-} \barν_l$ processes using 34.6 fb$^{-1}$ of collision events recorded by the Belle II experiment at the SuperKEKB asymmetric-energy $e^+ e^-$ collider. For the $B^-\to D^{0}\ell^-\barν_\ell$ channel, we present first studies that isolate this decay from other semileptonic processes and backgrounds. We…▽ MoreWe report measurements of the $\bar{B}^0 \to D^{*+} \ell^{-} \barν_l$ and $B^- \to D^{0} \ell^{-} \barν_l$ processes using 34.6 fb$^{-1}$ of collision events recorded by the Belle II experiment at the SuperKEKB asymmetric-energy $e^+ e^-$ collider. For the $B^-\to D^{0}\ell^-\barν_\ell$ channel, we present first studies that isolate this decay from other semileptonic processes and backgrounds. We report a measurement of the $\bar{B}^0 \to D^{*+} \ell^{-} \barν_l$ branching fraction and obtain ${\cal B}(\bar{B}^0 \to D^{*+} \ell^{-} \barν_l) = \left(4.60 \pm 0.05_{\mathrm{stat}}\pm0.17_{\mathrm{syst}} \pm 0.45_{π_s}\right) \%$, in agreement with the world average. Here, the uncertainties are statistical, systematic, and related to slow pion reconstruction, respectively. The systematic uncertainties are limited by the statistics of auxiliary measurements and will improve in the future. We also report differential branching fractions in five bins of the hadronic recoil parameter $w$ for $\bar{B}^0 \to D^{*+} \ell^{-} \barν_l$, unfolded to account for resolution and efficiency effects.△ Less"
DeepGIN: Deep Generative Inpainting Network for Extreme Image Inpainting,"Authors:Chu-Tak Li,Wan-Chi Siu,Zhi-Song Liu,Li-Wen Wang,Daniel Pak-Kong Lun","Abstract:…(BP) technique to enhance our inpainting results. Our DeepGIN outperforms the state-of-the-art approaches generally, including two publicly available datasets (FFHQ andOxfordBuildings), both quantitatively and qualitatively. We also demonstrate that our model is capable of completing masked images in the wild.▽ MoreThe degree of difficulty in image inpainting depends on the types and sizes of the missing parts. Existing image inpainting approaches usually encounter difficulties in completing the missing parts in the wild with pleasing visual and contextual results as they are trained for either dealing with one specific type of missing patterns (mask) or unilaterally assuming the shapes and/or sizes of the masked areas. We propose a deep generative inpainting network, named DeepGIN, to handle various types of masked images. We design a Spatial Pyramid Dilation (SPD) ResNet block to enable the use of distant features for reconstruction. We also employ Multi-Scale Self-Attention (MSSA) mechanism and Back Projection (BP) technique to enhance our inpainting results. Our DeepGIN outperforms the state-of-the-art approaches generally, including two publicly available datasets (FFHQ andOxfordBuildings), both quantitatively and qualitatively. We also demonstrate that our model is capable of completing masked images in the wild.△ Less"
A calibration of the Belle II hadronic tag-side reconstruction algorithm with $B \rightarrow X\ell ν$ decays,"Authors:Belle II Collaboration,F. Abudinén,I. Adachi,R. Adak,K. Adamczyk,P. Ahlburg,J. K. Ahn,H. Aihara,N. Akopov,A. Aloisio,F. Ameli,L. Andricek,N. Anh Ky,D. M. Asner,H. Atmacan,V. Aulchenko,T. Aushev,V. Aushev,T. Aziz,V. Babu,S. Bacher,S. Baehr,S. Bahinipati,A. M. Bakich,P. Bambade, et al. (518 additional authors not shown)","Abstract:Tag-side reconstruction is an important method for reconstructing $B$ meson decays with missing energy. The Belle II tag-side reconstruction algorithm, Full Event Interpretation, relies on a hierarchical reconstruction of $B$ meson decays with multivariate classification employed at each stage of reconstruction. Given the large numbers of classifiers employed and decay chains reconstructed, the pe…▽ MoreTag-side reconstruction is an important method for reconstructing $B$ meson decays with missing energy. The Belle II tag-side reconstruction algorithm, Full Event Interpretation, relies on a hierarchical reconstruction of $B$ meson decays with multivariate classification employed at each stage of reconstruction. Given the large numbers of classifiers employed and decay chains reconstructed, the performance of the algorithm on data and simulation differs significantly. Here, calibration factors are derived for hadronic tag-side $B$ decays by measuring a signal side decay, $B \rightarrow X\ell ν$, in $34.6$ fb$^{-1}$ of Belle II data. For a very loose selection on the tag-side $B$ multivariate classifier, the calibration factors are $0.65 \pm 0.02$ and $0.83 \pm 0.03$ for tag-side $B^{+}$ and $B^{0}$ mesons, respectively.△ Less"
$τ$ lepton mass measurement at Belle II,"Authors:Belle II Collaboration,F. Abudinén,I. Adachi,R. Adak,K. Adamczyk,P. Ahlburg,J. K. Ahn,H. Aihara,N. Akopov,A. Aloisio,F. Ameli,L. Andricek,N. Anh Ky,D. M. Asner,H. Atmacan,V. Aulchenko,T. Aushev,V. Aushev,T. Aziz,V. Babu,S. Bacher,S. Baehr,S. Bahinipati,A. M. Bakich,P. Bambade, et al. (517 additional authors not shown)","Abstract:The reconstruction of tau-pair production, $e^{+}e^{-} \to τ^{+}τ^{-}$, from the subsequent 3-prong ($τ^{+} \rightarrow π^{+} π^{-} π^{+} \barν_τ$) and 1-prong ($τ^{-} \to \ell^{-} \barν_{\ell} ν_τ$, $τ^{-} \to h^{-} ν_τ$ or $τ^{-} \to π^{-} π^0 ν_τ$) decays, is presented using 8.8 fb$^{-1}$ of $e^{+}e^{-}$ collision data of Belle II at the center-of-mass energy $\sqrt{s} = m_{Υ(4S)}$. The pseudom…▽ MoreThe reconstruction of tau-pair production, $e^{+}e^{-} \to τ^{+}τ^{-}$, from the subsequent 3-prong ($τ^{+} \rightarrow π^{+} π^{-} π^{+} \barν_τ$) and 1-prong ($τ^{-} \to \ell^{-} \barν_{\ell} ν_τ$, $τ^{-} \to h^{-} ν_τ$ or $τ^{-} \to π^{-} π^0 ν_τ$) decays, is presented using 8.8 fb$^{-1}$ of $e^{+}e^{-}$ collision data of Belle II at the center-of-mass energy $\sqrt{s} = m_{Υ(4S)}$. The pseudomass technique developed by the ARGUS experiment is used to measure the $τ$-lepton mass $m_τ$ in the 3-prong $τ^{+} \to π^{+} π^{-} π^{+} \barν_τ $ decay, resulting in $m_τ = 1777.28 \pm 0.75~{\rm (stat.)} \pm 0.33 ~{\rm (sys.)}~{\rm{MeV}/\rm{c}^2}$.△ Less"
Rediscovery of $B \to φK^{(*)}$ decays and measurement of the longitudinal polarization fraction $f_L$ in $B \to φK^{*}$ decays using the Summer 2020 Belle II dataset,"Authors:F. Abudinén,I. Adachi,R. Adak,K. Adamczyk,P. Ahlburg,J. K. Ahn,H. Aihara,N. Akopov,A. Aloisio,F. Ameli,L. Andricek,N. Anh Ky,D. M. Asner,H. Atmacan,V. Aulchenko,T. Aushev,V. Aushev,T. Aziz,V. Babu,S. Bacher,S. Baehr,S. Bahinipati,A. M. Bakich,P. Bambade,Sw. Banerjee, et al. (516 additional authors not shown)","Abstract:We utilize a sample of 34.6 fb$^{-1}$, collected by the Belle II experiment at the SuperKEKB asymmetric energy $e^+e^-$ collider, to search for the $B^+ \to φK^+$, $B^+ \to φK^{*+}$, $B^0 \to φK^0_S$, and $B^0 \to φK^{*0}$ decays. Charmless hadronic $B$ decays represent an important part of the Belle II physics program, and are an ideal benchmark to test the detector capabilities in terms of track…▽ MoreWe utilize a sample of 34.6 fb$^{-1}$, collected by the Belle II experiment at the SuperKEKB asymmetric energy $e^+e^-$ collider, to search for the $B^+ \to φK^+$, $B^+ \to φK^{*+}$, $B^0 \to φK^0_S$, and $B^0 \to φK^{*0}$ decays. Charmless hadronic $B$ decays represent an important part of the Belle II physics program, and are an ideal benchmark to test the detector capabilities in terms of tracking efficiency, charged particle identification, vertexing, and advanced analysis techniques. Each channel is observed with a significance that exceeds 5 standard deviations, and we obtain measurements of their branching ratios that are in good agreement with the world averages. For the $B \to φK^*$ modes, we also perform a measurement of the longitudinal polarization fraction $f_L$.△ Less"
First flavor tagging calibration using 2019 Belle II data,"Authors:Belle II Collaboration,F. Abudinén,I. Adachi,R. Adak,K. Adamczyk,P. Ahlburg,J. K. Ahn,H. Aihara,N. Akopov,A. Aloisio,F. Ameli,L. Andricek,N. Anh Ky,D. M. Asner,H. Atmacan,V. Aulchenko,T. Aushev,V. Aushev,T. Aziz,V. Babu,S. Bacher,S. Baehr,S. Bahinipati,A. M. Bakich,P. Bambade, et al. (518 additional authors not shown)","Abstract:We report on the first calibration of the standard Belle II $B$-flavor tagger using the full data set collected at the $Υ(4{\rm S})$ resonance in 2019 with the Belle II detector at the SuperKEKB collider, corresponding to 8.7 fb$^{-1}$ of integrated luminosity. The calibration is performed by reconstructing various hadronic charmed $B$-meson decays with flavor-specific final states. We use simulat…▽ MoreWe report on the first calibration of the standard Belle II $B$-flavor tagger using the full data set collected at the $Υ(4{\rm S})$ resonance in 2019 with the Belle II detector at the SuperKEKB collider, corresponding to 8.7 fb$^{-1}$ of integrated luminosity. The calibration is performed by reconstructing various hadronic charmed $B$-meson decays with flavor-specific final states. We use simulation to optimize our event selection criteria and to train the flavor tagging algorithm. We determine the tagging efficiency and the fraction of wrongly identified tag-side $B$~candidates from a measurement of the time-integrated $B^0-\overline{B}^0$ mixing probability. The total effective efficiency is measured to be $\varepsilon_{\rm eff} = \big(33.8 \pm 3.6(\text{stat}) \pm 1.6(\text{sys})\big)\%$, which is in good agreement with the predictions from simulation and comparable with the best one obtained by the Belle experiment. The results show a good understanding of the detector performance and offer a basis for future calibrations.△ Less"
Few shot domain adaptation for in situ macromolecule structural classification in cryo-electron tomograms,"Authors:Liangyong Yu,Ran Li,Xiangrui Zeng,Hongyi Wang,Jie Jin,Ge Yang,Rui Jiang,Min Xu","Abstract:Motivation: Cryo-Electron Tomography (cryo-ET) visualizes structure and spatial organization of macromolecules and their interactions with other subcellular components inside single cells in the close-to-native state at sub-molecular resolution. Such information is critical for the accurate understanding of cellular processes. However, subtomogram classification remains one of the major challenges…▽ MoreMotivation: Cryo-Electron Tomography (cryo-ET) visualizes structure and spatial organization of macromolecules and their interactions with other subcellular components inside single cells in the close-to-native state at sub-molecular resolution. Such information is critical for the accurate understanding of cellular processes. However, subtomogram classification remains one of the major challenges for the systematic recognition and recovery of the macromolecule structures in cryo-ET because of imaging limits and data quantity. Recently, deep learning has significantly improved the throughput and accuracy of large-scale subtomogram classification. However often it is difficult to get enough high-quality annotated subtomogram data for supervised training due to the enormous expense of labeling. To tackle this problem, it is beneficial to utilize another already annotated dataset to assist the training process. However, due to the discrepancy of image intensity distribution between source domain and target domain, the model trained on subtomograms in source domainmay perform poorly in predicting subtomogram classes in the target domain.
  Results: In this paper, we adapt a few shot domain adaptation method for deep learning based cross-domain subtomogram classification. The essential idea of our method consists of two parts: 1) take full advantage of the distribution of plentiful unlabeled target domain data, and 2) exploit the correlation between the whole source domain dataset and few labeled target domain data. Experiments conducted on simulated and real datasets show that our method achieves significant improvement on cross domain subtomogram classification compared with baseline methods.△ Less"
OpenSBLI: Automated code-generation for heterogeneous computing architectures applied to compressible fluid dynamics on structured grids,"Authors:David J. Lusher,Satya P. Jammy,Neil D. Sandham","Abstract:…by a choice of high-order Weighted Essentially Non-Oscillatory (WENO) or Targeted Essentially Non-Oscillatory (TENO) schemes. OpenSBLI generates a complete CFD solver in theOxfordParallel Structured (OPS) domain specific language. The OPS library is embedded in C code, enabling massively-parallel execution of the code on a variety of high-performance-compu…▽ MoreOpenSBLI is an open-source code-generation system for compressible fluid dynamics (CFD) on heterogeneous computing architectures. Written in Python, OpenSBLI is an explicit high-order finite-difference solver on structured curvilinear meshes. Shock-capturing is performed by a choice of high-order Weighted Essentially Non-Oscillatory (WENO) or Targeted Essentially Non-Oscillatory (TENO) schemes. OpenSBLI generates a complete CFD solver in theOxfordParallel Structured (OPS) domain specific language. The OPS library is embedded in C code, enabling massively-parallel execution of the code on a variety of high-performance-computing architectures, including GPUs. The present paper presents a code base that has been completely rewritten from the earlier proof of concept (Jacobs et al, JoCS 18 (2017), 12-23), allowing shock capturing, coordinate transformations for complex geometries, and a wide range of boundary conditions, including solid walls with and without heat transfer. A suite of validation and verification cases are presented, plus demonstration of a large-scale Direct Numerical Simulation (DNS) of a transitional Shockwave Boundary Layer Interaction (SBLI). The code is shown to have good weak and strong scaling on multi-GPU clusters. We demonstrate that code-generation and domain specific languages are suitable for performing efficient large-scale simulations of complex fluid flows on emerging computing architectures.△ Less"
Surjective homomorphisms from algebras of operators on long sequence spaces are automatically injective,"Authors:Bence Horváth,Tomasz Kania","Abstract:We study automatic injectivity of surjective algebra homomorphisms from $\mathscr{B}(X)$, the algebra of (bounded, linear) operators on $X$, to $\mathscr{B}(Y)$, where $X$ is one of the following \emph{long} sequence spaces: $c_0(λ)$, $\ell_{\infty}^c(λ)$, and $\ell_p(λ)$ ($1 \leqslant p < \infty$) and $Y$ is arbitrary. \textit{En route} to the proof that these spaces do indeed enjoy such a proper…▽ MoreWe study automatic injectivity of surjective algebra homomorphisms from $\mathscr{B}(X)$, the algebra of (bounded, linear) operators on $X$, to $\mathscr{B}(Y)$, where $X$ is one of the following \emph{long} sequence spaces: $c_0(λ)$, $\ell_{\infty}^c(λ)$, and $\ell_p(λ)$ ($1 \leqslant p < \infty$) and $Y$ is arbitrary. \textit{En route} to the proof that these spaces do indeed enjoy such a property, we classify two-sided ideals of the algebra of operators of any of the aforementioned Banach spaces that are closed with respect to the `sequential strong operator topology'.△ Less"
How Many Components? Quantifying the Complexity of the Metallicity Distribution in the Milky Way Bulge with APOGEE,"Authors:A. Rojas-Arriagada,G. Zasowski,M. Schultheis,M. Zoccali,S. Hasselquist,C. Chiappini,R. E. Cohen,K. Cunha,J. G. Fernández-Trincado,F. Fragkoudi,D. A. García-Hernández,D. Geisler,J. Lian,S. Majewski,D. Minniti,C. Nitschelm,A. B. A. Queiroz","Abstract:We use data of $\sim$13,000 stars from the SDSS/APOGEE survey to study the shape of the bulge MDF within the region $|\ell|\leq11^\circ$ and $|b|\leq13^\circ$, and spatially constrained to ${\rm R_{GC}\leq3.5}$ kpc. We apply Gaussian Mixture Modeling and Non-negative Matrix Factorization decomposition techniques to identify the optimal number and the properties of MDF components. We find the shape…▽ MoreWe use data of $\sim$13,000 stars from the SDSS/APOGEE survey to study the shape of the bulge MDF within the region $|\ell|\leq11^\circ$ and $|b|\leq13^\circ$, and spatially constrained to ${\rm R_{GC}\leq3.5}$ kpc. We apply Gaussian Mixture Modeling and Non-negative Matrix Factorization decomposition techniques to identify the optimal number and the properties of MDF components. We find the shape and spatial variations of the MDF (at ${\rm [Fe/H]\geq-1}$ dex) are well represented as a smoothly varying contribution of three overlapping components located at [Fe/H]=+$0.32$, $-0.17$ and $-0.66$ dex. The bimodal MDF found in previous studies is in agreement with our trimodal assessment once the limitations in sample size and individual measurement errors are taken into account. The shape of the MDF and its correlations with kinematics reveal different spatial distributions and kinematical structure for the three components co-existing in the bulge region. We confirm the consensus physical interpretation of metal-rich stars as associated with the secularly evolved disk into a boxy/peanut X-shape bar. On the other hand, metal-intermediate stars could be the product of in-situ formation at high redshift in a gas-rich environment characterized by violent and fast star formation. This interpretation would help to link a present-day structure with those observed in formation in the center of high redshift galaxies. Finally, metal-poor stars may correspond to the metal-rich tail of the population sampled at lower metallicity from the study of RR Lyrae stars. Conversely, they could be associated with the metal-poor tail of the early thick disc.△ Less"
Cascading Failures in Complex Networks,"Authors:Lucas D. Valdez,Louis Shekhtman,Cristian E. La Rocca,Xin Zhang,Sergey V. Buldyrev,Paul A. Trunfio,Lidia A. Braunstein,Shlomo Havlin","Abstract:Cascading failure is a potentially devastating process that spreads on real-world complex networks and can impact the integrity of wide-ranging infrastructures, natural systems, and societal cohesiveness. One of the essential features that create complex network vulnerability to failure propagation is the dependency among their components, exposing entire systems to significant risks from destabil…▽ MoreCascading failure is a potentially devastating process that spreads on real-world complex networks and can impact the integrity of wide-ranging infrastructures, natural systems, and societal cohesiveness. One of the essential features that create complex network vulnerability to failure propagation is the dependency among their components, exposing entire systems to significant risks from destabilizing hazards such as human attacks, natural disasters or internal breakdowns. Developing realistic models for cascading failures as well as strategies to halt and mitigate the failure propagation can point to new approaches to restoring and strengthening real-world networks. In this review, we summarize recent progress on models developed based on physics and complex network science to understand the mechanisms, dynamics and overall impact of cascading failures. We present models for cascading failures in single networks and interdependent networks and explain how different dynamic propagation mechanisms can lead to an abrupt collapse and a rich dynamic behavior. Finally, we close the review with novel emerging strategies for containing cascades of failures and discuss open questions that remain to be addressed.△ Less"
NSGANetV2: Evolutionary Multi-Objective Surrogate-Assisted Neural Architecture Search,"Authors:Zhichao Lu,Kalyanmoy Deb,Erik Goodman,Wolfgang Banzhaf,Vishnu Naresh Boddeti","Abstract:…more sample efficient. Furthermore, we demonstrate the effectiveness and versatility of the proposed method on six diverse non-standard datasets, e.g. STL-10, Flowers102,OxfordPets, FGVC Aircrafts etc. In all cases, NSGANetV2s improve the state-of-the-art (under mobile setting), suggesting that NAS can be a viable alternative to conventional transfer learn…▽ MoreIn this paper, we propose an efficient NAS algorithm for generating task-specific models that are competitive under multiple competing objectives. It comprises of two surrogates, one at the architecture level to improve sample efficiency and one at the weights level, through a supernet, to improve gradient descent training efficiency. On standard benchmark datasets (C10, C100, ImageNet), the resulting models, dubbed NSGANetV2, either match or outperform models from existing approaches with the search being orders of magnitude more sample efficient. Furthermore, we demonstrate the effectiveness and versatility of the proposed method on six diverse non-standard datasets, e.g. STL-10, Flowers102,OxfordPets, FGVC Aircrafts etc. In all cases, NSGANetV2s improve the state-of-the-art (under mobile setting), suggesting that NAS can be a viable alternative to conventional transfer learning approaches in handling diverse scenarios such as small-scale or fine-grained datasets. Code is available at https://github.com/mikelzc1990/nsganetv2△ Less"
Global Representation of the Conditional LATE Model: A Separability Result,"Authors:Yu-Chang Chen,Haitian Xie","Abstract:This paper studies the latent index representation of the conditional LATE model, making explicit the role of covariates in treatment selection. We find that if the directions of the monotonicity condition are the same across all values of the conditioning covariate, which is often assumed in the literature, then the treatment choice equation has to satisfy a separability condition between the ins…▽ MoreThis paper studies the latent index representation of the conditional LATE model, making explicit the role of covariates in treatment selection. We find that if the directions of the monotonicity condition are the same across all values of the conditioning covariate, which is often assumed in the literature, then the treatment choice equation has to satisfy a separability condition between the instrument and the covariate. This global representation result establishes testable restrictions imposed on the way covariates enter the treatment choice equation. We later extend the representation theorem to incorporate multiple ordered levels of treatment.△ Less"
Disentanglement of Color and Shape Representations for Continual Learning,"Authors:David Berga,Marc Masana,Joost Van de Weijer","Abstract:…perform explicit disentanglement of color and shape, by adjusting the network architecture. We tested classification accuracy and forgetting in a task-incremental setting withOxford-102 Flowers dataset. We combine our method with Elastic Weight Consolidation, Learning without Forgetting, Synaptic Intelligence and Memory Aware Synapses, and show that feature…▽ MoreWe hypothesize that disentangled feature representations suffer less from catastrophic forgetting. As a case study we perform explicit disentanglement of color and shape, by adjusting the network architecture. We tested classification accuracy and forgetting in a task-incremental setting withOxford-102 Flowers dataset. We combine our method with Elastic Weight Consolidation, Learning without Forgetting, Synaptic Intelligence and Memory Aware Synapses, and show that feature disentanglement positively impacts continual learning performance.△ Less"
Comprehensive assessment of error correction methods for high-throughput sequencing data,"Authors:Yun Heo,Gowthami Manikandan,Anand Ramachandran,Deming Chen","Abstract:…Ion Torrent, SOLiD sequencing etc. have brought about a quick and cheap way to sequence genomes. Recently, third generation sequencing (TGS) technologies like PacBio andOxfordNanopore Technology (ONT) have also been developed. Different technologies use different underlying methods for sequencing and are prone to different error rates. Though many tools e…▽ MoreThe advent of DNA and RNA sequencing has revolutionized the study of genomics and molecular biology. Next generation sequencing (NGS) technologies like Illumina, Ion Torrent, SOLiD sequencing etc. have brought about a quick and cheap way to sequence genomes. Recently, third generation sequencing (TGS) technologies like PacBio andOxfordNanopore Technology (ONT) have also been developed. Different technologies use different underlying methods for sequencing and are prone to different error rates. Though many tools exist for error correction of sequencing data from NGS and TGS methods, no standard method is available yet to evaluate the accuracy and effectiveness of these error-correction tools. In this study, we present a Software Package for Error Correction Tool Assessment on nuCLEic acid sequences (SPECTACLE) providing comprehensive algorithms to evaluate error-correction methods for DNA and RNA sequencing, for NGS and TGS platforms. We also present a compilation of sequencing datasets for Illumina, PacBio and ONT platforms that present challenging scenarios for error-correction tools. Using these datasets and SPECTACLE, we evaluate the performance of 23 different error-correction tools and present unique and helpful insights into their strengths and weaknesses. We hope that our methodology will standardize the evaluation of DNA and RNA error-correction tools in the future.△ Less"
Calabi-Yau Spaces in the String Landscape,Authors:Yang-Hui He,"Abstract:…the physics of the string landscape, have rendered the investigation of Calabi-Yau spaces one of the most exciting and inter-disciplinary fields.
  Invited contribution to theOxfordResearch Encyclopedia of Physics, B.~Foster Ed., OUP, 2020▽ MoreCalabi-Yau spaces, or Kahler spaces admitting zero Ricci curvature, have played a pivotal role in theoretical physics and pure mathematics for the last half-century. In physics, they constituted the first and natural solution to compactification of superstring theory to our 4-dimensional universe, primarily due to one of their equivalent definitions being the admittance of covariantly constant spinors. Since the mid-1980s, physicists and mathematicians have joined forces in creating explicit examples of Calabi-Yau spaces, compiling databases of formidable size, including the complete intersecion (CICY) dataset, the weighted hypersurfaces dataset, the elliptic-fibration dataset, the Kreuzer-Skarke toric hypersurface dataset, generalized CICYs etc., totaling at least on the order of 10^10 manifolds. These all contribute to the vast string landscape, the multitude of possible vacuum solutions to string compactification. More recently, this collaboration has been enriched by computer science and data science, the former, in bench-marking the complexity of the algorithms in computing geometric quantities and the latter, in applying techniques such as machine-learning in extracting unexpected information. These endeavours, inspired by the physics of the string landscape, have rendered the investigation of Calabi-Yau spaces one of the most exciting and inter-disciplinary fields.
  Invited contribution to theOxfordResearch Encyclopedia of Physics, B.~Foster Ed., OUP, 2020△ Less"
Access-based Intuitionistic Knowledge,Authors:Steffen Lewitzka,"Abstract:We introduce the concept of access-based intuitionistic knowledge which relies on the intuition that agent $i$ knows $\varphi$ if $i$ has found access to a proof of $\varphi$. Basic principles are distribution and factivity of knowledge as well as $\square\varphi\rightarrow K_i\varphi$ and $K_i(\varphi\veeψ) \rightarrow (K_i\varphi\vee K_iψ)$, where $\square\varphi$ reads `$\varphi$ is proved'. Th…▽ MoreWe introduce the concept of access-based intuitionistic knowledge which relies on the intuition that agent $i$ knows $\varphi$ if $i$ has found access to a proof of $\varphi$. Basic principles are distribution and factivity of knowledge as well as $\square\varphi\rightarrow K_i\varphi$ and $K_i(\varphi\veeψ) \rightarrow (K_i\varphi\vee K_iψ)$, where $\square\varphi$ reads `$\varphi$ is proved'. The formalization extends a family of classical modal logics designed in [Lewitzka 2015, 2017, 2019] as combinations of $IPC$ and $CPC$ and as systems for the reasoning about proof, i.e. intuitionistic truth. We adopt a formalization of common knowledge from [Lewitzka 2011] and interpret it here as access-based common knowledge. We compare our proposal with recent approaches to intuitionistic knowledge [Artemov and Protopopescu 2016; Lewitzka 2017, 2019] and bring together these different concepts in a unifying semantic framework based on Heyting algebra expansions.△ Less"
String-based methods for tonal harmony: A corpus study of Haydn's string quartets,Authors:David R. W. Sears,"Abstract:This chapter considers how string-based methods might be adapted to address music-analytic questions related to the discovery of musical organization, with particular attention devoted to the analysis of tonal harmony. I begin by applying the taxonomy of mental organization proposed by Mandler (1979) to the concept of musical organization. Using this taxonomy as a guide, I then present evidence fo…▽ MoreThis chapter considers how string-based methods might be adapted to address music-analytic questions related to the discovery of musical organization, with particular attention devoted to the analysis of tonal harmony. I begin by applying the taxonomy of mental organization proposed by Mandler (1979) to the concept of musical organization. Using this taxonomy as a guide, I then present evidence for three principles of tonal harmony -- recurrence, syntax, and recursion -- using a corpus of Haydn string quartets.△ Less"
Bombus Species Image Classification,"Authors:Venkat Margapuri,George Lavezzi,Robert Stewart,Dan Wagner","Abstract:…a taxonomic expert for proper categorization. We investigated whether an image classification system derived from transfer learning can do this task. We used Google Inception,OxfordVGG16 and VGG19 and Microsoft ResNet 50. We found Inception and VGG classifiers were able to make some progress at identifying bumble bee species from the available data, wherea…▽ MoreEntomologists, ecologists and others struggle to rapidly and accurately identify the species of bumble bees they encounter in their field work and research. The current process requires the bees to be mounted, then physically shipped to a taxonomic expert for proper categorization. We investigated whether an image classification system derived from transfer learning can do this task. We used Google Inception,OxfordVGG16 and VGG19 and Microsoft ResNet 50. We found Inception and VGG classifiers were able to make some progress at identifying bumble bee species from the available data, whereas ResNet was not. Individual classifiers achieved accuracies of up to 23% for single species identification and 44% top-3 labels, where a composite model performed better, 27% and 50%. We feel the performance was most hampered by our limited data set of 5,000-plus labeled images of 29 species, with individual species represented by 59 -315 images.△ Less"
Robot Perception enables Complex Navigation Behavior via Self-Supervised Learning,"Authors:Marvin Chancán,Michael Milford","Abstract:…self-supervision from a single image sequence - to enable complex goal-oriented navigation skills. We demonstrate our approach on two real-world driving dataset, KITTI andOxfordRobotCar, using the new interactive CityLearn framework. The results show that our method can accurately generalize to extreme environmental changes such as day to night cycles with…▽ MoreLearning visuomotor control policies in robotic systems is a fundamental problem when aiming for long-term behavioral autonomy. Recent supervised-learning-based vision and motion perception systems, however, are often separately built with limited capabilities, while being restricted to few behavioral skills such as passive visual odometry (VO) or mobile robot visual localization. Here we propose an approach to unify those successful robot perception systems for active target-driven navigation tasks via reinforcement learning (RL). Our method temporally incorporates compact motion and visual perception data - directly obtained using self-supervision from a single image sequence - to enable complex goal-oriented navigation skills. We demonstrate our approach on two real-world driving dataset, KITTI andOxfordRobotCar, using the new interactive CityLearn framework. The results show that our method can accurately generalize to extreme environmental changes such as day to night cycles with up to an 80% success rate, compared to 30% for a vision-only navigation systems.△ Less"
EDGE COVID-19: A Web Platform to generate submission-ready genomes for SARS-CoV-2 sequencing efforts,"Authors:Chien-Chi Lo,Migun Shakya,Karen Davenport,Mark Flynn,Adán Myers y Gutiérrez,Bin Hu,Po-E Li,Elais Player Jackson,Yan Xu,Patrick S. G. Chain","Abstract:…making. To address this need, we have developed a bioinformatic workflow to standardize the analysis of SARS-CoV-2 sequencing data generated with either the Illumina orOxfordNanopore platforms. Using an intuitive web-based interface, this workflow automates SARS-CoV-2 reference-based genome assembly, variant calling, lineage determination, and provides the…▽ MoreGenomics has become an essential technology for surveilling emerging infectious disease outbreaks. A wide range of technologies and strategies for pathogen genome enrichment and sequencing are being used by laboratories worldwide, together with different, and sometimes ad hoc, analytical procedures for generating genome sequences. As a result, public repositories now contain non-standard entries of varying quality. A standardized analytical process for consensus genome sequence determination, particularly for outbreaks such as the ongoing COVID-19 pandemic, is critical to provide a solid genomic basis for epidemiological analyses and well-informed decision making. To address this need, we have developed a bioinformatic workflow to standardize the analysis of SARS-CoV-2 sequencing data generated with either the Illumina orOxfordNanopore platforms. Using an intuitive web-based interface, this workflow automates SARS-CoV-2 reference-based genome assembly, variant calling, lineage determination, and provides the ability to submit the consensus sequence and necessary metadata to GenBank or GISAID. Given a raw Illumina orOxfordNanopore FASTQ read file, this web-based platform enables non-bioinformatics experts to automatically produce a SARS-CoV-2 genome that is ready for submission to GISAID or GenBank.
  Availability:https://edge-covid19.edgebioinformatics.org;https://github.com/LANL-Bioinformatics/EDGE/tree/SARS-CoV2△ Less"
FenceMask: A Data Augmentation Approach for Pre-extracted Image Features,"Authors:Pu Li,Xiangyang Li,Xiang Long","Abstract:…simulate object occlusion approaches. We tested it on CIFAR10, CIFAR100 and ImageNet datasets for Coarse-grained classification, COCO2017 and VisDrone datasets for detection,OxfordFlowers, Cornel Leaf and Stanford Dogs datasets for Fine-Grained Visual Categorization. Our method achieved significant performance improvement on Fine-Grained Visual Categorizat…▽ MoreWe propose a novel data augmentation method named 'FenceMask' that exhibits outstanding performance in various computer vision tasks. It is based on the 'simulation of object occlusion' strategy, which aim to achieve the balance between object occlusion and information retention of the input data. By enhancing the sparsity and regularity of the occlusion block, our augmentation method overcome the difficulty of small object augmentation and notably improve performance over baselines. Sufficient experiments prove the performance of our method is better than other simulate object occlusion approaches. We tested it on CIFAR10, CIFAR100 and ImageNet datasets for Coarse-grained classification, COCO2017 and VisDrone datasets for detection,OxfordFlowers, Cornel Leaf and Stanford Dogs datasets for Fine-Grained Visual Categorization. Our method achieved significant performance improvement on Fine-Grained Visual Categorization task and VisDrone dataset.△ Less"
Charmless $B$ decay reconstruction in 2019 Belle II data,"Authors:Belle II Collaboration,F. Abudinén,I. Adachi,R. Adak,K. Adamczyk,P. Ahlburg,J. K. Ahn,H. Aihara,N. Akopov,A. Aloisio,F. Ameli,L. Andricek,N. Anh Ky,D. M. Asner,H. Atmacan,V. Aulchenko,T. Aushev,V. Aushev,T. Aziz,V. Babu,S. Bacher,S. Baehr,S. Bahinipati,A. M. Bakich,P. Bambade, et al. (511 additional authors not shown)","Abstract:We report on the reconstruction of various charmless $B$ decays from electron-positron collisions at the energy corresponding to the $Υ(4S)$ resonance collected with the Belle II detector at the SuperKEKB collider. We use simulation to devise optimized event selections and apply them to the full data set collected in 2019, corresponding to 8.7\,fb$^{-1}$ of integrated luminosity. We fit the differ…▽ MoreWe report on the reconstruction of various charmless $B$ decays from electron-positron collisions at the energy corresponding to the $Υ(4S)$ resonance collected with the Belle II detector at the SuperKEKB collider. We use simulation to devise optimized event selections and apply them to the full data set collected in 2019, corresponding to 8.7\,fb$^{-1}$ of integrated luminosity. We fit the difference between half of the collision energy and the $B$ candidate energy (in the $Υ(4S)$ frame) for events restricted to a signal-rich range in beam-energy-constrained mass to search for charmless signals. Signal yields of approximately 80, 15, 20, 30, 90, and 160 decays are reconstructed for the channels $B^0 \to K^+π^-$, $B^0 \to π^+π^-$, $B^+ \to K^0_S(\to π^+π^-)π^+$, $B^+ \to K^+π^0(\to γγ)$, $B^+ \to K^+K^-K^+$, and $B^+ \to K^+π^-π^+$, respectively. Yields and background contaminations are compatible with those expected from simulation and comparable with those obtained by the Belle experiment. The results show a good understanding of the detector performance and offer a reliable basis to assess projections for future reach.△ Less"
Out of Nowhere: Duality,"Authors:Nick Huggett,Christian Wüthrich","Abstract:…monograph ""Out of Nowhere: The Emergence of Spacetime in Quantum Theories of Gravity"", co-authored by Nick Huggett and Christian Wüthrich and under contract withOxfordUniversity Press. (More information at www.beyondspacetime.net.) This chapter investigates the meaning and significance of string theoretic dualities, arguing that they reveal a surpr…▽ MoreThis is a chapter of the planned monograph ""Out of Nowhere: The Emergence of Spacetime in Quantum Theories of Gravity"", co-authored by Nick Huggett and Christian Wüthrich and under contract withOxfordUniversity Press. (More information at www.beyondspacetime.net.) This chapter investigates the meaning and significance of string theoretic dualities, arguing that they reveal a surprising physical indeterminateness to spacetime.△ Less"
SegAttnGAN: Text to Image Generation with Segmentation Attention,"Authors:Yuchuan Gou,Qiancheng Wu,Minghao Li,Bo Gong,Mei Han","Abstract:…better realism quality and higher quantitative measures compared with the previous state-of-art methods. We achieved Inception Score of 4.84 on the CUB dataset and 3.52 on theOxford-102 dataset. Besides, we tested the self-attention SegAttnGAN which uses generated segmentation data instead of masks from datasets for attention and achieved similar high-quali…▽ MoreIn this paper, we propose a novel generative network (SegAttnGAN) that utilizes additional segmentation information for the text-to-image synthesis task. As the segmentation data introduced to the model provides useful guidance on the generator training, the proposed model can generate images with better realism quality and higher quantitative measures compared with the previous state-of-art methods. We achieved Inception Score of 4.84 on the CUB dataset and 3.52 on theOxford-102 dataset. Besides, we tested the self-attention SegAttnGAN which uses generated segmentation data instead of masks from datasets for attention and achieved similar high-quality results, suggesting that our model can be adapted for the text-to-image synthesis task.△ Less"
Out of Nowhere: The 'emergence' of spacetime in string theory,"Authors:Nick Huggett,Christian Wüthrich","Abstract:…monograph ""Out of Nowhere: The Emergence of Spacetime in Quantum Theories of Gravity"", co-authored by Nick Huggett and Christian Wüthrich and under contract withOxfordUniversity Press. (More information at www.beyondspacetime.net.) This chapter analyses the nature and derivation of spacetime topology and geometry according to string theory.▽ MoreThis is a chapter of the planned monograph ""Out of Nowhere: The Emergence of Spacetime in Quantum Theories of Gravity"", co-authored by Nick Huggett and Christian Wüthrich and under contract withOxfordUniversity Press. (More information at www.beyondspacetime.net.) This chapter analyses the nature and derivation of spacetime topology and geometry according to string theory.△ Less"
Out of Nowhere: Spacetime from causality: causal set theory,"Authors:Christian Wüthrich,Nick Huggett","Abstract:…monograph ""Out of Nowhere: The Emergence of Spacetime in Quantum Theories of Gravity"", co-authored by Nick Huggett and Christian Wüthrich and under contract withOxfordUniversity Press. (More information at www.beyondspacetime.net.) This chapter introduces causal set theory and identifies and articulates a 'problem of space' in this theory.▽ MoreThis is a chapter of the planned monograph ""Out of Nowhere: The Emergence of Spacetime in Quantum Theories of Gravity"", co-authored by Nick Huggett and Christian Wüthrich and under contract withOxfordUniversity Press. (More information at www.beyondspacetime.net.) This chapter introduces causal set theory and identifies and articulates a 'problem of space' in this theory.△ Less"
On embedding Lambek calculus into commutative categorial grammars,Authors:Sergey Slavnov,"Abstract:We consider tensor grammars, which are an example of \commutative"" grammars, based on the classical (rather than intuitionistic) linear logic. They can be seen as a surface representation of abstract categorial grammars ACG in the sense that derivations of ACG translate to derivations of tensor grammars and this translation is isomorphic on the level of string languages. The basic ingredient are t…▽ MoreWe consider tensor grammars, which are an example of \commutative"" grammars, based on the classical (rather than intuitionistic) linear logic. They can be seen as a surface representation of abstract categorial grammars ACG in the sense that derivations of ACG translate to derivations of tensor grammars and this translation is isomorphic on the level of string languages. The basic ingredient are tensor terms, which can be seen as encoding and generalizing proof-nets. Using tensor terms makes the syntax extremely simple and a direct geometric meaning becomes transparent. Then we address the problem of encoding noncommutative operations in our setting. This turns out possible after enriching the system with new unary operators. The resulting system allows representing both ACG and Lambek grammars as conservative fragments, while the formalism remains, as it seems to us, rather simple and intuitive.△ Less"
"Hanburry Brown and Twiss, Hong Ou and Mandel effects and other landmarks in Quantum Optics: from photons to atoms",Authors:Alain Aspect,"Abstract:In this lecture, I first present my views on the second vs the first quantum revolution, then describe the Hanbury Brown and Twiss effect with photons, and indicate why it was so important in the development of modern quantum optics. The presentation of our experiments on the HBT effect with atoms will allow me to emphasize the analogies but also the increased richness of the effect when going fro…▽ MoreIn this lecture, I first present my views on the second vs the first quantum revolution, then describe the Hanbury Brown and Twiss effect with photons, and indicate why it was so important in the development of modern quantum optics. The presentation of our experiments on the HBT effect with atoms will allow me to emphasize the analogies but also the increased richness of the effect when going from photons to atoms. I will similarly describe the HOM effect for photons and its significance, and then present the analogous experiment with atoms. In conclusion, I will put these two effects in the long list of landmarks in the development of quantum optics, and indicate what has been done and what remains to be done with atoms in lieu of photons.△ Less"
Measurement of the $B^0$ lifetime using fully reconstructed hadronic decays in the 2019 Belle II dataset,"Authors:Belle II Collaboration,F. Abudinén,I. Adachi,R. Adak,K. Adamczyk,P. Ahlburg,J. K. Ahn,H. Aihara,N. Akopov,A. Aloisio,F. Ameli,L. Andricek,N. Anh Ky,D. M. Asner,H. Atmacan,V. Aulchenko,T. Aushev,V. Aushev,T. Aziz,V. Babu,S. Bacher,S. Baehr,S. Bahinipati,A. M. Bakich,P. Bambade, et al. (512 additional authors not shown)","Abstract:This document presents the measurement of $B^0$ meson lifetimes using the 2019 Belle II dataset that corresponds to an integrated luminosity of $8.7 \pm 0.2$ fb$^{-1}$. Each candidate is fully reconstructed with hadronic decay final states on the signal side, while the rest-of-event technique allows to infer the decay vertex position on the other (tag) side. $B^0$ lifetime is extracted from an unb…▽ MoreThis document presents the measurement of $B^0$ meson lifetimes using the 2019 Belle II dataset that corresponds to an integrated luminosity of $8.7 \pm 0.2$ fb$^{-1}$. Each candidate is fully reconstructed with hadronic decay final states on the signal side, while the rest-of-event technique allows to infer the decay vertex position on the other (tag) side. $B^0$ lifetime is extracted from an unbinned maximum likelihood fit to the distribution of the difference between the signal side $B^0$ candidate and the tag side decay times. The measured lifetime is $τ_{B^0} = 1.48 \pm 0.28 \pm 0.06 $ ps, where the first uncertainty is statistical and the second is systematic.△ Less"
S2IGAN: Speech-to-Image Generation via Adversarial Learning,"Authors:Xinsheng Wang,Tingting Qiao,Jihua Zhu,Alan Hanjalic,Odette Scharenborg","Abstract:…the proposed RDG synthesizes images that are semantically consistent with the corresponding speech descriptions. Extensive experiments on two public benchmark datasets CUB andOxford-102 demonstrate the effectiveness of the proposed S2IGAN on synthesizing high-quality and semantically-consistent images from the speech signal, yielding a good performance and…▽ MoreAn estimated half of the world's languages do not have a written form, making it impossible for these languages to benefit from any existing text-based technologies. In this paper, a speech-to-image generation (S2IG) framework is proposed which translates speech descriptions to photo-realistic images without using any text information, thus allowing unwritten languages to potentially benefit from this technology. The proposed S2IG framework, named S2IGAN, consists of a speech embedding network (SEN) and a relation-supervised densely-stacked generative model (RDG). SEN learns the speech embedding with the supervision of the corresponding visual information. Conditioned on the speech embedding produced by SEN, the proposed RDG synthesizes images that are semantically consistent with the corresponding speech descriptions. Extensive experiments on two public benchmark datasets CUB andOxford-102 demonstrate the effectiveness of the proposed S2IGAN on synthesizing high-quality and semantically-consistent images from the speech signal, yielding a good performance and a solid baseline for the S2IG task.△ Less"
The Information & Mutual Information Ratio for Counting Image Features and Their Matches,"Authors:Ali Khajegili Mirabadi,Stefano Rini","Abstract:…measures, are discussed. Finally, the effectiveness of these features is tested through feature extraction over INRIA Copydays datasets and feature matching over theOxfordsAffine Covariant Regions. These numerical evaluations validate the relevance of the IR and MIR in practical computer vision tasks▽ MoreFeature extraction and description is an important topic of computer vision, as it is the starting point of a number of tasks such as image reconstruction, stitching, registration, and recognition among many others. In this paper, two new image features are proposed: the Information Ratio (IR) and the Mutual Information Ratio (MIR). The IR is a feature of a single image, while the MIR describes features common across two or more images.We begin by introducing the IR and the MIR and motivate these features in an information theoretical context as the ratio of the self-information of an intensity level over the information contained over the pixels of the same intensity. Notably, the relationship of the IR and MIR with the image entropy and mutual information, classic information measures, are discussed. Finally, the effectiveness of these features is tested through feature extraction over INRIA Copydays datasets and feature matching over theOxfordsAffine Covariant Regions. These numerical evaluations validate the relevance of the IR and MIR in practical computer vision tasks△ Less"
Do We Need Fully Connected Output Layers in Convolutional Networks?,"Authors:Zhongchao Qian,Tyler L. Hayes,Kushal Kafle,Christopher Kanan","Abstract:…We are able to achieve comparable performance to a traditionally learned fully connected classification output layer on the ImageNet-1K, CIFAR-100, Stanford Cars-196, andOxfordFlowers-102 datasets, while not having a fully connected output layer at all.▽ MoreTraditionally, deep convolutional neural networks consist of a series of convolutional and pooling layers followed by one or more fully connected (FC) layers to perform the final classification. While this design has been successful, for datasets with a large number of categories, the fully connected layers often account for a large percentage of the network's parameters. For applications with memory constraints, such as mobile devices and embedded platforms, this is not ideal. Recently, a family of architectures that involve replacing the learned fully connected output layer with a fixed layer has been proposed as a way to achieve better efficiency. In this paper we examine this idea further and demonstrate that fixed classifiers offer no additional benefit compared to simply removing the output layer along with its parameters. We further demonstrate that the typical approach of having a fully connected final output layer is inefficient in terms of parameter count. We are able to achieve comparable performance to a traditionally learned fully connected classification output layer on the ImageNet-1K, CIFAR-100, Stanford Cars-196, andOxfordFlowers-102 datasets, while not having a fully connected output layer at all.△ Less"
Environmental Economics and Uncertainty: Review and a Machine Learning Outlook,"Authors:Ruda Zhang,Patrick Wingo,Rodrigo Duran,Kelly Rose,Jennifer Bauer,Roger Ghanem","Abstract:Economic assessment in environmental science concerns the measurement or valuation of environmental impacts, adaptation, and vulnerability. Integrated assessment modeling is a unifying framework of environmental economics, which attempts to combine key elements of physical, ecological, and socioeconomic systems. Uncertainty characterization in integrated assessment varies by component models: unce…▽ MoreEconomic assessment in environmental science concerns the measurement or valuation of environmental impacts, adaptation, and vulnerability. Integrated assessment modeling is a unifying framework of environmental economics, which attempts to combine key elements of physical, ecological, and socioeconomic systems. Uncertainty characterization in integrated assessment varies by component models: uncertainties associated with mechanistic physical models are often assessed with an ensemble of simulations or Monte Carlo sampling, while uncertainties associated with impact models are evaluated by conjecture or econometric analysis. Manifold sampling is a machine learning technique that constructs a joint probability model of all relevant variables which may be concentrated on a low-dimensional geometric structure. Compared with traditional density estimation methods, manifold sampling is more efficient especially when the data is generated by a few latent variables. The manifold-constrained joint probability model helps answer policy-making questions from prediction, to response, and prevention. Manifold sampling is applied to assess risk of offshore drilling in the Gulf of Mexico.△ Less"
Measurement of the branching fraction ${\cal B}(\bar B^0\to D^{*+}\ell^-\barν_\ell)$ with early Belle II data,"Authors:Belle II Collaboration,F. Abudinén,I. Adachi,R. Adak,K. Adamczyk,P. Ahlburg,J. K. Ahn,H. Aihara,N. Akopov,A. Aloisio,F. Ameli,L. Andricek,N. Anh Ky,D. M. Asner,H. Atmacan,V. Aulchenko,T. Aushev,V. Aushev,T. Aziz,V. Babu,S. Bacher,S. Baehr,S. Bahinipati,A. M. Bakich,P. Bambade, et al. (511 additional authors not shown)","Abstract:We measure the branching fractions for the decays $\bar{B}^0 \to D^{*+} e^{-} \barν_e$ and $\bar{B}^0 \to D^{*+} μ^{-} \barν_μ$ using $8.70 \pm 0.09~{\rm fb}^{-1}$ of data collected by the Belle II experiment at the SuperKEKB asymmetric-energy $e^+ e^-$ collider. Candidate signal decays are reconstructed with the subsequent decays $D^{*+}\to D^0 π^+$ and $D^0\to K^-π^+$. We obtain the results…▽ MoreWe measure the branching fractions for the decays $\bar{B}^0 \to D^{*+} e^{-} \barν_e$ and $\bar{B}^0 \to D^{*+} μ^{-} \barν_μ$ using $8.70 \pm 0.09~{\rm fb}^{-1}$ of data collected by the Belle II experiment at the SuperKEKB asymmetric-energy $e^+ e^-$ collider. Candidate signal decays are reconstructed with the subsequent decays $D^{*+}\to D^0 π^+$ and $D^0\to K^-π^+$. We obtain the results ${\cal B}(\bar{B}^0 \to D^{*+} e^{-} \barν_e)$ = $(4.55\pm0.14(\mathrm{stat})\pm0.35 (\mathrm{syst})) \%$ and ${\cal B}(\bar{B}^0 \to D^{*+} μ^{-} \barν_μ)$ = $(4.84\pm 0.13(\mathrm{stat})\pm0.37(\mathrm{syst})) \%$, in agreement with the world averages. The measurements serve to validate the full chain of detector operation and calibration, data collection and processing, and production of physics results in the case of semileptonic $B$-meson decays.△ Less"
The turbulent dynamics of Jupiter's and Saturn's weather layers: order out of chaos?,"Authors:Peter L Read,Roland M B Young,Daniel Kennedy","Abstract:…condensates, as well as by upwelling heat from their planetary interiors. They are also the most accessible regions of those planets to direct observations. Recent analyses inOxfordof cloud-tracked winds on Jupiter have demonstrated that kinetic energy is injected into the weather layer at scales comparable to the Rossby radius of deformation and cascades…▽ MoreThe weather layers of the gas giant planets, Jupiter and Saturn, comprise the shallow atmospheric layers that are influenced energetically by a combination of incoming solar radiation and localised latent heating of condensates, as well as by upwelling heat from their planetary interiors. They are also the most accessible regions of those planets to direct observations. Recent analyses inOxfordof cloud-tracked winds on Jupiter have demonstrated that kinetic energy is injected into the weather layer at scales comparable to the Rossby radius of deformation and cascades both upscale, mostly into the extra-tropical zonal jets, and downscale to the smallest resolvable scales in Cassini images. The large-scale flow on both Jupiter and Saturn appears to equilibrate towards a state which is close to marginal instability according to Arnol'd's 2nd stability theorem. This scenario is largely reproduced in a hierarchy of numerical models of giant planet weather layers, including relatively realistic models which seek to predict thermal and dynamical structures using a full set of parameterisations of radiative transfer, interior heat sources and even moist convection. Such models include the Jason GCM, developed inOxford, which also represents the formation of (energetically passive) clouds of NH3, NH4SH and H2O condensates and the transport of condensable tracers. Recent results show some promise in comparison with observations from the Cassini and Juno missions, but some observed features (such as Jupiter's Great Red Spot and other compact ovals) are not yet captured spontaneously by any weather layer model. We review recent work in this vein and discuss a number of open questions for future study.△ Less"
TResNet: High Performance GPU-Dedicated Architecture,"Authors:Tal Ridnik,Hussam Lawen,Asaf Noy,Emanuel Ben Baruch,Gilad Sharir,Itamar Friedman","Abstract:…transfer well and achieve state-of-the-art accuracy on competitive single-label classification datasets such as Stanford cars (96.0%), CIFAR-10 (99.0%), CIFAR-100 (91.5%) andOxford-Flowers (99.1%). They also perform well on multi-label classification and object detection tasks. Implementation is available at: https://github.com/mrT23/TResNet.▽ MoreMany deep learning models, developed in recent years, reach higher ImageNet accuracy than ResNet50, with fewer or comparable FLOPS count. While FLOPs are often seen as a proxy for network efficiency, when measuring actual GPU training and inference throughput, vanilla ResNet50 is usually significantly faster than its recent competitors, offering better throughput-accuracy trade-off.
  In this work, we introduce a series of architecture modifications that aim to boost neural networks' accuracy, while retaining their GPU training and inference efficiency. We first demonstrate and discuss the bottlenecks induced by FLOPs-optimizations. We then suggest alternative designs that better utilize GPU structure and assets. Finally, we introduce a new family of GPU-dedicated models, called TResNet, which achieve better accuracy and efficiency than previous ConvNets.
  Using a TResNet model, with similar GPU throughput to ResNet50, we reach 80.8 top-1 accuracy on ImageNet. Our TResNet models also transfer well and achieve state-of-the-art accuracy on competitive single-label classification datasets such as Stanford cars (96.0%), CIFAR-10 (99.0%), CIFAR-100 (91.5%) andOxford-Flowers (99.1%). They also perform well on multi-label classification and object detection tasks. Implementation is available at: https://github.com/mrT23/TResNet.△ Less"
Ramsey's contributions to probability and legal theory,Authors:F. E. Guerra-Pujol,"Abstract:Review of Cheryl Misak, Frank Ramsey: A Sheer Excess of Powers (OxfordUniversity Press, 2020).Review of Cheryl Misak, Frank Ramsey: A Sheer Excess of Powers (OxfordUniversity Press, 2020).△ Less"
ClusterVO: Clustering Moving Instances and Estimating Visual Odometry for Self and Surroundings,"Authors:Jiahui Huang,Sheng Yang,Tai-Jiang Mu,Shi-Min Hu","Abstract:…cluster segmentations online for every frame. The poses of camera and dynamic objects are instantly solved through a sliding-window optimization. Our system is evaluated onOxfordMultimotion and KITTI dataset both quantitatively and qualitatively, reaching comparable results to state-of-the-art solutions on both odometry and dynamic trajectory recovery.▽ MoreWe present ClusterVO, a stereo Visual Odometry which simultaneously clusters and estimates the motion of both ego and surrounding rigid clusters/objects. Unlike previous solutions relying on batch input or imposing priors on scene structure or dynamic object models, ClusterVO is online, general and thus can be used in various scenarios including indoor scene understanding and autonomous driving. At the core of our system lies a multi-level probabilistic association mechanism and a heterogeneous Conditional Random Field (CRF) clustering approach combining semantic, spatial and motion information to jointly infer cluster segmentations online for every frame. The poses of camera and dynamic objects are instantly solved through a sliding-window optimization. Our system is evaluated onOxfordMultimotion and KITTI dataset both quantitatively and qualitatively, reaching comparable results to state-of-the-art solutions on both odometry and dynamic trajectory recovery.△ Less"
Unsupervised Word Polysemy Quantification with Multiresolution Grids of Contextual Embeddings,"Authors:Christos Xypolopoulos,Antoine J. -P. Tixier,Michalis Vazirgiannis","Abstract:…our rankings are well correlated (with strong statistical significance) with 6 different rankings derived from famous human-constructed resources such as WordNet, OntoNotes,Oxford, Wikipedia etc., for 6 different standard metrics. We also visualize and analyze the correlation between the human rankings. A valuable by-product of our method is the ability to…▽ MoreThe number of senses of a given word, or polysemy, is a very subjective notion, which varies widely across annotators and resources. We propose a novel method to estimate polysemy, based on simple geometry in the contextual embedding space. Our approach is fully unsupervised and purely data-driven. We show through rigorous experiments that our rankings are well correlated (with strong statistical significance) with 6 different rankings derived from famous human-constructed resources such as WordNet, OntoNotes,Oxford, Wikipedia etc., for 6 different standard metrics. We also visualize and analyze the correlation between the human rankings. A valuable by-product of our method is the ability to sample, at no extra cost, sentences containing different senses of a given word. Finally, the fully unsupervised nature of our method makes it applicable to any language.
  Code and data are publicly available at https://github.com/ksipos/polysemy-assessment .
  The paper was accepted as a long paper at EACL 2021.△ Less"
"The Newer College Dataset: Handheld LiDAR, Inertial and Vision with Ground Truth","Authors:Milad Ramezani,Yiduo Wang,Marco Camurri,David Wisth,Matias Mattamala,Maurice Fallon","Abstract:…we present a large dataset with a variety of mobile mapping sensors collected using a handheld device carried at typical walking speeds for nearly 2.2 km through New College,Oxford. The dataset includes data from two commercially available devices - a stereoscopic-inertial camera and a multi-beam 3D LiDAR, which also provides inertial measurements. Addition…▽ MoreIn this paper we present a large dataset with a variety of mobile mapping sensors collected using a handheld device carried at typical walking speeds for nearly 2.2 km through New College,Oxford. The dataset includes data from two commercially available devices - a stereoscopic-inertial camera and a multi-beam 3D LiDAR, which also provides inertial measurements. Additionally, we used a tripod-mounted survey grade LiDAR scanner to capture a detailed millimeter-accurate 3D map of the test location (containing $\sim$290 million points). Using the map we inferred centimeter-accurate 6 Degree of Freedom (DoF) ground truth for the position of the device for each LiDAR scan to enable better evaluation of LiDAR and vision localisation, mapping and reconstruction systems. This ground truth is the particular novel contribution of this dataset and we believe that it will enable systematic evaluation which many similar datasets have lacked. The dataset combines both built environments, open spaces and vegetated areas so as to test localization and mapping systems such as vision-based navigation, visual and LiDAR SLAM, 3D LIDAR reconstruction and appearance-based place recognition. The dataset is available at: ori.ox.ac.uk/datasets/newer-college-dataset△ Less"
LiDAR Lateral Localisation Despite Challenging Occlusion from Traffic,"Authors:Tarlan Suleymanov,Matthew Gadd,Lars Kunze,Paul Newman","Abstract:…We show an example application in which fusion of a camera stream is used to initialise the lateral localisation. We demonstrate over four driven forays through centralOxford- totalling 40 km of driving - a gain in performance that inferring of occluded road boundaries brings.▽ MoreThis paper presents a system for improving the robustness of LiDAR lateral localisation systems. This is made possible by including detections of road boundaries which are invisible to the sensor (due to occlusion, e.g. traffic) but can be located by our Occluded Road Boundary Inference Deep Neural Network. We show an example application in which fusion of a camera stream is used to initialise the lateral localisation. We demonstrate over four driven forays through centralOxford- totalling 40 km of driving - a gain in performance that inferring of occluded road boundaries brings.△ Less"
"Quantization Conditions, 1900-1927","Authors:Anthony Duncan,Michel Janssen",Abstract:We trace the evolution of quantization conditions from Planck's introduction of a new fundamental constant (h) in his treatment of blackbody radiation in 1900 to Heisenberg's interpretation of the commutation relations of modern quantum mechanics in terms of his uncertainty principle in 1927.We trace the evolution of quantization conditions from Planck's introduction of a new fundamental constant (h) in his treatment of blackbody radiation in 1900 to Heisenberg's interpretation of the commutation relations of modern quantum mechanics in terms of his uncertainty principle in 1927.△ Less
Rectifying Pseudo Label Learning via Uncertainty Estimation for Domain Adaptive Semantic Segmentation,"Authors:Zhedong Zheng,Yi Yang","Abstract:…synthetic-to-real semantic segmentation benchmarks, i.e., GTA5 -> Cityscapes and SYNTHIA -> Cityscapes, as well as one cross-city benchmark, i.e., Cityscapes ->OxfordRobotCar. We demonstrate through extensive experiments that the proposed approach (1) dynamically sets different confidence thresholds according to the prediction variance, (2) rectif…▽ MoreThis paper focuses on the unsupervised domain adaptation of transferring the knowledge from the source domain to the target domain in the context of semantic segmentation. Existing approaches usually regard the pseudo label as the ground truth to fully exploit the unlabeled target-domain data. Yet the pseudo labels of the target-domain data are usually predicted by the model trained on the source domain. Thus, the generated labels inevitably contain the incorrect prediction due to the discrepancy between the training domain and the test domain, which could be transferred to the final adapted model and largely compromises the training process. To overcome the problem, this paper proposes to explicitly estimate the prediction uncertainty during training to rectify the pseudo label learning for unsupervised semantic segmentation adaptation. Given the input image, the model outputs the semantic segmentation prediction as well as the uncertainty of the prediction. Specifically, we model the uncertainty via the prediction variance and involve the uncertainty into the optimization objective. To verify the effectiveness of the proposed method, we evaluate the proposed method on two prevalent synthetic-to-real semantic segmentation benchmarks, i.e., GTA5 -> Cityscapes and SYNTHIA -> Cityscapes, as well as one cross-city benchmark, i.e., Cityscapes ->OxfordRobotCar. We demonstrate through extensive experiments that the proposed approach (1) dynamically sets different confidence thresholds according to the prediction variance, (2) rectifies the learning from noisy pseudo labels, and (3) achieves significant improvements over the conventional pseudo label learning and yields competitive performance on all three benchmarks.△ Less"
PointLoc: Deep Pose Regressor for LiDAR Point Cloud Localization,"Authors:Wei Wang,Bing Wang,Peijun Zhao,Changhao Chen,Ronald Clark,Bo Yang,Andrew Markham,Niki Trigoni","Abstract:…a novel PointNet-style architecture with self-attention to efficiently estimate 6-DoF poses from 360° LiDAR input frames.Extensive experiments on recently released challengingOxfordRadar RobotCar dataset and real-world robot experiments demonstrate that the proposedmethod can achieve accurate relocalization performance.▽ MoreIn this paper, we present a novel end-to-end learning-based LiDAR relocalization framework, termed PointLoc, which infers 6-DoF poses directly using only a single point cloud as input, without requiring a pre-built map. Compared to RGB image-based relocalization, LiDAR frames can provide rich and robust geometric information about a scene. However, LiDAR point clouds are unordered and unstructured making it difficult to apply traditional deep learning regression models for this task. We address this issue by proposing a novel PointNet-style architecture with self-attention to efficiently estimate 6-DoF poses from 360° LiDAR input frames.Extensive experiments on recently released challengingOxfordRadar RobotCar dataset and real-world robot experiments demonstrate that the proposedmethod can achieve accurate relocalization performance.△ Less"
MVP: Unified Motion and Visual Self-Supervised Learning for Large-Scale Robotic Navigation,"Authors:Marvin Chancán,Michael Milford","Abstract:…to those from VO or optimized radar odometry (RO), to efficiently learn self-supervised navigation policies via RL. We evaluate our method on two large real-world datasets,OxfordRobotcar and Nordland Railway, over a range of weather (e.g. overcast, night, snow, sun, rain, clouds) and seasonal (e.g. winter, spring, fall, summer) conditions using the new Cit…▽ MoreAutonomous navigation emerges from both motion and local visual perception in real-world environments. However, most successful robotic motion estimation methods (e.g. VO, SLAM, SfM) and vision systems (e.g. CNN, visual place recognition-VPR) are often separately used for mapping and localization tasks. Conversely, recent reinforcement learning (RL) based methods for visual navigation rely on the quality of GPS data reception, which may not be reliable when directly using it as ground truth across multiple, month-spaced traversals in large environments. In this paper, we propose a novel motion and visual perception approach, dubbed MVP, that unifies these two sensor modalities for large-scale, target-driven navigation tasks. Our MVP-based method can learn faster, and is more accurate and robust to both extreme environmental changes and poor GPS data than corresponding vision-only navigation methods. MVP temporally incorporates compact image representations, obtained using VPR, with optimized motion estimation data, including but not limited to those from VO or optimized radar odometry (RO), to efficiently learn self-supervised navigation policies via RL. We evaluate our method on two large real-world datasets,OxfordRobotcar and Nordland Railway, over a range of weather (e.g. overcast, night, snow, sun, rain, clouds) and seasonal (e.g. winter, spring, fall, summer) conditions using the new CityLearn framework; an interactive environment for efficiently training navigation agents. Our experimental results, on traversals of theOxfordRobotCar dataset with no GPS data, show that MVP can achieve 53% and 93% navigation success rate using VO and RO, respectively, compared to 7% for a vision-only method. We additionally report a trade-off between the RL success rate and the motion estimation precision.△ Less"
Augmenting Visual Place Recognition with Structural Cues,"Authors:Amadeus Oertel,Titus Cieslewski,Davide Scaramuzza","Abstract:…point cloud. We evaluate different methods for fusing the 2D and 3D features and obtain best performance with global average pooling and simple concatenation. On theOxfordRobotCar dataset, the resulting descriptor exhibits superior recognition performance compared to descriptors extracted from only one of the input modalities, including state-of-the-art im…▽ MoreIn this paper, we propose to augment image-based place recognition with structural cues. Specifically, these structural cues are obtained using structure-from-motion, such that no additional sensors are needed for place recognition. This is achieved by augmenting the 2D convolutional neural network (CNN) typically used for image-based place recognition with a 3D CNN that takes as input a voxel grid derived from the structure-from-motion point cloud. We evaluate different methods for fusing the 2D and 3D features and obtain best performance with global average pooling and simple concatenation. On theOxfordRobotCar dataset, the resulting descriptor exhibits superior recognition performance compared to descriptors extracted from only one of the input modalities, including state-of-the-art image-based descriptors. Especially at low descriptor dimensionalities, we outperform state-of-the-art descriptors by up to 90%.△ Less"
Freeze the Discriminator: a Simple Baseline for Fine-Tuning GANs,"Authors:Sangwoo Mo,Minsu Cho,Jinwoo Shin","Abstract:…both unconditional and conditional GANs. We demonstrate the consistent effect using StyleGAN and SNGAN-projection architectures on several datasets of Animal Face, Anime Face,OxfordFlower, CUB-200-2011, and Caltech-256 datasets. The code and results are available at https://github.com/sangwoomo/FreezeD.▽ MoreGenerative adversarial networks (GANs) have shown outstanding performance on a wide range of problems in computer vision, graphics, and machine learning, but often require numerous training data and heavy computational resources. To tackle this issue, several methods introduce a transfer learning technique in GAN training. They, however, are either prone to overfitting or limited to learning small distribution shifts. In this paper, we show that simple fine-tuning of GANs with frozen lower layers of the discriminator performs surprisingly well. This simple baseline, FreezeD, significantly outperforms previous techniques used in both unconditional and conditional GANs. We demonstrate the consistent effect using StyleGAN and SNGAN-projection architectures on several datasets of Animal Face, Anime Face,OxfordFlower, CUB-200-2011, and Caltech-256 datasets. The code and results are available at https://github.com/sangwoomo/FreezeD.△ Less"
Real-time Kinematic Ground Truth for theOxfordRobotCar Dataset,"Authors:Will Maddern,Geoffrey Pascoe,Matthew Gadd,Dan Barnes,Brian Yeomans,Paul Newman","Abstract:We describe the release of reference data towards a challenging long-term localisation and mapping benchmark based on the large-scaleOxfordRobotCar Dataset. The release includes 72 traversals of a route throughOxford, UK, gathered in all illumination, weather and traffic conditions, and is representative of the cond…▽ MoreWe describe the release of reference data towards a challenging long-term localisation and mapping benchmark based on the large-scaleOxfordRobotCar Dataset. The release includes 72 traversals of a route throughOxford, UK, gathered in all illumination, weather and traffic conditions, and is representative of the conditions an autonomous vehicle would be expected to operate reliably in. Using post-processed raw GPS, IMU, and static GNSS base station recordings, we have produced a globally-consistent centimetre-accurate ground truth for the entire year-long duration of the dataset. Coupled with a planned online benchmarking service, we hope to enable quantitative evaluation and comparison of different localisation and mapping approaches focusing on long-term autonomy for road vehicles in urban environments challenged by changing weather.△ Less"
Computational Design with Crowds,"Authors:Yuki Koyama,Takeo Igarashi","Abstract:Computational design is aimed at supporting or automating design processes using computational techniques. However, some classes of design tasks involve criteria that are difficult to handle only with computers. For example, visual design tasks seeking to fulfill aesthetic goals are difficult to handle purely with computers. One promising approach is to leverage human computation; that is, to inco…▽ MoreComputational design is aimed at supporting or automating design processes using computational techniques. However, some classes of design tasks involve criteria that are difficult to handle only with computers. For example, visual design tasks seeking to fulfill aesthetic goals are difficult to handle purely with computers. One promising approach is to leverage human computation; that is, to incorporate human input into the computation process. Crowdsourcing platforms provide a convenient way to integrate such human computation into a working system.
  In this chapter, we discuss such computational design with crowds in the domain of parameter tweaking tasks in visual design. Parameter tweaking is often performed to maximize the aesthetic quality of designed objects. Computational design powered by crowds can solve this maximization problem by leveraging human computation. We discuss the opportunities and challenges of computational design with crowds with two illustrative examples: (1) estimating the objective function (specifically, preference learning from crowds' pairwise comparisons) to facilitate interactive design exploration by a designer and (2) directly searching for the optimal parameter setting that maximizes the objective function (specifically, crowds-in-the-loop Bayesian optimization).△ Less"
Controlled time series generation for automotive software-in-the-loop testing using GANs,"Authors:Dhasarathy Parthasarathy,Karl Bäckström,Jens Henriksson,Sólrún Einarsdóttir","Abstract:Testing automotive mechatronic systems partly uses the software-in-the-loop approach, where systematically covering inputs of the system-under-test remains a major challenge. In current practice, there are two major techniques of input stimulation. One approach is to craft input sequences which eases control and feedback of the test process but falls short of exposing the system to realistic scena…▽ MoreTesting automotive mechatronic systems partly uses the software-in-the-loop approach, where systematically covering inputs of the system-under-test remains a major challenge. In current practice, there are two major techniques of input stimulation. One approach is to craft input sequences which eases control and feedback of the test process but falls short of exposing the system to realistic scenarios. The other is to replay sequences recorded from field operations which accounts for reality but requires collecting a well-labeled dataset of sufficient capacity for widespread use, which is expensive. This work applies the well-known unsupervised learning framework of Generative Adversarial Networks (GAN) to learn an unlabeled dataset of recorded in-vehicle signals and uses it for generation of synthetic input stimuli. Additionally, a metric-based linear interpolation algorithm is demonstrated, which guarantees that generated stimuli follow a customizable similarity relationship with specified references. This combination of techniques enables controlled generation of a rich range of meaningful and realistic input patterns, improving virtual test coverage and reducing the need for expensive field tests.△ Less"
2-Dimensional Categories,"Authors:Niles Johnson,Donald Yau","Abstract:This book is an introduction to 2-categories and bicategories, assuming only the most elementary aspects of category theory. A review of basic category theory is followed by a systematic discussion of 2-/bicategories, pasting diagrams, lax functors, 2-/bilimits, the Duskin nerve, 2-nerve, adjunctions and monads in bicategories, 2-monads, biequivalences, the Bicategorical Yoneda Lemma, and the Cohe…▽ MoreThis book is an introduction to 2-categories and bicategories, assuming only the most elementary aspects of category theory. A review of basic category theory is followed by a systematic discussion of 2-/bicategories, pasting diagrams, lax functors, 2-/bilimits, the Duskin nerve, 2-nerve, adjunctions and monads in bicategories, 2-monads, biequivalences, the Bicategorical Yoneda Lemma, and the Coherence Theorem for bicategories. Grothendieck fibrations and the Grothendieck construction are discussed next, followed by tricategories, monoidal bicategories, the Gray tensor product, and double categories. Completely detailed proofs of several fundamental but hard-to-find results are presented for the first time. With exercises and plenty of motivation and explanation, this book is useful for both beginners and experts.△ Less"
Change actions: from incremental computation to discrete derivatives,Authors:Mario Alvarez-Picallo,"Abstract:The goal of this thesis is threefold: first, to provide a general semantic setting for reasoning about incremental computation. Second, to establish and clarify the connection between derivatives in the incremental sense and derivatives in the analytic sense, that is to say, to provide a common definition of derivative of which the previous two are particular instances. Third, to give a theoretica…▽ MoreThe goal of this thesis is threefold: first, to provide a general semantic setting for reasoning about incremental computation. Second, to establish and clarify the connection between derivatives in the incremental sense and derivatives in the analytic sense, that is to say, to provide a common definition of derivative of which the previous two are particular instances. Third, to give a theoretically sound calculus for this general setting. To this end we define and explore the notions of change actions and differential maps between change actions and show how these notions relate to incremental computation through the concrete example of the semi-naive evaluation of Datalog queries. We also introduce the notion of a change action model as a setting for higher-order differentiation, and exhibit some interesting examples. Finally, we show how Cartesian difference categories, a family of particularly well-behaved change action models, generalise Cartesian differential categories and give rise to a calculus in the spirit of Ehrhard and Regnier's differential lambda-calculus.△ Less"
Human Creativity and Consciousness: Unintended Consequences of the Brain's Extraordinary Energy Efficiency?,Authors:T. N. Palmer,"Abstract:It is proposed that both human creativity and human consciousness are (unintended) consequences of the human brain's extraordinary energy efficiency. The topics of creativity and consciousness are treated separately, though have a common sub-structure. It is argued that creativity arises from a synergy between two cognitive modes of the human brain (which broadly coincide with Kahneman's Systems 1…▽ MoreIt is proposed that both human creativity and human consciousness are (unintended) consequences of the human brain's extraordinary energy efficiency. The topics of creativity and consciousness are treated separately, though have a common sub-structure. It is argued that creativity arises from a synergy between two cognitive modes of the human brain (which broadly coincide with Kahneman's Systems 1 and 2). In the first, available energy is spread across a relatively large network of neurons. As such, the amount of energy per active neuron is so small that the operation of such neurons is susceptible to thermal (ultimately quantum decoherent) noise. In the second, available energy is focussed on a small enough subset of neurons to guarantee a deterministic operation. An illustration of how this synergy can lead to creativity with implications for computing in silicon are discussed. Starting with a discussion of the concept of free will, the notion of consciousness is defined in terms of an awareness of what are perceived to be nearby counterfactual worlds in state space. It is argued that such awareness arises from an interplay between our memories on the one hand, and quantum physical mechanisms (where, unlike in classical physics, nearby counterfactual worlds play an indispensable dynamical role) in the ion channels of neural networks. As with the brain's susceptibility to noise, it is argued that in situations where quantum physics plays a role in the brain, it does so for reasons of energy efficiency. As an illustration of this definition of consciousness, a novel proposal is outlined as to why quantum entanglement appears so counter-intuitive.△ Less"
A Deep Learning Approach to Automate High-Resolution Blood Vessel Reconstruction on Computerized Tomography Images With or Without the Use of Contrast Agent,"Authors:Anirudh Chandrashekar,Ashok Handa,Natesh Shivakumar,Pierfrancesco Lapolla,Vicente Grau,Regent Lee","Abstract:…blood vessels in CT images acquired with or without the use of a contrast agent. Twenty-six patients with paired non-contrast and contrast-enhanced CT images within the ongoingOxfordAbdominal Aortic Aneurysm (OxAAA) study were randomly selected, manually annotated and used for model training and evaluation (13/13). Data augmentation methods were implemente…▽ MoreExisting methods to reconstruct vascular structures from a computed tomography (CT) angiogram rely on injection of intravenous contrast to enhance the radio-density within the vessel lumen. However, pathological changes can be present in the blood lumen, vessel wall or a combination of both that prevent accurate reconstruction. In the example of aortic aneurysmal disease, a blood clot or thrombus adherent to the aortic wall within the expanding aneurysmal sac is present in 70-80% of cases. These deformations prevent the automatic extraction of vital clinically relevant information by current methods. In this study, we implemented a modified U-Net architecture with attention-gating to establish a high-throughput and automated segmentation pipeline of pathological blood vessels in CT images acquired with or without the use of a contrast agent. Twenty-six patients with paired non-contrast and contrast-enhanced CT images within the ongoingOxfordAbdominal Aortic Aneurysm (OxAAA) study were randomly selected, manually annotated and used for model training and evaluation (13/13). Data augmentation methods were implemented to diversify the training data set in a ratio of 10:1. The performance of our Attention-based U-Net in extracting both the inner lumen and the outer wall of the aortic aneurysm from CT angiograms (CTA) was compared against a generic 3-D U-Net and displayed superior results. Subsequent implementation of this network architecture within the aortic segmentation pipeline from both contrast-enhanced CTA and non-contrast CT images has allowed for accurate and efficient extraction of the entire aortic volume. This extracted volume can be used to standardize current methods of aneurysmal disease management and sets the foundation for subsequent complex geometric and morphological analysis. Furthermore, the proposed pipeline can be extended to other vascular pathologies.△ Less"
Multi-Fusion Chinese WordNet (MCW) : Compound of Machine Learning and Manual Correction,"Authors:Mingchen Li,Zili Zhou,Yanna Wang","Abstract:…of PWN. So we decided to make a new Chinese wordnet called Multi-Fusion Chinese Wordnet (MCW) to make up those shortcomings. The key idea is to extend the SEW with the help ofOxfordbilingual dictionary and Xinhua bilingual dictionary, and then correct it. More specifically, we used machine learning and manual adjustment in our corrections. Two standards we…▽ MorePrinceton WordNet (PWN) is a lexicon-semantic network based on cognitive linguistics, which promotes the development of natural language processing. Based on PWN, five Chinese wordnets have been developed to solve the problems of syntax and semantics. They include: Northeastern University Chinese WordNet (NEW), Sinica Bilingual Ontological WordNet (BOW), Southeast University Chinese WordNet (SEW), Taiwan University Chinese WordNet (CWN), Chinese Open WordNet (COW). By using them, we found that these word networks have low accuracy and coverage, and cannot completely portray the semantic network of PWN. So we decided to make a new Chinese wordnet called Multi-Fusion Chinese Wordnet (MCW) to make up those shortcomings. The key idea is to extend the SEW with the help ofOxfordbilingual dictionary and Xinhua bilingual dictionary, and then correct it. More specifically, we used machine learning and manual adjustment in our corrections. Two standards were formulated to help our work. We conducted experiments on three tasks including relatedness calculation, word similarity and word sense disambiguation for the comparison of lemma's accuracy, at the same time, coverage also was compared. The results indicate that MCW can benefit from coverage and accuracy via our method. However, it still has room for improvement, especially with lemmas. In the future, we will continue to enhance the accuracy of MCW and expand the concepts in it.△ Less"
Symplectic Manifolds and Isomonodromic Deformations,Authors:Philip Boalch,"Abstract:We study moduli spaces of meromorphic connections (with arbitrary order poles) over Riemann surfaces together with the corresponding spaces of monodromy data (involving Stokes matrices). Natural symplectic structures are found and described both explicitly and from an infinite dimensional viewpoint (generalising the Atiyah-Bott approach). This enables us to give an intrinsic symplectic description…▽ MoreWe study moduli spaces of meromorphic connections (with arbitrary order poles) over Riemann surfaces together with the corresponding spaces of monodromy data (involving Stokes matrices). Natural symplectic structures are found and described both explicitly and from an infinite dimensional viewpoint (generalising the Atiyah-Bott approach). This enables us to give an intrinsic symplectic description of the isomonodromic deformation equations of Jimbo, Miwa and Ueno, thereby putting the existing results for the six Painleve equations and Schlesinger's equations into a uniform framework.△ Less"
AutoFCL: Automatically Tuning Fully Connected Layers for Handling Small Dataset,"Authors:S. H. Shabbeer Basha,Sravan Kumar Vinakota,Shiv Ram Dubey,Viswanath Pulabaigari,Snehasis Mukherjee","Abstract:…five pre-trained CNN models such as VGG-16, ResNet, DenseNet, MobileNet, and NASNetMobile. The experiments are conducted on three benchmark datasets, namely CalTech-101,Oxford-102 Flowers, and UC Merced Land Use datasets. Fine-tuning the newly learned (target-dependent) FC layers leads to state-of-the-art performance, according to the experiments carried ou…▽ MoreDeep Convolutional Neural Networks (CNN) have evolved as popular machine learning models for image classification during the past few years, due to their ability to learn the problem-specific features directly from the input images. The success of deep learning models solicits architecture engineering rather than hand-engineering the features. However, designing state-of-the-art CNN for a given task remains a non-trivial and challenging task, especially when training data size is less. To address this phenomena, transfer learning has been used as a popularly adopted technique. While transferring the learned knowledge from one task to another, fine-tuning with the target-dependent Fully Connected (FC) layers generally produces better results over the target task. In this paper, the proposed AutoFCL model attempts to learn the structure of FC layers of a CNN automatically using Bayesian optimization. To evaluate the performance of the proposed AutoFCL, we utilize five pre-trained CNN models such as VGG-16, ResNet, DenseNet, MobileNet, and NASNetMobile. The experiments are conducted on three benchmark datasets, namely CalTech-101,Oxford-102 Flowers, and UC Merced Land Use datasets. Fine-tuning the newly learned (target-dependent) FC layers leads to state-of-the-art performance, according to the experiments carried out in this research. The proposed AutoFCL method outperforms the existing methods over CalTech-101 andOxford-102 Flowers datasets by achieving the accuracy of 94.38% and 98.89%, respectively. However, our method achieves comparable performance on the UC Merced Land Use dataset with 96.83% accuracy. The source codes of this research are available at https://github.com/shabbeersh/AutoFCL.△ Less"
Understanding Profunctor Optics: a representation theorem,Authors:Guillaume Boisseau,"Abstract:Optics, aka functional references, are classes of tools that allow composable access into compound data structures. Usually defined as programming language libraries, they provide combinators to manipulate different shapes of data such as sums, products and collections, that can be composed to operate on larger structures. Together they form a powerful language to describe transformations of data.…▽ MoreOptics, aka functional references, are classes of tools that allow composable access into compound data structures. Usually defined as programming language libraries, they provide combinators to manipulate different shapes of data such as sums, products and collections, that can be composed to operate on larger structures. Together they form a powerful language to describe transformations of data. Among the different approaches to describing optics, one particular type of optics, called profunctor optics, stands out. It describes alternative but equivalent representations of most of the common combinators, and enhances them with elegant composability properties via a higher-order encoding. Notably, it enables easy composition across different optic families. Unfortunately, profunctor optics are difficult to reason about, and linking usual optics with an equivalent profunctor representation has so far been done on a case-by-case basis, with definitions that sometimes seem very ad hoc. This makes it hard both to analyse properties of existing profunctor optics and to define new ones. This thesis presents an equivalent representation of profunctor optics, called isomorphism optics, that is both closer to intuition and easier to reason about. This tool enables powerful theorems to be derived generically about profunctor optics. Finally, this thesis develops a framework to ease deriving new profunctor encodings from concrete optic families.△ Less"
Magnetic Fluctuations and the Spin-Orbit Interaction in Mott Insulating CoO,"Authors:P. M. Sarte,S. D. Wilson,J. P. Attfield,C. Stock","Abstract:…using conventional pseudo-bosonic approaches. It would not be for another 40 years that Roger, now atOxfordand motivated by the discovery of the high-$T_{c}$ cuprate superconductors [Bednorz & Muller 1986 Z. Phys. B $\mathbf{64}$ 189], would make another attempt at the parameterisation of the magnetic excitation spectrum that had previously alluded hi…▽ MoreMotivated by the presence of an unquenched orbital angular momentum in CoO, a team at Chalk River, including a recently hired research officer Roger Cowley, performed the first inelastic neutron scattering experiments on the classic Mott insulator [Sakurai $\textit{et al.}$ 1968 Phys. Rev. $\mathbf{167}$ 510]. Despite identifying magnon modes at the zone boundary, the team was unable to parameterise the low energy magnetic excitation spectrum below $T\rm{_{N}}$ using conventional pseudo-bosonic approaches. It would not be for another 40 years that Roger, now atOxfordand motivated by the discovery of the high-$T_{c}$ cuprate superconductors [Bednorz & Muller 1986 Z. Phys. B $\mathbf{64}$ 189], would make another attempt at the parameterisation of the magnetic excitation spectrum that had previously alluded him. Upon his return to CoO, Roger found a system embroiled in controversy, with some of its most fundamental parameters still remaining undetermined. Faced with such a formidable task, Roger performed a series of inelastic neutron scattering experiments in the early 2010's on both CoO and a magnetically dilute structural analogue MgO. These experiments would prove instrumental in the determination of both single-ion [Cowley $\textit{et al.}$ 2013 Phys. Rev. B $\mathbf{88}$ 205117] and cooperative magnetic parameters [Sarte $\textit{et al.}$ 2018 Phys. Rev. B $\mathbf{98}$ 024415] for CoO. Both these sets of parameters would eventually be used in a spin-orbit exciton model [Sarte $\textit{et al.}$ 2019 Phys. Rev. B $\mathbf{100}$ 075143], developed by his longtime friend and collaborator Bill Buyers, to successfully parameterise the complex spectrum that both measured at Chalk River almost 50 years prior. The story of CoO is of one that has come full circle, one filled with both spectacular failures and intermittent, yet profound, little victories.△ Less"
Under the Radar: Learning to Predict Robust Keypoints for Odometry Estimation and Metric Localisation in Radar,"Authors:Dan Barnes,Ingmar Posner","Abstract:…optimised for our application. Furthermore the architecture is sensor agnostic and can be applied to most modalities. We run experiments on 280km of real world driving from theOxfordRadar RobotCar Dataset and improve on the state-of-the-art in point-based radar odometry, reducing errors by up to 45% whilst running an order of magnitude faster, simultaneous…▽ MoreThis paper presents a self-supervised framework for learning to detect robust keypoints for odometry estimation and metric localisation in radar. By embedding a differentiable point-based motion estimator inside our architecture, we learn keypoint locations, scores and descriptors from localisation error alone. This approach avoids imposing any assumption on what makes a robust keypoint and crucially allows them to be optimised for our application. Furthermore the architecture is sensor agnostic and can be applied to most modalities. We run experiments on 280km of real world driving from theOxfordRadar RobotCar Dataset and improve on the state-of-the-art in point-based radar odometry, reducing errors by up to 45% whilst running an order of magnitude faster, simultaneously solving metric loop closures. Combining these outputs, we provide a framework capable of full mapping and localisation with radar in urban environments.△ Less"
Profunctor optics and traversals,Authors:Mario Román,"Abstract:Optics are bidirectional accessors of data structures; they provide a powerful abstraction of many common data transformations. This abstraction is compositional thanks to a representation in terms of profunctors endowed with an algebraic structure called Tambara module. There exists a general definition of optic in terms of coends that, after some elementary application of the Yoneda lemma, parti…▽ MoreOptics are bidirectional accessors of data structures; they provide a powerful abstraction of many common data transformations. This abstraction is compositional thanks to a representation in terms of profunctors endowed with an algebraic structure called Tambara module. There exists a general definition of optic in terms of coends that, after some elementary application of the Yoneda lemma, particularizes in each one of the basic optics. Traversals used to be the exception; we show an elementary derivation of traversals and discuss some other new derivations for optics. We relate our characterization of traversals to the previous ones showing that the coalgebras of a comonad that represents and split into shape and contents are traversable functors. The representation of optics in terms of profunctors has many different proofs in the literature; we discuss two ways of proving it, generalizing both to the case of mixed optics for an arbitrary action. Categories of optics can be seen as Eilenberg-Moore categories for a monad described by Pastro and Street. This gives us two different approaches to composition between profunctor optics of different families: using distributive laws between the monads defining them, and using coproducts of monads. The second one is the one implicitly used in Haskell programming; but we show that a refinement of the notion of optic is required in order to model it faithfully. We provide experimental implementations of a library of optics in Haskell and partial Agda formalizations of the profunctor representation theorem.△ Less"
Unifying Deep Local and Global Features for Image Search,"Authors:Bingyi Cao,Andre Araujo,Jack Sim","Abstract:…into the model, improving training efficiency and matching performance. Comprehensive experiments show that our model achieves state-of-the-art image retrieval on the RevisitedOxfordand Paris datasets, and state-of-the-art single-model instance-level recognition on the Google Landmarks dataset v2. Code and models are available at https://github.com/tensorf…▽ MoreImage retrieval is the problem of searching an image database for items that are similar to a query image. To address this task, two main types of image representations have been studied: global and local image features. In this work, our key contribution is to unify global and local features into a single deep model, enabling accurate retrieval with efficient feature extraction. We refer to the new model as DELG, standing for DEep Local and Global features. We leverage lessons from recent feature learning work and propose a model that combines generalized mean pooling for global features and attentive selection for local features. The entire network can be learned end-to-end by carefully balancing the gradient flow between two heads -- requiring only image-level labels. We also introduce an autoencoder-based dimensionality reduction technique for local features, which is integrated into the model, improving training efficiency and matching performance. Comprehensive experiments show that our model achieves state-of-the-art image retrieval on the RevisitedOxfordand Paris datasets, and state-of-the-art single-model instance-level recognition on the Google Landmarks dataset v2. Code and models are available at https://github.com/tensorflow/models/tree/master/research/delf .△ Less"
Towards Interpretable and Robust Hand Detection via Pixel-wise Prediction,"Authors:Dan Liu,Libo Zhang,Tiejian Luo,Lili Tao,Yanjun Wu","Abstract:…and derotation layers. (4) Auxiliary supervision accelerates the training process, which saves more than 10 hours in our experiments. Experimental results on the VIVA andOxfordhand detection and tracking datasets show competitive accuracy of our method compared with state-of-the-art methods with higher speed.▽ MoreThe lack of interpretability of existing CNN-based hand detection methods makes it difficult to understand the rationale behind their predictions. In this paper, we propose a novel neural network model, which introduces interpretability into hand detection for the first time. The main improvements include: (1) Detect hands at pixel level to explain what pixels are the basis for its decision and improve transparency of the model. (2) The explainable Highlight Feature Fusion block highlights distinctive features among multiple layers and learns discriminative ones to gain robust performance. (3) We introduce a transparent representation, the rotation map, to learn rotation features instead of complex and non-transparent rotation and derotation layers. (4) Auxiliary supervision accelerates the training process, which saves more than 10 hours in our experiments. Experimental results on the VIVA andOxfordhand detection and tracking datasets show competitive accuracy of our method compared with state-of-the-art methods with higher speed.△ Less"
"Deep Learning based Pedestrian Inertial Navigation: Methods, Dataset and On-Device Inference","Authors:Changhao Chen,Peijun Zhao,Chris Xiaoxuan Lu,Wei Wang,Andrew Markham,Niki Trigoni","Abstract:…of sufficient labelled data for training and evaluating architecture benchmarks has limited the adoption of DNNs in IMU-based tasks. In this paper, we present and release theOxfordInertial Odometry Dataset (OxIOD), a first-of-its-kind public dataset for deep learning based inertial navigation research, with fine-grained ground-truth on all sequences. Furth…▽ MoreModern inertial measurements units (IMUs) are small, cheap, energy efficient, and widely employed in smart devices and mobile robots. Exploiting inertial data for accurate and reliable pedestrian navigation supports is a key component for emerging Internet-of-Things applications and services. Recently, there has been a growing interest in applying deep neural networks (DNNs) to motion sensing and location estimation. However, the lack of sufficient labelled data for training and evaluating architecture benchmarks has limited the adoption of DNNs in IMU-based tasks. In this paper, we present and release theOxfordInertial Odometry Dataset (OxIOD), a first-of-its-kind public dataset for deep learning based inertial navigation research, with fine-grained ground-truth on all sequences. Furthermore, to enable more efficient inference at the edge, we propose a novel lightweight framework to learn and reconstruct pedestrian trajectories from raw IMU data. Extensive experiments show the effectiveness of our dataset and methods in achieving accurate data-driven pedestrian inertial navigation on resource-constrained devices.△ Less"
AutoDiscern: Rating the Quality of Online Health Information with Hierarchical Encoder Attention-based Neural Networks,"Authors:Laura Kinkead,Ahmed Allam,Michael Krauthammer","Abstract:…risks to the patient in the form of misinformation and a possibly poorer relationship with their physician. To address this, the DISCERN criteria (developed at University ofOxford) are used to evaluate the quality of online health information. However, patients are unlikely to take the time to apply these criteria to the health websites they visit. We built…▽ MorePatients increasingly turn to search engines and online content before, or in place of, talking with a health professional. Low quality health information, which is common on the internet, presents risks to the patient in the form of misinformation and a possibly poorer relationship with their physician. To address this, the DISCERN criteria (developed at University ofOxford) are used to evaluate the quality of online health information. However, patients are unlikely to take the time to apply these criteria to the health websites they visit. We built an automated implementation of the DISCERN instrument (Brief version) using machine learning models. We compared the performance of a traditional model (Random Forest) with that of a hierarchical encoder attention-based neural network (HEA) model using two language embeddings, BERT and BioBERT. The HEA BERT and BioBERT models achieved average F1-macro scores across all criteria of 0.75 and 0.74, respectively, outperforming the Random Forest model (average F1-macro = 0.69). Overall, the neural network based models achieved 81% and 86% average accuracy at 100% and 80% coverage, respectively, compared to 94% manual rating accuracy. The attention mechanism implemented in the HEA architectures not only provided 'model explainability' by identifying reasonable supporting sentences for the documents fulfilling the Brief DISCERN criteria, but also boosted F1 performance by 0.05 compared to the same architecture without an attention mechanism. Our research suggests that it is feasible to automate online health information quality assessment, which is an important step towards empowering patients to become informed partners in the healthcare process.△ Less"
On selected developments in the theory of natural dualities,Authors:Miroslav Haviar,"Abstract:…to make with his foreign colleagues several breakthroughs and move the theory forward. It is aimed as author's reflection on his works on the natural dualities inOxfordand Melbourne over the period of twenty years 1993-2012 (before his attention with the colleagues in universal algebra and lattice theory has been fully focused on the theory of canonica…▽ MoreThis is a survey on selected developments in the theory of natural dualities where the author had the opportunity to make with his foreign colleagues several breakthroughs and move the theory forward. It is aimed as author's reflection on his works on the natural dualities inOxfordand Melbourne over the period of twenty years 1993-2012 (before his attention with the colleagues in universal algebra and lattice theory has been fully focused on the theory of canonical extensions and the theory of bilattices). It is also meant as a remainder that the main problems of the theory of natural dualities, Dualisability Problem and Decidability Problem for Dualisability, remain still open.
  Theory of natural dualities is a general theory for quasi-varieties of algebras that generalizes `classical' dualities such as Stone duality for Boolean algebras, Pontryagin duality for abelian groups, Priestley duality for distributive lattices, and Hofmann-Mislove-Stralka duality for semilattices. We present a brief background of the theory and then illustrate its applications on our study of Entailment Problem, Problem of Endodualisability versus Endoprimality and then a famous Full versus Strong Problem with related developments.△ Less"
Unsupervised Scene Adaptation with Memory Regularization in vivo,"Authors:Zhedong Zheng,Yi Yang","Abstract:…+11.1% and +11.3% mIoU improvement over the baseline model, respectively. Besides, a similar +12.0% mIoU improvement is observed on the cross-city benchmark: Cityscapes ->OxfordRobotCar.▽ MoreWe consider the unsupervised scene adaptation problem of learning from both labeled source data and unlabeled target data. Existing methods focus on minoring the inter-domain gap between the source and target domains. However, the intra-domain knowledge and inherent uncertainty learned by the network are under-explored. In this paper, we propose an orthogonal method, called memory regularization in vivo to exploit the intra-domain knowledge and regularize the model training. Specifically, we refer to the segmentation model itself as the memory module, and minor the discrepancy of the two classifiers, i.e., the primary classifier and the auxiliary classifier, to reduce the prediction inconsistency. Without extra parameters, the proposed method is complementary to the most existing domain adaptation methods and could generally improve the performance of existing methods. Albeit simple, we verify the effectiveness of memory regularization on two synthetic-to-real benchmarks: GTA5 -> Cityscapes and SYNTHIA -> Cityscapes, yielding +11.1% and +11.3% mIoU improvement over the baseline model, respectively. Besides, a similar +12.0% mIoU improvement is observed on the cross-city benchmark: Cityscapes ->OxfordRobotCar.△ Less"
Formula size games for modal logic and $μ$-calculus,"Authors:Lauri Hella,Miikka Vilander","Abstract:We propose a new version of formula size game for modal logic. The game characterizes the equivalence of pointed Kripke-models up to formulas of given numbers of modal operators and binary connectives. Our game is similar to the well-known Adler-Immerman game. However, due to a crucial difference in the definition of positions of the game, its winning condition is simpler, and the second player do…▽ MoreWe propose a new version of formula size game for modal logic. The game characterizes the equivalence of pointed Kripke-models up to formulas of given numbers of modal operators and binary connectives. Our game is similar to the well-known Adler-Immerman game. However, due to a crucial difference in the definition of positions of the game, its winning condition is simpler, and the second player does not have a trivial optimal strategy. Thus, unlike the Adler-Immerman game, our game is a genuine two-person game. We illustrate the use of the game by proving a non-elementary succinctness gap between bisimulation invariant first-order logic $\mathrm{FO}$ and (basic) modal logic $\mathrm{ML}$. We also present a version of the game for the modal $μ$-calculus $\mathrm{L}_μ$ and show that $\mathrm{FO}$ is also non-elementarily more succinct than $\mathrm{L}_μ$.△ Less"
Image Manipulation with Natural Language using Two-sidedAttentive Conditional Generative Adversarial Network,"Authors:Dawei Zhu,Aditya Mogadala,Dietrich Klakow","Abstract:…(GAN) based framework at different scales. Experimental results show that TEA-cGAN which generates 128x128 and 256x256 resolution images outperforms existing methods on CUB andOxford-102 datasets both quantitatively and qualitatively.▽ MoreAltering the content of an image with photo editing tools is a tedious task for an inexperienced user. Especially, when modifying the visual attributes of a specific object in an image without affecting other constituents such as background etc. To simplify the process of image manipulation and to provide more control to users, it is better to utilize a simpler interface like natural language. Therefore, in this paper, we address the challenge of manipulating images using natural language description. We propose the Two-sidEd Attentive conditional Generative Adversarial Network (TEA-cGAN) to generate semantically manipulated images while preserving other contents such as background intact. TEA-cGAN uses fine-grained attention both in the generator and discriminator of Generative Adversarial Network (GAN) based framework at different scales. Experimental results show that TEA-cGAN which generates 128x128 and 256x256 resolution images outperforms existing methods on CUB andOxford-102 datasets both quantitatively and qualitatively.△ Less"
MTRNet++: One-stage Mask-based Scene Text Eraser,"Authors:Osman Tursun,Simon Denman,Rui Zeng,Sabesan Sivapalan,Sridha Sridharan,Clinton Fookes","Abstract:…branches, and attention blocks. With this architecture, MTRNet++ can remove text either with or without an external mask. It achieves state-of-the-art results on both theOxfordand SCUT datasets without using external ground-truth masks. The results of ablation studies demonstrate that the proposed multi-branch architecture with attention blocks is effectiv…▽ MoreA precise, controllable, interpretable and easily trainable text removal approach is necessary for both user-specific and large-scale text removal applications. To achieve this, we propose a one-stage mask-based text inpainting network, MTRNet++. It has a novel architecture that includes mask-refine, coarse-inpainting and fine-inpainting branches, and attention blocks. With this architecture, MTRNet++ can remove text either with or without an external mask. It achieves state-of-the-art results on both theOxfordand SCUT datasets without using external ground-truth masks. The results of ablation studies demonstrate that the proposed multi-branch architecture with attention blocks is effective and essential. It also demonstrates controllability and interpretability.△ Less"
An Economical Business-Cycle Model,"Authors:Pascal Michaillat,Emmanuel Saez","Abstract:This paper develops a new model of business cycles. The model is economical in that it is solved with an aggregate demand-aggregate supply diagram, and the effects of shocks and policies are obtained by comparative statics. The model builds on two unconventional assumptions. First, producers and consumers meet through a matching function. Thus, the model features unemployment, which fluctuates in…▽ MoreThis paper develops a new model of business cycles. The model is economical in that it is solved with an aggregate demand-aggregate supply diagram, and the effects of shocks and policies are obtained by comparative statics. The model builds on two unconventional assumptions. First, producers and consumers meet through a matching function. Thus, the model features unemployment, which fluctuates in response to aggregate demand and supply shocks. Second, wealth enters the utility function, so the model allows for permanent zero-lower-bound episodes. In the model, the optimal monetary policy is to set the interest rate at the level that eliminates the unemployment gap. This optimal interest rate is computed from the prevailing unemployment gap and monetary multiplier (the effect of the nominal interest rate on the unemployment rate). If the unemployment gap is exceedingly large, monetary policy cannot eliminate it before reaching the zero lower bound, but a wealth tax can.△ Less"
Improving Interpretability of Word Embeddings by Generating Definition and Usage,"Authors:Haitong Zhang,Yongping Du,Jiaxin Sun,Qingxiao Li","Abstract:…task model to multi-task setting and investigate several joint multi-task models to combine usage modeling and definition modeling together. Experimental results on existingOxforddataset and a new collectedOxford-2019 dataset show that our single-task model achieves the state-of-the-art result in definition modeling…▽ MoreWord embeddings are substantially successful in capturing semantic relations among words. However, these lexical semantics are difficult to be interpreted. Definition modeling provides a more intuitive way to evaluate embeddings by utilizing them to generate natural language definitions of corresponding words. This task is of great significance for practical application and in-depth understanding of word representations. We propose a novel framework for definition modeling, which can generate reasonable and understandable context-dependent definitions. Moreover, we introduce usage modeling and study whether it is possible to utilize embeddings to generate example sentences of words. These ways are a more direct and explicit expression of embedding's semantics for better interpretability. We extend the single task model to multi-task setting and investigate several joint multi-task models to combine usage modeling and definition modeling together. Experimental results on existingOxforddataset and a new collectedOxford-2019 dataset show that our single-task model achieves the state-of-the-art result in definition modeling and the multi-task learning methods are helpful for two tasks to improve the performance.△ Less"
SKD: Keypoint Detection for Point Clouds using Saliency Estimation,"Authors:Georgi Tinchev,Adrian Penate-Sanchez,Maurice Fallon","Abstract:…solely as a result of the geometry surrounding a point, but also take into account the descriptor's response. The approach was evaluated on two large LIDAR datasets - theOxfordRobotCar dataset and the KITTI dataset, where we obtain up to 50% improvement over the state-of-the-art in both matchability and repeatability. When performing sparse matching wi…▽ MoreWe present SKD, a novel keypoint detector that uses saliency to determine the best candidates from a point cloud for tasks such as registration and reconstruction. The approach can be applied to any differentiable deep learning descriptor by using the gradients of that descriptor with respect to the 3D position of the input points as a measure of their saliency. The saliency is combined with the original descriptor and context information in a neural network, which is trained to learn robust keypoint candidates. The key intuition behind this approach is that keypoints are not extracted solely as a result of the geometry surrounding a point, but also take into account the descriptor's response. The approach was evaluated on two large LIDAR datasets - theOxfordRobotCar dataset and the KITTI dataset, where we obtain up to 50% improvement over the state-of-the-art in both matchability and repeatability. When performing sparse matching with the keypoints computed by our method we achieve a higher inlier ratio and faster convergence.△ Less"
A spider timing model: accounting for quadrupole deformations and relativity in close pulsar binaries,"Authors:Guillaume Voisin,René Breton,Charlotte Summers","Abstract:Spider millisecond pulsars are, along with some eclipsing post-common envelope systems and cataclysmic variables, part of an expanding category of compact binaries with low-mass companions for which puzzling timing anomalies have been observed. The most prominent type of irregularities seen in them are orbital period variations, a phenomenon which has been proposed to originate from changes in the…▽ MoreSpider millisecond pulsars are, along with some eclipsing post-common envelope systems and cataclysmic variables, part of an expanding category of compact binaries with low-mass companions for which puzzling timing anomalies have been observed. The most prominent type of irregularities seen in them are orbital period variations, a phenomenon which has been proposed to originate from changes in the gravitational quadrupole moment of the companion star. A physically sound modelling of the timing of these systems is key to understanding their structure and evolution. In this paper we argue that a complete timing model must account for relativistic corrections as well as rotationally and tidally induced quadrupole distortions. We solve for the resulting orbital dynamics using perturbation theory and derive the corresponding timing model in the low eccentricity limit. We find that the expected strong quadrupole deformation of the companion star results in an effective minimum orbital eccentricity. It is accompanied by a fast periastron precession which, if not taken into account, averages out any measurement of the said eccentricity. We show that, with our model, detection of both eccentricity and precession is likely to be made in many if not all spider pulsar systems. Combined with optical light curves, this will allow us to measure the apsidal motion constant, connecting the quadrupole deformation to the internal structure, and thus opening a new window into probing the nature of their exotic stellar interiors. Moreover, more accurate timing may eventually lead spider pulsars to be used for high-precision timing experiments such as pulsar timing arrays.△ Less"
A Categorical Approach to Subgroups of Quantum Groups and Their Crystal Bases,Authors:Rhiannon Savage,"Abstract:Suppose that we have a semisimple, connected, simply connected algebraic group $G$ with corresponding Lie algebra $\mathfrak{g}$. There is a Hopf pairing between the universal enveloping algebra $U(\mathfrak{g})$ and the coordinate ring $O(G)$. By introducing a parameter $q$, we can consider quantum deformations $U_q(\mathfrak{g})$ and $O_q(G)$ respectively, between which there again exists a Hopf…▽ MoreSuppose that we have a semisimple, connected, simply connected algebraic group $G$ with corresponding Lie algebra $\mathfrak{g}$. There is a Hopf pairing between the universal enveloping algebra $U(\mathfrak{g})$ and the coordinate ring $O(G)$. By introducing a parameter $q$, we can consider quantum deformations $U_q(\mathfrak{g})$ and $O_q(G)$ respectively, between which there again exists a Hopf pairing. We show that the category of crystals associated with $U_q(\mathfrak{g})$ is a monoidal category. We define subgroups of $U_q(\mathfrak{g})$ to be right coideal subalgebras, and subgroups of $O_q(G)$ to be quotient left $O_q(G)$-module coalgebras. Furthermore, we discuss a categorical approach to subgroups of quantum groups which we hope will provide us with a link to crystal basis theory.△ Less"
GPU Support for Automatic Generation of Finite-Differences Stencil Kernels,"Authors:Vitor Hugo Mickus Rodrigues,Lucas Cavalcante,Maelso Bruno Pereira,Fabio Luporini,István Reguly,Gerard Gorman,Samuel Xavier de Souza","Abstract:…present an extension for an open source compiler designed to produce highly optimized finite difference kernels for use in inversion methods named Devito. We embed it with theOxfordParallel Domain Specific Language (OP-DSL) in order to enable automatic code generation for GPU architectures from a high-level representation. We aim to enable users coding in…▽ MoreThe growth of data to be processed in the Oil & Gas industry matches the requirements imposed by evolving algorithms based on stencil computations, such as Full Waveform Inversion and Reverse Time Migration. Graphical processing units (GPUs) are an attractive architectural target for stencil computations because of its high degree of data parallelism. However, the rapid architectural and technological progression makes it difficult for even the most proficient programmers to remain up-to-date with the technological advances at a micro-architectural level. In this work, we present an extension for an open source compiler designed to produce highly optimized finite difference kernels for use in inversion methods named Devito. We embed it with theOxfordParallel Domain Specific Language (OP-DSL) in order to enable automatic code generation for GPU architectures from a high-level representation. We aim to enable users coding in a symbolic representation level to effortlessly get their implementations leveraged by the processing capacities of GPU architectures. The implemented backend is evaluated on a NVIDIA GTX Titan Z, and on a NVIDIA Tesla V100 in terms of operational intensity through the roof-line model for varying space-order discretization levels of 3D acoustic isotropic wave propagation stencil kernels with and without symbolic optimizations. It achieves approximately 63% of V100's peak performance and 24% of Titan Z's peak performance for stencil kernels over grids with 256 points. Our study reveals that improving memory usage should be the most efficient strategy for leveraging the performance of the implemented solution on the evaluated architectures.△ Less"
Creativity,Authors:Liane Gabora,"Abstract:Creativity is perhaps what most differentiates humans from other species. It involves the capacity to shift between divergent and convergent modes of thought in response to task demands. Divergent thought has been characterized as the kind of thinking needed to generate multiple solutions, while convergent thought has been characterized as the kind of thinking needed for tasks in with one solution…▽ MoreCreativity is perhaps what most differentiates humans from other species. It involves the capacity to shift between divergent and convergent modes of thought in response to task demands. Divergent thought has been characterized as the kind of thinking needed to generate multiple solutions, while convergent thought has been characterized as the kind of thinking needed for tasks in with one solution. Divergent thought has been conceived of as reflecting on the task from unconventional perspectives, while convergent thought has been conceived of as reflecting on it from conventional perspectives. Personality traits correlated with creativity include openness to experience, tolerance of ambiguity, and self-confidence. Evidence that creativity is linked with affective disorders is mixed. Neuroscientific research using electroencephalography (EEG) or functional magnetic resonance imaging (fMRI) suggests that creativity is associated with a loosening of cognitive control and decreased arousal. The distributed, content-addressable structure of associative memory is conducive to bringing task-relevant items to mind without the need for explicit search. Human creativity dates back to the earliest stone tools over three million years ago, with the Paleolithic marking the onset of art, science, and religion. Areas of controversy concern the relative contributions of expertise, chance, and intuition, the importance of process versus product, whether creativity is domain-specific versus domain-general, the extent to which creativity is correlated with affective disorders, and whether divergent thought entails the generation of multiple ideas or the honing of a single initially ambiguous mental representation that may manifest as different external outputs. Areas for further research include computational modeling, the biological basis of creativity, and studies that track ideation processes over time.△ Less"
On the Status of Quantum State Realism,Authors:Wayne C. Myrvold,"Abstract:I argue that we have good reason for being realist about quantum states. Though a research programme of attempting to construct a plausible theory that accounts for quantum phenomena without ontic quantum states is well-motivated, that research programme is confronted by considerable obstacles. Two theorems are considered that place restrictions on a theory of that sort: a theorem due to Barrett,…▽ MoreI argue that we have good reason for being realist about quantum states. Though a research programme of attempting to construct a plausible theory that accounts for quantum phenomena without ontic quantum states is well-motivated, that research programme is confronted by considerable obstacles. Two theorems are considered that place restrictions on a theory of that sort: a theorem due to Barrett, Cavalcanti, Lal, and Maroney, and an extension, by the author, of the Pusey-Barrett-Rudolph theorem, that employs an assumption weaker than their Cartesian Product Assumption. These theorems have assumptions, of course. If there were powerful evidence against the conclusion that quantum states correspond to something in physical reality, it might be reasonable to reject these assumptions. But the situation we find ourselves in is the opposite: there is no evidence at all supporting irrealism about quantum states.△ Less"
On the Upper Bound of the Kullback-Leibler Divergence and Cross Entropy,"Authors:Min Chen,Mateu Sbert","Abstract:This archiving article consists of several short reports on the discussions between the two authors over the past two years atOxfordand Madrid, and their work carried out during that period on the upper bound of the Kullback-Leibler divergence and cross entropy. The work was motivated by the cost-benefit ratio proposed by Chen and Golan [1], and the less d…▽ MoreThis archiving article consists of several short reports on the discussions between the two authors over the past two years atOxfordand Madrid, and their work carried out during that period on the upper bound of the Kullback-Leibler divergence and cross entropy. The work was motivated by the cost-benefit ratio proposed by Chen and Golan [1], and the less desirable property that the Kullback-Leibler (KL) divergence used in the measure is unbounded. The work subsequently (i) confirmed that the KL-divergence used in the cost-benefit ratio should exhibit a bounded property, (ii) proposed a new divergence measure, and (iii) compared this new divergence measure with a few other bounded measures.△ Less"
Quantum theory and functional analysis,Authors:Klaas Landsman,"Abstract:Quantum theory and functional analysis were created and put into essentially their final form during similar periods ending around 1930. Each was also a key outcome of the major revolutions that both physics and mathematics as a whole underwent at the time. This paper studies their interaction in this light, emphasizing the leading roles played by Hilbert in preparing the ground and by von Neumann…▽ MoreQuantum theory and functional analysis were created and put into essentially their final form during similar periods ending around 1930. Each was also a key outcome of the major revolutions that both physics and mathematics as a whole underwent at the time. This paper studies their interaction in this light, emphasizing the leading roles played by Hilbert in preparing the ground and by von Neumann in bringing them together during the crucial year of 1927, when he gave the modern, abstract definition of a Hilbert space and applied this concept to quantum mechanics (consolidated in his famous monograph from 1932). Subsequently, I give a very brief overview of three areas of functional analysis that have had fruitful interactions with quantum theory since 1932, namely unbounded operators, operator algebras, and distributions. The paper closes with some musings about the role of functional analysis in actual physics.△ Less"
Decision Procedures for Guarded Logics,Authors:Kevin Kappelmann,"Abstract:An important class of decidable first-order logic fragments are those satisfying a guardedness condition, such as the guarded fragment (GF). Usually, decidability for these logics is closely linked to the tree-like model property - the fact that satisfying models can be taken to have tree-like form. Decision procedures for the guarded fragment based on the tree-like model property are difficult to…▽ MoreAn important class of decidable first-order logic fragments are those satisfying a guardedness condition, such as the guarded fragment (GF). Usually, decidability for these logics is closely linked to the tree-like model property - the fact that satisfying models can be taken to have tree-like form. Decision procedures for the guarded fragment based on the tree-like model property are difficult to implement. An alternative approach, based on restricting first-order resolution, has been proposed, and this shows more promise from the point of view of implementation. In this work, we connect the tree-like model property of the guarded fragment with the resolution-based approach. We derive efficient resolution-based rewriting algorithms that solve the Quantifier-Free Query Answering Problem under Guarded Tuple Generating Dependencies (GTGDs) and Disjunctive Guarded Tuple Generating Dependencies (DisGTGDs). The Query Answering Problem for these classes subsumes many cases of GF satisfiability. Our algorithms, in addition to making the connection to the tree-like model property clear, give a natural account of the selection and ordering strategies used by resolution procedures for the guarded fragment. We also believe that our rewriting algorithm for the special case of GTGDs may prove itself valuable in practice as it does not require any Skolemisation step and its theoretical runtime outperforms those of known GF resolution procedures in case of fixed dependencies. Moreover, we show a novel normalisation procedure for the widely used chase procedure in case of (disjunctive) GTGDs, which could be useful for future studies.△ Less"
Hierarchical Mixtures of Generators for Adversarial Learning,"Authors:Alper Ahmetoğlu,Ethem Alpaydın","Abstract:…and can be trained using gradient-based optimization, just like the original GAN model. Our experiments on five image data sets, namely, MNIST, FashionMNIST, UTZap50K,OxfordFlowers, and CelebA, show that our proposed model generates samples of high quality and diversity in terms of popular GAN evaluation metrics. The learned hierarchical structure also lea…▽ MoreGenerative adversarial networks (GANs) are deep neural networks that allow us to sample from an arbitrary probability distribution without explicitly estimating the distribution. There is a generator that takes a latent vector as input and transforms it into a valid sample from the distribution. There is also a discriminator that is trained to discriminate such fake samples from true samples of the distribution; at the same time, the generator is trained to generate fakes that the discriminator cannot tell apart from the true samples. Instead of learning a global generator, a recent approach involves training multiple generators each responsible from one part of the distribution. In this work, we review such approaches and propose the hierarchical mixture of generators, inspired from the hierarchical mixture of experts model, that learns a tree structure implementing a hierarchical clustering with soft splits in the decision nodes and local generators in the leaves. Since the generators are combined softly, the whole model is continuous and can be trained using gradient-based optimization, just like the original GAN model. Our experiments on five image data sets, namely, MNIST, FashionMNIST, UTZap50K,OxfordFlowers, and CelebA, show that our proposed model generates samples of high quality and diversity in terms of popular GAN evaluation metrics. The learned hierarchical structure also leads to knowledge extraction.△ Less"
Definability patterns and their symmetries,Authors:Ehud Hrushovski,"Abstract:We identify a canonical structure J associated to any first-order theory, the {\it space of definability patterns}. It generalizes the imaginary algebraic closure in a stable theory, and the hyperimaginary bounded closure in simple theories. J admits a compact topology, not necessarily Hausdorff, but the Hausdorff part can already be bigger than the Kim-Pillay space. Using it, we obtain simple pro…▽ MoreWe identify a canonical structure J associated to any first-order theory, the {\it space of definability patterns}. It generalizes the imaginary algebraic closure in a stable theory, and the hyperimaginary bounded closure in simple theories. J admits a compact topology, not necessarily Hausdorff, but the Hausdorff part can already be bigger than the Kim-Pillay space. Using it, we obtain simple proofs of a number of results previously obtained using topological dynamics, but working one power set level lower. The Lascar neighbour relation is represented by a canonical relation on the compact Hausdorff part J; the general Lascar group can be read off this compact structure. This gives concrete form to results of Krupiński, Newelski, Pillay, Rzepecki and Simon, who used topological dynamics applied to large models to show the existence of compact groups mapping onto the Lascar group. In an appendix, we show that a construction analogous to the above but using infinitary patterns recovers the Ellis group of \cite{kns}, and use this to sharpen the cardinality bound for their Ellis group from\beth_5to\beth_3, showing the latter is optimal. There is also a close connection to another school of topological dynamics, set theory and model theory, centered around the Kechris-Pestov-Todor\v cević correspondence. We define the Ramsey property for a first order theory, and show - as a simple application of the construction applied to an auxiliary theory - that any theory admits a canonical minimal Ramsey expansion. This was envisaged and proved for certain Fraissé classes, first by Kechris-Pestov-Todor\v cević for expansions by orderings, then by Melleray, Nguyen Van Thé, Tsankov and Zucker for more general expansions.△ Less"
Knowledge of Uncertain Worlds: Programming with Logical Constraints,"Authors:Yanhong A. Liu,Scott D. Stoller","Abstract:Programming with logic for sophisticated applications must deal with recursion and negation, which together have created significant challenges in logic, leading to many different, conflicting semantics of rules. This paper describes a unified language, DA logic, for design and analysis logic, based on the unifying founded semantics and constraint semantics, that support the power and ease of prog…▽ MoreProgramming with logic for sophisticated applications must deal with recursion and negation, which together have created significant challenges in logic, leading to many different, conflicting semantics of rules. This paper describes a unified language, DA logic, for design and analysis logic, based on the unifying founded semantics and constraint semantics, that support the power and ease of programming with different intended semantics. The key idea is to provide meta-constraints, supports the use of uncertain information in the form of either undefined values or possible combinations of values or both, and promote the use of knowledge units that can be instantiated by any new predicates, including predicates with additional arguments.△ Less"
Machine Learning and Big Scientific Data,"Authors:Tony Hey,Keith Butler,Sam Jackson,Jeyarajan Thiyagalingam","Abstract:…huge growth of experimental data generated by the new generation of large-scale experiments at UK national facilities at the Rutherford Appleton Laboratory site at Harwell nearOxford. Such ""Big Scientific Data"" comes from the Diamond Light Source and Electron Microscopy Facilities, the ISIS Neutron and Muon Facility, and the UK's Central Laser F…▽ MoreThis paper reviews some of the challenges posed by the huge growth of experimental data generated by the new generation of large-scale experiments at UK national facilities at the Rutherford Appleton Laboratory site at Harwell nearOxford. Such ""Big Scientific Data"" comes from the Diamond Light Source and Electron Microscopy Facilities, the ISIS Neutron and Muon Facility, and the UK's Central Laser Facility. Increasingly, scientists are now needing to use advanced machine learning and other AI technologies both to automate parts of the data pipeline and also to help find new scientific discoveries in the analysis of their data. For commercially important applications, such as object recognition, natural language processing and automatic translation, deep learning has made dramatic breakthroughs. Google's DeepMind has now also used deep learning technology to develop their AlphaFold tool to make predictions for protein folding. Remarkably, they have been able to achieve some spectacular results for this specific scientific problem. Can deep learning be similarly transformative for other scientific problems? After a brief review of some initial applications of machine learning at the Rutherford Appleton Laboratory, we focus on challenges and opportunities for AI in advancing materials science. Finally, we discuss the importance of developing some realistic machine learning benchmarks using Big Scientific Data coming from a number of different scientific domains. We conclude with some initial examples of our ""SciML"" benchmark suite and of the research challenges these benchmarks will enable.△ Less"
Global visual localization in LiDAR-maps through shared 2D-3D embedding space,"Authors:Daniele Cattaneo,Matteo Vaghi,Simone Fontana,Augusto Luis Ballardini,Domenico Giorgio Sorrenti","Abstract:…to assess the effectiveness of the approach w.r.t. different learning paradigms, network architectures, and loss functions. All the evaluations have been performed using theOxfordRobotcar Dataset, which encompasses a wide range of weather and light conditions.▽ MoreGlobal localization is an important and widely studied problem for many robotic applications. Place recognition approaches can be exploited to solve this task, e.g., in the autonomous driving field. While most vision-based approaches match an image w.r.t. an image database, global visual localization within LiDAR-maps remains fairly unexplored, even though the path toward high definition 3D maps, produced mainly from LiDARs, is clear. In this work we leverage Deep Neural Network (DNN) approaches to create a shared embedding space between images and LiDAR-maps, allowing for image to 3D-LiDAR place recognition. We trained a 2D and a 3D DNN that create embeddings, respectively from images and from point clouds, that are close to each other whether they refer to the same place. An extensive experimental activity is presented to assess the effectiveness of the approach w.r.t. different learning paradigms, network architectures, and loss functions. All the evaluations have been performed using theOxfordRobotcar Dataset, which encompasses a wide range of weather and light conditions.△ Less"
Quantum Error Correction,Authors:Todd A. Brun,"Abstract:Quantum error correction is a set of methods to protect quantum information--that is, quantum states--from unwanted environmental interactions (decoherence) and other forms of noise. The information is stored in a quantum error-correcting code, which is a subspace in a larger Hilbert space. This code is designed so that the most common errors move the state into an error space orthogonal to the or…▽ MoreQuantum error correction is a set of methods to protect quantum information--that is, quantum states--from unwanted environmental interactions (decoherence) and other forms of noise. The information is stored in a quantum error-correcting code, which is a subspace in a larger Hilbert space. This code is designed so that the most common errors move the state into an error space orthogonal to the original code space while preserving the information in the state. It is possible to determine whether an error has occurred by a suitable measurement and to apply a unitary correction that returns the state to the code space, without measuring (and hence disturbing) the protected state itself. In general, codewords of a quantum code are entangled states. No code that stores information can protect against all possible errors; instead, codes are designed to correct a specific error set, which should be chosen to match the most likely types of noise. An error set is represented by a set of operators that can multiply the codeword state. Quantum error correction is used to protect information in quantum communication (where quantum states pass through noisy channels) and quantum computation (where quantum states are transformed through a sequence of imperfect computational steps in the presence of environmental decoherence to solve a computational problem). In quantum computation, error correction is just one component of fault-tolerant design.△ Less"
The Partonic Content of Nucleons and Nuclei,Authors:Juan Rojo,"Abstract:Deepening our knowledge of the partonic content of nucleons and nuclei represents a central endeavour of modern high-energy and nuclear physics, with ramifications in related disciplines such as astroparticle physics. There are two main scientific drivers motivating these investigations of the partonic structure of hadrons. On the one hand, addressing fundamental open issues in our understanding i…▽ MoreDeepening our knowledge of the partonic content of nucleons and nuclei represents a central endeavour of modern high-energy and nuclear physics, with ramifications in related disciplines such as astroparticle physics. There are two main scientific drivers motivating these investigations of the partonic structure of hadrons. On the one hand, addressing fundamental open issues in our understanding in the strong interactions such as the origin of the nucleon mass, spin, and transverse structure; the presence of heavy quarks in the nucleon wave function; and the possible onset of novel gluon-dominated dynamical regimes. On the other hand, pinning down with the highest possible precision the substructure of nucleons and nuclei is a central component for theoretical predictions in a wide range of experiments, from proton and heavy ion collisions at the Large Hadron Collider to ultra-high energy neutrino interactions at neutrino telescopes. This Article presents a succinct non-technical overview of our modern understanding of the quark, gluon, and photon substructure of nucleons and nuclei, focusing on recent trends and results and discussing future perspectives for the field.△ Less"
One-Dimensional Lieb-OxfordBounds,"Authors:Andre Laestadius,Fabian M Faulstich","Abstract:We investigate and prove Lieb-Oxfordbounds in one dimension by studying convex potentials that approximate the ill-defined Coulomb potential. A Lieb-Oxfordinequality establishes a bound of the indirect interaction energy for electrons in terms of the one-body particle densityρ_ψof a wave functionψ. Our results…▽ MoreWe investigate and prove Lieb-Oxfordbounds in one dimension by studying convex potentials that approximate the ill-defined Coulomb potential. A Lieb-Oxfordinequality establishes a bound of the indirect interaction energy for electrons in terms of the one-body particle densityρ_ψof a wave functionψ. Our results include modified soft Coulomb potential and regularized Coulomb potential. For these potentials, we establish Lieb-Oxford-type bounds utilizing logarithmic expressions of the particle density. Furthermore, a previous conjectured formI_\mathrm{xc}(ψ)\geq - C_1 \int_{\mathbb R} ρ_ψ(x)^{2} \mathrm{d}xis discussed for different convex potentials.△ Less"
Quillen's Theorem A and the Whitehead theorem for bicategories,"Authors:Niles Johnson,Donald Yau","Abstract:We prove a bicategorical analogue of Quillen's Theorem A. As an application, we deduce the well-known result that a pseudofunctor is a biequivalence if and only if it is essentially surjective on objects, essentially full on 1-cells, and fully faithful on 2-cells.We prove a bicategorical analogue of Quillen's Theorem A. As an application, we deduce the well-known result that a pseudofunctor is a biequivalence if and only if it is essentially surjective on objects, essentially full on 1-cells, and fully faithful on 2-cells.△ Less"
A bicategorical pasting theorem,"Authors:Niles Johnson,Donald Yau","Abstract:We provide an elementary proof of a bicategorical pasting theorem that does not rely on Power's 2-categorical pasting theorem, the bicategorical coherence theorem, or the local characterization of a biequivalence.We provide an elementary proof of a bicategorical pasting theorem that does not rely on Power's 2-categorical pasting theorem, the bicategorical coherence theorem, or the local characterization of a biequivalence.△ Less"
Toy Models of Top Down Causation,Authors:Adrian Kent,"Abstract:Models in which causation arises from higher level structures as well as from microdynamics may be relevant to unifying quantum theory with classical physics or general relativity. They also give a way of defining a form of panprotopsychist property dualism, in which consciousness and material physics causally affect one another. I describe toy models based on probabilistic cellular automata that…▽ MoreModels in which causation arises from higher level structures as well as from microdynamics may be relevant to unifying quantum theory with classical physics or general relativity. They also give a way of defining a form of panprotopsychist property dualism, in which consciousness and material physics causally affect one another. I describe toy models based on probabilistic cellular automata that illustrate possibilities and difficulties with these ideas.△ Less"
Deep Mangoes: from fruit detection to cultivar identification in colour images of mango trees,"Authors:Philippe Borianne,Frederic Borne,Julien Sarron,Emile Faye","Abstract:This paper presents results on the detection and identification mango fruits from colour images of trees. We evaluate the behaviour and the performances of the Faster R-CNN network to determine whether it is robust enough to ""detect and classify"" fruits under particularly heterogeneous conditions in terms of plant cultivars, plantation scheme, and visual information acquisition contexts. The netwo…▽ MoreThis paper presents results on the detection and identification mango fruits from colour images of trees. We evaluate the behaviour and the performances of the Faster R-CNN network to determine whether it is robust enough to ""detect and classify"" fruits under particularly heterogeneous conditions in terms of plant cultivars, plantation scheme, and visual information acquisition contexts. The network is trained to distinguish the 'Kent', 'Keitt', and ""Boucodiekhal"" mango cultivars from 3,000 representative labelled fruit annotations. The validation set composed of about 7,000 annotations was then tested with a confidence threshold of 0.7 and a Non-Maximal-Suppression threshold of 0.25. With a F1-score of 0.90, the Faster R-CNN is well suitable to the simple fruit detection in tiles of 500x500 pixels. We then combine a multi-tiling approach with a Jaccard matrix to merge the different parts of objects detected several times, and thus report the detections made at the tile scale to the native 6,000x4,000 pixel size images. Nonetheless with a F1-score of 0.56, the cultivar identification Faster R-CNN network presents some limitations for simultaneously detecting the mango fruits and identifying their respective cultivars. Despite the proven errors in fruit detection, the cultivar identification rates of the detected mango fruits are in the order of 80%. The ideal solution could combine a Mask R-CNN for the image pre-segmentation of trees and a double-stream Faster R-CNN for detecting the mango fruits and identifying their respective cultivar to provide predictions more relevant to users' expectations.△ Less"
Deep Aggregation of Regional Convolutional Activations for Content Based Image Retrieval,"Authors:Konstantin Schall,Kai Uwe Barthel,Nico Hezel,Klaus Jung","Abstract:…the backbone neural network and to learn the aggregation weights. Our method achieves state-of-the-art results for the INRIA Holidays data set and competitive results for theOxfordBuildings and Paris data sets while reducing the training time significantly.▽ MoreOne of the key challenges of deep learning based image retrieval remains in aggregating convolutional activations into one highly representative feature vector. Ideally, this descriptor should encode semantic, spatial and low level information. Even though off-the-shelf pre-trained neural networks can already produce good representations in combination with aggregation methods, appropriate fine tuning for the task of image retrieval has shown to significantly boost retrieval performance. In this paper, we present a simple yet effective supervised aggregation method built on top of existing regional pooling approaches. In addition to the maximum activation of a given region, we calculate regional average activations of extracted feature maps. Subsequently, weights for each of the pooled feature vectors are learned to perform a weighted aggregation to a single feature vector. Furthermore, we apply our newly proposed NRA loss function for deep metric learning to fine tune the backbone neural network and to learn the aggregation weights. Our method achieves state-of-the-art results for the INRIA Holidays data set and competitive results for theOxfordBuildings and Paris data sets while reducing the training time significantly.△ Less"
Dynamic survival prediction in intensive care units from heterogeneous time series without the need for variable selection or pre-processing,"Authors:Jacob Deasy,Pietro Liò,Ari Ercole","Abstract:…Neural Network (RNN) to provide early prediction of in-patient mortality risk. We compared mortality predictions with the Simplified Acute Physiology Score II (SAPS II) and theOxfordAcute Severity of Illness Score (OASIS). Data were split into an independent test set (10%) and a ten-fold cross-validation was carried out during training to avoid overfitting…▽ MoreWe present a machine learning pipeline and model that uses the entire uncurated EHR for prediction of in-hospital mortality at arbitrary time intervals, using all available chart, lab and output events, without the need for pre-processing or feature engineering. Data for more than 45,000 American ICU patients from the MIMIC-III database were used to develop an ICU mortality prediction model. All chart, lab and output events were treated by the model in the same manner inspired by Natural Language Processing (NLP). Patient events were discretized by percentile and mapped to learnt embeddings before being passed to a Recurrent Neural Network (RNN) to provide early prediction of in-patient mortality risk. We compared mortality predictions with the Simplified Acute Physiology Score II (SAPS II) and theOxfordAcute Severity of Illness Score (OASIS). Data were split into an independent test set (10%) and a ten-fold cross-validation was carried out during training to avoid overfitting. 13,233 distinct variables with heterogeneous data types were included without manual selection or pre-processing. Recordings in the first few hours of a patient's stay were found to be strongly predictive of mortality, outperforming models using SAPS II and OASIS scores within just 2 hours and achieving a state of the art Area Under the Receiver Operating Characteristic (AUROC) value of 0.80 (95% CI 0.79-0.80) at 12 hours vs 0.70 and 0.66 for SAPS II and OASIS at 24 hours respectively. Our model achieves a very strong performance of AUROC 0.86 (95% CI 0.85-0.86) for in-patient mortality prediction after 48 hours on the MIMIC-III dataset. Predictive performance increases over the first 48 hours of the ICU stay, but suffers from diminishing returns, providing rationale for time-limited trials of critical care and suggesting that the timing of decision making can be optimised and individualised.△ Less"
CalBehav: A Machine Learning based Personalized Calendar Behavioral Model using Time-Series Smartphone Data,"Authors:Iqbal H. Sarker,Alan Colman,Jun Han,A. S. M. Kayes,Paul Watters","Abstract:The electronic calendar is a valuable resource nowadays for managing our daily life appointments or schedules, also known as events, ranging from professional to highly personal. Researchers have studied various types of calendar events to predict smartphone user behavior for incoming mobile communications. However, these studies typically do not take into account behavioral variations between ind…▽ MoreThe electronic calendar is a valuable resource nowadays for managing our daily life appointments or schedules, also known as events, ranging from professional to highly personal. Researchers have studied various types of calendar events to predict smartphone user behavior for incoming mobile communications. However, these studies typically do not take into account behavioral variations between individuals. In the real world, smartphone users can differ widely from each other in how they respond to incoming communications during their scheduled events. Moreover, an individual user may respond the incoming communications differently in different contexts subject to what type of event is scheduled in her personal calendar. Thus, a static calendar-based behavioral model for individual smartphone users does not necessarily reflect their behavior to the incoming communications. In this paper, we present a machine learning based context-aware model that is personalized and dynamically identifies individual's dominant behavior for their scheduled events using logged time-series smartphone data, and shortly name as ``CalBehav''. The experimental results based on real datasets from calendar and phone logs, show that this data-driven personalized model is more effective for intelligently managing the incoming mobile communications compared to existing calendar-based approaches.△ Less"
The formation of the Martian moons,"Authors:Pascal Rosenblatt,Ryuki Hyodo,Francesco C. Pignatale,Antony Trinh,Sébastien Charnoz,Kevin M. Dunseath,Mariko Terao-Dunseath,Hidenori Genda","Abstract:Almost all the planets of our solar system have moons. Each planetary system has however unique characteristics. The Martian system has not one single big moon like the Earth, not tens of moons of various sizes like for the giant planets, but two small moons: Phobos and Deimos. How did form such a system? This question is still being investigated on the basis of the Earth-based and space-borne obs…▽ MoreAlmost all the planets of our solar system have moons. Each planetary system has however unique characteristics. The Martian system has not one single big moon like the Earth, not tens of moons of various sizes like for the giant planets, but two small moons: Phobos and Deimos. How did form such a system? This question is still being investigated on the basis of the Earth-based and space-borne observations of the Martian moons and of the more modern theories proposed to account for the formation of other moon systems. The most recent scenario of formation of the Martian moons relies on a giant impact occurring at early Mars history and having also formed the so-called hemispheric crustal dichotomy. This scenario accounts for the current orbits of both moons unlike the scenario of capture of small size asteroids. It also predicts a composition of disk material as a mixture of Mars and impactor materials that is in agreement with remote sensing observations of both moon surfaces, which suggests a composition different from Mars. The composition of the Martian moons is however unclear, given the ambiguity on the interpretation of the remote sensing observations. The study of the formation of the Martian moon system has improved our understanding of moon formation of terrestrial planets: The giant collision scenario can have various outcomes and not only a big moon as for the Earth. This scenario finds a natural place in our current vision of the early solar system when conditions were favorable for giant collisions to occur. The next step in exploration of Martian moon is a sample return mission to test the giant collision scenario for their origin, and to provide tests of models of early solar system dynamics since Mars may retain material exchanged between the inner and outer solar system.△ Less"
Phoretic Active Matter,Authors:Ramin Golestanian,"Abstract:These notes are an account of a series of lectures given at the Les Houches Summer School ""Active Matter and Non-equilibrium Statistical Physics"" during August and September 2018. The lectures can be viewed online at [http://www.ds.mpg.de/lmp/lectures/les-houches-2018].These notes are an account of a series of lectures given at the Les Houches Summer School ""Active Matter and Non-equilibrium Statistical Physics"" during August and September 2018. The lectures can be viewed online at [http://www.ds.mpg.de/lmp/lectures/les-houches-2018].△ Less"
TheOxfordRadar RobotCar Dataset: A Radar Extension to theOxfordRobotCar Dataset,"Authors:Dan Barnes,Matthew Gadd,Paul Murcutt,Paul Newman,Ingmar Posner","Abstract:In this paper we present TheOxfordRadar RobotCar Dataset, a new dataset for researching scene understanding using Millimetre-Wave FMCW scanning radar data. The target application is autonomous vehicles where this modality is robust to environmental conditions such as fog, rain, snow, or lens flare, which typically challenge other sensor modalities such as…▽ MoreIn this paper we present TheOxfordRadar RobotCar Dataset, a new dataset for researching scene understanding using Millimetre-Wave FMCW scanning radar data. The target application is autonomous vehicles where this modality is robust to environmental conditions such as fog, rain, snow, or lens flare, which typically challenge other sensor modalities such as vision and LIDAR.
  The data were gathered in January 2019 over thirty-two traversals of a centralOxfordroute spanning a total of 280km of urban driving. It encompasses a variety of weather, traffic, and lighting conditions. This 4.7TB dataset consists of over 240,000 scans from a Navtech CTS350-X radar and 2.4 million scans from two Velodyne HDL-32E 3D LIDARs; along with six cameras, two 2D LIDARs, and a GPS/INS receiver. In addition we release ground truth optimised radar odometry to provide an additional impetus to research in this domain. The full dataset is available for download at: ori.ox.ac.uk/datasets/radar-robotcar-dataset△ Less"
No Fear of the Dark: Image Retrieval under Varying Illumination Conditions,"Authors:Tomas Jenicek,Ondřej Chum","Abstract:…outperform standard approaches in varying illumination conditions, while staying on par with the state-of-the-art methods on daylight illumination benchmarks, such asOxfordor Paris datasets.▽ MoreImage retrieval under varying illumination conditions, such as day and night images, is addressed by image preprocessing, both hand-crafted and learned. Prior to extracting image descriptors by a convolutional neural network, images are photometrically normalised in order to reduce the descriptor sensitivity to illumination changes. We propose a learnable normalisation based on the U-Net architecture, which is trained on a combination of single-camera multi-exposure images and a newly constructed collection of similar views of landmarks during day and night. We experimentally show that both hand-crafted normalisation based on local histogram equalisation and the learnable normalisation outperform standard approaches in varying illumination conditions, while staying on par with the state-of-the-art methods on daylight illumination benchmarks, such asOxfordor Paris datasets.△ Less"
OxfordHandbook on AI Ethics Book Chapter on Race and Gender,Authors:Timnit Gebru,"Abstract:From massive face-recognition-based surveillance and machine-learning-based decision systems predicting crime recidivism rates, to the move towards automated health diagnostic systems, artificial intelligence (AI) is being used in scenarios that have serious consequences in people's lives. However, this rapid permeation of AI into society has not been accompanied by a thorough investigation of the…▽ MoreFrom massive face-recognition-based surveillance and machine-learning-based decision systems predicting crime recidivism rates, to the move towards automated health diagnostic systems, artificial intelligence (AI) is being used in scenarios that have serious consequences in people's lives. However, this rapid permeation of AI into society has not been accompanied by a thorough investigation of the sociopolitical issues that cause certain groups of people to be harmed rather than advantaged by it. For instance, recent studies have shown that commercial face recognition systems have much higher error rates for dark skinned women while having minimal errors on light skinned men. A 2016 ProPublica investigation uncovered that machine learning based tools that assess crime recidivism rates in the US are biased against African Americans. Other studies show that natural language processing tools trained on newspapers exhibit societal biases (e.g. finishing the analogy ""Man is to computer programmer as woman is to X"" by homemaker). At the same time, books such as Weapons of Math Destruction and Automated Inequality detail how people in lower socioeconomic classes in the US are subjected to more automated decision making tools than those who are in the upper class. Thus, these tools are most often used on people towards whom they exhibit the most bias. While many technical solutions have been proposed to alleviate bias in machine learning systems, we have to take a holistic and multifaceted approach. This includes standardization bodies determining what types of systems can be used in which scenarios, making sure that automated decision tools are created by people from diverse backgrounds, and understanding the historical and political factors that disadvantage certain groups who are subjected to these tools.△ Less"
Towards automated symptoms assessment in mental health,Authors:Maxim Osipov,"Abstract:Activity and motion analysis has the potential to be used as a diagnostic tool for mental disorders. However, to-date, little work has been performed in turning stratification measures of activity into useful symptom markers. The research presented in this thesis has focused on the identification of objective activity and behaviour metrics that could be useful for the analysis of mental health sym…▽ MoreActivity and motion analysis has the potential to be used as a diagnostic tool for mental disorders. However, to-date, little work has been performed in turning stratification measures of activity into useful symptom markers. The research presented in this thesis has focused on the identification of objective activity and behaviour metrics that could be useful for the analysis of mental health symptoms in the above mentioned dimensions. Particular attention is given to the analysis of objective differences between disorders, as well as identification of clinical episodes of mania and depression in bipolar patients, and deterioration in borderline personality disorder patients. A principled framework is proposed for mHealth monitoring of psychiatric patients, based on measurable changes in behaviour, represented in physical activity time series, collected via mobile and wearable devices. The framework defines methods for direct computational analysis of symptoms in disorganisation and psychomotor dimensions, as well as measures for indirect assessment of mood, using patterns of physical activity, sleep and circadian rhythms. The approach of computational behaviour analysis, proposed in this thesis, has the potential for early identification of clinical deterioration in ambulatory patients, and allows for the specification of distinct and measurable behavioural phenotypes, thus enabling better understanding and treatment of mental disorders.△ Less"
Dual Adversarial Inference for Text-to-Image Synthesis,"Authors:Qicheng Lao,Mohammad Havaei,Ahmad Pesaranghader,Francis Dutil,Lisa Di Jorio,Thomas Fevens","Abstract:…certain meaningful information present in the image that are not well described in the text. The new framework also improves the quality of synthesized images when evaluated onOxford-102, CUB and COCO datasets.▽ MoreSynthesizing images from a given text description involves engaging two types of information: the content, which includes information explicitly described in the text (e.g., color, composition, etc.), and the style, which is usually not well described in the text (e.g., location, quantity, size, etc.). However, in previous works, it is typically treated as a process of generating images only from the content, i.e., without considering learning meaningful style representations. In this paper, we aim to learn two variables that are disentangled in the latent space, representing content and style respectively. We achieve this by augmenting current text-to-image synthesis frameworks with a dual adversarial inference mechanism. Through extensive experiments, we show that our model learns, in an unsupervised manner, style representations corresponding to certain meaningful information present in the image that are not well described in the text. The new framework also improves the quality of synthesized images when evaluated onOxford-102, CUB and COCO datasets.△ Less"
Local Supports Global: Deep Camera Relocalization with Sequence Enhancement,"Authors:Fei Xue,Xin Wang,Zike Yan,Qiuyuan Wang,Junqiu Wang,Hongbin Zha","Abstract:…poses provided by the VO component as additional motion constraints. Thus, the global consistency can be guaranteed. Experiments on the public indoor 7-Scenes and outdoorOxfordRobotCar benchmark datasets demonstrate that benefited from local information inherent in the sequence, our approach outperforms state-of-the-art methods, especially in some challeng…▽ MoreWe propose to leverage the local information in image sequences to support global camera relocalization. In contrast to previous methods that regress global poses from single images, we exploit the spatial-temporal consistency in sequential images to alleviate uncertainty due to visual ambiguities by incorporating a visual odometry (VO) component. Specifically, we introduce two effective steps called content-augmented pose estimation and motion-based refinement. The content-augmentation step focuses on alleviating the uncertainty of pose estimation by augmenting the observation based on the co-visibility in local maps built by the VO stream. Besides, the motion-based refinement is formulated as a pose graph, where the camera poses are further optimized by adopting relative poses provided by the VO component as additional motion constraints. Thus, the global consistency can be guaranteed. Experiments on the public indoor 7-Scenes and outdoorOxfordRobotCar benchmark datasets demonstrate that benefited from local information inherent in the sequence, our approach outperforms state-of-the-art methods, especially in some challenging cases, e.g., insufficient texture, highly repetitive textures, similar appearances, and over-exposure.△ Less"
Unraveling the hidden organisation of urban systems and their mobility flows,"Authors:Riccardo Gallotti,Giulia Bertagnolli,Manlio De Domenico","Abstract:Increasing evidence suggests that cities are complex systems, with structural and dynamical features responsible for a broad spectrum of emerging phenomena. Here we use a unique data set of human flows and couple it with information on the underlying street network to study, simultaneously, the structural and functional organisation of 10 world megacities. We quantify the efficiency of flow exchan…▽ MoreIncreasing evidence suggests that cities are complex systems, with structural and dynamical features responsible for a broad spectrum of emerging phenomena. Here we use a unique data set of human flows and couple it with information on the underlying street network to study, simultaneously, the structural and functional organisation of 10 world megacities. We quantify the efficiency of flow exchange between areas of a city in terms of integration and segregation using well defined measures. Results reveal unexpected complex patterns that shed new light on urban organisation. Large cities tend to be more segregated and less integrated, while their overall topological organisation resembles that of small world networks. At the same time, the heterogeneity of flows distribution might act as a catalyst for further integrating a city. Our analysis unravels how human behaviour influences, and is influenced by, the urban environment, suggesting quantitative indicators to control integration and segregation of human flows that can be used, among others, for restriction policies to adopt during emergencies and, as an interesting byproduct, allows us to characterise functional (dis)similarities of different metropolitan areas, countries, and cultures.△ Less"
Envelopes of embedded super-Earths I. Two-dimensional simulations,"Authors:William Béthune,Roman R. Rafikov","Abstract:Measurements of exoplanetary masses and radii have revealed a population of massive super-Earths --- planets sufficiently large that, according to one dimensional models, they should have turned into gas giants. To better understand the origin of these objects, we carry out hydrodynamical simulations of planetary cores embedded in a nascent protoplanetary disk. In this first paper of a series, to…▽ MoreMeasurements of exoplanetary masses and radii have revealed a population of massive super-Earths --- planets sufficiently large that, according to one dimensional models, they should have turned into gas giants. To better understand the origin of these objects, we carry out hydrodynamical simulations of planetary cores embedded in a nascent protoplanetary disk. In this first paper of a series, to gain intuition as well as to develop useful diagnostics, we focus on two-dimensional simulations of the flow around protoplanetary cores. We use the pluto code to study isothermal and adiabatic envelopes around cores of sub- to super-thermal masses, fully resolving the envelope properties down to the core surface. Owing to the conservation of vortensity, envelopes acquire a substantial degree of rotational support when the core mass increases beyond the thermal mass, suggesting a limited applicability of one-dimensional models for describing the envelope structure. The finite size of the core (relatively large for super-Earths) also controls the amount of rotational support in the entire envelope. Steady non-axisymmetric shocks develop in the supersonic envelopes of high-mass cores, triggering mass accretion and turbulent mixing in their interiors. We also examine the influence of the gas self-gravity on the envelope structure. Although it only weakly alters the properties of the envelopes, the gas gravity has significant effect on the properties of the density waves triggered by the core in the protoplanetary disk.△ Less"
"Filter Early, Match Late: Improving Network-Based Visual Place Recognition","Authors:Stephen Hausler,Adam Jacobson,Michael Milford","Abstract:…experimental analysis is performed to determine the full scope of causality between early layer filtering and late layer extraction. For validity, we use three datasets:OxfordRobotCar, Nordland, and Gardens Point, achieving overall superior performance to NetVLAD. The work provides a number of new avenues for exploring CNN optimizations, without full re-tr…▽ MoreCNNs have excelled at performing place recognition over time, particularly when the neural network is optimized for localization in the current environmental conditions. In this paper we investigate the concept of feature map filtering, where, rather than using all the activations within a convolutional tensor, only the most useful activations are used. Since specific feature maps encode different visual features, the objective is to remove feature maps that are detract from the ability to recognize a location across appearance changes. Our key innovation is to filter the feature maps in an early convolutional layer, but then continue to run the network and extract a feature vector using a later layer in the same network. By filtering early visual features and extracting a feature vector from a higher, more viewpoint invariant later layer, we demonstrate improved condition and viewpoint invariance. Our approach requires image pairs for training from the deployment environment, but we show that state-of-the-art performance can regularly be achieved with as little as a single training image pair. An exhaustive experimental analysis is performed to determine the full scope of causality between early layer filtering and late layer extraction. For validity, we use three datasets:OxfordRobotCar, Nordland, and Gardens Point, achieving overall superior performance to NetVLAD. The work provides a number of new avenues for exploring CNN optimizations, without full re-training.△ Less"
REMAP: Multi-layer entropy-guided pooling of dense CNN features for image retrieval,"Authors:Syed Sameed Husain,Miroslaw Bober","Abstract:…controlled by SGD. The entire framework is trained in an end-to-end fashion, outperforming the latest state-of-the-art results. On image retrieval datasets Holidays,Oxfordand MPEG, the REMAP descriptor achieves mAP of 95.5%, 91.5%, and 80.1% respectively, outperforming any results published to date. REMAP also formed the core of the winning submission to t…▽ MoreThis paper addresses the problem of very large-scale image retrieval, focusing on improving its accuracy and robustness. We target enhanced robustness of search to factors such as variations in illumination, object appearance and scale, partial occlusions, and cluttered backgrounds - particularly important when search is performed across very large datasets with significant variability. We propose a novel CNN-based global descriptor, called REMAP, which learns and aggregates a hierarchy of deep features from multiple CNN layers, and is trained end-to-end with a triplet loss. REMAP explicitly learns discriminative features which are mutually-supportive and complementary at various semantic levels of visual abstraction. These dense local features are max-pooled spatially at each layer, within multi-scale overlapping regions, before aggregation into a single image-level descriptor. To identify the semantically useful regions and layers for retrieval, we propose to measure the information gain of each region and layer using KL-divergence. Our system effectively learns during training how useful various regions and layers are and weights them accordingly. We show that such relative entropy-guided aggregation outperforms classical CNN-based aggregation controlled by SGD. The entire framework is trained in an end-to-end fashion, outperforming the latest state-of-the-art results. On image retrieval datasets Holidays,Oxfordand MPEG, the REMAP descriptor achieves mAP of 95.5%, 91.5%, and 80.1% respectively, outperforming any results published to date. REMAP also formed the core of the winning submission to the Google Landmark Retrieval Challenge on Kaggle.△ Less"
Scale Invariant Fully Convolutional Network: Detecting Hands Efficiently,"Authors:Dan Liu,Dawei Du,Libo Zhang,Tiejian Luo,Yanjun Wu,Feiyue Huang,Siwei Lyu","Abstract:…with the state-of-the-art methods, our algorithm shows comparable accuracy and runs a 4.23 times faster speed on the VIVA dataset and achieves better average precision onOxfordhand detection dataset at a speed of 62.5 fps.▽ MoreExisting hand detection methods usually follow the pipeline of multiple stages with high computation cost, i.e., feature extraction, region proposal, bounding box regression, and additional layers for rotated region detection. In this paper, we propose a new Scale Invariant Fully Convolutional Network (SIFCN) trained in an end-to-end fashion to detect hands efficiently. Specifically, we merge the feature maps from high to low layers in an iterative way, which handles different scales of hands better with less time overhead comparing to concatenating them simply. Moreover, we develop the Complementary Weighted Fusion (CWF) block to make full use of the distinctive features among multiple layers to achieve scale invariance. To deal with rotated hand detection, we present the rotation map to get rid of complex rotation and derotation layers. Besides, we design the multi-scale loss scheme to accelerate the training process significantly by adding supervision to the intermediate layers of the network. Compared with the state-of-the-art methods, our algorithm shows comparable accuracy and runs a 4.23 times faster speed on the VIVA dataset and achieves better average precision onOxfordhand detection dataset at a speed of 62.5 fps.△ Less"
Four Things Everyone Should Know to Improve Batch Normalization,"Authors:Cecilia Summers,Michael J. Dinneen","Abstract:…for very small batch sizes by combining the strengths of Batch and Group Normalization. We validate our results empirically on six datasets: CIFAR-100, SVHN, Caltech-256,OxfordFlowers-102, CUB-2011, and ImageNet.▽ MoreA key component of most neural network architectures is the use of normalization layers, such as Batch Normalization. Despite its common use and large utility in optimizing deep architectures, it has been challenging both to generically improve upon Batch Normalization and to understand the circumstances that lend themselves to other enhancements. In this paper, we identify four improvements to the generic form of Batch Normalization and the circumstances under which they work, yielding performance gains across all batch sizes while requiring no additional computation during training. These contributions include proposing a method for reasoning about the current example in inference normalization statistics, fixing a training vs. inference discrepancy; recognizing and validating the powerful regularization effect of Ghost Batch Normalization for small and medium batch sizes; examining the effect of weight decay regularization on the scaling and shifting parameters gamma and beta; and identifying a new normalization algorithm for very small batch sizes by combining the strengths of Batch and Group Normalization. We validate our results empirically on six datasets: CIFAR-100, SVHN, Caltech-256,OxfordFlowers-102, CUB-2011, and ImageNet.△ Less"
Generalising the Wallis Product,Authors:Joshua W. E. Farrell,"Abstract:In 1655, John Wallis whilst at the University ofOxforddiscovered the famous and beautiful formula for pi, now known as Wallis' Product. Since then, several analogous formulae have been discovered generalising the original. One more modern proof of the Wallis Product and its relatives directly uses the Gamma Function. This short paper will use similar t…▽ MoreIn 1655, John Wallis whilst at the University ofOxforddiscovered the famous and beautiful formula for pi, now known as Wallis' Product. Since then, several analogous formulae have been discovered generalising the original. One more modern proof of the Wallis Product and its relatives directly uses the Gamma Function. This short paper will use similar techniques to understand certain related classes of infinite products. Almost all results within this paper are new findings made by myself; when I should be revising or completing assignment work I find myself always going back to this.△ Less"
Higher Correlations and the Alternative Hypothesis,"Authors:Jeffrey C. Lagarias,Brad Rodgers","Abstract:The Alternative Hypothesis concerns a hypothetical and unlikely picture of how zeros of the Riemann zeta function are spaced which one would like to rule out. In the Alternative Hypothesis, the renormalized distance between nontrivial zeros is supposed to always lie at a half integer. It is known that the Alternative Hypothesis is compatible with what is known about the pair correlation function o…▽ MoreThe Alternative Hypothesis concerns a hypothetical and unlikely picture of how zeros of the Riemann zeta function are spaced which one would like to rule out. In the Alternative Hypothesis, the renormalized distance between nontrivial zeros is supposed to always lie at a half integer. It is known that the Alternative Hypothesis is compatible with what is known about the pair correlation function of zeta zeros. We ask whether what is currently known about higher correlation functions of the zeros is sufficient to rule out the Alternative Hypothesis and show by construction of an explicit counterexample point process that it is not. A similar result was recently independently obtained by T. Tao, using slightly different methods.
  We also apply the ergodic theorem to this point process to show there exists a deterministic collection of points lying in\tfrac{1}{2}\mathbb{Z}which satisfy the Alternative Hypothesis spacing but mimic all statistics which are currently known about zeros of the zeta function.△ Less"
Disparity-based HDR imaging,"Authors:Jennifer Bonnard,Gilles Valette,Céline Loscos","Abstract:High-dynamic range imaging permits to extend the dynamic range of intensity values to get close to what the human eye is able to perceive. Although there has been a huge progress in the digital camera sensor range capacity, the need of capturing several exposures in order to reconstruct high-dynamic range values persist. In this paper, we present a study on how to acquire high-dynamic range values…▽ MoreHigh-dynamic range imaging permits to extend the dynamic range of intensity values to get close to what the human eye is able to perceive. Although there has been a huge progress in the digital camera sensor range capacity, the need of capturing several exposures in order to reconstruct high-dynamic range values persist. In this paper, we present a study on how to acquire high-dynamic range values for multi-stereo images. In many papers, disparity has been used to register pixels of different images and guide the reconstruction. In this paper, we show the limitations of such approaches and propose heuristics as solutions to identified problematic cases.△ Less"
Modelling baryonic feedback for survey cosmology,"Authors:Nora Elisa Chisari,Alexander J. Mead,Shahab Joudaki,Pedro Ferreira,Aurel Schneider,Joseph Mohr,Tilman Tröster,David Alonso,Ian G. McCarthy,Sergio Martin-Alvarez,Julien Devriendt,Adrianne Slyz,Marcel P. van Daalen","Abstract:…, as a function of scale and redshift, and of the observables derived from this quantity. This work is the result of a workshop held at the University ofOxfordin November of 2018.▽ MoreObservational cosmology in the next decade will rely on probes of the distribution of matter in the redshift range between0<z<3to elucidate the nature of dark matter and dark energy. In this redshift range, galaxy formation is known to have a significant impact on observables such as two-point correlations of galaxy shapes and positions, altering their amplitude and scale dependence beyond the expected statistical uncertainty of upcoming experiments at separations under 10 Mpc. Successful extraction of information in such a regime thus requires, at the very least, unbiased models for the impact of galaxy formation on the matter distribution, and can benefit from complementary observational priors. This work reviews the current state of the art in the modelling of baryons for cosmology, from numerical methods to approximate analytical prescriptions, and makes recommendations for studies in the next decade, including a discussion of potential probe combinations that can help constrain the role of baryons in cosmological studies. We focus, in particular, on the modelling of the matter power spectrum,P(k,z), as a function of scale and redshift, and of the observables derived from this quantity. This work is the result of a workshop held at the University ofOxfordin November of 2018.△ Less"
Proceedings ML Family / OCaml Users and Developers workshops,"Authors:Sam Lindley,Gabriel Scherer","Abstract:This volume contains the joint post-proceedings of the 2017 editions of the ML Family Workshop and the OCaml Users and Developers Workshop which took place inOxford, UK, September 2017, and which were colocated with the ICFP 2017 conference.This volume contains the joint post-proceedings of the 2017 editions of the ML Family Workshop and the OCaml Users and Developers Workshop which took place inOxford, UK, September 2017, and which were colocated with the ICFP 2017 conference.△ Less"
Occlusion-Robust MVO: Multimotion Estimation Through Occlusion Via Motion Closure,"Authors:Kevin M. Judd,Jonathan D. Gammell","Abstract:…the reappearance of motions through motion closure. The performance of this occlusion-robust multimotion visual odometry (MVO) pipeline is evaluated on real-world data and theOxfordMultimotion Dataset.▽ MoreVisual motion estimation is an integral and well-studied challenge in autonomous navigation. Recent work has focused on addressing multimotion estimation, which is especially challenging in highly dynamic environments. Such environments not only comprise multiple, complex motions but also tend to exhibit significant occlusion.
  Previous work in object tracking focuses on maintaining the integrity of object tracks but usually relies on specific appearance-based descriptors or constrained motion models. These approaches are very effective in specific applications but do not generalize to the full multimotion estimation problem.
  This paper presents a pipeline for estimating multiple motions, including the camera egomotion, in the presence of occlusions. This approach uses an expressive motion prior to estimate the SE (3) trajectory of every motion in the scene, even during temporary occlusions, and identify the reappearance of motions through motion closure. The performance of this occlusion-robust multimotion visual odometry (MVO) pipeline is evaluated on real-world data and theOxfordMultimotion Dataset.△ Less"
"Comparative analysis of radiotherapy LINAC downtime and failure modes in the UK, Nigeria and Botswana","Authors:Laurence M. Wroe,Taofeeq A. Ige,Obinna C. Asogwa,Simeon C. Aruah,Surbhi Grover,Remigio Makufa,Matthew Fitz-Gibbon,Suzanne L. Sheehy","Abstract:…any correlations between LINAC environment and performance. Logbooks kept by radiotherapy personnel on the operation of their LINAC were obtained and analysed from centres inOxford(UK), Abuja, Benin, Enugu, Lagos, Sokoto (Nigeria) and Gaborone (Botswana). By deconstructing the LINAC into 12 different subsystems, it is found that the vacuum subsystem only f…▽ MoreThe lack of radiotherapy linear accelerators (LINACs) in Low- and Middle- Income Countries (LMICs) has been recognised as a major barrier to providing quality cancer care in these regions, along with a shortfall in the number of highly qualified personnel. It is expected that additional challenges will be faced in operating precise, high tech radiotherapy equipment in these environments, and anecdotal evidence suggests that LINACs have greater downtime and higher failure rates of components than their counterparts in High-Income Countries. To guide future developments such as the design of a LINAC tailored for use in LMIC environments, it is important to take a data-driven approach to any re-engineering of the technology. However, no detailed statistical data on LINAC downtime and failure modes has been previously collected or presented in the literature. This work presents the first known comparative analysis of failure modes and downtime of current generation LINACs in radiotherapy centres, with the aim of determining any correlations between LINAC environment and performance. Logbooks kept by radiotherapy personnel on the operation of their LINAC were obtained and analysed from centres inOxford(UK), Abuja, Benin, Enugu, Lagos, Sokoto (Nigeria) and Gaborone (Botswana). By deconstructing the LINAC into 12 different subsystems, it is found that the vacuum subsystem only fails in the LMIC centres and the failure rate in an LMIC environment is more than twice as large in 6 of the 12 subsystems compared to the High Income Country (HIC). Additionally, it is shown that despite accounting for only 3.4% of total number of faults, the LINAC faults which take more than an hour to repair account for 74.6% of the total downtime. The results of this study inform future attempts to mitigate the problems affecting LINACs in LMIC environments.△ Less"
Distance Metric Learned Collaborative Representation Classifier,"Authors:Tapabrata Chakraborti,Brendan McCane,Steven Mills,Umapada Pal","Abstract:…convolutional network as the feature learner. The proposed method DML-CRC gives state-of-the-art performance on benchmark fine-grained classification datasets CUB Birds,OxfordFlowers andOxford-IIIT Pets using the VGG-19 deep network. The method is network agnostic and can be used for any similar classification tasks…▽ MoreAny generic deep machine learning algorithm is essentially a function fitting exercise, where the network tunes its weights and parameters to learn discriminatory features by minimizing some cost function. Though the network tries to learn the optimal feature space, it seldom tries to learn an optimal distance metric in the cost function, and hence misses out on an additional layer of abstraction. We present a simple effective way of achieving this by learning a generic Mahalanabis distance in a collaborative loss function in an end-to-end fashion with any standard convolutional network as the feature learner. The proposed method DML-CRC gives state-of-the-art performance on benchmark fine-grained classification datasets CUB Birds,OxfordFlowers andOxford-IIIT Pets using the VGG-19 deep network. The method is network agnostic and can be used for any similar classification tasks.△ Less"
MinCall - MinION end2end convolutional deep learning basecaller,"Authors:Neven Miculinić,Marko Ratković,Mile Šikić","Abstract:TheOxfordNanopore Technologies's MinION is the first portable DNA sequencing device. It is capable of producing long reads, over 100 kBp were reported. However, it has significantly higher error rate than other methods. In this study, we present MinCall, an end2end basecaller model for the MinION. The model is based on deep learning and uses convolutio…▽ MoreTheOxfordNanopore Technologies's MinION is the first portable DNA sequencing device. It is capable of producing long reads, over 100 kBp were reported. However, it has significantly higher error rate than other methods. In this study, we present MinCall, an end2end basecaller model for the MinION. The model is based on deep learning and uses convolutional neural networks (CNN) in its implementation. For extra performance, it uses cutting edge deep learning techniques and architectures, batch normalization and Connectionist Temporal Classification (CTC) loss. The best performing deep learning model achieves 91.4% median match rate on E. Coli dataset using R9 pore chemistry and 1D reads.△ Less"
2D3D-MatchNet: Learning to Match Keypoints Across 2D Image and 3D Point Cloud,"Authors:Mengdan Feng,Sixing Hu,Marcelo Ang,Gim Hee Lee","Abstract:…As a result, we are able to directly match and establish 2D-3D correspondences from the query image and 3D point cloud reference map for visual pose estimation. We create ourOxford2D-3D Patches dataset from theOxfordRobotcar dataset with the ground truth camera poses and 2D-3D image to point cloud correspondences…▽ MoreLarge-scale point cloud generated from 3D sensors is more accurate than its image-based counterpart. However, it is seldom used in visual pose estimation due to the difficulty in obtaining 2D-3D image to point cloud correspondences. In this paper, we propose the 2D3D-MatchNet - an end-to-end deep network architecture to jointly learn the descriptors for 2D and 3D keypoint from image and point cloud, respectively. As a result, we are able to directly match and establish 2D-3D correspondences from the query image and 3D point cloud reference map for visual pose estimation. We create ourOxford2D-3D Patches dataset from theOxfordRobotcar dataset with the ground truth camera poses and 2D-3D image to point cloud correspondences for training and testing the deep network. Experimental results verify the feasibility of our approach.△ Less"
Proceedings Twelfth Workshop on Developments in Computational Models and Ninth Workshop on Intersection Types and Related Systems,"Authors:Michele Pagani,Sandra Alves","Abstract:…at Twelfth Workshop on Developments in Computational Models (DCM 2018) and the Ninth Workshop on Intersection Types and Related Systems (ITRS 2018), held on July 8, 2018 inOxford, in affiliation with FLOC 2018.▽ MoreThis volume contains a final and revised selection of papers presented at Twelfth Workshop on Developments in Computational Models (DCM 2018) and the Ninth Workshop on Intersection Types and Related Systems (ITRS 2018), held on July 8, 2018 inOxford, in affiliation with FLOC 2018.△ Less"
Proceedings Joint International Workshop on Linearity & Trends in Linear Logic and Applications,"Authors:Thomas Ehrhard,Maribel Fernández,Valeria de Paiva,Lorenzo Tortora de Falco","Abstract:This volume contains a selection of papers presented at Linearity/TLLA 2018: Joint Linearity and TLLA workshops (part of FLOC 2018) held on July 7-8, 2018 inOxford. Linearity has been a key feature in several lines of research in both theoretical and practical approaches to computer science. On the theoretical side there is much work stemming from linear lo…▽ MoreThis volume contains a selection of papers presented at Linearity/TLLA 2018: Joint Linearity and TLLA workshops (part of FLOC 2018) held on July 7-8, 2018 inOxford. Linearity has been a key feature in several lines of research in both theoretical and practical approaches to computer science. On the theoretical side there is much work stemming from linear logic dealing with proof technology, complexity classes and more recently quantum computation. On the practical side there is work on program analysis, expressive operational semantics for programming languages, linear programming languages, program transformation, update analysis and efficient implementation techniques. Linear logic is not only a theoretical tool to analyse the use of resources in logic and computation. It is also a corpus of tools, approaches, and methodologies (proof nets, exponential decomposition, geometry of interaction, coherent spaces, relational models, etc.) that were originally developed for the study of linear logic's syntax and semantics and are nowadays applied in several other fields.△ Less"
Lessons from early Earth: UV surface radiation should not limit the habitability of active M star systems,"Authors:Jack T. O'Malley-James,Lisa Kaltenegger","Abstract:The closest potentially habitable worlds outside our Solar system orbit a different kind of star than our Sun: smaller red dwarf stars. Such stars can flare frequently, bombarding their planets with biologically damaging high-energy UV radiation, placing planetary atmospheres at risk of erosion and bringing the habitability of these worlds into question. However, the surface UV flux on these world…▽ MoreThe closest potentially habitable worlds outside our Solar system orbit a different kind of star than our Sun: smaller red dwarf stars. Such stars can flare frequently, bombarding their planets with biologically damaging high-energy UV radiation, placing planetary atmospheres at risk of erosion and bringing the habitability of these worlds into question. However, the surface UV flux on these worlds is unknown. Here we show the first models of the surface UV environments of the four closest potentially habitable exoplanets: Proxima-b, TRAPPIST-1e, Ross-128b, and LHS-1140b assuming different atmospheric compositions, spanning Earth-analogue to eroded and anoxic atmospheres and compare them to levels for Earth throughout its geological evolution. Even for planet models with eroded and anoxic atmospheres, surface UV radiation remains below early Earth levels, even during flares. Given that the early Earth was inhabited, we show that UV radiation should not be a limiting factor for the habitability of planets orbiting M stars. Our closest neighbouring worlds remain intriguing targets for the search for life beyond our Solar system.△ Less"
Proceedings 7th International Workshop on Theorem proving components for Educational software,"Authors:Pedro Quaresma,Walther Neuper","Abstract:The 7th International Workshop on Theorem proving components for Educational software (ThEdu'18) was held inOxford, United Kingdom, on 18 July 2018. It was associated to the conference, Federated Logic Conference 2018 (FLoC2018).
  The major aim of the ThEdu workshop series was to link developers interested in adapting Computer Theorem Proving (TP) to t…▽ MoreThe 7th International Workshop on Theorem proving components for Educational software (ThEdu'18) was held inOxford, United Kingdom, on 18 July 2018. It was associated to the conference, Federated Logic Conference 2018 (FLoC2018).
  The major aim of the ThEdu workshop series was to link developers interested in adapting Computer Theorem Proving (TP) to the needs of education and to inform mathematicians and mathematics educators about TP's potential for educational software. Topics of interest include: methods of automated deduction applied to checking students' input; methods of automated deduction applied to prove post-conditions for particular problem solutions; combinations of deduction and computation enabling systems to propose next steps; automated provers specific for dynamic geometry systems; proof and proving in mathematics education.
  ThEdu'18 was a vibrant workshop, with one invited talk and six contributions. It triggered the post-proceedings at hand.△ Less"
Auto-Embedding Generative Adversarial Networks for High Resolution Image Synthesis,"Authors:Yong Guo,Qi Chen,Jian Chen,Qingyao Wu,Qinfeng Shi,Mingkui Tan","Abstract:…input noise. The resultant images exhibit better perceptual photo-realism, i.e., with sharper structure and richer details, than other baselines on several datasets, includingOxford-102 Flowers, Caltech-UCSD Birds (CUB), High-Quality Large-scale CelebFaces Attributes (CelebA-HQ), Large-scale Scene Understanding (LSUN) and ImageNet.▽ MoreGenerating images via the generative adversarial network (GAN) has attracted much attention recently. However, most of the existing GAN-based methods can only produce low-resolution images of limited quality. Directly generating high-resolution images using GANs is nontrivial, and often produces problematic images with incomplete objects. To address this issue, we develop a novel GAN called Auto-Embedding Generative Adversarial Network (AEGAN), which simultaneously encodes the global structure features and captures the fine-grained details. In our network, we use an autoencoder to learn the intrinsic high-level structure of real images and design a novel denoiser network to provide photo-realistic details for the generated images. In the experiments, we are able to produce 512x512 images of promising quality directly from the input noise. The resultant images exhibit better perceptual photo-realism, i.e., with sharper structure and richer details, than other baselines on several datasets, includingOxford-102 Flowers, Caltech-UCSD Birds (CUB), High-Quality Large-scale CelebFaces Attributes (CelebA-HQ), Large-scale Scene Understanding (LSUN) and ImageNet.△ Less"
Active Materials: Biological Benchmarks and Transport Limitations,Authors:Eric R. Dufresne,"Abstract:These lecture notes were prepared for the 2018 Summer School on `Active Matter and Non-equilibrium Statistical Physics' at l'École de Physique des Houches. They survey metabolic activity across a wide range of living organisms, and consider size limitations due to the transport of fuel, waste, and heat for active materials at biomimetic levels of activity.These lecture notes were prepared for the 2018 Summer School on `Active Matter and Non-equilibrium Statistical Physics' at l'École de Physique des Houches. They survey metabolic activity across a wide range of living organisms, and consider size limitations due to the transport of fuel, waste, and heat for active materials at biomimetic levels of activity.△ Less"
PProCRC: Probabilistic Collaboration of Image Patches,"Authors:Tapabrata Chakraborti,Brendan McCane,Steven Mills,Umapada Pal","Abstract:…earlier CRC formulations: patch based (PCRC, GP-CRC) as well as the state-of-the-art probabilistic (ProCRC and EProCRC) on three fine-grained species recognition datasets (OxfordFlowers,Oxford-IIIT Pets and CUB Birds) using two CNN backbones (Vgg-19 and ResNet-50).▽ MoreWe present a conditional probabilistic framework for collaborative representation of image patches. It incorporates background compensation and outlier patch suppression into the main formulation itself, thus doing away with the need for pre-processing steps to handle the same. A closed form non-iterative solution of the cost function is derived. The proposed method (PProCRC) outperforms earlier CRC formulations: patch based (PCRC, GP-CRC) as well as the state-of-the-art probabilistic (ProCRC and EProCRC) on three fine-grained species recognition datasets (OxfordFlowers,Oxford-IIIT Pets and CUB Birds) using two CNN backbones (Vgg-19 and ResNet-50).△ Less"
Cooling rate effect on thermoremanent magnetization in archaeological baked clays: an experimental study on modern bricks,"Authors:Gwenaël Hervé,Annick Chauvin,Philippe Lanos,Pierre Rochette,Mireille Perrin,Michel Perron d'arc","Abstract:The influence of cooling rate on the intensity of thermoremanent magnetization (TRM) and the necessity to correct archaeo/palaeointensities for this effect have long been recognized. However the reliability of the correction is still questioned. We studied 35 bricks baked in two modern kilns (SK and BK) in known experimental conditions and with measurements of the direction and intensity of the ge…▽ MoreThe influence of cooling rate on the intensity of thermoremanent magnetization (TRM) and the necessity to correct archaeo/palaeointensities for this effect have long been recognized. However the reliability of the correction is still questioned. We studied 35 bricks baked in two modern kilns (SK and BK) in known experimental conditions and with measurements of the direction and intensity of the geomagnetic field at the site. The smallest kiln (SK, 0.2 m 3) cooled in around 12 hours and the biggest kiln (BK, 8 m 3) in around 40 hours. Thermomagnetic, hysteresis and backfield curves indicated that the main magnetic carriers were Ti-poor titanomagnetites and Tipoortitanohematites. The fraction of the TRM carried by Ti-poor titanohematites is the maindifference between the two sets of bricks. This fraction is around 5-10% in bricks from BK kilnand up to 40% in those from SK kiln. Intensities of the Earth's magnetic field were determinedusing the original Thellier-Thellier protocol with correction of TRM anisotropy. The averageintensities overestimate the expected field intensity by 5% (SK) and 6% (BK). This resultemphasizes the necessity of the cooling rate correction. In order to have a detailed evaluation ofthe cooling rate effect, we used several slow cooling rates: 0.8, 0.4, 0.2 and 0.1{\textdegree}C/min. Thecorrection factors obtained with the 0.8{\textdegree}C/min cooling ranged between -2% and 21% and wereproportional to the TRM fraction carried by Ti-poor titanohematite. The higher proportion ofthese grains in bricks from SK kiln led to an overestimate of the correction factor and anunderestimate of the intensity by 7%. However, the expected intensity is recovered whentemperature steps higher than 580{\textdegree}C (i.e. in the range of Ti-poor titanohematites unblockingtemperatures) were excluded from the calculation of archaeointensity and cooling rate correction.In the case of the BK kiln bricks, for which Ti-poor titanohematites does not contributesignificantly to the TRM, all tested cooling rates give average intensities close to the expectedvalue. Incorrectly estimating the duration of the archaeological cooling has therefore a lowimpact on the accuracy of the archaeointensity data on these kinds of material.△ Less"
Turbulent velocity measurements in high Reynolds cryogenic helium facilities at Service des Basses Temperatures (SBT),"Authors:Swapnil Kharche,Jean-Paul Moro,Christophe Baudet,Bernard Rousset,Andre Fuchs,Joachim Peinke,Alain Girard","Abstract:Due to its very low viscosity, cryogenic helium has been used for years to generate high Reynolds turbulent flows. The measurement of velocity fluctuations in such high Reynolds flows is however a challenging issue, as it is necessary to develop small size (typically micron-length), robust sensors, to measure the whole spectrum of fluctuations of the velocity, which may extend to hundreds of kHz a…▽ MoreDue to its very low viscosity, cryogenic helium has been used for years to generate high Reynolds turbulent flows. The measurement of velocity fluctuations in such high Reynolds flows is however a challenging issue, as it is necessary to develop small size (typically micron-length), robust sensors, to measure the whole spectrum of fluctuations of the velocity, which may extend to hundreds of kHz and possibly higher. SBT has developed for years different facilities, in particular taking benefit of the refrigeration capacities available at CEA Grenoble. In this article we present the current status of developments of hot wire sensors at CEA. Different characterizations of Wollaston hot wires are shown, and measurements of velocity fluctuations in different conditions, in normal helium as well as in superfluid helium are shown.△ Less"
The Swampland: Introduction and Review,Authors:Eran Palti,"Abstract:The Swampland program aims to distinguish effective theories which can be completed into quantum gravity in the ultraviolet from those which cannot. This article forms an introduction to the field, assuming only a knowledge of quantum field theory and general relativity. It also forms a comprehensive review, covering the range of ideas that are part of the field, from the Weak Gravity Conjecture,…▽ MoreThe Swampland program aims to distinguish effective theories which can be completed into quantum gravity in the ultraviolet from those which cannot. This article forms an introduction to the field, assuming only a knowledge of quantum field theory and general relativity. It also forms a comprehensive review, covering the range of ideas that are part of the field, from the Weak Gravity Conjecture, through compactifications of String Theory, to the de Sitter conjecture.△ Less"
MILDNet: A Lightweight Single Scaled Deep Ranking Architecture,Authors:Anirudha Vishvakarma,"Abstract:…and significant reduction in inference time. The significance of intermediate layers on image retrieval task has also been shown to be performing on popular datasets Holidays,Oxford, Paris [5]. So even though our experiments are done on ecommerce domain, it is applicable to other domains as well. We further did an ablation study to validate our hypothesis b…▽ MoreMulti-scale deep CNN architecture [1, 2, 3] successfully captures both fine and coarse level image descriptors for visual similarity task, but they come up with expensive memory overhead and latency. In this paper, we propose a competing novel CNN architecture, called MILDNet, which merits by being vastly compact (about 3 times). Inspired by the fact that successive CNN layers represent the image with increasing levels of abstraction, we compressed our deep ranking model to a single CNN by coupling activations from multiple intermediate layers along with the last layer. Trained on the famous Street2shop dataset [4], we demonstrate that our approach performs as good as the current state-of-the-art models with only one third of the parameters, model size, training time and significant reduction in inference time. The significance of intermediate layers on image retrieval task has also been shown to be performing on popular datasets Holidays,Oxford, Paris [5]. So even though our experiments are done on ecommerce domain, it is applicable to other domains as well. We further did an ablation study to validate our hypothesis by checking the impact on adding each intermediate layer. With this we also present two more useful variants of MILDNet, a mobile model (12 times smaller) for on-edge devices and a compactly featured model (512-d feature embeddings) for systems with less RAMs and to reduce the ranking cost. Further we present an intuitive way to automatically create a tailored in-house triplet training dataset, which is very hard to create manually. This solution too can also be deployed as an all-inclusive visual similarity solution. Finally, we present our entire production level architecture which currently powers visual similarity at Fynd.△ Less"
A Generative Map for Image-based Camera Localization,"Authors:Mingpan Guo,Stefan Matthes,Jiaojiao Ye,Hao Shen","Abstract:…Kalman filter, which also allows it to incorporate additional sensor information such as stereo visual odometry. For evaluation, we use real world images from the 7-Scenes andOxfordRobotCar datasets. We demonstrate that our Generative Map can be queried with a pose of interest from the test sequence to predict an image, which closely resembles the true sce…▽ MoreIn image-based camera localization systems, information about the environment is usually stored in some representation, which can be referred to as a map. Conventionally, most maps are built upon hand-crafted features. Recently, neural networks have attracted attention as a data-driven map representation, and have shown promising results in visual localization. However, these neural network maps are generally hard to interpret by human. A readable map is not only accessible to humans, but also provides a way to be verified when the ground truth pose is unavailable. To tackle this problem, we propose Generative Map, a new framework for learning human-readable neural network maps, by combining a generative model with the Kalman filter, which also allows it to incorporate additional sensor information such as stereo visual odometry. For evaluation, we use real world images from the 7-Scenes andOxfordRobotCar datasets. We demonstrate that our Generative Map can be queried with a pose of interest from the test sequence to predict an image, which closely resembles the true scene. For localization, we show that Generative Map achieves comparable performance with current regression models. Moreover, our framework is trained completely from scratch, unlike regression models which rely on large ImageNet pretrained networks.△ Less"
DeepLO: Geometry-Aware Deep LiDAR Odometry,"Authors:Younggun Cho,Giseop Kim,Ayoung Kim","Abstract:…loss functions that allow switching between supervised and unsupervised learning depending on the ground-truth validity in the training phase. An evaluation using the KITTI andOxfordRobotCar dataset demonstrates the prominent performance and efficiency of the proposed method when achieving pose accuracy.▽ MoreRecently, learning-based ego-motion estimation approaches have drawn strong interest from studies mostly focusing on visual perception. These groundbreaking works focus on unsupervised learning for odometry estimation but mostly for visual sensors. Compared to images, a learning-based approach using Light Detection and Ranging (LiDAR) has been reported in a few studies where, most often, a supervised learning framework is proposed. In this paper, we propose a novel approach to geometry-aware deep LiDAR odometry trainable via both supervised and unsupervised frameworks. We incorporate the Iterated Closest Point (ICP) algorithm into a deep-learning framework and show the reliability of the proposed pipeline. We provide two loss functions that allow switching between supervised and unsupervised learning depending on the ground-truth validity in the training phase. An evaluation using the KITTI andOxfordRobotCar dataset demonstrates the prominent performance and efficiency of the proposed method when achieving pose accuracy.△ Less"
An extension of the Bourgain-Sarnak-Ziegler theorem with modular applications,"Authors:Mattia Cafferata,Alberto Perelli,Alessandro Zaccagnini",Abstract:We prove an extension of the Bourgain-Sarnak-Ziegler theorem and then apply it to bound certain polynomial exponential sums with modular coefficients.We prove an extension of the Bourgain-Sarnak-Ziegler theorem and then apply it to bound certain polynomial exponential sums with modular coefficients.△ Less
Rotating double-diffusive convection in stably stratified planetary cores,"Authors:Rémy Monville,Jérémie Vidal,David Cébron,Nathanaël Schaeffer","Abstract:In planetary fluid cores, the density depends on temperature and chemical composition, which diffuse at very different rates. This leads to various instabilities, bearing the name of double-diffusive convection. We investigate rotating double-diffusive convection (RDDC) in fluid spheres. We use the Boussinesq approximation with homogeneous internal thermal and compositional source terms. We focus…▽ MoreIn planetary fluid cores, the density depends on temperature and chemical composition, which diffuse at very different rates. This leads to various instabilities, bearing the name of double-diffusive convection. We investigate rotating double-diffusive convection (RDDC) in fluid spheres. We use the Boussinesq approximation with homogeneous internal thermal and compositional source terms. We focus on the finger regime, in which the thermal gradient is stabilising whereas the compositional one is destabilising. First, we perform a global linear stability analysis in spheres. The critical Rayleigh numbers drastically drop for stably stratified fluids, yielding large-scale convective motions where local analyses predict stability. We evidence the inviscid nature of this large-scale double-diffusive instability, enabling the determination of the marginal stability curve at realistic planetary regimes. In particular, we show that in stably stratified spheres, the Rayleigh numbers $Ra$ at the onset evolve like $Ra \sim Ek^{-1}$, where $Ek$ is the Ekman number. This differs from rotating convection in unstably stratified spheres, for which $Ra \sim Ek^{-4/3}$. The domain of existence of inviscid convection thus increases as $Ek^{-1/3}$. Second, we perform nonlinear simulations. We find a transition between two regimes of RDDC, controlled by the strength of the stratification. Furthermore, far from the RDDC onset, we find a dominating equatorially anti-symmetric, large-scale zonal flow slightly above the associated linear onset. Unexpectedly, a purely linear mechanism can explain this phenomenon, even far from the instability onset, yielding a symmetry breaking of the nonlinear flow at saturation. For even stronger stable stratification, the flow becomes mainly equatorially-symmetric and intense zonal jets develop. Finally, we apply our results to the early Earth core. Double diffusion can reduce the critical Rayleigh number by four decades for realistic core conditions. We suggest that the early Earth core was prone to turbulent RDDC, with large-scale zonal flows.△ Less"
Optical analysis of a CMB cosmic string candidate,"Authors:O. S. Sazhina,D. Scognamiglio,M. V. Sazhin,M. Capaccioli","Abstract:The complexity of the cosmological scenario regarding cosmic strings (CSs) stands still in the way of a complete understanding. We describe here a promising strategy for the possible detection of these elusive physical entities. It is based on the search of strong gravitational lensing events in the location area of the CS candidate (CSc-1), which was declared in a previous work by CMB analysis. U…▽ MoreThe complexity of the cosmological scenario regarding cosmic strings (CSs) stands still in the way of a complete understanding. We describe here a promising strategy for the possible detection of these elusive physical entities. It is based on the search of strong gravitational lensing events in the location area of the CS candidate (CSc-1), which was declared in a previous work by CMB analysis. Using photometric and geometric criteria, we identified pairs of candidates of lensed galaxies (LGCs) in the ""string field"" (SF), which were then compared with the average density of background galaxy pairs in a set of ""control fields"" (CFs). We found an excess of $22\%$ (per sq. deg.) of the LGCs in SF, which exceeds the estimated cosmic dispersion. We also found that the number of LGCs is in excess of $29.2\%$ in the angular separation bin $[8'', 9'']$. Finally, we analysed the possibility of a preferred orientation of the line connecting the centres of the LGCs. The orientation is statistically significant for an angular separation bin $[4'',6'']$. Therefore, we found two ""windows"" for the preferred angular separation for LGCs along the possible CS. However, the confirmation of the gravitational lensing origin of our LGCs requires spectroscopic observations which seem to be justified by the present results. We plan to acquire their spectra as well as to continue the study of the spectral and morphological features of the LGCs in the CSc-1 field and to analyse the other CS-candidates using the same strategy.△ Less"
Proceedings Fifth International Workshop on Rewriting Techniques for Program Transformations and Evaluation,"Authors:Joachim Niehren,David Sabel","Abstract:…contains the formal proceedings of the 5th International Workshop on Rewriting Techniques for Program Transformations and Evaluation (WPTE 2018), held on 8th of Juli 2018 inOxford, United Kingdom, and affiliated with FLoC 2018 and FSCD 2018.
  Scope of WPTE:
  Rewriting techniques are of great help for studying correctness of program transformations, transl…▽ MoreThis volume contains the formal proceedings of the 5th International Workshop on Rewriting Techniques for Program Transformations and Evaluation (WPTE 2018), held on 8th of Juli 2018 inOxford, United Kingdom, and affiliated with FLoC 2018 and FSCD 2018.
  Scope of WPTE:
  Rewriting techniques are of great help for studying correctness of program transformations, translations and evaluation, and the aim of WPTE is to bring together the researchers working on program transformations, evaluation, and operationally-based programming language semantics, using rewriting methods, in order to share the techniques and recent developments and to exchange ideas to encourage further activation of research in this area. Topics in the scope of WPTE include the correctness of program transformations, optimisations and translations; program transformations for proving termination, confluence and other properties; correctness of evaluation strategies; operational semantics of programs, operationally-based program equivalences such as contextual equivalences and bisimulations; cost-models for reasoning about the optimizing power of transformations and the costs of evaluation; program transformations for verification and theorem proving purposes; translation, simulation, equivalence of programs with different formalisms, and evaluation strategies; program transformations for applying rewriting techniques to programs in specific programming languages; program transformations for program inversions and program synthesis; program transformation and evaluation for Haskell and rewriting.
  Research Paper Selection:
  At the workshop six research papers were presented of which five were accepted for the postproceedings. Each submission was reviewed by three or four members of the Program Committee in two to three rounds, one round for workshop presentation and at most two rounds for publication to the postproceedings.
  The program also included one invited talk by Jean-Pierre Jouannaud (Polytec, Palaiseau, Grand Paris, France) on a framework for graph rewriting; the abstract of this talk is included in this volume.△ Less"
On the 2-head of the colored Jones polynomial for pretzel knots,Authors:Paul Beirne,"Abstract:In this paper, we prove a formula for the 2-head of the colored Jones polynomial for an infinite family of pretzel knots. Following Hall, the proof utilizes skein-theoretic techniques and a careful examination of higher order stability properties for coefficients of the colored Jones polynomial.In this paper, we prove a formula for the 2-head of the colored Jones polynomial for an infinite family of pretzel knots. Following Hall, the proof utilizes skein-theoretic techniques and a careful examination of higher order stability properties for coefficients of the colored Jones polynomial.△ Less"
Program Equivalence for Algebraic Effects via Modalities,Authors:Cristina Matache,"Abstract:This dissertation is concerned with the study of program equivalence and algebraic effects as they arise in the theory of programming languages. Algebraic effects represent impure behaviour in a functional programming language, such as input and output, exceptions, nondeterminism etc. all treated in a generic way. Program equivalence aims to identify which programs can be considered equal in some…▽ MoreThis dissertation is concerned with the study of program equivalence and algebraic effects as they arise in the theory of programming languages. Algebraic effects represent impure behaviour in a functional programming language, such as input and output, exceptions, nondeterminism etc. all treated in a generic way. Program equivalence aims to identify which programs can be considered equal in some sense. This question has been studied for a long time but has only recently been extended to languages with algebraic effects, which are a newer development. Much work remains to be done in order to understand program equivalence in the presence of algebraic effects. In particular, there is no characterisation of contextual equivalence using a logic. We define a logic whose formulas express properties of higher-order programs with algebraic effects. We then investigate three notions of program equivalence for algebraic effects: logical equivalence induced by the aforementioned logic, applicative bisimilarity and contextual equivalence. For the programming language used in this dissertation, we prove that they all coincide. Therefore, the main novel contribution of the dissertation is defining the first logic for algebraic effects whose induced program equivalence coincides with contextual equivalence.△ Less"
Categorical Operational Physics,Authors:Sean Tull,"Abstract:Many insights into the quantum world can be found by studying it from amongst more general operational theories of physics. In this thesis, we develop an approach to the study of such theories purely in terms of the behaviour of their processes, as described mathematically through the language of category theory. This extends a framework for quantum processes known as categorical quantum mechanics…▽ MoreMany insights into the quantum world can be found by studying it from amongst more general operational theories of physics. In this thesis, we develop an approach to the study of such theories purely in terms of the behaviour of their processes, as described mathematically through the language of category theory. This extends a framework for quantum processes known as categorical quantum mechanics (CQM) due to Abramsky and Coecke.
  We first consider categorical frameworks for operational theories. We introduce a notion of such theory, based on those of Chiribella, D'Ariano and Perinotti (CDP), but more general than the probabilistic ones typically considered. We establish a correspondence between these and what we call ""operational categories"", using features introduced by Jacobs et al. in effectus theory, an area of categorical logic to which we provide an operational interpretation. We then see how to pass to a broader category of ""super-causal"" processes, allowing for the powerful diagrammatic features of CQM.
  Next we study operational theories themselves. We survey numerous principles that a theory may satisfy, treating them in a basic diagrammatic setting, and relating notions from probabilistic theories, CQM and effectus theory. We provide a new description of superpositions in the category of pure quantum processes, using this to give an abstract construction of the category of Hilbert spaces and linear maps.
  Finally, we reconstruct finite-dimensional quantum theory itself. More broadly, we give a recipe for recovering a class of generalised quantum theories, before instantiating it with operational principles inspired by an earlier reconstruction due to CDP. This reconstruction is fully categorical, not requiring the usual technical assumptions of probabilistic theories. Specialising to such theories recovers both standard quantum theory and that over real Hilbert spaces.△ Less"
Towards an Advanced Linear International Collider,Authors:ALEGRO collaboration,"Abstract:…The preparation of this document was coordinated by the Advanced LinEar collider study GROup, ALEGRO. The content was defined through discussions at the ALEGRO workshop inOxfordUK, March 2018, and an advanced draft was discussed during a one day meeting prior to the AAC workshop in Breckenridge, CO, USA, August 2018. This document was submitted as an adde…▽ MoreThis document provides detailed information on the status of Advanced and Novel Accelerators techniques and describes the steps that need to be envisaged for their implementation in future accelerators, in particular for high energy physics applications. It complements the overview prepared for the update of the European Strategy for particle physics, and provides a detailed description of the field. The scientific priorities of the community are described for each technique of acceleration able to achieve accelerating gradient in the GeV~range or above. ALEGRO working group leaders have coordinated the preparation of their working group contribution and contributed to editing the documents. The preparation of this document was coordinated by the Advanced LinEar collider study GROup, ALEGRO. The content was defined through discussions at the ALEGRO workshop inOxfordUK, March 2018, and an advanced draft was discussed during a one day meeting prior to the AAC workshop in Breckenridge, CO, USA, August 2018. This document was submitted as an addendum to the ALEGRO submission to the European Strategy for Particle Physics.△ Less"
Information-theoretic foundations of thermodynamics in general probabilistic theories,Authors:Carlo Maria Scandolo,"Abstract:We study the informational underpinnings of thermodynamics and statistical mechanics, using an abstract framework, general probabilistic theories, capable of describing arbitrary physical theories. This allows one to abstract the informational content of a theory from the concrete details of its formalism. In this framework, we extend the treatment of microcanonical thermodynamics, namely the ther…▽ MoreWe study the informational underpinnings of thermodynamics and statistical mechanics, using an abstract framework, general probabilistic theories, capable of describing arbitrary physical theories. This allows one to abstract the informational content of a theory from the concrete details of its formalism. In this framework, we extend the treatment of microcanonical thermodynamics, namely the thermodynamics of systems with a well-defined energy, beyond the known cases of classical and quantum theory, formulating two necessary requirements for a well-defined thermodynamics. We adopt the recent approach of resource theories, where one studies the transitions between states that can be accomplished with a restricted set of physical operations. We formulate three different resource theories, differing in the choice of the restricted set of physical operations. To bridge the gap between the objective dynamics of particles and the subjective world of probabilities, one of the core issues in the foundations of statistical mechanics, we propose four information-theoretic axioms. They are satisfied by quantum theory and more exotic alternatives, including a suitable extension of classical theory where classical systems interact with each other creating entangled states. The axioms identify a class of theories where every mixed state can be modelled as the reduced state of a pure entangled state. In these theories it is possible to introduce well-behaved notions of majorisation, entropy, and Gibbs states, allowing for an information-theoretic derivation of Landauer's principle. The three resource theories define the same notion of resource if and only if, on top of the four axioms, the dynamics of the underlying theory satisfy a condition called ""unrestricted reversibility"". Under this condition we derive a duality between microcanonical thermodynamics and pure bipartite entanglement.△ Less"
Signed Network Structural Analysis and Applications with a Focus on Balance Theory,Authors:Samin Aref,"Abstract:We analyse signed networks from the perspective of balance theory which predicts structural balance as a global structure for signed social networks that represent groups of friends and enemies. The scarcity of balanced networks encouraged us to define the notion of partial balance in order to quantify the extent to which a network is balanced. We evaluate several numerical measures of partial bal…▽ MoreWe analyse signed networks from the perspective of balance theory which predicts structural balance as a global structure for signed social networks that represent groups of friends and enemies. The scarcity of balanced networks encouraged us to define the notion of partial balance in order to quantify the extent to which a network is balanced. We evaluate several numerical measures of partial balance and recommend using the frustration index, a measure that satisfies key axiomatic properties and allows us to analyse graphs based on their levels of partial balance. The exact algorithms used in the literature to compute the frustration index, also called the line index of balance, are not scalable and cannot process graphs with a few hundred edges. We formulate computing the frustration index as a graph optimisation problem to find the minimum number of edges whose removal results in a balanced network given binary decision variables associated with graph nodes and edges. We use our first optimisation model to analyse graphs with up to 3000 edges. Reformulating the optimisation problem, we develop three more efficient binary linear programming models. Equipping the models with valid inequalities and prioritised branching as speed-up techniques allows us to process graphs with 15000 edges on inexpensive hardware. Besides making exact computations possible for large graphs, we show that our models outperform heuristics and approximation algorithms suggested in the literature by orders of magnitude. We extend the concepts of balance and frustration in signed networks to applications beyond the classic friend-enemy interpretation of balance theory in social context. Using a high-performance computer, we analyse graphs with up to 100000 edges to investigate a range of applications from biology and chemistry to finance, international relations, and physics.△ Less"
"The Group Element of Cybercrime: Types, Dynamics, and Criminal Operations","Authors:Jason R. C. Nurse,Maria Bada","Abstract:While cybercrime can often be an individual activity pursued by lone hackers, it has increasingly grown into a group activity, with networks across the world. This chapter critically examines the group element of cybercrime from several perspectives. It identifies the platforms that online groups---cybercriminal and otherwise---use to interact, and considers groups as both perpetrators and victims…▽ MoreWhile cybercrime can often be an individual activity pursued by lone hackers, it has increasingly grown into a group activity, with networks across the world. This chapter critically examines the group element of cybercrime from several perspectives. It identifies the platforms that online groups---cybercriminal and otherwise---use to interact, and considers groups as both perpetrators and victims of cybercrime. A key novelty is the discovery of new types of online groups whose collective actions border on criminality. The chapter also analyzes how online cybercrime groups form, organize, and operate. It explores issues such as trust, motives, and means, and draws on several poignant examples, from Anonymous to LulzSec, to illustrate the arguments.△ Less"
TheOxfordMultimotion Dataset: Multiple SE(3) Motions with Ground Truth,"Authors:Kevin M. Judd,Jonathan D. Gammell","Abstract:…evaluation of these multimotion estimation techniques requires datasets consisting of multiple, complex motions that also contain ground truth for every moving body.
  TheOxfordMultimotion Dataset provides a number of multimotion estimation problems of varying complexity. It includes both complex problems that challenge existing algorithms as well as a num…▽ MoreDatasets advance research by posing challenging new problems and providing standardized methods of algorithm comparison. High-quality datasets exist for many important problems in robotics and computer vision, including egomotion estimation and motion/scene segmentation, but not for techniques that estimate every motion in a scene. Metric evaluation of these multimotion estimation techniques requires datasets consisting of multiple, complex motions that also contain ground truth for every moving body.
  TheOxfordMultimotion Dataset provides a number of multimotion estimation problems of varying complexity. It includes both complex problems that challenge existing algorithms as well as a number of simpler problems to support development. These include observations from both static and dynamic sensors, a varying number of moving bodies, and a variety of different 3D motions. It also provides a number of experiments designed to isolate specific challenges of the multimotion problem, including rotation about the optical axis and occlusion.
  In total, theOxfordMultimotion Dataset contains over 110 minutes of multimotion data consisting of stereo and RGB-D camera images, IMU data, and Vicon ground-truth trajectories. The dataset culminates in a complex toy car segment representative of many challenging real-world scenarios. This paper describes each experiment with a focus on its relevance to the multimotion estimation problem.△ Less"
Pull out all the stops: Textual analysis via punctuation sequences,"Authors:Alexandra N. M. Darmon,Marya Bazzi,Sam D. Howison,Mason A. Porter","Abstract:…the lucid prose of a favorite author or slogging through some other writer's cumbersome, heavy-set prattle (full of parentheses, em dashes, compound adjectives, andOxfordcommas), readers will notice stylistic signatures not only in word choice and grammar, but also in punctuation itself. Indeed, visual sequences of punctuation from different authors pr…▽ MoreWhether enjoying the lucid prose of a favorite author or slogging through some other writer's cumbersome, heavy-set prattle (full of parentheses, em dashes, compound adjectives, andOxfordcommas), readers will notice stylistic signatures not only in word choice and grammar, but also in punctuation itself. Indeed, visual sequences of punctuation from different authors produce marvelously different (and visually striking) sequences. Punctuation is a largely overlooked stylistic feature in ""stylometry"", the quantitative analysis of written text. In this paper, we examine punctuation sequences in a corpus of literary documents and ask the following questions: Are the properties of such sequences a distinctive feature of different authors? Is it possible to distinguish literary genres based on their punctuation sequences? Do the punctuation styles of authors evolve over time? Are we on to something interesting in trying to do stylometry without words, or are we full of sound and fury (signifying nothing)?△ Less"
Associative $n$-categories,Authors:Christoph Dorn,"Abstract:We define novel fully combinatorial models of higher categories. Our definitions are based on a connection of higher categories to ""directed spaces"". Directed spaces are locally modelled on manifold diagrams, which are stratifications of the n-cube such that strata are transversal to the flag foliation of the n-cube. The first part of this thesis develops a combinatorial model for manifold diagram…▽ MoreWe define novel fully combinatorial models of higher categories. Our definitions are based on a connection of higher categories to ""directed spaces"". Directed spaces are locally modelled on manifold diagrams, which are stratifications of the n-cube such that strata are transversal to the flag foliation of the n-cube. The first part of this thesis develops a combinatorial model for manifold diagrams called singular n-cubes. In the second part we apply this model to build our notions of higher categories.
  Singular n-cubes are ""directed triangulations"" of space together with a decomposition into a collection of subspaces or strata. Singular n-cubes can be naturally organised into two categories. The first, whose morphisms are bundles themselves, is used for the inductive definition of singular (n+1)-cubes. The second, whose morphisms are ""open"" base changes, admits an (epi,mono) factorisation system. Monomorphisms will be called embeddings of cubes. Epimorphisms will be called collapses and describe how triangulations can be coarsened. Each cube has a unique coarsest triangulation called its normal form. The existence of normal forms makes the equality relation of (combinatorially represented) manifold diagrams decidable.
  As the main application of the resulting combinatorial framework for manifold diagrams, we give algebraic definitions of various notions of higher categories. Namely, we define associative n-categories, presented associative n-categories and presented associative n-groupoids. All three notions will have strict units and associators; the only weak coherences are homotopies, but we develop a mechanism for recovering the usual coherence data of weak n-categories, such as associators and pentagonators and their higher analogues. This will motivate the conjecture that the theory of associative higher categories is equivalent to its fully weak counterpart.△ Less"
SIAN: software for structural identifiability analysis of ODE models,"Authors:Hoon Hong,Alexey Ovchinnikov,Gleb Pogudin,Chee Yap","Abstract:Biological processes are often modeled by ordinary differential equations with unknown parameters. The unknown parameters are usually estimated from experimental data. In some cases, due to the structure of the model, this estimation problem does not have a unique solution even in the case of continuous noise-free data. It is therefore desirable to check the uniqueness a priori before carrying out…▽ MoreBiological processes are often modeled by ordinary differential equations with unknown parameters. The unknown parameters are usually estimated from experimental data. In some cases, due to the structure of the model, this estimation problem does not have a unique solution even in the case of continuous noise-free data. It is therefore desirable to check the uniqueness a priori before carrying out actual experiments. We present a new software SIAN (Structural Identifiability ANalyser) that does this. Our software can tackle problems that could not be tackled by previously developed packages.△ Less"
Parallel Clustering of Single Cell Transcriptomic Data with Split-Merge Sampling on Dirichlet Process Mixtures,"Authors:Tiehang Duan,José P. Pinto,Xiaohui Xie","Abstract:Motivation: With the development of droplet based systems, massive single cell transcriptome data has become available, which enables analysis of cellular and molecular processes at single cell resolution and is instrumental to understanding many biological processes. While state-of-the-art clustering methods have been applied to the data, they face challenges in the following aspects: (1) the clu…▽ MoreMotivation: With the development of droplet based systems, massive single cell transcriptome data has become available, which enables analysis of cellular and molecular processes at single cell resolution and is instrumental to understanding many biological processes. While state-of-the-art clustering methods have been applied to the data, they face challenges in the following aspects: (1) the clustering quality still needs to be improved; (2) most models need prior knowledge on number of clusters, which is not always available; (3) there is a demand for faster computational speed. Results: We propose to tackle these challenges with Parallel Split Merge Sampling on Dirichlet Process Mixture Model (the Para-DPMM model). Unlike classic DPMM methods that perform sampling on each single data point, the split merge mechanism samples on the cluster level, which significantly improves convergence and optimality of the result. The model is highly parallelized and can utilize the computing power of high performance computing (HPC) clusters, enabling massive clustering on huge datasets. Experiment results show the model outperforms current widely used models in both clustering quality and computational speed. Availability: Source code is publicly available on https://github.com/tiehangd/Para_DPMM/tree/master/Para_DPMM_package△ Less"
Simulation Modeling,Authors:Florian Hartig,"Abstract:With the rise of computers, simulation models have emerged beside the more traditional statistical and mathematical models as a third pillar for ecological analysis. Broadly speaking, a simulation model is an algorithm, typically implemented as a computer program, which propagates the states of a system forward. Unlike in a mathematical model, however, this propagation does not employ the methods…▽ MoreWith the rise of computers, simulation models have emerged beside the more traditional statistical and mathematical models as a third pillar for ecological analysis. Broadly speaking, a simulation model is an algorithm, typically implemented as a computer program, which propagates the states of a system forward. Unlike in a mathematical model, however, this propagation does not employ the methods of calculus but rather a set of rules or formulae that directly prescribe the next state. Such an algorithmic model specification is particularly suited for describing systems that are difficult to capture or analyze with differential equations such as: (a) systems that are highly nonlinear or chaotic; (b) discrete systems, for example networks or groups of distinct individuals; (c) systems that are stochastic; and (d) systems that are too complex to be successfully treated with classical calculus. As these situations are frequently encountered in ecology, simulation models are now widely applied across the discipline. They have been instrumental in developing new insights into classical questions of species' coexistence, community assembly, population dynamics, biogeography, and many more. The methods for this relatively young field are still being actively developed, and practical work with simulation models requires ecologists to learn new skills such as coding, sensitivity analysis, calibration, validation, and forecasting uncertainties. Moreover, scientific inquiry with complex systems has led to subtle changes to the philosophical and epistemological views regarding simplicity, reductionism, and the relationship between prediction and understanding.△ Less"
"Luminous AGB variables in the dwarf Irregular Galaxy, NGC 3109","Authors:J. W. Menzies,P. A. Whitelock,M. W. Feast,N. Matsunaga","Abstract:In a shallow near-infrared survey of the dwarf Irregular galaxy, NGC 3109, near the periphery of the Local Group, we have found eight Mira variables, seven of which appear to be oxygen-rich (O-Miras). The periods range from about 430 days to almost 1500 days. Because of our relatively bright limiting magnitude, only 45 of the more than 400 known carbon stars were measured, but none was found to be…▽ MoreIn a shallow near-infrared survey of the dwarf Irregular galaxy, NGC 3109, near the periphery of the Local Group, we have found eight Mira variables, seven of which appear to be oxygen-rich (O-Miras). The periods range from about 430 days to almost 1500 days. Because of our relatively bright limiting magnitude, only 45 of the more than 400 known carbon stars were measured, but none was found to be a large amplitude variable. One of the Miras may be an unrecognised C star. Five of the O-Miras are probably hot-bottom burning stars considering that they are brighter than expected from the period--luminosity relation of Miras and that, by comparison with theoretical evolutionary tracks, they appear to have masses >~4 Msun. A census of very long period (P>1000 days) Miras in the Galaxy and Magellanic Clouds is presented and discussed together with the newly discovered long period, but relatively blue, variables in NGC 3109. New $JHKL$ photometry is presented for three O-rich long period Miras i n the SMC (including a candidate super-AGB star).△ Less"
The Interiors of Jupiter and Saturn,Authors:Ravit Helled,"Abstract:Probing the interiors of the gas giant planets in our Solar System is not an easy task. It requires a set of accurate measurements combined with theoretical models that are used to infer the planetary composition and its depth dependence. The masses of Jupiter and Saturn are 317.83 and 95.16 Earth masses, respectively, and since a few decades, we know that they mostly consist of hydrogen and heliu…▽ MoreProbing the interiors of the gas giant planets in our Solar System is not an easy task. It requires a set of accurate measurements combined with theoretical models that are used to infer the planetary composition and its depth dependence. The masses of Jupiter and Saturn are 317.83 and 95.16 Earth masses, respectively, and since a few decades, we know that they mostly consist of hydrogen and helium. It is the mass of heavy elements (all elements heavier than helium) that is not well determined, as well as their distribution within the planets. While the heavy elements are not the dominating materials in Jupiter and Saturn they are the key for our understanding of their formation and evolution histories.
  The planetary internal structure is inferred from theoretical models that fit the available observational constraints by using theoretical equations of states (EOSs) for hydrogen, helium, their mixtures, and heavier elements (typically rocks and/or ices). However, there is no unique solution for the planetary structure and the results depend on the used EOSs and the model assumptions imposed by the modeler. Major model assumptions that can affect the derived internal structure include the number of layers, the heat transport mechanism within the planet (and its entropy), the nature of the core (compact vs. diluted), and the location (pressure) of separation between the two envelopes. Alternative structure models assume a less distinct division between the layers and/or a non-homogenous distribution of the heavy elements. Today, with accurate measurements of the gravitational fields of Jupiter and Saturn from the Juno and Cassini missions, structure models can be further constrained. At the same time, these measurements introduce new challenges for planetary modellers.△ Less"
Adversarial Learning of Semantic Relevance in Text to Image Synthesis,"Authors:Miriam Cha,Youngjune L. Gwon,H. T. Kung","Abstract:…of selecting random training examples, we perform negative sampling based on the semantic distance from a positive example in the class. We evaluate our approach using theOxford-102 flower dataset, adopting the inception score and multi-scale structural similarity index (MS-SSIM) metrics to assess discriminability and diversity of the generated images. The…▽ MoreWe describe a new approach that improves the training of generative adversarial nets (GANs) for synthesizing diverse images from a text input. Our approach is based on the conditional version of GANs and expands on previous work leveraging an auxiliary task in the discriminator. Our generated images are not limited to certain classes and do not suffer from mode collapse while semantically matching the text input. A key to our training methods is how to form positive and negative training examples with respect to the class label of a given image. Instead of selecting random training examples, we perform negative sampling based on the semantic distance from a positive example in the class. We evaluate our approach using theOxford-102 flower dataset, adopting the inception score and multi-scale structural similarity index (MS-SSIM) metrics to assess discriminability and diversity of the generated images. The empirical results indicate greater diversity in the generated images, especially when we gradually select more negative training examples closer to a positive example in the semantic space.△ Less"
Proceedings of the eleventh Workshop on Answer Set Programming and Other Computing Paradigms 2018,"Authors:Jorge Fandinno,Johannes Fichte","Abstract:This is the Proceedings of the eleventh Workshop on Answer Set Programming and Other Computing Paradigms (ASPOCP) 2018, which was held inOxford, UK, July 18th, 2018.This is the Proceedings of the eleventh Workshop on Answer Set Programming and Other Computing Paradigms (ASPOCP) 2018, which was held inOxford, UK, July 18th, 2018.△ Less"
Termination of $λ$$Π$ modulo rewriting using the size-change principle (work in progress),"Authors:Frédéric Blanqui,Guillaume Genestier","Abstract:The Size-Change Termination principle was first introduced to study the termination of first-order functional programs. In this work, we show that it can also be used to study the termination of higher-order rewriting in a system of dependent types extending LF.The Size-Change Termination principle was first introduced to study the termination of first-order functional programs. In this work, we show that it can also be used to study the termination of higher-order rewriting in a system of dependent types extending LF.△ Less"
Detect-to-Retrieve: Efficient Regional Aggregation for Image Search,"Authors:Marvin Teichmann,Andre Araujo,Menglong Zhu,Jack Sim","Abstract:…systems that index image regions independently. Our complete image retrieval system improves upon the previous state-of-the-art by significant margins on the RevisitedOxfordand Paris datasets. Code and data available at the project webpage: https://github.com/tensorflow/models/tree/master/research/delf.▽ MoreRetrieving object instances among cluttered scenes efficiently requires compact yet comprehensive regional image representations. Intuitively, object semantics can help build the index that focuses on the most relevant regions. However, due to the lack of bounding-box datasets for objects of interest among retrieval benchmarks, most recent work on regional representations has focused on either uniform or class-agnostic region selection. In this paper, we first fill the void by providing a new dataset of landmark bounding boxes, based on the Google Landmarks dataset, that includes $86k$ images with manually curated boxes from $15k$ unique landmarks. Then, we demonstrate how a trained landmark detector, using our new dataset, can be leveraged to index image regions and improve retrieval accuracy while being much more efficient than existing regional methods. In addition, we introduce a novel regional aggregated selective match kernel (R-ASMK) to effectively combine information from detected regions into an improved holistic image representation. R-ASMK boosts image retrieval accuracy substantially with no dimensionality increase, while even outperforming systems that index image regions independently. Our complete image retrieval system improves upon the previous state-of-the-art by significant margins on the RevisitedOxfordand Paris datasets. Code and data available at the project webpage: https://github.com/tensorflow/models/tree/master/research/delf.△ Less"
The Right (Angled) Perspective: Improving the Understanding of Road Scenes Using Boosted Inverse Perspective Mapping,"Authors:Tom Bruls,Horia Porav,Lars Kunze,Paul Newman","Abstract:…objects are automatically removed from the scene, thus revealing the underlying road layout in an improved fashion. We demonstrate our framework using real-world data from theOxfordRobotCar Dataset and show that scene understanding tasks directly benefit from our boosted IPM approach.▽ MoreMany tasks performed by autonomous vehicles such as road marking detection, object tracking, and path planning are simpler in bird's-eye view. Hence, Inverse Perspective Mapping (IPM) is often applied to remove the perspective effect from a vehicle's front-facing camera and to remap its images into a 2D domain, resulting in a top-down view. Unfortunately, however, this leads to unnatural blurring and stretching of objects at further distance, due to the resolution of the camera, limiting applicability. In this paper, we present an adversarial learning approach for generating a significantly improved IPM from a single camera image in real time. The generated bird's-eye-view images contain sharper features (e.g. road markings) and a more homogeneous illumination, while (dynamic) objects are automatically removed from the scene, thus revealing the underlying road layout in an improved fashion. We demonstrate our framework using real-world data from theOxfordRobotCar Dataset and show that scene understanding tasks directly benefit from our boosted IPM approach.△ Less"
The Indus Script and Economics. A Role for Indus Seals and Tablets in Rationing and Administration of Labor,Authors:Rajesh P. N. Rao,"Abstract:The Indus script remains one of the last major undeciphered scripts of the ancient world. We focus here on Indus inscriptions on a group of miniature tablets discovered by Meadow and Kenoyer in Harappa in 1997. By drawing parallels with proto-Elamite and proto-Cuneiform inscriptions, we explore how these miniature tablets may have been used to record rations allocated to porters or laborers. We th…▽ MoreThe Indus script remains one of the last major undeciphered scripts of the ancient world. We focus here on Indus inscriptions on a group of miniature tablets discovered by Meadow and Kenoyer in Harappa in 1997. By drawing parallels with proto-Elamite and proto-Cuneiform inscriptions, we explore how these miniature tablets may have been used to record rations allocated to porters or laborers. We then show that similar inscriptions are found on stamp seals, leading to the potentially provocative conclusion that rather than simply indicating ownership of property, Indus seals may have been used for generating tokens, tablets and sealings for repetitive economic transactions such as rations and exchange of canonical amounts of goods, grains, animals, and labor in a barter-based economy.△ Less"
XMM-Newton and INTEGRAL analysis of the Supergiant Fast X-ray Transient IGR J17354-3255,"Authors:M. E. Goossens,A. J. Bird,A. B. Hill,V. Sguera,S. P. Drave","Abstract:We present the results of combined INTEGRAL and XMM-Newton observations of the supergiant fast X-ray transient (SFXT) IGR J17354$-$3255. Three XMM-Newton observations of lengths 33.4 ks, 32.5 ks and 21.9 ks were undertaken, the first an initial pointing to identify the correct source in the field of view and the latter two performed around periastron. Simultaneous INTEGRAL observations across…▽ MoreWe present the results of combined INTEGRAL and XMM-Newton observations of the supergiant fast X-ray transient (SFXT) IGR J17354$-$3255. Three XMM-Newton observations of lengths 33.4 ks, 32.5 ks and 21.9 ks were undertaken, the first an initial pointing to identify the correct source in the field of view and the latter two performed around periastron. Simultaneous INTEGRAL observations across $\sim66\%$ of the orbital cycle were analysed but the source was neither detected by IBIS/ISGRI nor by JEM-X. The XMM-Newton light curves display a range of moderately bright X-ray activity but there are no particularly strong flares or outbursts in any of the three observations. We show that the spectral shape measured by XMM-Newton can be fitted by a consistent model throughout the observation, suggesting that the observed flux variations are driven by obscuration from a wind of varying density rather than changes in accretion mode. The simultaneous INTEGRAL data rule out simple extrapolation of the simple powerlaw model beyond the XMM-Newton energy range.△ Less"
The Making of a Creative Worldview,Authors:Liane Gabora,"Abstract:Research at the interface between cognitive psychology, neuroscience, and the science of complex, dynamical systems, is piecing together an understanding of the creative process, including how it works, how it can be fostered, and the developmental antecedents and personality traits of particularly creative people. This chapter examines the workings of creative minds, those with the potential to s…▽ MoreResearch at the interface between cognitive psychology, neuroscience, and the science of complex, dynamical systems, is piecing together an understanding of the creative process, including how it works, how it can be fostered, and the developmental antecedents and personality traits of particularly creative people. This chapter examines the workings of creative minds, those with the potential to significantly impact the evolution of human culture.△ Less"
Individualized Time-Series Segmentation for Mining Mobile Phone User Behavior,"Authors:Iqbal H. Sarker,Alan Colman,MA Kabir,Jun Han","Abstract:Mobile phones can record individual's daily behavioral data as a time-series. In this paper, we present an effective time-series segmentation technique that extracts optimal time segments of individual's similar behavioral characteristics utilizing their mobile phone data. One of the determinants of an individual's behavior is the various activities undertaken at various times-of-the-day and days-…▽ MoreMobile phones can record individual's daily behavioral data as a time-series. In this paper, we present an effective time-series segmentation technique that extracts optimal time segments of individual's similar behavioral characteristics utilizing their mobile phone data. One of the determinants of an individual's behavior is the various activities undertaken at various times-of-the-day and days-of-the-week. In many cases, such behavior will follow temporal patterns. Currently, researchers use either equal or unequal interval-based segmentation of time for mining mobile phone users' behavior. Most of them take into account static temporal coverage of 24-h-a-day and few of them take into account the number of incidences in time-series data. However, such segmentations do not necessarily map to the patterns of individual user activity and subsequent behavior because of not taking into account the diverse behaviors of individuals over time-of-the-week. Therefore, we propose a behavior-oriented time segmentation (BOTS) technique that takes into account not only the temporal coverage of the week but also the number of incidences of diverse behaviors dynamically for producing similar behavioral time segments over the week utilizing time-series data. Experiments on the real mobile phone datasets show that our proposed segmentation technique better captures the user's dominant behavior at various times-of-the-day and days-of-the-week enabling the generation of high confidence temporal rules in order to mine individual mobile phone users' behavior.△ Less"
Self Paced Adversarial Training for Multimodal Few-shot Learning,"Authors:Frederik Pahde,Oleksiy Ostapenko,Patrick Jähnichen,Tassilo Klein,Moin Nabi","Abstract:…approach builds upon the idea of cross-modal data generation in order to alleviate the data sparsity problem. We improve few-shot learning accuracies on the finegrained CUB andOxford-102 datasets.▽ MoreState-of-the-art deep learning algorithms yield remarkable results in many visual recognition tasks. However, they still fail to provide satisfactory results in scarce data regimes. To a certain extent this lack of data can be compensated by multimodal information. Missing information in one modality of a single data point (e.g. an image) can be made up for in another modality (e.g. a textual description). Therefore, we design a few-shot learning task that is multimodal during training (i.e. image and text) and single-modal during test time (i.e. image). In this regard, we propose a self-paced class-discriminative generative adversarial network incorporating multimodality in the context of few-shot learning. The proposed approach builds upon the idea of cross-modal data generation in order to alleviate the data sparsity problem. We improve few-shot learning accuracies on the finegrained CUB andOxford-102 datasets.△ Less"
Proceedings 4th Workshop on Formal Integrated Development Environment,"Authors:Paolo Masci,Rosemary Monahan,Virgile Prevosto","Abstract:…proceedings of F-IDE 2018, the fourth international workshop on Formal Integrated Development Environment, which was held as a FLoC 2018 satellite event, on July 14, 2018, inOxford, England. 
  High levels of safety, security and also privacy standards require the use of formal methods to specify and develop compliant software (sub)systems. Any standard com…▽ MoreThis volume contains the proceedings of F-IDE 2018, the fourth international workshop on Formal Integrated Development Environment, which was held as a FLoC 2018 satellite event, on July 14, 2018, inOxford, England. 
  High levels of safety, security and also privacy standards require the use of formal methods to specify and develop compliant software (sub)systems. Any standard comes with an assessment process, which requires a complete documentation of the application in order to ease the justification of design choices and the review of code and proofs. Thus tools are needed for handling specifications, program constructs and verification artifacts. The aim of the F-IDE workshop is to provide a forum for presenting and discussing research efforts as well as experience returns on design, development and usage of formal IDE aiming at making formal methods ""easier"" for both specialists and non-specialists.△ Less"
Graded Poisson Algebras,"Authors:Alberto S. Cattaneo,Domenico Fiorenza,Riccardo Longoni","Abstract:This note is an expanded and updated version of our entry with the same title for the 2006 Encyclopedia of Mathematical Physics. We give a brief overview of graded Poisson algebras, their main properties and their main applications, in the contexts of super differentiable and of derived algebraic geometry.This note is an expanded and updated version of our entry with the same title for the 2006 Encyclopedia of Mathematical Physics. We give a brief overview of graded Poisson algebras, their main properties and their main applications, in the contexts of super differentiable and of derived algebraic geometry.△ Less"
Ising-PageRank model of opinion formation on social networks,"Authors:Klaus M. Frahm,Dima L. Shepelyansky","Abstract:…or 2DRank nodes of the network even if the elite fraction is very small. The analytical and numerical studies are preformed for the networks of English Wikipedia 2017 andOxfordUniversity 2006.▽ MoreWe propose a new Ising-PageRank model of opinion formation on a social network by introducing an Ising- or spin-like structure of the corresponding Google matrix. Each elector or node of the network has two components corresponding to a red or blue opinion in the society. Also each elector propagates either the red or the blue opinion on the network so that the links between electors are described by two by two matrices favoring one or the other of the two opinions. An elector votes for red or blue depending on the dominance of its red or blue PageRank vector components. We determine the dependence of the final society vote on the fraction of nodes with red (or blue) influence allowing to determine the transition for the election outcome border between the red or blue option. We show that this transition border is significantly affected by the opinion of society elite electors composed of the top PageRank, CheiRank or 2DRank nodes of the network even if the elite fraction is very small. The analytical and numerical studies are preformed for the networks of English Wikipedia 2017 andOxfordUniversity 2006.△ Less"
"Universal regularization methods - varying the power, the smoothness and the accuracy","Authors:Coralia Cartis,Nicholas I. M. Gould,Philippe L. Toint","Abstract:Adaptive cubic regularization methods have emerged as a credible alternative to linesearch and trust-region for smooth nonconvex optimization, with optimal complexity amongst second-order methods. Here we consider a general/new class of adaptive regularization methods, that use first- or higher-order local Taylor models of the objective regularized by a(ny) power of the step size and applied to co…▽ MoreAdaptive cubic regularization methods have emerged as a credible alternative to linesearch and trust-region for smooth nonconvex optimization, with optimal complexity amongst second-order methods. Here we consider a general/new class of adaptive regularization methods, that use first- or higher-order local Taylor models of the objective regularized by a(ny) power of the step size and applied to convexly-constrained optimization problems. We investigate the worst-case evaluation complexity/global rate of convergence of these algorithms, when the level of sufficient smoothness of the objective may be unknown or may even be absent. We find that the methods accurately reflect in their complexity the degree of smoothness of the objective and satisfy increasingly better bounds with improving accuracy of the models. The bounds vary continuously and robustly with respect to the regularization power and accuracy of the model and the degree of smoothness of the objective.△ Less"
Cybercrime and You: How Criminals Attack and the Human Factors That They Seek to Exploit,Authors:Jason R. C. Nurse,"Abstract:Cybercrime is a significant challenge to society, but it can be particularly harmful to the individuals who become victims. This chapter engages in a comprehensive and topical analysis of the cybercrimes that target individuals. It also examines the motivation of criminals that perpetrate such attacks and the key human factors and psychological aspects that help to make cybercriminals successful.…▽ MoreCybercrime is a significant challenge to society, but it can be particularly harmful to the individuals who become victims. This chapter engages in a comprehensive and topical analysis of the cybercrimes that target individuals. It also examines the motivation of criminals that perpetrate such attacks and the key human factors and psychological aspects that help to make cybercriminals successful. Key areas assessed include social engineering (e.g., phishing, romance scams, catfishing), online harassment (e.g., cyberbullying, trolling, revenge porn, hate crimes), identity-related crimes (e.g., identity theft, doxing), hacking (e.g., malware, cryptojacking, account hacking), and denial-of-service crimes. As a part of its contribution, the chapter introduces a summary taxonomy of cybercrimes against individuals and a case for why they will continue to occur if concerted interdisciplinary efforts are not pursued.△ Less"
Systematic errors in Gaia DR2 astrometry and their impact on measurements of internal kinematics of star clusters,Authors:Eugene Vasiliev,"Abstract:We use stellar proper motions (PM) from Gaia Data Release 2 for studying the internal kinematics of Milky Way globular clusters. In addition to statistical measurement errors, there are significant spatially correlated systematic errors, which cannot be ignored when studying the internal kinematics. We develop a mathematically consistent procedure for incorporating the spatial correlations in any…▽ MoreWe use stellar proper motions (PM) from Gaia Data Release 2 for studying the internal kinematics of Milky Way globular clusters. In addition to statistical measurement errors, there are significant spatially correlated systematic errors, which cannot be ignored when studying the internal kinematics. We develop a mathematically consistent procedure for incorporating the spatial correlations in any model-fitting approach, and use it to determine rotation and velocity dispersion profiles of a few dozen clusters. We confirm detection of rotation in the sky plane for ~10 clusters reported in previous studies, and discover a few more clusters with rotation amplitudes exceeding ~0.05 mas/yr. However, in more than half of these cases the significance of this rotation signature is rather low when taking into account the systematic errors. We find that the PM dispersion is not sensitive to systematic errors in PM, however, it is quite sensitive to the selection criteria on the input sample, most importantly, in crowded central regions. When using the cleanest possible samples, PM dispersion can be reliably measured down to 0.1 mas/yr for ~60 clusters.△ Less"
Imprinting control regions (ICRs) are marked by mono-allelic bivalent chromatin when transcriptionally inactive,"Authors:Stéphanie Maupetit-Méhouas,Bertille Montibus,David Nury,Chiharu Tayama,Michel Wassef,Satya Kota,Anne Fogli,Fabiana Cerqueira Campos,Kenichiro Hata,Robert Feil,Raphaël Margueron,Kazuhiko Nakabayashi,Franck Court,Philippe Arnaud","Abstract:Parental allele-specific expression of imprinted genes is mediated by imprinting control regions (ICRs) that are constitutively marked by DNA methylation imprints on the maternal or paternal allele. Mono-allelic DNA methylation is strictly required for the process of imprinting and has to be faithfully maintained during the entire lifespan. While the regulation of DNA methylation itself is well un…▽ MoreParental allele-specific expression of imprinted genes is mediated by imprinting control regions (ICRs) that are constitutively marked by DNA methylation imprints on the maternal or paternal allele. Mono-allelic DNA methylation is strictly required for the process of imprinting and has to be faithfully maintained during the entire lifespan. While the regulation of DNA methylation itself is well understood, the mechanisms whereby the opposite allele remains unmethylated are unclear. Here, we show that in the mouse, at maternally methylated ICRs, the paternal allele, which is constitutively associated with H3K4me2/3, is marked by default by H3K27me3 when these ICRs are transcriptionally inactive, leading to the formation of a bivalent chromatin signature. Our data suggest that at ICRs, chromatin bivalency has a protective role by ensuring that DNA on the paternal allele remains unmethylated and protected against spurious and unscheduled gene expression. Moreover , they provide the proof of concept that, beside pluripotent cells, chromatin bivalency is the default state of transcriptionally inactive CpG island promoters , regardless of the developmental stage, thereby contributing to protect cell identity.△ Less"
Automatic Thresholding of SIFT Descriptors,Authors:Matthew R. Kirchner,"Abstract:We introduce a method to perform automatic thresholding of SIFT descriptors that improves matching performance by at least 15.9% on theOxfordimage matching benchmark. The method uses a contrario methodology to determine a unique bin magnitude threshold. This is done by building a generative uniform background model for descriptors and determining when bin…▽ MoreWe introduce a method to perform automatic thresholding of SIFT descriptors that improves matching performance by at least 15.9% on theOxfordimage matching benchmark. The method uses a contrario methodology to determine a unique bin magnitude threshold. This is done by building a generative uniform background model for descriptors and determining when bin magnitudes have reached a sufficient level. The presented method, called meaningful clamping, contrasts from the current SIFT implementation by efficiently computing a clamping threshold that is unique for every descriptor.△ Less"
Learning to Describe Phrases with Local and Global Contexts,"Authors:Shonosuke Ishiwatari,Hiroaki Hayashi,Naoki Yoshinaga,Graham Neubig,Shoetsu Sato,Masashi Toyoda,Masaru Kitsuregawa","Abstract:…2017; Gadetsky+ 2018], our model appropriately takes important clues from both local and global contexts. Experimental results on three existing datasets (including WordNet,Oxfordand Urban Dictionaries) and a dataset newly created from Wikipedia demonstrate the effectiveness of our method over previous work.▽ MoreWhen reading a text, it is common to become stuck on unfamiliar words and phrases, such as polysemous words with novel senses, rarely used idioms, internet slang, or emerging entities. If we humans cannot figure out the meaning of those expressions from the immediate local context, we consult dictionaries for definitions or search documents or the web to find other global context to help in interpretation. Can machines help us do this work? Which type of context is more important for machines to solve the problem? To answer these questions, we undertake a task of describing a given phrase in natural language based on its local and global contexts. To solve this task, we propose a neural description model that consists of two context encoders and a description decoder. In contrast to the existing methods for non-standard English explanation [Ni+ 2017] and definition generation [Noraset+ 2017; Gadetsky+ 2018], our model appropriately takes important clues from both local and global contexts. Experimental results on three existing datasets (including WordNet,Oxfordand Urban Dictionaries) and a dataset newly created from Wikipedia demonstrate the effectiveness of our method over previous work.△ Less"
Text-Adaptive Generative Adversarial Networks: Manipulating Images with Natural Language,"Authors:Seonghyeon Nam,Yunji Kim,Seon Joo Kim","Abstract:…learns to generate images where only regions that correspond to the given text are modified. Experimental results show that our method outperforms existing methods on CUB andOxford-102 datasets, and our results were mostly preferred on a user study. Extensive analysis shows that our method is able to effectively disentangle visual attributes and produce ple…▽ MoreThis paper addresses the problem of manipulating images using natural language description. Our task aims to semantically modify visual attributes of an object in an image according to the text describing the new visual appearance. Although existing methods synthesize images having new attributes, they do not fully preserve text-irrelevant contents of the original image. In this paper, we propose the text-adaptive generative adversarial network (TAGAN) to generate semantically manipulated images while preserving text-irrelevant contents. The key to our method is the text-adaptive discriminator that creates word-level local discriminators according to input text to classify fine-grained attributes independently. With this discriminator, the generator learns to generate images where only regions that correspond to the given text are modified. Experimental results show that our method outperforms existing methods on CUB andOxford-102 datasets, and our results were mostly preferred on a user study. Extensive analysis shows that our method is able to effectively disentangle visual attributes and produce pleasing outputs.△ Less"
Free Lunch,Authors:George Svetlichny,"Abstract:The Free Lunch Principle: Nature thrives on freebies. She chooses nothing, and no one helps Her. She must use canonical mathematical structures as there is no one to tell Her otherwise. With this I show where variational principles are superfluous and Noether's theorem is trivial.The Free Lunch Principle: Nature thrives on freebies. She chooses nothing, and no one helps Her. She must use canonical mathematical structures as there is no one to tell Her otherwise. With this I show where variational principles are superfluous and Noether's theorem is trivial.△ Less"
Two-axis goniometer for single-crystal nuclear magnetic resonance measurements,"Authors:Tonči Cvitanić,Marin Lukas,Mihael S. Grbić",Abstract:…of operating with high precision at both low and high temperatures and in high magnetic fields. It was mounted on the custom made nuclear magnetic resonance probe for use inOxfordInstruments wide-bore variable field superconducting magnet.▽ MoreWe report the design and construction of a two-axis goniometer capable of any sample orientation with respect to the external magnetic field. The advantage of this design is that it allows free rotations around a single axis independent of the other which minimizes rotational error without reduction of angle range. Goniometer is capable of operating with high precision at both low and high temperatures and in high magnetic fields. It was mounted on the custom made nuclear magnetic resonance probe for use inOxfordInstruments wide-bore variable field superconducting magnet.△ Less
Proceedings 18th Refinement Workshop,"Authors:John Derrick,Brijesh Dongol,Steve Reeves","Abstract:…also allow the possibility of precise and verifiable development, as captured by the concept of refinement.
  The 18th Refinement Workshop was held as part of FLoC 2018 atOxford, UK.▽ MoreRefinement is one of the cornerstones of a formal approach to software engineering. Refinement is the process of developing a more detailed design or implementation from an abstract specification through a sequence of mathematically-based steps that maintain correctness with respect to the original specification. Work on the foundations of languages such as Z, B, VDM and CSP have led to their widespread use in certain industrial sectors, e.g., those with security or safety critical concerns. In addition to precise specification, formal methods also allow the possibility of precise and verifiable development, as captured by the concept of refinement.
  The 18th Refinement Workshop was held as part of FLoC 2018 atOxford, UK.△ Less"
Redefining Revolutions,Authors:Andrew Aberdein,"Abstract:…some putative mathematical revolutions.
  [1] Andrew Aberdein and Stephen Read, The philosophy of alternative logics, The Development of Modern Logic (Leila Haaparanta, ed.),OxfordUniversity Press,…▽ MoreIn their account of theory change in logic, Aberdein and Read distinguish 'glorious' from 'inglorious' revolutions--only the former preserves all 'the key components of a theory' [1]. A widespread view, expressed in these terms, is that empirical science characteristically exhibits inglorious revolutions but that revolutions in mathematics are at most glorious [2]. Here are three possible responses:
  0. Accept that empirical science and mathematics are methodologically discontinuous;
  1. Argue that mathematics can exhibit inglorious revolutions;
  2. Deny that inglorious revolutions are characteristic of science.
  Where Aberdein and Read take option 1, option 2 is preferred by Mizrahi [3]. This paper seeks to resolve this disagreement through consideration of some putative mathematical revolutions.
  [1] Andrew Aberdein and Stephen Read, The philosophy of alternative logics, The Development of Modern Logic (Leila Haaparanta, ed.),OxfordUniversity Press,Oxford, 2009, pp. 613-723.
  [2] Donald Gillies (ed.), Revolutions in Mathematics,OxfordUniversity Press,Oxford, 1992.
  [3] Moti Mizrahi, Kuhn's incommensurability thesis: What's the argument?, Social Epistemology 29 (2015), no. 4, 361-378.△ Less"
CNN-SVO: Improving the Mapping in Semi-Direct Visual Odometry Using Single-Image Depth Prediction,"Authors:Shing Yan Loo,Ali Jahani Amiri,Syamsiah Mashohor,Sai Hong Tang,Hong Zhang","Abstract:…correspondence between views and fast convergence to the true depth in order to create new map points. We evaluate our method with two outdoor datasets: KITTI dataset andOxfordRobotcar dataset. The experimental results indicate that the improved SVO mapping results in increased robustness and camera tracking accuracy.▽ MoreReliable feature correspondence between frames is a critical step in visual odometry (VO) and visual simultaneous localization and mapping (V-SLAM) algorithms. In comparison with existing VO and V-SLAM algorithms, semi-direct visual odometry (SVO) has two main advantages that lead to state-of-the-art frame rate camera motion estimation: direct pixel correspondence and efficient implementation of probabilistic mapping method. This paper improves the SVO mapping by initializing the mean and the variance of the depth at a feature location according to the depth prediction from a single-image depth prediction network. By significantly reducing the depth uncertainty of the initialized map point (i.e., small variance centred about the depth prediction), the benefits are twofold: reliable feature correspondence between views and fast convergence to the true depth in order to create new map points. We evaluate our method with two outdoor datasets: KITTI dataset andOxfordRobotcar dataset. The experimental results indicate that the improved SVO mapping results in increased robustness and camera tracking accuracy.△ Less"
Interest point detectors stability evaluation on ApolloScape dataset,"Authors:Jacek Komorowski,Konrad Czarnota,Tomasz Trzcinski,Lukasz Dabala,Simon Lynen","Abstract:…a lack of a standard benchmark to evaluate suitability of these novel keypoint detectors for real-live applications such as autonomous driving. Traditional benchmarks (e.g.OxfordVGG) are rather limited, as they consist of relatively few images of mostly planar scenes taken in favourable conditions. In this paper we verify if the recent, deep-learning based…▽ MoreIn the recent years, a number of novel, deep-learning based, interest point detectors, such as LIFT, DELF, Superpoint or LF-Net was proposed. However there's a lack of a standard benchmark to evaluate suitability of these novel keypoint detectors for real-live applications such as autonomous driving. Traditional benchmarks (e.g.OxfordVGG) are rather limited, as they consist of relatively few images of mostly planar scenes taken in favourable conditions. In this paper we verify if the recent, deep-learning based interest point detectors have the advantage over the traditional, hand-crafted keypoint detectors. To this end, we evaluate stability of a number of hand crafted and recent, learning-based interest point detectors on the street-level view ApolloScape dataset.△ Less"
C4Synth: Cross-Caption Cycle-Consistent Text-to-Image Synthesis,"Authors:K J Joseph,Arghya Pal,Sailaja Rajanala,Vineeth N Balasubramanian","Abstract:…Cycle Consistency' between the multiple captions and the generated image(s). We report quantitative and qualitative results on the standard Caltech-UCSD Birds (CUB) andOxford-102 Flowers datasets to validate the efficacy of the proposed approach.▽ MoreGenerating an image from its description is a challenging task worth solving because of its numerous practical applications ranging from image editing to virtual reality. All existing methods use one single caption to generate a plausible image. A single caption by itself, can be limited, and may not be able to capture the variety of concepts and behavior that may be present in the image. We propose two deep generative models that generate an image by making use of multiple captions describing it. This is achieved by ensuring 'Cross-Caption Cycle Consistency' between the multiple captions and the generated image(s). We report quantitative and qualitative results on the standard Caltech-UCSD Birds (CUB) andOxford-102 Flowers datasets to validate the efficacy of the proposed approach.△ Less"
OxIOD: The Dataset for Deep Inertial Odometry,"Authors:Changhao Chen,Peijun Zhao,Chris Xiaoxuan Lu,Wei Wang,Andrew Markham,Niki Trigoni","Abstract:…sufficient labelled data for training and testing various architectures limits the proliferation of adopting DNNs in IMU-based tasks. In this paper, we propose and release theOxfordInertial Odometry Dataset (OxIOD), a first-of-its-kind data collection for inertial-odometry research, with all sequences having ground-truth labels. Our dataset contains 158 se…▽ MoreAdvances in micro-electro-mechanical (MEMS) techniques enable inertial measurements units (IMUs) to be small, cheap, energy efficient, and widely used in smartphones, robots, and drones. Exploiting inertial data for accurate and reliable navigation and localization has attracted significant research and industrial interest, as IMU measurements are completely ego-centric and generally environment agnostic. Recent studies have shown that the notorious issue of drift can be significantly alleviated by using deep neural networks (DNNs), e.g. IONet. However, the lack of sufficient labelled data for training and testing various architectures limits the proliferation of adopting DNNs in IMU-based tasks. In this paper, we propose and release theOxfordInertial Odometry Dataset (OxIOD), a first-of-its-kind data collection for inertial-odometry research, with all sequences having ground-truth labels. Our dataset contains 158 sequences totalling more than 42 km in total distance, much larger than previous inertial datasets. Another notable feature of this dataset lies in its diversity, which can reflect the complex motions of phone-based IMUs in various everyday usage. The measurements were collected with four different attachments (handheld, in the pocket, in the handbag and on the trolley), four motion modes (halting, walking slowly, walking normally, and running), five different users, four types of off-the-shelf consumer phones, and large-scale localization from office buildings. Deep inertial tracking experiments were conducted to show the effectiveness of our dataset in training deep neural network models and evaluate learning-based and model-based algorithms. The OxIOD Dataset is available at: http://deepio.cs.ox.ac.uk△ Less"
The Method of Alternating Projections,Authors:Omer Ginat,"Abstract:The method of alternating projections involves orthogonally projecting an element of a Hilbert space onto a collection of closed subspaces. It is known that the resulting sequence always converges in norm if the projections are taken periodically, or even quasiperiodically. We present proofs of such well known results, and offer an original proof for the case of two closed subspaces, known as von…▽ MoreThe method of alternating projections involves orthogonally projecting an element of a Hilbert space onto a collection of closed subspaces. It is known that the resulting sequence always converges in norm if the projections are taken periodically, or even quasiperiodically. We present proofs of such well known results, and offer an original proof for the case of two closed subspaces, known as von Neumann's theorem. Additionally, it is known that this sequence always converges with respect to the weak topology, regardless of the order projections are taken in. By focusing on projections directly, rather than the more general case of contractions considered previously in the literature, we are able to give a simpler proof of this result. We end by presenting a technical construction taken from a recent paper, of a sequence for which we do not have convergence in norm.△ Less"
"Precessing spherical shells: flows, dissipation, dynamo and the lunar core","Authors:David Cébron,Raphaël Laguerre,Jerome Noir,Nathanaël Schaeffer","Abstract:Precession of planets or moons affects internal liquid layers by driving flows, instabilities and possibly dynamos.The energy dissipated by these phenomena can influence orbital parameters such as the planet's spin rate.However, there is no systematic study of these flows in the spherical shell geometry relevant for planets, and the lack of scaling law prevents convincing extrapolation to celestia…▽ MorePrecession of planets or moons affects internal liquid layers by driving flows, instabilities and possibly dynamos.The energy dissipated by these phenomena can influence orbital parameters such as the planet's spin rate.However, there is no systematic study of these flows in the spherical shell geometry relevant for planets, and the lack of scaling law prevents convincing extrapolation to celestial bodies.We have run more than 900 simulations of fluid spherical shells affected by precession, to systematically study basic flows, instabilities, turbulence, and magnetic field generation.We observe no significant effects of the inner core on the onset of the instabilities.We obtain an analytical estimate of the viscous dissipation, mostly due to boundary layer friction in our simulations.We propose theoretical onsets for hydrodynamic instabilities, and document the intensity of turbulent fluctuations.We extend previous precession dynamo studies towards lower viscosities, at the limits of today's computers.In the low viscosity regime, precession dynamos rely on the presence of large-scale vortices, and the surface magnetic fields are dominated by small scales.Interestingly, intermittent and self-killing dynamos are observed.Our results suggest that large-scale planetary magnetic fields are unlikely to be produced by a precession-driven dynamo in a spherical core.But this question remains open as planetary cores are not exactly spherical, and thus the coupling between the fluid and the boundary does not vanish in the relevant limit of small viscosity.Moreover, the fully turbulent dissipation regime has not yet been reached in simulations.Our results suggest that the melted lunar core has been in a turbulent state throughout its history.Furthermore, in the view of recent experimental results, we propose updated formulas predicting the fluid mean rotation vector and the associated dissipation in both the laminar and the turbulent regimes.△ Less"
The Cosmic Spiderweb and General Origami Tessellation Design,Authors:Mark Neyrinck,"Abstract:The cosmic web (the arrangement of matter in the universe), spider's webs, and origami tessellations are linked by their geometry (specifically, of sectional-Voronoi tessellations). This motivates origami and textile artistic representations of the cosmic web. It also relates to the scientific insights origami can bring to the cosmic web; we show results of some cosmological computer simulations,…▽ MoreThe cosmic web (the arrangement of matter in the universe), spider's webs, and origami tessellations are linked by their geometry (specifically, of sectional-Voronoi tessellations). This motivates origami and textile artistic representations of the cosmic web. It also relates to the scientific insights origami can bring to the cosmic web; we show results of some cosmological computer simulations, with some origami-tessellation properties. We also adapt software developed for cosmic-web research to provide an interactive tool for general origami-tessellation design.△ Less"
Heterotic String Models on Smooth Calabi-Yau Threefolds,Authors:Andrei Constantin,"Abstract:This thesis contributes with a number of topics to the subject of string compactifications. In the first half of the work, I discuss the Hodge plot of Calabi-Yau threefolds realised as hypersurfaces in toric varieties. The intricate structure of this plot is explained by the existence of certain webs of elliptic-K3 fibrations. Such manifolds arise from reflexive polytopes that can be cut into two…▽ MoreThis thesis contributes with a number of topics to the subject of string compactifications. In the first half of the work, I discuss the Hodge plot of Calabi-Yau threefolds realised as hypersurfaces in toric varieties. The intricate structure of this plot is explained by the existence of certain webs of elliptic-K3 fibrations. Such manifolds arise from reflexive polytopes that can be cut into two parts along K3 slices. Any two half-polytopes over a given slice can be combined into a reflexive polytope. This fact, together with a remarkable relation on the additivity of Hodge numbers, give to the Hodge plot the appearance of a fractal. Moving on, I discuss a different type of web by looking at smooth $\mathbb{Z}_3$-quotients of complete intersection Calabi-Yau threefolds in products of projective spaces. In the second half of the work, I explore an algorithmic approach to constructing heterotic compactifications with holomorphic and polystable sums of line bundles over complete intersection Calabi-Yau threefolds that admit freely acting discrete symmetries. Such abelian bundles lead to $N=1$ GUT theories with gauge group $SU(5)\times U(4)$. The extra $U(1)$ symmetries are generically Green-Schwarz anomalous and survive in the low energy theory as global symmetries that constrain the allowed operators. The line bundle construction allows for a systematic computer search resulting in a plethora of models with one or more pairs of Higgs doublets and no exotic fields charged under the Standard Model group. In the last part of the thesis I focus on the case study of the tetraquadric threefold and address the question of the finiteness of the class of consistent and physically viable line bundle models on this manifold. Line bundle sums provide an accessible window into the moduli space of non-abelian bundles. I explore this moduli space around the abelian locus using monad bundles.△ Less"
Measuring the Volatility of the Political agenda in Public Opinion and News Media,"Authors:Chico Q. Camargo,Scott A. Hale,Peter John,Helen Z. Margetts","Abstract:Recent election surprises, regime changes, and political shocks indicate that political agendas have become more fast-moving and volatile. The ability to measure the complex dynamics of agenda change and capture the nature and extent of volatility in political systems is therefore more crucial than ever before. This study proposes a definition and operationalization of volatility that combines ins…▽ MoreRecent election surprises, regime changes, and political shocks indicate that political agendas have become more fast-moving and volatile. The ability to measure the complex dynamics of agenda change and capture the nature and extent of volatility in political systems is therefore more crucial than ever before. This study proposes a definition and operationalization of volatility that combines insights from political science, communications, information theory, and computational techniques. The proposed measures of fractionalization and agenda change encompass the shifting salience of issues in the agenda as a whole and allow the study of agendas across different domains. We evaluate these metrics and compare them to other measures such as issue-level survival rates and the Pedersen Index, which uses public-opinion poll data to measure public agendas, as well as traditional media content to measure media agendas in the UK and Germany. We show how these measures complement existing approaches and could be employed in future agenda-setting research.△ Less"
Evolving Algebras 1993: Lipari Guide,Authors:Yuri Gurevich,"Abstract:Computation models and specification methods seem to be worlds apart. The project on abstract state machines (in short ASMs, also known as evolving algebras) started as an attempt to bridge the gap by improving on Turing's thesis. We sought more versatile machines which would be able to step-for-step simulate arbitrary algorithms on their natural abstraction levels. The ASM thesis asserts that ASM…▽ MoreComputation models and specification methods seem to be worlds apart. The project on abstract state machines (in short ASMs, also known as evolving algebras) started as an attempt to bridge the gap by improving on Turing's thesis. We sought more versatile machines which would be able to step-for-step simulate arbitrary algorithms on their natural abstraction levels. The ASM thesis asserts that ASMs are such versatile machines. The guide provides the definitions of sequential, parallel and distributed ASMs.△ Less"
Dagger completions and bornological torsion-freeness,"Authors:Ralf Meyer,Devarshi Mukherjee","Abstract:We define a dagger algebra as a bornological algebra over a discrete valuation ring with three properties that are typical of Monsky-Washnitzer algebras, namely, completeness, bornological torsion-freeness and a certain spectral radius condition. We study inheritance properties of the three properties that define a dagger algebra. We describe dagger completions of bornological algebras in general…▽ MoreWe define a dagger algebra as a bornological algebra over a discrete valuation ring with three properties that are typical of Monsky-Washnitzer algebras, namely, completeness, bornological torsion-freeness and a certain spectral radius condition. We study inheritance properties of the three properties that define a dagger algebra. We describe dagger completions of bornological algebras in general and compute some noncommutative examples.△ Less"
Pure states statistical mechanics: On its foundations and applications to quantum gravity,Authors:Fabio Anza,"Abstract:The project concerns the interplay among quantum mechanics, statistical mechanics and thermodynamics, in isolated quantum systems. The underlying goal is to improve our understanding of the concept of thermal equilibrium in quantum systems. First, I investigated the role played by observables and measurements in the emergence of thermal behaviour. This led to a new notion of thermal equilibrium wh…▽ MoreThe project concerns the interplay among quantum mechanics, statistical mechanics and thermodynamics, in isolated quantum systems. The underlying goal is to improve our understanding of the concept of thermal equilibrium in quantum systems. First, I investigated the role played by observables and measurements in the emergence of thermal behaviour. This led to a new notion of thermal equilibrium which is specific for a given observable, rather than for the whole state of the system. The equilibrium picture that emerges is a generalization of statistical mechanics in which we are not interested in the state of the system but only in the outcome of the measurement process. I investigated how this picture relates to one of the most promising approaches for the emergence of thermal behaviour in isolated quantum systems: the Eigenstate Thermalization Hypothesis. Then, I applied the results to study some equilibrium properties of many-body localised systems. Despite the localization phenomenon, which prevents thermalization of subsystems, I was able to show that we can still use the predictions of statistical mechanics to describe the equilibrium of some observables. Moreover, the intuition developed in the process led me to propose an experimentally accessible way to unravel the interacting nature of many-body localised systems. Second, I exploited the ""Concentration of Measure"" phenomenon to study the macroscopic properties of the basis states of Loop Quantum Gravity. These techniques were previously used to explain why the thermal behaviour in quantum systems is such an ubiquitous phenomenon, at the macroscopic scale. I focused on the local properties, their thermodynamic behaviour and interplay with the semiclassical limit. This was motivated by the necessity to understand, from a quantum gravity perspective, how and why a classical horizon exhibits thermal properties.△ Less"
Improving Transferability of Deep Neural Networks,"Authors:Parijat Dube,Bishwaranjan Bhattacharjee,Elisabeth Petit-Bois,Matthew Hill",Abstract:…source dataset are chosen well. One of the important parameters is the learning rate for the layers of the neural network. We show through experiments on the ImageNet22k andOxfordFlowers datasets that improvements in accuracy in range of 127% can be obtained by proper choice of learning rates. We also show that the images/label parameter for a dataset can…▽ MoreLearning from small amounts of labeled data is a challenge in the area of deep learning. This is currently addressed by Transfer Learning where one learns the small data set as a transfer task from a larger source dataset. Transfer Learning can deliver higher accuracy if the hyperparameters and source dataset are chosen well. One of the important parameters is the learning rate for the layers of the neural network. We show through experiments on the ImageNet22k andOxfordFlowers datasets that improvements in accuracy in range of 127% can be obtained by proper choice of learning rates. We also show that the images/label parameter for a dataset can potentially be used to determine optimal learning rates for the layers to get the best overall accuracy. We additionally validate this method on a sample of real-world image classification tasks from a public visual recognition API.△ Less
A Banach space whose algebra of operators is Dedekind-finite but it does not have stable rank one,Authors:Bence Horváth,"Abstract:…constructed by Tarbard (Ph.D. Thesis, University ofOxford, 2013), the algebra of operators $B(X_{\infty})$ is Dedekind-finite but does not have stable rank one. While this sheds some light on the Banach space structure of $X_{\infty}$ itself, we observe that the indecomposable but not hereditarily indecomposable Banach space constructed by Gowers and Maure…▽ MoreIn this note we examine the connection between the stable rank one and Dedekind-finite property of the algebra of operators on a Banach space $X$. We show that for the indecomposable but not hereditarily indecomposable Banach space $X_{\infty}$ constructed by Tarbard (Ph.D. Thesis, University ofOxford, 2013), the algebra of operators $B(X_{\infty})$ is Dedekind-finite but does not have stable rank one. While this sheds some light on the Banach space structure of $X_{\infty}$ itself, we observe that the indecomposable but not hereditarily indecomposable Banach space constructed by Gowers and Maurey (Math. Ann., 1997) does not possess this property.△ Less"
Proper motions and dynamics of the Milky Way globular cluster system from Gaia DR2,Authors:Eugene Vasiliev,"Abstract:We use Gaia Data Release 2 to determine the mean proper motions for 150 Milky Way globular clusters (almost the entire known population), with a typical uncertainty of 0.05 mas/yr limited mainly by systematic errors. Combining them with distance and line-of-sight velocity measurements from the literature, we analyze the distribution of globular clusters in the 6d phase space, using both position/v…▽ MoreWe use Gaia Data Release 2 to determine the mean proper motions for 150 Milky Way globular clusters (almost the entire known population), with a typical uncertainty of 0.05 mas/yr limited mainly by systematic errors. Combining them with distance and line-of-sight velocity measurements from the literature, we analyze the distribution of globular clusters in the 6d phase space, using both position/velocity and action/angle coordinates. The population of clusters in the central 10 kpc has a mean rotational velocity reaching 50-80 km/s, and a nearly isotropic velocity dispersion 100-120 km/s, while in the outer galaxy the cluster orbits are strongly radially anisotropic. We confirm a concentration of clusters at high radial action in the outer region of the Galaxy. Finally, we explore a range of equilibrium distribution function-based models for the entire globular cluster system, and the information they provide about the potential of the Milky Way. The dynamics of clusters is best described by models with the circular velocity between 10 and 50 kpc staying in the range 210-240 km/s.△ Less"
The Vadalog System: Datalog-based Reasoning for Knowledge Graphs,"Authors:Luigi Bellomarini,Georg Gottlob,Emanuel Sallinger","Abstract:…present the Vadalog system, a Datalog-based system for performing complex logic reasoning tasks, such as those required in advanced knowledge graphs. The Vadalog system isOxford'scontribution to the VADA research programme, a joint effort of the universities ofOxford, Manchester and Edinburgh and around 20 indus…▽ MoreOver the past years, there has been a resurgence of Datalog-based systems in the database community as well as in industry. In this context, it has been recognized that to handle the complex knowl\-edge-based scenarios encountered today, such as reasoning over large knowledge graphs, Datalog has to be extended with features such as existential quantification. Yet, Datalog-based reasoning in the presence of existential quantification is in general undecidable. Many efforts have been made to define decidable fragments. Warded Datalog+/- is a very promising one, as it captures PTIME complexity while allowing ontological reasoning. Yet so far, no implementation of Warded Datalog+/- was available. In this paper we present the Vadalog system, a Datalog-based system for performing complex logic reasoning tasks, such as those required in advanced knowledge graphs. The Vadalog system isOxford'scontribution to the VADA research programme, a joint effort of the universities ofOxford, Manchester and Edinburgh and around 20 industrial partners. As the main contribution of this paper, we illustrate the first implementation of Warded Datalog+/-, a high-performance Datalog+/- system utilizing an aggressive termination control strategy. We also provide a comprehensive experimental evaluation.△ Less"
Quantile contours and allometric modelling for risk classification of abnormal ratios with an application to asymmetric growth-restriction in preterm infants,"Authors:Marco Geraci,Nansi S. Boghossian,Alessio Farcomeni,Jeffrey D. Horbar","Abstract:…definition of directional quantile that can be used as cutoff for risk assessment. We show the application of the proposed approach using a large dataset from the VermontOxfordNetwork containing observations of birthweight (BW) and head circumference (HC) for more than 150,000 preterm infants. Our analysis suggests that disproportionately growth-restricted…▽ MoreWe develop an approach to risk classification based on quantile contours and allometric modelling of multivariate anthropometric measurements. We propose the definition of allometric direction tangent to the directional quantile envelope, which divides ratios of measurements into half-spaces. This in turn provides an operational definition of directional quantile that can be used as cutoff for risk assessment. We show the application of the proposed approach using a large dataset from the VermontOxfordNetwork containing observations of birthweight (BW) and head circumference (HC) for more than 150,000 preterm infants. Our analysis suggests that disproportionately growth-restricted infants with a larger HC-to-BW ratio are at increased mortality risk as compared to proportionately growth-restricted infants. The role of maternal hypertension is also investigated.△ Less"
ELT HARMONI: Image Slicer Preliminary Design,"Authors:Florence Laurent,Didier Boudon,Johan Kosmalski,Magali Loupias,Guillaume Raffault,Alban Remillieux,Niranjan Thatte,Ian Bryson,Hermine Schnetler,Fraser Clarke,Matthias Tecza","Abstract:…instantaneous wavelength coverage spanning the 0.47 - 2.45 μm wavelength range of the instrument. The consortium consists of several institutes in Europe under leadership ofOxfordUniversity. Harmoni is starting its Final Design Phase after a Preliminary Design Phase in November, 2017. The CRAL has the responsibility of the Integral Field Unit design linkin…▽ MoreHarmoni is the ELT's first light visible and near-infrared integral field spectrograph. It will provide four different spatial scales, ranging from coarse spaxels of 60 x 30 mas best suited for seeing limited observations, to 4 mas spaxels that Nyquist sample the diffraction limited point spread function of the ELT at near-infrared wavelengths. Each spaxel scale may be combined with eleven spectral settings, that provide a range of spectral resolving powers from R 3500 to R 20000 and instantaneous wavelength coverage spanning the 0.47 - 2.45 μm wavelength range of the instrument. The consortium consists of several institutes in Europe under leadership ofOxfordUniversity. Harmoni is starting its Final Design Phase after a Preliminary Design Phase in November, 2017. The CRAL has the responsibility of the Integral Field Unit design linking the Preoptics to the 4 Spectrographs. It is composed of a field splitter associated with a relay system and an image slicer that create from a rectangular Field of View a very long (540mm) output slit for each spectrograph. In this paper, the preliminary design and performances of Harmoni Image Slicer will be presented including image quality, pupil distortion and slit geometry. It has been designed by CRAL for Harmoni PDR in November, 2017. Special emphases will be put on straylight analysis and slice diffraction. The optimisation of the manufacturing and slit geometry will also be reported.△ Less"
Levels of spacetime emergence in quantum gravity,Authors:Daniele Oriti,"Abstract:We explore the issue of spacetime emergence in quantum gravity, by articulating several levels at which this can be intended. These levels correspond to the reconstruction moves that are needed to recover the classical and continuum notion of space and time, which are progressively lost in a progressively deeper sense in the more fundamental quantum gravity description. They can also be understood…▽ MoreWe explore the issue of spacetime emergence in quantum gravity, by articulating several levels at which this can be intended. These levels correspond to the reconstruction moves that are needed to recover the classical and continuum notion of space and time, which are progressively lost in a progressively deeper sense in the more fundamental quantum gravity description. They can also be understood as successive steps in a process of widening of the perspective, revealing new details and new questions at each step. Each level carries indeed new technical issues and opportunities, and raises new conceptual issues. This deepens the scope of the debate on the nature of spacetime, both philosophically and physically.△ Less"
Proceedings of the 7th Workshop on Mathematically Structured Functional Programming,"Authors:Robert Atkey,Sam Lindley","Abstract:…in data and control.
  The seventh workshop on Mathematically Structured Functional Programming was held on 8th July 2018 affiliated with FSCD 2018 as part of FLoC 2018 inOxford, UK. 
  There were two invited talks. In addition four full papers and two extended abstracts were selected by the programme committee for presentation.▽ MoreThe seventh workshop on Mathematically Structured Functional Programming is devoted to the derivation of functionality from structure. It is a celebration of the direct impact of Theoretical Computer Science on programs as we write them today. Modern programming languages, and in particular functional languages, support the direct expression of mathematical structures, equipping programmers with tools of remarkable power and abstraction. Where would Haskell be without monads? Functional reactive programming without arrows? Call-by-push-value without adjunctions? The list goes on. This workshop is a forum for researchers who seek to reflect mathematical phenomena in data and control.
  The seventh workshop on Mathematically Structured Functional Programming was held on 8th July 2018 affiliated with FSCD 2018 as part of FLoC 2018 inOxford, UK. 
  There were two invited talks. In addition four full papers and two extended abstracts were selected by the programme committee for presentation.△ Less"
Deep Saliency Hashing,"Authors:Sheng Jin,Hongxun Yao,Xiaoshuai Sun,Shangchen Zhou,Lei Zhang,Xiansheng Hua","Abstract:…We conduct extensive experiments on both fine-grained and general retrieval datasets for performance evaluation. Experimental results on fine-grained datasets, includingOxfordFlowers-17, Stanford Dogs-120, and CUB Bird demonstrate that our DSaH performs the best for fine-grained retrieval task and beats the strongest competitor (DTQ) by approximately 10%…▽ MoreIn recent years, hashing methods have been proved to be effective and efficient for the large-scale Web media search. However, the existing general hashing methods have limited discriminative power for describing fine-grained objects that share similar overall appearance but have subtle difference. To solve this problem, we for the first time introduce the attention mechanism to the learning of fine-grained hashing codes. Specifically, we propose a novel deep hashing model, named deep saliency hashing (DSaH), which automatically mines salient regions and learns semantic-preserving hashing codes simultaneously. DSaH is a two-step end-to-end model consisting of an attention network and a hashing network. Our loss function contains three basic components, including the semantic loss, the saliency loss, and the quantization loss. As the core of DSaH, the saliency loss guides the attention network to mine discriminative regions from pairs of images. We conduct extensive experiments on both fine-grained and general retrieval datasets for performance evaluation. Experimental results on fine-grained datasets, includingOxfordFlowers-17, Stanford Dogs-120, and CUB Bird demonstrate that our DSaH performs the best for fine-grained retrieval task and beats the strongest competitor (DTQ) by approximately 10% on both Stanford Dogs-120 and CUB Bird. DSaH is also comparable to several state-of-the-art hashing methods on general datasets, including CIFAR-10 and NUS-WIDE.△ Less"
Proceedings of the 13th International Workshop on Logical Frameworks and Meta-Languages: Theory and Practice,"Authors:Frédéric Blanqui,Giselle Reis","Abstract:…a selection of papers presented at LFMTP 2018, the 13th international Workshop on Logical Frameworks and Meta-Languages: Theory and Practice (LFMTP), held on July 7, 2018, inOxford, UK. The workshop was affiliated with the 3rd international conference on Formal Structures for Computation and Deduction (FSCD) within the 7th Federated Logic Conference (FLoC).▽ MoreThis volume contains a selection of papers presented at LFMTP 2018, the 13th international Workshop on Logical Frameworks and Meta-Languages: Theory and Practice (LFMTP), held on July 7, 2018, inOxford, UK. The workshop was affiliated with the 3rd international conference on Formal Structures for Computation and Deduction (FSCD) within the 7th Federated Logic Conference (FLoC).△ Less"
Generate the corresponding Image from Text Description using Modified GAN-CLS Algorithm,"Authors:Fuzhou Gong,Zigeng Xia","Abstract:…through inference. Then we correct the GAN-CLS algorithm according to the inference by modifying the objective function of the model. Finally, we do the experiments on theOxford-102 dataset and the CUB dataset. As a result, our modified algorithm can generate images which are more plausible than the GAN-CLS algorithm in some cases. Also, some of the generat…▽ MoreSynthesizing images or texts automatically is a useful research area in the artificial intelligence nowadays. Generative adversarial networks (GANs), which are proposed by Goodfellow in 2014, make this task to be done more efficiently by using deep neural networks. We consider generating corresponding images from an input text description using a GAN. In this paper, we analyze the GAN-CLS algorithm, which is a kind of advanced method of GAN proposed by Scott Reed in 2016. First, we find the problem with this algorithm through inference. Then we correct the GAN-CLS algorithm according to the inference by modifying the objective function of the model. Finally, we do the experiments on theOxford-102 dataset and the CUB dataset. As a result, our modified algorithm can generate images which are more plausible than the GAN-CLS algorithm in some cases. Also, some of the generated images match the input texts better.△ Less"
The Monge-Ampère Equation,Authors:Connor Mooney,"Abstract:…two mini-courses given by the author in May 2018. One was for ""Advanced Lectures in Nonlinear Analysis"" at l'Università degli Studi di Torino, and the other for theOxfordPDE CDT.▽ MoreIn this survey article we discuss the interior and boundary regularity of Alexandrov solutions to $\det D^2u = 1$. We include some topics which it seems were not recently revisited in similar articles, including Calabi's interior $C^3$ estimate, and the approaches of Cheng-Yau and Lions to obtain classical solutions to the Dirichlet problem.
  The survey grew from two mini-courses given by the author in May 2018. One was for ""Advanced Lectures in Nonlinear Analysis"" at l'Università degli Studi di Torino, and the other for theOxfordPDE CDT.△ Less"
Identification and analysis of the young population in the starburst galaxy NGC 253,"Authors:M. J. Rodríguez,G. Baume,C. Feinstein","Abstract:We present a study of the young population in the starburst galaxy NGC 253. In particular, we focused our attention on searching young star groups, obtaining their main properties and studying their hierarchical organization. For this task, we used multiband images and their corresponding photometric data obtained with the Advanced Camera for Surveys of the Hubble Space Telescope (ACS/HST).
  We h…▽ MoreWe present a study of the young population in the starburst galaxy NGC 253. In particular, we focused our attention on searching young star groups, obtaining their main properties and studying their hierarchical organization. For this task, we used multiband images and their corresponding photometric data obtained with the Advanced Camera for Surveys of the Hubble Space Telescope (ACS/HST).
  We have first derived the absorption affecting the different regions of the galaxy. Then, we applied an automatic and objective searching method over the corrected data in order to detect young star groups. We complemented this result with the construction of the stellar density map for the blue young population. A statistical procedure to decontaminate the photometric diagrams from field stars was applied over the detected groups and we estimated their fundamental parameters.
  As a result, we built a catalog of 875 new identified young groups with their main characteristics, including coordinates, sizes, estimated number of members, stellar densities, luminosity function (LF) slopes and galactocentric distances. We observed these groups delineate different structures of the galaxy, and they are the last step in the hierarchical way in which the young population is organized. From their size distribution, we found they have typical radius of $\sim 40 - 50$ pc. These values are consistent with those ones found in others nearby galaxies. We estimated a mean value of the LF slope of 0.21 and an average density of 0.0006 stars/pc$^3$ for the identified young groups taking into account stars earlier than B6.△ Less"
Part-Aware Fine-grained Object Categorization using Weakly Supervised Part Detection Network,"Authors:Yabin Zhang,Kui Jia,Zhixin Wang","Abstract:…(DPP) that directly targets for proposing candidates of discriminative local parts, with no bridging via object-level proposals. Experiments on the benchmark CUB-200-2011 andOxfordFlower 102 datasets show the efficacy of our proposed method for both discriminative part detection and fine-grained categorization. In particular, we achieve the new state-of-th…▽ MoreFine-grained object categorization aims for distinguishing objects of subordinate categories that belong to the same entry-level object category. The task is challenging due to the facts that (1) training images with ground-truth labels are difficult to obtain, and (2) variations among different subordinate categories are subtle. It is well established that characterizing features of different subordinate categories are located on local parts of object instances. In fact, careful part annotations are available in many fine-grained categorization datasets. However, manually annotating object parts requires expertise, which is also difficult to generalize to new fine-grained categorization tasks. In this work, we propose a Weakly Supervised Part Detection Network (PartNet) that is able to detect discriminative local parts for use of fine-grained categorization. A vanilla PartNet builds on top of a base subnetwork two parallel streams of upper network layers, which respectively compute scores of classification probabilities (over subordinate categories) and detection probabilities (over a specified number of discriminative part detectors) for local regions of interest (RoIs). The image-level prediction is obtained by aggregating element-wise products of these region-level probabilities. To generate a diverse set of RoIs as inputs of PartNet, we propose a simple Discretized Part Proposals module (DPP) that directly targets for proposing candidates of discriminative local parts, with no bridging via object-level proposals. Experiments on the benchmark CUB-200-2011 andOxfordFlower 102 datasets show the efficacy of our proposed method for both discriminative part detection and fine-grained categorization. In particular, we achieve the new state-of-the-art performance on CUB-200-2011 dataset when ground-truth part annotations are not available.△ Less"
Deep Lip Reading: a comparison of models and an online application,"Authors:Triantafyllos Afouras,Joon Son Chung,Andrew Zisserman","Abstract:…language model for decoding, the transformer is a sequence-to-sequence model. Our best performing model improves the state-of-the-art word error rate on the challenging BBC-OxfordLip Reading Sentences 2 (LRS2) benchmark dataset by over 20 percent.
  As a further contribution we investigate the fully convolutional model when used for online (real time) lip r…▽ MoreThe goal of this paper is to develop state-of-the-art models for lip reading -- visual speech recognition. We develop three architectures and compare their accuracy and training times: (i) a recurrent model using LSTMs; (ii) a fully convolutional model; and (iii) the recently proposed transformer model. The recurrent and fully convolutional models are trained with a Connectionist Temporal Classification loss and use an explicit language model for decoding, the transformer is a sequence-to-sequence model. Our best performing model improves the state-of-the-art word error rate on the challenging BBC-OxfordLip Reading Sentences 2 (LRS2) benchmark dataset by over 20 percent.
  As a further contribution we investigate the fully convolutional model when used for online (real time) lip reading of continuous speech, and show that it achieves high performance with low latency.△ Less"
Generating Image Sequence from Description with LSTM Conditional GAN,"Authors:Xu Ouyang,Xi Zhang,Di Ma,Gady Agam","Abstract:…we propose a new neural network architecture of LSTM Conditional Generative Adversarial Networks to generate images of real-life objects. Our proposed model is trained on theOxford-102 Flowers and Caltech-UCSD Birds-200-2011 datasets. We demonstrate that our proposed model produces the better results surpassing other state-of-art approaches.▽ MoreGenerating images from word descriptions is a challenging task. Generative adversarial networks(GANs) are shown to be able to generate realistic images of real-life objects. In this paper, we propose a new neural network architecture of LSTM Conditional Generative Adversarial Networks to generate images of real-life objects. Our proposed model is trained on theOxford-102 Flowers and Caltech-UCSD Birds-200-2011 datasets. We demonstrate that our proposed model produces the better results surpassing other state-of-art approaches.△ Less"
Contextuality and noncommutative geometry in quantum mechanics,"Authors:Nadish de Silva,Rui Soares Barbosa","Abstract:Observable properties of a classical physical system can be modelled deterministically as functions from the space of pure states to outcomes; dually, states can be modelled as functions from the algebra of observables to outcomes. The probabilistic predictions of quantum physics are contextual in that they preclude this classical assumption of reality: noncommuting observables, which are not assu…▽ MoreObservable properties of a classical physical system can be modelled deterministically as functions from the space of pure states to outcomes; dually, states can be modelled as functions from the algebra of observables to outcomes. The probabilistic predictions of quantum physics are contextual in that they preclude this classical assumption of reality: noncommuting observables, which are not assumed to be comeasurable, cannot be consistently ascribed deterministic values even if one enriches the description of a quantum state.
  Here, we consider the geometrically dual objects of noncommutative algebras of observables as being generalisations of classical state spaces to the quantum setting and argue that these generalised geometric spaces represent the objects of study of noncommutative operator geometry. By adapting the spectral presheaf of Hamilton-Isham-Butterfield, a formulation of quantum state space that collates contextual data, we reconstruct tools of noncommutative geometry in an explicitly geometric fashion. In this way, we bridge the foundations of quantum mechanics with the foundations of noncommutative geometry à la Connes et al.
  To each unital $C^*$-algebra we associate a geometric object acting as a generalised Gel'fand spectrum. We show how any functor $F$ from compact Hausdorff spaces to a suitable target category can be applied directly to these geometric objects to automatically yield an extension $\tilde{F}$ acting on all unital $C^*$-algebras.
  This procedure is used to give a novel formulation of the operator $K_0$-functor in terms of the topological $K$-functor. We then delineate a $C^*$-algebraic conjecture that the extension of the functor that assigns to a topological space its lattice of open sets assigns to a unital $C^*$-algebra its lattice of closed, two-sided ideals. We prove the von Neumann algebraic analogue of this conjecture.△ Less"
An introduction to stochastic processes associated with resistance forms and their scaling limits,Authors:David A. Croydon,"Abstract:…from `Time-changes of stochastic processes associated with resistance forms', which was written jointly with T.\ Kumagai (Kyoto University) and B.~M.~Hambly (University ofOxford).▽ MoreWe introduce and summarise results from the recent paper `Scaling limits of stochastic processes associated with resistance forms', and also applications from `Time-changes of stochastic processes associated with resistance forms', which was written jointly with T.\ Kumagai (Kyoto University) and B.~M.~Hambly (University ofOxford).△ Less"
The Planets in Indigenous Australian Traditions,"Authors:Duane W. Hamacher,Kirsten Banks","Abstract:Studies in Australian Indigenous astronomical knowledge reveal few accounts of the visible planets in the sky. However, what information we do have tells us that Aboriginal people were close observers of planets and their motions, noting the relative brightness of the planets, their motions along the ecliptic, retrograde motion, the relationship between Venus and its proximity to the Sun, Venus' c…▽ MoreStudies in Australian Indigenous astronomical knowledge reveal few accounts of the visible planets in the sky. However, what information we do have tells us that Aboriginal people were close observers of planets and their motions, noting the relative brightness of the planets, their motions along the ecliptic, retrograde motion, the relationship between Venus and its proximity to the Sun, Venus' connection to the Sun through zodiacal light, and the synodic cycle of Venus, particularly as it transitions from the Evening Star to the Morning Star. The dearth of descriptions of planets in Aboriginal traditions may be due to the gross incompleteness of recorded astronomical traditions, and of ethnographic bias and misidentification in the anthropological record. Ethnographic fieldwork with Aboriginal and Torres Strait Islander communities is revealing new, previously unrecorded knowledge about the planets and their related phenomena.△ Less"
Knoto-ID: a tool to study the entanglement of open protein chains using the concept of knotoids,"Authors:Julien Dorier,Dimos Goundaroulis,Fabrizio Benedetti,Andrzej Stasiak","Abstract:The backbone of most proteins forms an open curve. To study their entanglement, a common strategy consists in searching for the presence of knots in their backbones using topological invariants. However, this approach requires to close the curve into a loop, which alters the geometry of curve. Knoto-ID allows evaluating the entanglement of open curves without the need to close them, using the rece…▽ MoreThe backbone of most proteins forms an open curve. To study their entanglement, a common strategy consists in searching for the presence of knots in their backbones using topological invariants. However, this approach requires to close the curve into a loop, which alters the geometry of curve. Knoto-ID allows evaluating the entanglement of open curves without the need to close them, using the recent concept of knotoids which is a generalization of the classical knot theory to open curves. Knoto-ID can analyse the global topology of the full chain as well as the local topology by exhaustively studying all subchains or only determining the knotted core. Knoto-ID permits to localize topologically non-trivial protein folds that are not detected by informatics tools detecting knotted protein folds.△ Less"
Learning Several Languages from Labeled Strings: State Merging and Evolutionary Approaches,Authors:Alexis Linard,"Abstract:The problem of learning pairwise disjoint deterministic finite automata (DFA) from positive examples has been recently addressed. In this paper, we address the problem of identifying a set of DFAs from labeled strings and come up with two methods. The first is based on state merging and a heuristic related to the size of each state merging iteration. State merging operations involving a large numb…▽ MoreThe problem of learning pairwise disjoint deterministic finite automata (DFA) from positive examples has been recently addressed. In this paper, we address the problem of identifying a set of DFAs from labeled strings and come up with two methods. The first is based on state merging and a heuristic related to the size of each state merging iteration. State merging operations involving a large number of states are extracted, to provide sub-DFAs. The second method is based on a multi-objective evolutionary algorithm whose fitness function takes into account the accuracy of the DFA w.r.t. the learning sample, as well as the desired number of DFAs. We evaluate our methods on a dataset originated from industry.△ Less"
On Dualities and Equivalences Between Physical Theories,Authors:Jeremy Butterfield,"Abstract:The main aim of this paper is to make a remark about the relation between (i) dualities between theories, as `duality' is understood in physics and (ii) equivalence of theories, as `equivalence' is understood in logic and philosophy. The remark is that in physics, two theories can be dual, and accordingly get called `the same theory', though we interpret them as disagreeing---so that they are cert…▽ MoreThe main aim of this paper is to make a remark about the relation between (i) dualities between theories, as `duality' is understood in physics and (ii) equivalence of theories, as `equivalence' is understood in logic and philosophy. The remark is that in physics, two theories can be dual, and accordingly get called `the same theory', though we interpret them as disagreeing---so that they are certainly not equivalent, as `equivalent' is normally understood. So the remark is simple: but, I shall argue, worth stressing---since often neglected. My argument for this is based on the account of duality developed by De Haro: which is illustrated here with several examples, from both elementary physics and string theory. Thus I argue that in some examples, including in string theory, two dual theories disagree in their claims about the world. I also spell out how this remark implies a limitation of proposals (both traditional and recent) to understand theoretical equivalence as either logical equivalence or a weakening of it.△ Less"
Analysis of Fast Structured Dictionary Learning,"Authors:Saiprasad Ravishankar,Anna Ma,Deanna Needell","Abstract:Sparsity-based models and techniques have been exploited in many signal processing and imaging applications. Data-driven methods based on dictionary and sparsifying transform learning enable learning rich image features from data, and can outperform analytical models. In particular, alternating optimization algorithms have been popular for learning such models. In this work, we focus on alternatin…▽ MoreSparsity-based models and techniques have been exploited in many signal processing and imaging applications. Data-driven methods based on dictionary and sparsifying transform learning enable learning rich image features from data, and can outperform analytical models. In particular, alternating optimization algorithms have been popular for learning such models. In this work, we focus on alternating minimization for a specific structured unitary sparsifying operator learning problem, and provide a convergence analysis. While the algorithm converges to the critical points of the problem generally, our analysis establishes under mild assumptions, the local linear convergence of the algorithm to the underlying sparsifying model of the data. Analysis and numerical simulations show that our assumptions hold for standard probabilistic data models. In practice, the algorithm is robust to initialization.△ Less"
Classical realizability as a classifier for nondeterminism,Authors:Guillaume Geoffroy,"Abstract:We show how the language of Krivine's classical realizability may be used to specify various forms of nondeterminism and relate them with properties of realizability models. More specifically, we introduce an abstract notion of multi-evaluation relation which allows us to finely describe various nondeterministic behaviours. This defines a hierarchy of computational models, ordered by their degree…▽ MoreWe show how the language of Krivine's classical realizability may be used to specify various forms of nondeterminism and relate them with properties of realizability models. More specifically, we introduce an abstract notion of multi-evaluation relation which allows us to finely describe various nondeterministic behaviours. This defines a hierarchy of computational models, ordered by their degree of nondeterminism, similar to Sazonov's degrees of parallelism. What we show is a duality between the structure of the characteristic Boolean algebra of a realizability model and the degree of nondeterminism in its underlying computational model. ACM Reference Format: Guillaume Geoffroy. 2018. Classical realizability as a classifier for nondeter-minism.△ Less"
A sequent calculus with dependent types for classical arithmetic,Authors:Étienne Miquey,"Abstract:In a recent paper, Herbelin developed a calculus dPA$^ω$ in which constructive proofs for the axioms of countable and dependent choices could be derived via the encoding of a proof of countable universal quantification as a stream of it components. However, the property of normalization (and therefore the one of soundness) was only conjectured. The difficulty for the proof of normalization is due…▽ MoreIn a recent paper, Herbelin developed a calculus dPA$^ω$ in which constructive proofs for the axioms of countable and dependent choices could be derived via the encoding of a proof of countable universal quantification as a stream of it components. However, the property of normalization (and therefore the one of soundness) was only conjectured. The difficulty for the proof of normalization is due to the simultaneous presence of dependent dependent types (for the constructive part of the choice), of control operators (for classical logic), of coinductive objects (to encode functions of type $N\rightarrow A$ into streams $(a_0,a_1,\ldots)$) and of lazy evaluation with sharing (for these coinductive objects).Building on previous works, we introduce in this paper a variant of dPA$^ω$ presented as a sequent calculus. On the one hand, we take advantage of a variant of Krivine classical realizability we developed to prove the normalization of classical call-by-need. On the other hand, we benefit of dL, a classical sequent calculus with dependent types in which type safety is ensured using delimited continuations together with a syntactic restriction. By combining the techniques developed in these papers, we manage to define a realizability interpretation {à} la Krivine of our calculus that allows us to prove normalization and soundness.△ Less"
AutoAugment: Learning Augmentation Policies from Data,"Authors:Ekin D. Cubuk,Barret Zoph,Dandelion Mane,Vijay Vasudevan,Quoc V. Le","Abstract:…Augmentation policies we find are transferable between datasets. The policy learned on ImageNet transfers well to achieve significant improvements on other datasets, such asOxfordFlowers, Caltech-101,Oxford-IIT Pets, FGVC Aircraft, and Stanford Cars.▽ MoreData augmentation is an effective technique for improving the accuracy of modern image classifiers. However, current data augmentation implementations are manually designed. In this paper, we describe a simple procedure called AutoAugment to automatically search for improved data augmentation policies. In our implementation, we have designed a search space where a policy consists of many sub-policies, one of which is randomly chosen for each image in each mini-batch. A sub-policy consists of two operations, each operation being an image processing function such as translation, rotation, or shearing, and the probabilities and magnitudes with which the functions are applied. We use a search algorithm to find the best policy such that the neural network yields the highest validation accuracy on a target dataset. Our method achieves state-of-the-art accuracy on CIFAR-10, CIFAR-100, SVHN, and ImageNet (without additional data). On ImageNet, we attain a Top-1 accuracy of 83.5% which is 0.4% better than the previous record of 83.1%. On CIFAR-10, we achieve an error rate of 1.5%, which is 0.6% better than the previous state-of-the-art. Augmentation policies we find are transferable between datasets. The policy learned on ImageNet transfers well to achieve significant improvements on other datasets, such asOxfordFlowers, Caltech-101,Oxford-IIT Pets, FGVC Aircraft, and Stanford Cars.△ Less"
Generative Adversarial Forests for Better Conditioned Adversarial Learning,"Authors:Yan Zuo,Gil Avraham,Tom Drummond","Abstract:…clear qualitative and quantitative evidence of the effectiveness of our approach, gaining significant performance improvements over several popular GAN-based approaches on theOxfordFlowers and Aligned Celebrity Faces datasets.▽ MoreIn recent times, many of the breakthroughs in various vision-related tasks have revolved around improving learning of deep models; these methods have ranged from network architectural improvements such as Residual Networks, to various forms of regularisation such as Batch Normalisation. In essence, many of these techniques revolve around better conditioning, allowing for deeper and deeper models to be successfully learned. In this paper, we look towards better conditioning Generative Adversarial Networks (GANs) in an unsupervised learning setting. Our method embeds the powerful discriminating capabilities of a decision forest into the discriminator of a GAN. This results in a better conditioned model which learns in an extremely stable way. We demonstrate empirical results which show both clear qualitative and quantitative evidence of the effectiveness of our approach, gaining significant performance improvements over several popular GAN-based approaches on theOxfordFlowers and Aligned Celebrity Faces datasets.△ Less"
Strategy-Proof Incentives for Predictions,Authors:Amir Ban,"Abstract:Our aim is to design mechanisms that motivate all agents to reveal their predictions truthfully and promptly. For myopic agents, proper scoring rules induce truthfulness. However, as has been described in the literature, when agents take into account long-term effects of their actions, deception and reticence may appear. No simple rules exist to distinguish between the truthful and the untruthful…▽ MoreOur aim is to design mechanisms that motivate all agents to reveal their predictions truthfully and promptly. For myopic agents, proper scoring rules induce truthfulness. However, as has been described in the literature, when agents take into account long-term effects of their actions, deception and reticence may appear. No simple rules exist to distinguish between the truthful and the untruthful situations, and a determination has been done in isolated cases only.
  This is of relevance to prediction markets, where the market value is a common prediction, and more generally in informal public prediction forums, such as stock-market estimates by analysts. We describe three mechanisms that are strategy-proof with non-myopic considerations, and show that one of them meets all our requirements from a mechanism in almost all prediction settings. We formulate rules to distinguish truthful from untruthful settings, and use them to extensively classify prediction settings with continuous outcomes. We show how our proposed mechanism restores prompt truthfulness where incumbent mechanisms fail, and offer guidelines to implementing it in a prediction market.△ Less"
"A Continuous, Full-scope, Spatio-temporal Tracking Metric based on KL-divergence",Authors:Terrence Adams,"Abstract:…are recast to handle various types of tracking errors (i.e., false alarms, missed detections, merges, splits). Scoring results are given on a standard tracking dataset (OxfordTown Centre Dataset), as well as several simulated scenarios. Also, this new metric is compared with several other metrics including the commonly used Multiple Object Tracking Accuracy…▽ MoreA unified metric is given for the evaluation of object tracking systems. The metric is inspired by KL-divergence or relative entropy, which is commonly used to evaluate clustering techniques. Since tracking problems are fundamentally different from clustering, the components of KL-divergence are recast to handle various types of tracking errors (i.e., false alarms, missed detections, merges, splits). Scoring results are given on a standard tracking dataset (OxfordTown Centre Dataset), as well as several simulated scenarios. Also, this new metric is compared with several other metrics including the commonly used Multiple Object Tracking Accuracy metric. In the final section, advantages of this metric are given including the fact that it is continuous, parameter-less and comprehensive.△ Less"
MC-GAN: Multi-conditional Generative Adversarial Network for Image Synthesis,"Authors:Hyojin Park,YoungJoon Yoo,Nojun Kwak","Abstract:…the amount of the background information from the given base image using the foreground information from the text attributes. From the experiments with Caltech-200 bird andOxford-102 flower datasets, we show that our model is able to generate photo-realistic images with a resolution of 128 x 128. The source code of MC-GAN is released.▽ MoreIn this paper, we introduce a new method for generating an object image from text attributes on a desired location, when the base image is given. One step further to the existing studies on text-to-image generation mainly focusing on the object's appearance, the proposed method aims to generate an object image preserving the given background information, which is the first attempt in this field. To tackle the problem, we propose a multi-conditional GAN (MC-GAN) which controls both the object and background information jointly. As a core component of MC-GAN, we propose a synthesis block which disentangles the object and background information in the training stage. This block enables MC-GAN to generate a realistic object image with the desired background by controlling the amount of the background information from the given base image using the foreground information from the text attributes. From the experiments with Caltech-200 bird andOxford-102 flower datasets, we show that our model is able to generate photo-realistic images with a resolution of 128 x 128. The source code of MC-GAN is released.△ Less"
Spectral clustering algorithms for the detection of clusters in block-cyclic and block-acyclic graphs,"Authors:H. Van Lierde,T. W. S. Chow,J. -C. Delvenne","Abstract:We propose two spectral algorithms for partitioning nodes in directed graphs respectively with a cyclic and an acyclic pattern of connection between groups of nodes. Our methods are based on the computation of extremal eigenvalues of the transition matrix associated to the directed graph. The two algorithms outperform state-of-the art methods for directed graph clustering on synthetic datasets, in…▽ MoreWe propose two spectral algorithms for partitioning nodes in directed graphs respectively with a cyclic and an acyclic pattern of connection between groups of nodes. Our methods are based on the computation of extremal eigenvalues of the transition matrix associated to the directed graph. The two algorithms outperform state-of-the art methods for directed graph clustering on synthetic datasets, including methods based on blockmodels, bibliometric symmetrization and random walks. Our algorithms have the same space complexity as classical spectral clustering algorithms for undirected graphs and their time complexity is also linear in the number of edges in the graph. One of our methods is applied to a trophic network based on predator-prey relationships. It successfully extracts common categories of preys and predators encountered in food chains. The same method is also applied to highlight the hierarchical structure of a worldwide network of Autonomous Systems depicting business agreements between Internet Service Providers.△ Less"
Functional ASP with Intensional Sets: Application to Gelfond-Zhang Aggregates,"Authors:Pedro Cabalar,Jorge Fandinno,Luis Fariñas del Cerro,David Pearce","Abstract:In this paper, we propose a variant of Answer Set Programming (ASP) with evaluable functions that extends their application to sets of objects, something that allows a fully logical treatment of aggregates. Formally, we start from the syntax of First Order Logic with equality and the semantics of Quantified Equilibrium Logic with evaluable functions (QELF). Then, we proceed to incorporate a new ki…▽ MoreIn this paper, we propose a variant of Answer Set Programming (ASP) with evaluable functions that extends their application to sets of objects, something that allows a fully logical treatment of aggregates. Formally, we start from the syntax of First Order Logic with equality and the semantics of Quantified Equilibrium Logic with evaluable functions (QELF). Then, we proceed to incorporate a new kind of logical term, intensional set (a construct commonly used to denote the set of objects characterised by a given formula), and to extend QELF semantics for this new type of expression. In our extended approach, intensional sets can be arbitrarily used as predicate or function arguments or even nested inside other intensional sets, just as regular first-order logical terms. As a result, aggregates can be naturally formed by the application of some evaluable function (count, sum, maximum, etc) to a set of objects expressed as an intensional set. This approach has several advantages. First, while other semantics for aggregates depend on some syntactic transformation (either via a reduct or a formula translation), the QELF interpretation treats them as regular evaluable functions, providing a compositional semantics and avoiding any kind of syntactic restriction. Second, aggregates can be explicitly defined now within the logical language by the simple addition of formulas that fix their meaning in terms of multiple applications of some (commutative and associative) binary operation. For instance, we can use recursive rules to define sum in terms of integer addition. Last, but not least, we prove that the semantics we obtain for aggregates coincides with the one defined by Gelfond and Zhang for the Alog language, when we restrict to that syntactic fragment. (Under consideration for acceptance in TPLP)△ Less"
Translating LPOD and CR-Prolog2 into Standard Answer Set Programs,"Authors:Joohyung Lee,Zhun Yang","Abstract:Logic Programs with Ordered Disjunction (LPOD) is an extension of standard answer set programs to handle preference using the construct of ordered disjunction, and CR-Prolog2 is an extension of standard answer set programs with consistency restoring rules and LPOD-like ordered disjunction. We present reductions of each of these languages into the standard ASP language, which gives us an alternativ…▽ MoreLogic Programs with Ordered Disjunction (LPOD) is an extension of standard answer set programs to handle preference using the construct of ordered disjunction, and CR-Prolog2 is an extension of standard answer set programs with consistency restoring rules and LPOD-like ordered disjunction. We present reductions of each of these languages into the standard ASP language, which gives us an alternative way to understand the extensions in terms of the standard ASP language.△ Less"
A Probabilistic Extension of Action Language BC+,"Authors:Joohyung Lee,Yi Wang","Abstract:We present a probabilistic extension of action language BC+. Just like BC+ is defined as a high-level notation of answer set programs for describing transition systems, the proposed language, which we call pBC+, is defined as a high-level notation of LPMLN programs---a probabilistic extension of answer set programs. We show how probabilistic reasoning about transition systems, such as prediction,…▽ MoreWe present a probabilistic extension of action language BC+. Just like BC+ is defined as a high-level notation of answer set programs for describing transition systems, the proposed language, which we call pBC+, is defined as a high-level notation of LPMLN programs---a probabilistic extension of answer set programs. We show how probabilistic reasoning about transition systems, such as prediction, postdiction, and planning problems, as well as probabilistic diagnosis for dynamic domains, can be modeled in pBC+ and computed using an implementation of LPMLN.△ Less"
Spatio-temporal Patterns of Indian Monsoon Rainfall,"Authors:Adway Mitra,Amit Apte,Rama Govindarajan,Vishal Vasan,Sreekar Vadlamani","Abstract:The primary objective of this paper is to analyze a set of canonical spatial patterns that approximate the daily rainfall across the Indian region, as identified in the companion paper where we developed a discrete representation of the Indian summer monsoon rainfall using state variables with spatio-temporal coherence maintained using a Markov Random Field prior. In particular, we use these spati…▽ MoreThe primary objective of this paper is to analyze a set of canonical spatial patterns that approximate the daily rainfall across the Indian region, as identified in the companion paper where we developed a discrete representation of the Indian summer monsoon rainfall using state variables with spatio-temporal coherence maintained using a Markov Random Field prior. In particular, we use these spatio-temporal patterns to study the variation of rainfall during the monsoon season. Firstly, the ten patterns are divided into three families of patterns distinguished by their total rainfall amount and geographic spread. These families are then used to establish `active' and `break' spells of the Indian monsoon at the all-India level. Subsequently, we characterize the behavior of these patterns in time by estimating probabilities of transition from one pattern to another across days in a season. Patterns tend to be `sticky': the self-transition is the most common. We also identify most commonly occurring sequences of patterns. This leads to a simple seasonal evolution model for the summer monsoon rainfall. The discrete representation introduced in the companion paper also identifies typical temporal rainfall patterns for individual locations. This enables us to determine wet and dry spells at local and regional scales. Lastly, we specify sets of locations that tend to have such spells simultaneously, and thus come up with a new regionalization of the landmass.△ Less"
A Discrete View of the Indian Monsoon to Identify Spatial Patterns of Rainfall,"Authors:Adway Mitra,Amit Apte,Rama Govindarajan,Vishal Vasan,Sreekar Vadlamani","Abstract:We propose a representation of the Indian summer monsoon rainfall in terms of a probabilistic model based on a Markov Random Field, consisting of discrete state variables representing low and high rainfall at grid-scale and daily rainfall patterns across space and in time. These discrete states are conditioned on observed daily gridded rainfall data from the period 2000-2007. The model gives us a…▽ MoreWe propose a representation of the Indian summer monsoon rainfall in terms of a probabilistic model based on a Markov Random Field, consisting of discrete state variables representing low and high rainfall at grid-scale and daily rainfall patterns across space and in time. These discrete states are conditioned on observed daily gridded rainfall data from the period 2000-2007. The model gives us a set of 10 spatial patterns of daily monsoon rainfall over India, which are robust over a range of user-chosen parameters as well as coherent in space and time. Each day in the monsoon season is assigned precisely one of the spatial patterns, that approximates the spatial distribution of rainfall on that day. Such approximations are quite accurate for nearly 95% of the days. Remarkably, these patterns are representative (with similar accuracy) of the monsoon seasons from 1901 to 2000 as well. Finally, we compare the proposed model with alternative approaches to extract spatial patterns of rainfall, using empirical orthogonal functions as well as clustering algorithms such as K-means and spectral clustering.△ Less"
"Phylotastic: An Experiment in Creating, Manipulating, and Evolving Phylogenetic Biology Workflows Using Logic Programming","Authors:Thanh Hai Nguyen,Enrico Pontelli,Tran Cao Son","Abstract:Evolutionary Biologists have long struggled with the challenge of developing analysis workflows in a flexible manner, thus facilitating the reuse of phylogenetic knowledge. An evolutionary biology workflow can be viewed as a plan which composes web services that can retrieve, manipulate, and produce phylogenetic trees. The Phylotastic project was launched two years ago as a collaboration between e…▽ MoreEvolutionary Biologists have long struggled with the challenge of developing analysis workflows in a flexible manner, thus facilitating the reuse of phylogenetic knowledge. An evolutionary biology workflow can be viewed as a plan which composes web services that can retrieve, manipulate, and produce phylogenetic trees. The Phylotastic project was launched two years ago as a collaboration between evolutionary biologists and computer scientists, with the goal of developing an open architecture to facilitate the creation of such analysis workflows. While composition of web services is a problem that has been extensively explored in the literature, including within the logic programming domain, the incarnation of the problem in Phylotastic provides a number of additional challenges. Along with the need to integrate preferences and formal ontologies in the description of the desired workflow, evolutionary biologists tend to construct workflows in an incremental manner, by successively refining the workflow, by indicating desired changes (e.g., exclusion of certain services, modifications of the desired output). This leads to the need of successive iterations of incremental replanning, to develop a new workflow that integrates the requested changes while minimizing the changes to the original workflow. This paper illustrates how Phylotastic has addressed the challenges of creating and refining phylogenetic analysis workflows using logic programming technology and how such solutions have been used within the general framework of the Phylotastic project. Under consideration in Theory and Practice of Logic Programming (TPLP).△ Less"
Exploiting Answer Set Programming with External Sources for Meta-Interpretive Learning,"Authors:Tobias Kaminski,Thomas Eiter,Katsumi Inoue","Abstract:Meta-Interpretive Learning (MIL) learns logic programs from examples by instantiating meta-rules, which is implemented by the Metagol system based on Prolog. Viewing MIL-problems as combinatorial search problems, they can alternatively be solved by employing Answer Set Programming (ASP), which may result in performance gains as a result of efficient conflict propagation. However, a straightforward…▽ MoreMeta-Interpretive Learning (MIL) learns logic programs from examples by instantiating meta-rules, which is implemented by the Metagol system based on Prolog. Viewing MIL-problems as combinatorial search problems, they can alternatively be solved by employing Answer Set Programming (ASP), which may result in performance gains as a result of efficient conflict propagation. However, a straightforward ASP-encoding of MIL results in a huge search space due to a lack of procedural bias and the need for grounding. To address these challenging issues, we encode MIL in the HEX-formalism, which is an extension of ASP that allows us to outsource the background knowledge, and we restrict the search space to compensate for a procedural bias in ASP. This way, the import of constants from the background knowledge can for a given type of meta-rules be limited to relevant ones. Moreover, by abstracting from term manipulations in the encoding and by exploiting the HEX interface mechanism, the import of such constants can be entirely avoided in order to mitigate the grounding bottleneck. An experimental evaluation shows promising results.△ Less"
Proof-relevant Horn Clauses for Dependent Type Inference and Term Synthesis,"Authors:František Farka,Ekaterina Komendantskya,Kevin Hammond","Abstract:First-order resolution has been used for type inference for many years, including in Hindley- Milner type inference, type-classes, and constrained data types. Dependent types are a new trend in functional languages. In this paper, we show that proof-relevant first-order resolution can play an important role in automating type inference and term synthesis for dependently typed languages. We propose…▽ MoreFirst-order resolution has been used for type inference for many years, including in Hindley- Milner type inference, type-classes, and constrained data types. Dependent types are a new trend in functional languages. In this paper, we show that proof-relevant first-order resolution can play an important role in automating type inference and term synthesis for dependently typed languages. We propose a calculus that translates type inference and term synthesis problems in a dependently typed language to a logic program and a goal in the proof-relevant first-order Horn clause logic. The computed answer substitution and proof term then provide a solution to the given type inference and term synthesis problem. We prove the decidability and soundness of our method. The paper is under consideration for acceptance in TPLP.△ Less"
Constraint Answer Set Programming without Grounding,"Authors:Joaquín Arias,Manuel Carro,Elmer Salazar,Kyle Marple,Gopal Gupta","Abstract:Extending ASP with constraints (CASP) enhances its expressiveness and performance. This extension is not straightforward as the grounding phase, present in most ASP systems, removes variables and the links among them, and also causes a combinatorial explosion in the size of the program. Several methods to overcome this issue have been devised: restricting the constraint domains (e.g., discrete ins…▽ MoreExtending ASP with constraints (CASP) enhances its expressiveness and performance. This extension is not straightforward as the grounding phase, present in most ASP systems, removes variables and the links among them, and also causes a combinatorial explosion in the size of the program. Several methods to overcome this issue have been devised: restricting the constraint domains (e.g., discrete instead of dense), or the type (or number) of models that can be returned. In this paper we propose to incorporate constraints into s(ASP), a goal-directed, top-down execution model which implements ASP while retaining logical variables both during execution and in the answer sets. The resulting model, s(CASP), can constrain variables that, as in CLP, are kept during the execution and in the answer sets. s(CASP) inherits and generalizes the execution model of s(ASP) and is parametric w.r.t. the constraint solver. We describe this novel execution model and show through several examples the enhanced expressiveness of s(CASP) w.r.t. ASP, CLP, and other CASP systems. We also report improved performance w.r.t. other very mature, highly optimized ASP systems in some benchmarks. This paper is under consideration for publication in Theory and Practice of Logic Programming (TPLP).△ Less"
Optimal Scheduling for Exposed Datapath Architectures with Buffered Processing Units by ASP,"Authors:Marc Dahlem,Anoop Bhagyanath,Klaus Schneider","Abstract:Conventional processor architectures are restricted in exploiting instruction level parallelism (ILP) due to the relatively low number of programmer-visible registers. Therefore, more recent processor architectures expose their datapaths so that the compiler (1) can schedule parallel instructions to different processing units and (2) can make effective use of local storage of the processing units.…▽ MoreConventional processor architectures are restricted in exploiting instruction level parallelism (ILP) due to the relatively low number of programmer-visible registers. Therefore, more recent processor architectures expose their datapaths so that the compiler (1) can schedule parallel instructions to different processing units and (2) can make effective use of local storage of the processing units. Among these architectures, the Synchronous Control Asynchronous Dataflow (SCAD) architecture is a new exposed datapath architecture whose processing units are equipped with first-in first-out (FIFO) buffers at their input and output ports.
  In contrast to register-based machines, the optimal code generation for SCAD is still a matter of research. In particular, SAT and SMT solvers were used to generate optimal resource constrained and optimal time constrained schedules for SCAD, respectively. As Answer Set Programming (ASP) offers better flexibility in handling such scheduling problems, we focus in this paper on using an answer set solver for both resource and time constrained optimal SCAD code generation. As a major benefit of using ASP, we are able to generate \emph{all} optimal schedules for a given program which allows one to study their properties. Furthermore, the experimental results of this paper demonstrate that the answer set solver can compete with SAT solvers and outperforms SMT solvers. \emph{This paper is under consideration for acceptance in TPLP.}△ Less"
Specifying and Verbalising Answer Set Programs in Controlled Natural Language,Authors:Rolf Schwitter,"Abstract:We show how a bi-directional grammar can be used to specify and verbalise answer set programs in controlled natural language. We start from a program specification in controlled natural language and translate this specification automatically into an executable answer set program. The resulting answer set program can be modified following certain naming conventions and the revised version of the pr…▽ MoreWe show how a bi-directional grammar can be used to specify and verbalise answer set programs in controlled natural language. We start from a program specification in controlled natural language and translate this specification automatically into an executable answer set program. The resulting answer set program can be modified following certain naming conventions and the revised version of the program can then be verbalised in the same subset of natural language that was used as specification language. The bi-directional grammar is parametrised for processing and generation, deals with referring expressions, and exploits symmetries in the data structure of the grammar rules whenever these grammar rules need to be duplicated. We demonstrate that verbalisation requires sentence planning in order to aggregate similar structures with the aim to improve the readability of the generated specification. Without modifications, the generated specification is always semantically equivalent to the original one; our bi-directional grammar is the first one that allows for semantic round-tripping in the context of controlled natural language processing. This paper is under consideration for acceptance in TPLP.△ Less"
Certified Graph View Maintenance with Regular Datalog,"Authors:Angela Bonifati,Stefania Dumbrava,Emilio Jesus Gallego Arias","Abstract:We employ the Coq proof assistant to develop a mechanically-certified framework for evaluating graph queries and incrementally maintaining materialized graph instances, also called views. The language we use for defining queries and views is Regular Datalog (RD) -- a notable fragment of non-recursive Datalog that can express complex navigational queries, with transitive closure as native operator.…▽ MoreWe employ the Coq proof assistant to develop a mechanically-certified framework for evaluating graph queries and incrementally maintaining materialized graph instances, also called views. The language we use for defining queries and views is Regular Datalog (RD) -- a notable fragment of non-recursive Datalog that can express complex navigational queries, with transitive closure as native operator. We first design and encode the theory of RD and then mechanize a RD-specific evaluation algorithm capable of fine-grained, incremental graph view computation, which we prove sound with respect to the declarative RD semantics. By using the Coq extraction mechanism, we test an Ocaml version of the verified engine on a set of preliminary benchmarks. Our development is particularly focused on leveraging existing verification and notational techniques to: a) define mechanized properties that can be easily understood by logicians and database researchers and b) attain formal verification with limited effort. Our work is the first step towards a unified, machine-verified, formal framework for dynamic graph query languages and their evaluation engines. This paper is under consideration for acceptance in TPLP.△ Less"
Routing Driverless Transport Vehicles in Car Assembly with Answer Set Programming,"Authors:Martin Gebser,Philipp Obermeier,Michel Ratsch-Heitmann,Mario Runge,Torsten Schaub","Abstract:Automated storage and retrieval systems are principal components of modern production and warehouse facilities. In particular, automated guided vehicles nowadays substitute human-operated pallet trucks in transporting production materials between storage locations and assembly stations. While low-level control systems take care of navigating such driverless vehicles along programmed routes and avo…▽ MoreAutomated storage and retrieval systems are principal components of modern production and warehouse facilities. In particular, automated guided vehicles nowadays substitute human-operated pallet trucks in transporting production materials between storage locations and assembly stations. While low-level control systems take care of navigating such driverless vehicles along programmed routes and avoid collisions even under unforeseen circumstances, in the common case of multiple vehicles sharing the same operation area, the problem remains how to set up routes such that a collection of transport tasks is accomplished most effectively. We address this prevalent problem in the context of car assembly at Mercedes-Benz Ludwigsfelde GmbH, a large-scale producer of commercial vehicles, where routes for automated guided vehicles used in the production process have traditionally been hand-coded by human engineers. Such ad-hoc methods may suffice as long as a running production process remains in place, while any change in the factory layout or production targets necessitates tedious manual reconfiguration, not to mention the missing portability between different production plants. Unlike this, we propose a declarative approach based on Answer Set Programming to optimize the routes taken by automated guided vehicles for accomplishing transport tasks. The advantages include a transparent and executable problem formalization, provable optimality of routes relative to objective criteria, as well as elaboration tolerance towards particular factory layouts and production targets. Moreover, we demonstrate that our approach is efficient enough to deal with the transport tasks evolving in realistic production processes at the car factory of Mercedes-Benz Ludwigsfelde GmbH.△ Less"
Experimenting with robotic intra-logistics domains,"Authors:Martin Gebser,Philipp Obermeier,Thomas Otto,Torsten Schaub,Orkunt Sabuncu,Van Nguyen,Tran Cao Son","Abstract:We introduce the asprilo [1] framework to facilitate experimental studies of approaches addressing complex dynamic applications. For this purpose, we have chosen the domain of robotic intra-logistics. This domain is not only highly relevant in the context of today's fourth industrial revolution but it moreover combines a multitude of challenging issues within a single uniform framework. This inclu…▽ MoreWe introduce the asprilo [1] framework to facilitate experimental studies of approaches addressing complex dynamic applications. For this purpose, we have chosen the domain of robotic intra-logistics. This domain is not only highly relevant in the context of today's fourth industrial revolution but it moreover combines a multitude of challenging issues within a single uniform framework. This includes multi-agent planning, reasoning about action, change, resources, strategies, etc. In return, asprilo allows users to study alternative solutions as regards effectiveness and scalability. Although asprilo relies on Answer Set Programming and Python, it is readily usable by any system complying with its fact-oriented interface format. This makes it attractive for benchmarking and teaching well beyond logic programming. More precisely, asprilo consists of a versatile benchmark generator, solution checker and visualizer as well as a bunch of reference encodings featuring various ASP techniques. Importantly, the visualizer's animation capabilities are indispensable for complex scenarios like intra-logistics in order to inspect valid as well as invalid solution candidates. Also, it allows for graphically editing benchmark layouts that can be used as a basis for generating benchmark suites.
  [1] asprilo stands for Answer Set Programming for robotic intra-logistics△ Less"
Constraint-Based Inference in Probabilistic Logic Programs,"Authors:Arun Nampally,Timothy Zhang,C. R. Ramakrishnan","Abstract:Probabilistic Logic Programs (PLPs) generalize traditional logic programs and allow the encoding of models combining logical structure and uncertainty. In PLP, inference is performed by summarizing the possible worlds which entail the query in a suitable data structure, and using it to compute the answer probability. Systems such as ProbLog, PITA, etc., use propositional data structures like expla…▽ MoreProbabilistic Logic Programs (PLPs) generalize traditional logic programs and allow the encoding of models combining logical structure and uncertainty. In PLP, inference is performed by summarizing the possible worlds which entail the query in a suitable data structure, and using it to compute the answer probability. Systems such as ProbLog, PITA, etc., use propositional data structures like explanation graphs, BDDs, SDDs, etc., to represent the possible worlds. While this approach saves inference time due to substructure sharing, there are a number of problems where a more compact data structure is possible. We propose a data structure called Ordered Symbolic Derivation Diagram (OSDD) which captures the possible worlds by means of constraint formulas. We describe a program transformation technique to construct OSDDs via query evaluation, and give procedures to perform exact and approximate inference over OSDDs. Our approach has two key properties. Firstly, the exact inference procedure is a generalization of traditional inference, and results in speedup over the latter in certain settings. Secondly, the approximate technique is a generalization of likelihood weighting in Bayesian Networks, and allows us to perform sampling-based inference with lower rejection rate and variance. We evaluate the effectiveness of the proposed techniques through experiments on several problems. This paper is under consideration for acceptance in TPLP.△ Less"
Temporal Answer Set Programming on Finite Traces,"Authors:Pedro Cabalar,Roland Kaminski,Torsten Schaub,Anna Schuhmann","Abstract:In this paper, we introduce an alternative approach to Temporal Answer Set Programming that relies on a variation of Temporal Equilibrium Logic (TEL) for finite traces. This approach allows us to even out the expressiveness of TEL over infinite traces with the computational capacity of (incremental) Answer Set Programming (ASP). Also, we argue that finite traces are more natural when reasoning abo…▽ MoreIn this paper, we introduce an alternative approach to Temporal Answer Set Programming that relies on a variation of Temporal Equilibrium Logic (TEL) for finite traces. This approach allows us to even out the expressiveness of TEL over infinite traces with the computational capacity of (incremental) Answer Set Programming (ASP). Also, we argue that finite traces are more natural when reasoning about action and change. As a result, our approach is readily implementable via multi-shot ASP systems and benefits from an extension of ASP's full-fledged input language with temporal operators. This includes future as well as past operators whose combination offers a rich temporal modeling language. For computation, we identify the class of temporal logic programs and prove that it constitutes a normal form for our approach. Finally, we outline two implementations, a generic one and an extension of clingo.△ Less"
First-order answer set programming as constructive proof search,"Authors:Aleksy Schubert,Paweł Urzyczyn","Abstract:We propose an interpretation of the first-order answer set programming (FOASP) in terms of intuitionistic proof theory. It is obtained by two polynomial translations between FOASP and the bounded-arity fragment of the Sigma_1 level of the Mints hierarchy in first-order intuitionistic logic. It follows that Sigma_1 formulas using predicates of fixed arity (in particular unary) is of the same streng…▽ MoreWe propose an interpretation of the first-order answer set programming (FOASP) in terms of intuitionistic proof theory. It is obtained by two polynomial translations between FOASP and the bounded-arity fragment of the Sigma_1 level of the Mints hierarchy in first-order intuitionistic logic. It follows that Sigma_1 formulas using predicates of fixed arity (in particular unary) is of the same strength as FOASP. Our construction reveals a close similarity between constructive provability and stable entailment, or equivalently, between the construction of an answer set and an intuitionistic refutation. This paper is under consideration for publication in Theory and Practice of Logic Programming△ Less"
An ASP Methodology for Understanding Narratives about Stereotypical Activities,"Authors:Daniela Inclezan,Qinglin Zhang,Marcello Balduccini,Ankush Israney","Abstract:We describe an application of Answer Set Programming to the understanding of narratives about stereotypical activities, demonstrated via question answering. Substantial work in this direction was done by Erik Mueller, who modeled stereotypical activities as scripts. His systems were able to understand a good number of narratives, but could not process texts describing exceptional scenarios. We pro…▽ MoreWe describe an application of Answer Set Programming to the understanding of narratives about stereotypical activities, demonstrated via question answering. Substantial work in this direction was done by Erik Mueller, who modeled stereotypical activities as scripts. His systems were able to understand a good number of narratives, but could not process texts describing exceptional scenarios. We propose addressing this problem by using a theory of intentions developed by Blount, Gelfond, and Balduccini. We present a methodology in which we substitute scripts by activities (i.e., hierarchical plans associated with goals) and employ the concept of an intentional agent to reason about both normal and exceptional scenarios. We exemplify the application of this methodology by answering questions about a number of restaurant stories. This paper is under consideration for acceptance in TPLP.△ Less"
Shape Neutral Analysis of Graph-based Data-structures,"Authors:Gregory J. Duck,Joxan Jaffar,Roland H. C. Yap","Abstract:Malformed data-structures can lead to runtime errors such as arbitrary memory access or corruption. Despite this, reasoning over data-structure properties for low-level heap manipulating programs remains challenging. In this paper we present a constraint-based program analysis that checks data-structure integrity, w.r.t. given target data-structure properties, as the heap is manipulated by the pro…▽ MoreMalformed data-structures can lead to runtime errors such as arbitrary memory access or corruption. Despite this, reasoning over data-structure properties for low-level heap manipulating programs remains challenging. In this paper we present a constraint-based program analysis that checks data-structure integrity, w.r.t. given target data-structure properties, as the heap is manipulated by the program. Our approach is to automatically generate a solver for properties using the type definitions from the target program. The generated solver is implemented using a Constraint Handling Rules (CHR) extension of built-in heap, integer and equality solvers. A key property of our program analysis is that the target data-structure properties are shape neutral, i.e., the analysis does not check for properties relating to a given data-structure graph shape, such as doubly-linked-lists versus trees. Nevertheless, the analysis can detect errors in a wide range of data-structure manipulating programs, including those that use lists, trees, DAGs, graphs, etc. We present an implementation that uses the Satisfiability Modulo Constraint Handling Rules (SMCHR) system. Experimental results show that our approach works well for real-world C programs.△ Less"
Solving Horn Clauses on Inductive Data Types Without Induction,"Authors:Emanuele De Angelis,Fabio Fioravanti,Alberto Pettorossi,Maurizio Proietti","Abstract:We address the problem of verifying the satisfiability of Constrained Horn Clauses (CHCs) based on theories of inductively defined data structures, such as lists and trees. We propose a transformation technique whose objective is the removal of these data structures from CHCs, hence reducing their satisfiability to a satisfiability problem for CHCs on integers and booleans. We propose a transforma…▽ MoreWe address the problem of verifying the satisfiability of Constrained Horn Clauses (CHCs) based on theories of inductively defined data structures, such as lists and trees. We propose a transformation technique whose objective is the removal of these data structures from CHCs, hence reducing their satisfiability to a satisfiability problem for CHCs on integers and booleans. We propose a transformation algorithm and identify a class of clauses where it always succeeds. We also consider an extension of that algorithm, which combines clause transformation with reasoning on integer constraints. Via an experimental evaluation we show that our technique greatly improves the effectiveness of applying the Z3 solver to CHCs. We also show that our verification technique based on CHC transformation followed by CHC solving, is competitive with respect to CHC solvers extended with induction. This paper is under consideration for acceptance in TPLP.△ Less"
Shared aggregate sets in answer set programming,"Authors:Mario Alviano,Carmine Dodaro,Marco Maratea","Abstract:Aggregates are among the most frequently used linguistic extensions of answer set programming. The result of an aggregation may introduce new constants during the instantiation of the input program, a feature known as value invention. When the aggregation involves literals whose truth value is undefined at instantiation time, modern grounders introduce several instances of the aggregate, one for e…▽ MoreAggregates are among the most frequently used linguistic extensions of answer set programming. The result of an aggregation may introduce new constants during the instantiation of the input program, a feature known as value invention. When the aggregation involves literals whose truth value is undefined at instantiation time, modern grounders introduce several instances of the aggregate, one for each possible interpretation of the undefined literals. This paper introduces new data structures and techniques to handle such cases, and more in general aggregations on the same aggregate set identified in the ground program in input. The proposed solution reduces the memory footprint of the solver without sacrificing efficiency. On the contrary, the performance of the solver may improve thanks to the addition of some simple entailed clauses which are not easily discovered otherwise, and since redundant computation is avoided during propagation. Empirical evidence of the potential impact of the proposed solution is given. (Under consideration for acceptance in TPLP).△ Less"
Cautious reasoning in ASP via minimal models and unsatisfiable cores,"Authors:Mario Alviano,Carmine Dodaro,Matti Järvisalo,Marco Maratea,Alessandro Previti","Abstract:Answer Set Programming (ASP) is a logic-based knowledge representation framework, supporting---among other reasoning modes---the central task of query answering. In the propositional case, query answering amounts to computing cautious consequences of the input program among the atoms in a given set of candidates, where a cautious consequence is an atom belonging to all stable models. Currently, th…▽ MoreAnswer Set Programming (ASP) is a logic-based knowledge representation framework, supporting---among other reasoning modes---the central task of query answering. In the propositional case, query answering amounts to computing cautious consequences of the input program among the atoms in a given set of candidates, where a cautious consequence is an atom belonging to all stable models. Currently, the most efficient algorithms either iteratively verify the existence of a stable model of the input program extended with the complement of one candidate, where the candidate is heuristically selected, or introduce a clause enforcing the falsity of at least one candidate, so that the solver is free to choose which candidate to falsify at any time during the computation of a stable model. This paper introduces new algorithms for the computation of cautious consequences, with the aim of driving the solver to search for stable models discarding more candidates. Specifically, one of such algorithms enforces minimality on the set of true candidates, where different notions of minimality can be used, and another takes advantage of unsatisfiable cores computation. The algorithms are implemented in WASP, and experiments on benchmarks from the latest ASP competitions show that the new algorithms perform better than the state of the art. (Under consideration for acceptance in TPLP).△ Less"
Top-down and Bottom-up Evaluation Procedurally Integrated,Authors:David S. Warren,"Abstract:This paper describes how XSB combines top-down and bottom-up computation through the mechanisms of variant tabling and subsumptive tabling with abstraction, respectively.
  It is well known that top-down evaluation of logical rules in Prolog has a procedural interpretation as recursive procedure invocation (Kowalski 1986). Tabling adds the intuition of short-circuiting redundant computations (Warr…▽ MoreThis paper describes how XSB combines top-down and bottom-up computation through the mechanisms of variant tabling and subsumptive tabling with abstraction, respectively.
  It is well known that top-down evaluation of logical rules in Prolog has a procedural interpretation as recursive procedure invocation (Kowalski 1986). Tabling adds the intuition of short-circuiting redundant computations (Warren 1992) .This paper shows how to introduce into tabled logic program evaluation a bottom-up component, whose procedural intuition is the initialization of a data structure, in which a relation is initially computed and filled, on first demand, and then used throughout the remainder of a larger computation for efficient lookup. This allows many Prolog programs to be expressed fully declaratively, programs which formerly required procedural features, such as assert, to be made efficient.
  This paper is under consideration for acceptance in ""Theory and Practice of Logic Programming (TPLP)"".△ Less"
Approximation Fixpoint Theory and the Well-Founded Semantics of Higher-Order Logic Programs,"Authors:Angelos Charalambidis,Panos Rondogiannis,Ioanna Symeonidou","Abstract:We define a novel, extensional, three-valued semantics for higher-order logic programs with negation. The new semantics is based on interpreting the types of the source language as three-valued Fitting-monotonic functions at all levels of the type hierarchy. We prove that there exists a bijection between such Fitting-monotonic functions and pairs of two-valued-result functions where the first memb…▽ MoreWe define a novel, extensional, three-valued semantics for higher-order logic programs with negation. The new semantics is based on interpreting the types of the source language as three-valued Fitting-monotonic functions at all levels of the type hierarchy. We prove that there exists a bijection between such Fitting-monotonic functions and pairs of two-valued-result functions where the first member of the pair is monotone-antimonotone and the second member is antimonotone-monotone. By deriving an extension of consistent approximation fixpoint theory (Denecker et al. 2004) and utilizing the above bijection, we define an iterative procedure that produces for any given higher-order logic program a distinguished extensional model. We demonstrate that this model is actually a minimal one. Moreover, we prove that our construction generalizes the familiar well-founded semantics for classical logic programs, making in this way our proposal an appealing formulation for capturing the well-founded semantics for higher-order logic programs. This paper is under consideration for acceptance in TPLP.△ Less"
Multi-scale prediction for robust hand detection and classification,"Authors:Ding Lu,Yong Wang,Robert Laganiere,Xinbin Luo,Shan Fu","Abstract:…generated at multiple scales. Our method is evaluated on challenging hand datasets, namely the Vision for Intelligent Vehicles and Applications (VIVA) Challenge and theOxfordhand dataset. It is compared against recent hand detection algorithms. The experimental results demonstrate that our proposed method achieves state-of-the-art detection for hands of va…▽ MoreIn this paper, we present a multi-scale Fully Convolutional Networks (MSP-RFCN) to robustly detect and classify human hands under various challenging conditions. In our approach, the input image is passed through the proposed network to generate score maps, based on multi-scale predictions. The network has been specifically designed to deal with small objects. It uses an architecture based on region proposals generated at multiple scales. Our method is evaluated on challenging hand datasets, namely the Vision for Intelligent Vehicles and Applications (VIVA) Challenge and theOxfordhand dataset. It is compared against recent hand detection algorithms. The experimental results demonstrate that our proposed method achieves state-of-the-art detection for hands of various sizes.△ Less"
A third strike against perfect phylogeny,"Authors:Leo van Iersel,Mark Jones,Steven Kelk","Abstract:Perfect phylogenies are fundamental in the study of evolutionary trees because they capture the situation when each evolutionary trait emerges only once in history; if such events are believed to be rare, then by Occam's Razor such parsimonious trees are preferable as a hypothesis of evolution. A classical result states that 2-state characters permit a perfect phylogeny precisely if each subset of…▽ MorePerfect phylogenies are fundamental in the study of evolutionary trees because they capture the situation when each evolutionary trait emerges only once in history; if such events are believed to be rare, then by Occam's Razor such parsimonious trees are preferable as a hypothesis of evolution. A classical result states that 2-state characters permit a perfect phylogeny precisely if each subset of 2 characters permits one. More recently, it was shown that for 3-state characters the same property holds but for size-3 subsets. A long-standing open problem asked whether such a constant exists for each number of states. More precisely, it has been conjectured that for any fixed integer $r$, there exists a constant $f(r)$ such that a set of $r$-state characters $C$ has a perfect phylogeny if and only if every subset of at most $f(r)$ characters has a perfect phylogeny. In this paper, we show that this conjecture is false. In particular, we show that for any constant $t$, there exists a set $C$ of $8$-state characters such that $C$ has no perfect phylogeny, but there exists a perfect phylogeny for every subset of $t$ characters. This negative result complements the two negative results (""strikes"") of Bodlaender et al. We reflect on the consequences of this third strike, pointing out that while it does close off some routes for efficient algorithm development, many others remain open.△ Less"
A Trajectory Calculus for Qualitative Spatial Reasoning Using Answer Set Programming,"Authors:George Baryannis,Ilias Tachmazidis,Sotiris Batsakis,Grigoris Antoniou,Mario Alviano,Timos Sellis,Pei-Wei Tsai","Abstract:Spatial information is often expressed using qualitative terms such as natural language expressions instead of coordinates; reasoning over such terms has several practical applications, such as bus routes planning. Representing and reasoning on trajectories is a specific case of qualitative spatial reasoning that focuses on moving objects and their paths. In this work, we propose two versions of a…▽ MoreSpatial information is often expressed using qualitative terms such as natural language expressions instead of coordinates; reasoning over such terms has several practical applications, such as bus routes planning. Representing and reasoning on trajectories is a specific case of qualitative spatial reasoning that focuses on moving objects and their paths. In this work, we propose two versions of a trajectory calculus based on the allowed properties over trajectories, where trajectories are defined as a sequence of non-overlapping regions of a partitioned map. More specifically, if a given trajectory is allowed to start and finish at the same region, 6 base relations are defined (TC-6). If a given trajectory should have different start and finish regions but cycles are allowed within, 10 base relations are defined (TC-10). Both versions of the calculus are implemented as ASP programs; we propose several different encodings, including a generalised program capable of encoding any qualitative calculus in ASP. All proposed encodings are experimentally evaluated using a real-world dataset. Experiment results show that the best performing implementation can scale up to an input of 250 trajectories for TC-6 and 150 trajectories for TC-10 for the problem of discovering a consistent configuration, a significant improvement compared to previous ASP implementations for similar qualitative spatial and temporal calculi. This manuscript is under consideration for acceptance in TPLP.△ Less"
An iterative approach to precondition inference using constrained Horn clauses,"Authors:Bishoksan Kafle,John P. Gallagher,Graeme Gange,Peter Schachte,Harald Sondergaard,Peter J. Stuckey","Abstract:We present a method for automatic inference of conditions on the initial states of a program that guarantee that the safety assertions in the program are not violated. Constrained Horn clauses (CHCs) are used to model the program and assertions in a uniform way, and we use standard abstract interpretations to derive an over-approximation of the set of unsafe initial states. The precondition then i…▽ MoreWe present a method for automatic inference of conditions on the initial states of a program that guarantee that the safety assertions in the program are not violated. Constrained Horn clauses (CHCs) are used to model the program and assertions in a uniform way, and we use standard abstract interpretations to derive an over-approximation of the set of unsafe initial states. The precondition then is the constraint corresponding to the complement of that set, under-approximating the set of safe initial states. This idea of complementation is not new, but previous attempts to exploit it have suffered from the loss of precision. Here we develop an iterative specialisation algorithm to give more precise, and in some cases optimal safety conditions. The algorithm combines existing transformations, namely constraint specialisation, partial evaluation and a trace elimination transformation. The last two of these transformations perform polyvariant specialisation, leading to disjunctive constraints which improve precision. The algorithm is implemented and tested on a benchmark suite of programs from the literature in precondition inference and software verification competitions.△ Less"
Higher order molecular organisation as a source of biological function,"Authors:Thomas Gaudelet,Noel Malod-Dognin,Natasa Przulj","Abstract:Molecular interactions have widely been modelled as networks. The local wiring patterns around molecules in molecular networks are linked with their biological functions. However, networks model only pairwise interactions between molecules and cannot explicitly and directly capture the higher order molecular organisation, such as protein complexes and pathways. Hence, we ask if hypergraphs (hypern…▽ MoreMolecular interactions have widely been modelled as networks. The local wiring patterns around molecules in molecular networks are linked with their biological functions. However, networks model only pairwise interactions between molecules and cannot explicitly and directly capture the higher order molecular organisation, such as protein complexes and pathways. Hence, we ask if hypergraphs (hypernetworks), that directly capture entire complexes and pathways along with protein-protein interactions (PPIs), carry additional functional information beyond what can be uncovered from networks of pairwise molecular interactions. The mathematical formalism of a hypergraph has long been known, but not often used in studying molecular networks due to the lack of sophisticated algorithms for mining the underlying biological information hidden in the wiring patterns of molecular systems modelled as hypernetworks.
  We propose a new, multi-scale, protein interaction hypernetwork model that utilizes hypergraphs to capture different scales of protein organization, including PPIs, protein complexes and pathways. In analogy to graphlets, we introduce hypergraphlets, small, connected, non-isomorphic, induced sub-hypergraphs of a hypergraph, to quantify the local wiring patterns of these multi-scale molecular hypergraphs and to mine them for new biological information. We apply them to model the multi-scale protein networks of baker yeast and human and show that the higher order molecular organisation captured by these hypergraphs is strongly related to the underlying biology. Importantly, we demonstrate that our new models and data mining tools reveal different, but complementary biological information compared to classical PPI networks. We apply our hypergraphlets to successfully predict biological functions of uncharacterised proteins.△ Less"
Semi-supervised Adversarial Learning to Generate Photorealistic Face Images of New Identities from 3D Morphable Model,"Authors:Baris Gecer,Binod Bhattarai,Josef Kittler,Tae-Kyun Kim","Abstract:…data sets: LFW and IJB-A. We observe that the generated images from our framework consistently improves over the performance of deep face recognition network trained withOxfordVGG Face dataset and achieves comparable results to the state-of-the-art.▽ MoreWe propose a novel end-to-end semi-supervised adversarial framework to generate photorealistic face images of new identities with wide ranges of expressions, poses, and illuminations conditioned by a 3D morphable model. Previous adversarial style-transfer methods either supervise their networks with large volume of paired data or use unpaired data with a highly under-constrained two-way generative framework in an unsupervised fashion. We introduce pairwise adversarial supervision to constrain two-way domain adaptation by a small number of paired real and synthetic images for training along with the large volume of unpaired data. Extensive qualitative and quantitative experiments are performed to validate our idea. Generated face images of new identities contain pose, lighting and expression diversity and qualitative results show that they are highly constraint by the synthetic input image while adding photorealism and retaining identity information. We combine face images generated by the proposed method with the real data set to train face recognition algorithms. We evaluated the model on two challenging data sets: LFW and IJB-A. We observe that the generated images from our framework consistently improves over the performance of deep face recognition network trained withOxfordVGG Face dataset and achieves comparable results to the state-of-the-art.△ Less"
RevisitingOxfordand Paris: Large-Scale Image Retrieval Benchmarking,"Authors:Filip Radenović,Ahmet Iscen,Giorgos Tolias,Yannis Avrithis,Ondřej Chum","Abstract:In this paper we address issues with image retrieval benchmarking on standard and popularOxford5k and Paris 6k datasets. In particular, annotation errors, the size of the dataset, and the level of challenge are addressed: new annotation for both datasets is created with an extra attention to the reliability of the ground truth. Three new protocols of varyi…▽ MoreIn this paper we address issues with image retrieval benchmarking on standard and popularOxford5k and Paris 6k datasets. In particular, annotation errors, the size of the dataset, and the level of challenge are addressed: new annotation for both datasets is created with an extra attention to the reliability of the ground truth. Three new protocols of varying difficulty are introduced. The protocols allow fair comparison between different methods, including those using a dataset pre-processing stage. For each dataset, 15 new challenging queries are introduced. Finally, a new set of 1M hard, semi-automatically cleaned distractors is selected.
  An extensive comparison of the state-of-the-art methods is performed on the new benchmark. Different types of methods are evaluated, ranging from local-feature-based to modern CNN based methods. The best results are achieved by taking the best of the two worlds. Most importantly, image retrieval appears far from being solved.△ Less"
"Duality, Fundamentality, and Emergence","Authors:Elena Castellani,Sebastian De Haro","Abstract:Dualities offer new possibilities for relating fundamentality and emergence. In particular, as is the aim of this chapter to show, it may happen that the relations of fundamentality and emergence between dual theories are inverted. In other words, the direction of emergence typically found in these cases is opposite to the direction of emergence followed in the standard accounts: that is, while th…▽ MoreDualities offer new possibilities for relating fundamentality and emergence. In particular, as is the aim of this chapter to show, it may happen that the relations of fundamentality and emergence between dual theories are inverted. In other words, the direction of emergence typically found in these cases is opposite to the direction of emergence followed in the standard accounts: that is, while the standard emergence direction is that of decreasing fundamentality---in that there is emergence of less fundamental, high-level entities, out of more fundamental, low-level entities---in these cases of duality, on the contrary, a more fundamental entity can emerge out of a less fundamental one. In fact, this possibility can be traced back to the existence of different classical limits in quantum field theories and string theories.△ Less"
Plasma Density Structures at Comet 67P/Churyumov-Gerasimenko,"Authors:I. A. D. Engelhardt,A. I. Eriksson,G. Stenberg Wieser,C. Goetz,M. Rubin,P. Henri,H. Nilsson,E. Odelstad,R. Hajra,X. Vallières","Abstract:We present Rosetta RPC case study from four events at various radial distance, phase angle and local time from autumn 2015, just after perihelion of comet 67P/Churyumov-Gerasimenko. Pulse like (high amplitude, up to minutes in time) signatures are seen with several RPC instruments in the plasma density (LAP, MIP), ion energy and flux (ICA) as well as magnetic field intensity (MAG). Furthermore the…▽ MoreWe present Rosetta RPC case study from four events at various radial distance, phase angle and local time from autumn 2015, just after perihelion of comet 67P/Churyumov-Gerasimenko. Pulse like (high amplitude, up to minutes in time) signatures are seen with several RPC instruments in the plasma density (LAP, MIP), ion energy and flux (ICA) as well as magnetic field intensity (MAG). Furthermore the cometocentric distance relative to the electron exobase is seen to be a good organizing parameter for the measured plasma variations. The closer Rosetta is to this boundary, the more pulses are measured. This is consistent with the pulses being filaments of plasma originating from the diamagnetic cavity boundary as predicted by simulations.△ Less"
Unsupervised Semantic Deep Hashing,Authors:Sheng Jin,"Abstract:…on CIFAR-10, NUSWIDE have demonstrated that \textbf{USDH} outperforms several state-of-the-art unsupervised hashing methods for image retrieval. We also conduct experiments onOxford17 datasets for fine-grained classification to verify its efficiency for other computer vision tasks.▽ MoreIn recent years, deep hashing methods have been proved to be efficient since it employs convolutional neural network to learn features and hashing codes simultaneously. However, these methods are mostly supervised. In real-world application, it is a time-consuming and overloaded task for annotating a large number of images. In this paper, we propose a novel unsupervised deep hashing method for large-scale image retrieval. Our method, namely unsupervised semantic deep hashing (\textbf{USDH}), uses semantic information preserved in the CNN feature layer to guide the training of network. We enforce four criteria on hashing codes learning based on VGG-19 model: 1) preserving relevant information of feature space in hashing space; 2) minimizing quantization loss between binary-like codes and hashing codes; 3) improving the usage of each bit in hashing codes by using maximum information entropy, and 4) invariant to image rotation. Extensive experiments on CIFAR-10, NUSWIDE have demonstrated that \textbf{USDH} outperforms several state-of-the-art unsupervised hashing methods for image retrieval. We also conduct experiments onOxford17 datasets for fine-grained classification to verify its efficiency for other computer vision tasks.△ Less"
Accretion Processes,Authors:Alessandro Morbidelli,"Abstract:In planetary science, accretion is the process in which solids agglomerate to form larger and larger objects and eventually planets are produced. The initial conditions are a disc of gas and microscopic solid particles, with a total mass of about 1% of the gas mass. Accretion has to be effective and fast. Effective, because the original total mass in solids in the solar protoplanetary disk was pro…▽ MoreIn planetary science, accretion is the process in which solids agglomerate to form larger and larger objects and eventually planets are produced. The initial conditions are a disc of gas and microscopic solid particles, with a total mass of about 1% of the gas mass. Accretion has to be effective and fast. Effective, because the original total mass in solids in the solar protoplanetary disk was probably of the order of ~ 300 Earth masses, and the mass incorporated into the planets is ~100 Earth masses. Fast, because the cores of the giant planets had to grow to tens of Earth masses in order to capture massive doses of hydrogen and helium from the disc before the dispersal of the latter, i.e. in a few millions of years. There is probably not one accretion process but several, depending on the scale at which accretion operates. A first process is the sticking of microscopic dust into larger grains and pebbles. A second process is the formation of an intermediate class of objects called planetesimals. A third accretion process has to lead from planetesimals to planets. Actually, several processes can be involved in this step, from collisional coagulation among planetesimals to the accretion of small particles under the effect of gas drag, to giant impacts between protoplanets. This chapter will detail all these processes, adopting a historical perspective: i.e. from the classic processes investigated in the past decades to those unveiled in the last years.△ Less"
Line Artist: A Multiple Style Sketch to Painting Synthesis Scheme,"Authors:Jinning Li,Siqi Liu,Mengyao Cao","Abstract:…VGG19 network and PageRank algorithm. The appealing artistic images are then generated by optimization iterations. Experiments are operated on the Kaggle Cats dataset and TheOxfordBuildings Dataset. Our synthesis results are proved to be artistic, beautiful and robust.▽ MoreDrawing a beautiful painting is a dream of many people since childhood. In this paper, we propose a novel scheme, Line Artist, to synthesize artistic style paintings with freehand sketch images, leveraging the power of deep learning and advanced algorithms. Our scheme includes three models. The Sketch Image Extraction (SIE) model is applied to generate the training data. It includes smoothing reality images and pencil sketch extraction. The Detailed Image Synthesis (DIS) model trains a conditional generative adversarial network to generate detailed real-world information. The Adaptively Weighted Artistic Style Transfer (AWAST) model is capable to combine multiple style images with a content with the VGG19 network and PageRank algorithm. The appealing artistic images are then generated by optimization iterations. Experiments are operated on the Kaggle Cats dataset and TheOxfordBuildings Dataset. Our synthesis results are proved to be artistic, beautiful and robust.△ Less"
Benchmarks for cyber-physical systems: A modular model library for building automation systems (Extended version),"Authors:Nathalie Cauchi,Alessandro Abate","Abstract:…where corresponding models are built and engaged with different analysis goals. The benchmarks, the model library and data collected from the BAS setup at the University ofOxford, are kept on-line at https://github.com/natchi92/BASBenchmarks.▽ MoreBuilding Automation Systems (BAS) are exemplars of Cyber-Physical Systems (CPS), incorporating digital control architectures over underlying continuous physical processes. We provide a modular model library for BAS drawn from expertise developed on a real BAS setup. The library allows to build models comprising of either physical quantities or digital control modules.% which are composable. The structure, operation, and dynamics of the model can be complex, incorporating (i) stochasticity, (ii) non-linearities, (iii) numerous continuous variables or discrete states, (iv) various input and output signals, and (v) a large number of possible discrete configurations. The modular composition of BAS components can generate useful CPS benchmarks. We display this use by means of three realistic case studies, where corresponding models are built and engaged with different analysis goals. The benchmarks, the model library and data collected from the BAS setup at the University ofOxford, are kept on-line at https://github.com/natchi92/BASBenchmarks.△ Less"
Minimum error correction-based haplotype assembly: considerations for long read data,"Authors:Sina Majidian,Mohammad Hossein Kahaei,Dick de Ridder","Abstract:…reconstructed haplotypes for devices that produce error-prone long reads. Specifically, we evaluate this approach for devices developed by Illumina, Pacific BioSciences andOxfordNanopore Technologies. We show that imprecise haplotypes may be reconstructed with a lower MEC than that of the exact haplotype. The performance of MEC is explored for different co…▽ MoreThe single nucleotide polymorphism (SNP) is the most widely studied type of genetic variation. A haplotype is defined as the sequence of alleles at SNP sites on each haploid chromosome. Haplotype information is essential in unravelling the genome-phenotype association. Haplotype assembly is a well-known approach for reconstructing haplotypes, exploiting reads generated by DNA sequencing devices. The Minimum Error Correction (MEC) metric is often used for reconstruction of haplotypes from reads. However, problems with the MEC metric have been reported. Here, we investigate the MEC approach to demonstrate that it may result in incorrectly reconstructed haplotypes for devices that produce error-prone long reads. Specifically, we evaluate this approach for devices developed by Illumina, Pacific BioSciences andOxfordNanopore Technologies. We show that imprecise haplotypes may be reconstructed with a lower MEC than that of the exact haplotype. The performance of MEC is explored for different coverage levels and error rates of data. Our simulation results reveal that in order to avoid incorrect MEC-based haplotypes, a coverage of 25 is needed for reads generated by Pacific BioSciences RS systems.△ Less"
Adversarial Training for Adverse Conditions: Robust Metric Localisation using Appearance Transfer,"Authors:Horia Porav,Will Maddern,Paul Newman","Abstract:…images are used to localize the robot against a previously built map using traditional sparse matching approaches. We benchmark our results using multiple traversals of theOxfordRobotCar Dataset over a year-long period, using one traversal as a map and the other to localise. We show that this method significantly improves place recognition and localisation…▽ MoreWe present a method of improving visual place recognition and metric localisation under very strong appear- ance change. We learn an invertable generator that can trans- form the conditions of images, e.g. from day to night, summer to winter etc. This image transforming filter is explicitly designed to aid and abet feature-matching using a new loss based on SURF detector and dense descriptor maps. A network is trained to output synthetic images optimised for feature matching given only an input RGB image, and these generated images are used to localize the robot against a previously built map using traditional sparse matching approaches. We benchmark our results using multiple traversals of theOxfordRobotCar Dataset over a year-long period, using one traversal as a map and the other to localise. We show that this method significantly improves place recognition and localisation under changing and adverse conditions, while reducing the number of mapping runs needed to successfully achieve reliable localisation.△ Less"
Identifying Sources and Sinks in the Presence of Multiple Agents with Gaussian Process Vector Calculus,"Authors:Adam D. Cobb,Richard Everett,Andrew Markham,Stephen J. Roberts","Abstract:In systems of multiple agents, identifying the cause of observed agent dynamics is challenging. Often, these agents operate in diverse, non-stationary environments, where models rely on hand-crafted environment-specific features to infer influential regions in the system's surroundings. To overcome the limitations of these inflexible models, we present GP-LAPLACE, a technique for locating sources…▽ MoreIn systems of multiple agents, identifying the cause of observed agent dynamics is challenging. Often, these agents operate in diverse, non-stationary environments, where models rely on hand-crafted environment-specific features to infer influential regions in the system's surroundings. To overcome the limitations of these inflexible models, we present GP-LAPLACE, a technique for locating sources and sinks from trajectories in time-varying fields. Using Gaussian processes, we jointly infer a spatio-temporal vector field, as well as canonical vector calculus operations on that field. Notably, we do this from only agent trajectories without requiring knowledge of the environment, and also obtain a metric for denoting the significance of inferred causal features in the environment by exploiting our probabilistic method. To evaluate our approach, we apply it to both synthetic and real-world GPS data, demonstrating the applicability of our technique in the presence of multiple agents, as well as its superiority over existing methods.△ Less"
Inter-Theory Relations in Physics: Case Studies from Quantum Mechanics and Quantum Field Theory,Authors:Joshua Rosaler,"Abstract:The relationship that is widely presumed to hold between physical theories and their successors, in which the successors in some sense explain the success of the theories they replace, is known commonly as 'reduction.' I argue that one traditional approach to theory reduction in physics, founded on the notion that a superseded theory should simply be a mathematical limit of the theory that superse…▽ MoreThe relationship that is widely presumed to hold between physical theories and their successors, in which the successors in some sense explain the success of the theories they replace, is known commonly as 'reduction.' I argue that one traditional approach to theory reduction in physics, founded on the notion that a superseded theory should simply be a mathematical limit of the theory that supersedes it, is misleading as a general picture of the relationship whereby one theory encompasses the domain of empirical validity of another. I defend an alternative account that builds upon a certain general type of relationship between dynamical systems models describing the same physical system. I demonstrate how this particular relationship resembles the methodological prescriptions set out by Ernest Nagel's more general approach to reduction across the sciences.
  After clarifying these points of general methodology, I go on to apply this approach to a number of particular inter-theory reductions in physics involving quantum theory. I consider three reductions: first, connecting classical mechanics and non-relativistic quantum mechanics; second, connecting classical electrodynamics and quantum electrodynamics; and third, connecting non-relativistic quantum mechanics and quantum electrodynamics. In all cases, a certain core set of mechanisms, employing decoherence together with variations of Ehrenfest's Theorem, serves to underwrite the occurrence of approximately classical behavior. For concreteness, I consider two particular realist interpretations of quantum theory - the Everett and Bohm theories - as potential bases for these reductions. However, many of the technical results concerning these reductions pertain also more generally to the bare, uninterpreted formalism of quantum theory.△ Less"
Agama reference documentation,Authors:Eugene Vasiliev,"Abstract:Agama (Action-based Galaxy Modelling Architecture) is a software library intended for a broad range of tasks within the field of stellar dynamics. As the name suggests, it is centered around the use of action/angle formalism to describe the structure of stellar systems, but this is only one of its many facets. The library contains a powerful framework for dealing with arbitrary density/potential p…▽ MoreAgama (Action-based Galaxy Modelling Architecture) is a software library intended for a broad range of tasks within the field of stellar dynamics. As the name suggests, it is centered around the use of action/angle formalism to describe the structure of stellar systems, but this is only one of its many facets. The library contains a powerful framework for dealing with arbitrary density/potential profiles and distribution functions (analytic, extracted from N-body models, or fitted to the data), a vast collection of general-purpose mathematical routines, and covers many aspects of galaxy dynamics up to the very high-level interface for constructing self-consistent galaxy models.
  This document serves two purposes. First of all, this is a detailed reference for the library itself. Second, it describes various mathematical and numerical methods that could be applicable in a broader context. These include: (1) one- and multidimensional interpolation using cubic and quintic splines; (2) penalized spline fitting of noisy data and penalized spline density estimation; (3) adaptive rejection sampling from multidimensional probability distributions; (4) computation of gravitational potentials using spherical- and azimuthal-harmonic expansions; (5) Staeckel fudge method for computing action/angle variables; (6) a general discussion about good programming practices.△ Less"
AGAMA: Action-based galaxy modelling architecture,Authors:Eugene Vasiliev,"Abstract:Agama is a publicly available software library for a broad range of applications in the field of stellar dynamics. It provides methods for computing the gravitational potential of arbitrary analytic density profiles or N-body models; orbit integration and analysis; transformations between position/velocity and action/angle variables; distribution functions expressed in terms of actions and their m…▽ MoreAgama is a publicly available software library for a broad range of applications in the field of stellar dynamics. It provides methods for computing the gravitational potential of arbitrary analytic density profiles or N-body models; orbit integration and analysis; transformations between position/velocity and action/angle variables; distribution functions expressed in terms of actions and their moments; iterative construction of self-consistent multicomponent galaxy models. Applications include the inference about the structure of Milky Way or other galaxies from observations of stellar kinematics; preparation of equilibrium initial conditions for N-body simulations; analysis of snapshots from simulations. The library is written in C++, provides a Python interface, and can be coupled to other stellar-dynamical software: Amuse, Galpy and Nemo.△ Less"
Proceedings Fourth International Workshop on Rewriting Techniques for Program Transformations and Evaluation,"Authors:Horatiu Cirstea,David Sabel","Abstract:…contains the formal proceedings of the 4th International Workshop on Rewriting Techniques for Program Transformations and Evaluation (WPTE 2017), held on 8th September 2017 inOxford, United Kingdom, and affiliated with the Second International Conference on Formal Structures for Computation and Deduction (FSCD 2017).▽ MoreThis volume contains the formal proceedings of the 4th International Workshop on Rewriting Techniques for Program Transformations and Evaluation (WPTE 2017), held on 8th September 2017 inOxford, United Kingdom, and affiliated with the Second International Conference on Formal Structures for Computation and Deduction (FSCD 2017).△ Less"
Simulation assisted machine learning,"Authors:Timo M. Deist,Andrew Patti,Zhaoqi Wang,David Krane,Taylor Sorenson,David Craft","Abstract:Motivation: In a predictive modeling setting, if sufficient details of the system behavior are known, one can build and use a simulation for making predictions. When sufficient system details are not known, one typically turns to machine learning, which builds a black-box model of the system using a large dataset of input sample features and outputs. We consider a setting which is between these tw…▽ MoreMotivation: In a predictive modeling setting, if sufficient details of the system behavior are known, one can build and use a simulation for making predictions. When sufficient system details are not known, one typically turns to machine learning, which builds a black-box model of the system using a large dataset of input sample features and outputs. We consider a setting which is between these two extremes: some details of the system mechanics are known but not enough for creating simulations that can be used to make high quality predictions. In this context we propose using approximate simulations to build a kernel for use in kernelized machine learning methods, such as support vector machines. The results of multiple simulations (under various uncertainty scenarios) are used to compute similarity measures between every pair of samples: sample pairs are given a high similarity score if they behave similarly under a wide range of simulation parameters. These similarity values, rather than the original high dimensional feature data, are used to build the kernel.
  Results: We demonstrate and explore the simulation based kernel (SimKern) concept using four synthetic complex systems--three biologically inspired models and one network flow optimization model. We show that, when the number of training samples is small compared to the number of features, the SimKern approach dominates over no-prior-knowledge methods. This approach should be applicable in all disciplines where predictive models are sought and informative yet approximate simulations are available.
  Availability: The Python SimKern software, the demonstration models (in MATLAB, R), and the datasets are available at https://github.com/davidcraft/SimKern.△ Less"
The Interhospital Transfer Network for Very Low Birth Weight Infants in the United States,"Authors:Munik Shrestha,Samuel V. Scarpino,Erika M. Edwards,Lucy T. Greenberg,Jeffrey D. Horbar","Abstract:…networks are important, both economically and for infant morbidity and mortality, the national-level pattern of neonatal transfers is largely unknown. Using data from VermontOxfordNetwork on 44,753 births, 2,122 hospitals, and 9,722 inter-hospital infant transfers from 2015, we performed the largest analysis to date on the inter-hospital transfer network f…▽ MoreVery low birth weight (VLBW) infants require specialized care in neonatal intensive care units. In the United States (U.S.), such infants frequently are transferred between hospitals. Although these neonatal transfer networks are important, both economically and for infant morbidity and mortality, the national-level pattern of neonatal transfers is largely unknown. Using data from VermontOxfordNetwork on 44,753 births, 2,122 hospitals, and 9,722 inter-hospital infant transfers from 2015, we performed the largest analysis to date on the inter-hospital transfer network for VLBW infants in the U.S. We find that transfers are organized around regional communities, but that despite being largely within state boundaries, most communities often contain at least two hospitals in different states. To classify the structural variation in transfer pattern amongst these communities, we applied a spectral measure for regionalization and found an association between a community's degree of regionalization and their infant transfer rate, which was not utilized in detecting communities. We also demonstrate that the established measures of network centrality and hierarchy, e.g., the community-wide entropy in PageRank or betweenness centrality and number of distinct `layers' within a community, correlate weakly with our regionalization index and were not significantly associated with metrics on infant transfer rate. Our results suggest that the regionalization index captures novel information about the structural properties of VLBW infant transfer networks, have the practical implication of characterizing neonatal care in the U.S., and may apply more broadly to the role of centralizing forces in organizing complex adaptive systems.△ Less"
The Higher-Order Prover Leo-III (Extended Version),"Authors:Alexander Steen,Christoph Benzmüller","Abstract:The automated theorem prover Leo-III for classical higher-order logic with Henkin semantics and choice is presented. Leo-III is based on extensional higher-order paramodulation and accepts every common TPTP dialect (FOF, TFF, THF), including their recent extensions to rank-1 polymorphism (TF1, TH1). In addition, the prover natively supports almost every normal higher-order modal logic. Leo-III coo…▽ MoreThe automated theorem prover Leo-III for classical higher-order logic with Henkin semantics and choice is presented. Leo-III is based on extensional higher-order paramodulation and accepts every common TPTP dialect (FOF, TFF, THF), including their recent extensions to rank-1 polymorphism (TF1, TH1). In addition, the prover natively supports almost every normal higher-order modal logic. Leo-III cooperates with first-order reasoning tools using translations to many-sorted first-order logic and produces verifiable proof certificates. The prover is evaluated on heterogeneous benchmark sets.△ Less"
Differential Equation Axiomatization: The Impressive Power of Differential Ghosts,"Authors:André Platzer,Yong Kiam Tan","Abstract:We prove the completeness of an axiomatization for differential equation invariants. First, we show that the differential equation axioms in differential dynamic logic are complete for all algebraic invariants. Our proof exploits differential ghosts, which introduce additional variables that can be chosen to evolve freely along new differential equations. Cleverly chosen differential ghosts are th…▽ MoreWe prove the completeness of an axiomatization for differential equation invariants. First, we show that the differential equation axioms in differential dynamic logic are complete for all algebraic invariants. Our proof exploits differential ghosts, which introduce additional variables that can be chosen to evolve freely along new differential equations. Cleverly chosen differential ghosts are the proof-theoretical counterpart of dark matter. They create new hypothetical state, whose relationship to the original state variables satisfies invariants that did not exist before. The reflection of these new invariants in the original system then enables its analysis.
  We then show that extending the axiomatization with existence and uniqueness axioms makes it complete for all local progress properties, and further extension with a real induction axiom makes it complete for all real arithmetic invariants. This yields a parsimonious axiomatization, which serves as the logical foundation for reasoning about invariants of differential equations. Moreover, our results are purely axiomatic, and so the axiomatization is suitable for sound implementation in foundational theorem provers.△ Less"
Quantum cohomology and closed-string mirror symmetry for toric varieties,Authors:Jack Smith,"Abstract:We give a short new computation of the quantum cohomology of an arbitrary smooth toric variety $X$, by showing directly that the Kodaira-Spencer map of Fukaya-Oh-Ohta-Ono defines an isomorphism onto a suitable Jacobian ring. The proof is based on the purely algebraic fact that a class of generalised Jacobian rings associated to $X$ are free as modules over the Novikov ring. In contrast to previous…▽ MoreWe give a short new computation of the quantum cohomology of an arbitrary smooth toric variety $X$, by showing directly that the Kodaira-Spencer map of Fukaya-Oh-Ohta-Ono defines an isomorphism onto a suitable Jacobian ring. The proof is based on the purely algebraic fact that a class of generalised Jacobian rings associated to $X$ are free as modules over the Novikov ring. In contrast to previous results of this kind, $X$ need not be compact. When $X$ is monotone the presentation we obtain is completely explicit, using only well-known computations with the standard complex structure.△ Less"
Variants of Schanuel's conjecture,Authors:Jonathan Kirby,"Abstract:…minor revisions now in January 2018. The treatment is far from exhaustive of the literature, and for the most part consists of statements which were under discussion in theOxfordlogic group between around 2002 and 2007.▽ MoreThis is a collection of variants of Schanuel's conjecture and the known dependencies between them. It was originally written in 2007, and made available for a time on my webpage. I have been asked by a few people to make it available again and have taken the opportunity to make some minor revisions now in January 2018. The treatment is far from exhaustive of the literature, and for the most part consists of statements which were under discussion in theOxfordlogic group between around 2002 and 2007.△ Less"
The Data Market: Policies for Decentralised Visual Localisation,"Authors:Matthew Gadd,Paul Newman","Abstract:…sections of the world, which the agents treat as ""products"" for appraisal and purchase. To this end, we demonstrate and evaluate our system using the publicly availableOxfordRobotCar Dataset, the hand-labelled data market catalogue (approaching 446km of fully indexed sections-of-interest) for which we plan to release alongside the existing raw ster…▽ MoreThis paper presents a mercantile framework for the decentralised sharing of navigation expertise amongst a fleet of robots which perform regular missions into a common but variable environment. We build on our earlier work and allow individual agents to intermittently initiate trades based on a real-time assessment of the nature of their missions or demand for localisation capability, and to choose trading partners with discrimination based on an internally evolving set of beliefs in the expected value of trading with each other member of the team. To this end, we suggest some obligatory properties that a formalisation of the distributed versioning of experience maps should exhibit, to ensure the eventual convergence in the state of each agent's map under a sequence of pairwise exchanges, as well as the uninterrupted integrity of the representation under versioning operations. To mitigate limitations in hardware and network resources, the ""data market"" is catalogued by distinct sections of the world, which the agents treat as ""products"" for appraisal and purchase. To this end, we demonstrate and evaluate our system using the publicly availableOxfordRobotCar Dataset, the hand-labelled data market catalogue (approaching 446km of fully indexed sections-of-interest) for which we plan to release alongside the existing raw stereo imagery. We show that, by refining market policies over time, agents achieve improved localisation in a directed and accelerated manner.△ Less"
Robots as Powerful Allies for the Study of Embodied Cognition from the Bottom Up,"Authors:Matej Hoffmann,Rolf Pfeifer","Abstract:A large body of compelling evidence has been accumulated demonstrating that embodiment - the agent's physical setup, including its shape, materials, sensors and actuators - is constitutive for any form of cognition and as a consequence, models of cognition need to be embodied. In contrast to methods from empirical sciences to study cognition, robots can be freely manipulated and virtually all key…▽ MoreA large body of compelling evidence has been accumulated demonstrating that embodiment - the agent's physical setup, including its shape, materials, sensors and actuators - is constitutive for any form of cognition and as a consequence, models of cognition need to be embodied. In contrast to methods from empirical sciences to study cognition, robots can be freely manipulated and virtually all key variables of their embodiment and control programs can be systematically varied. As such, they provide an extremely powerful tool of investigation. We present a robotic bottom-up or developmental approach, focusing on three stages: (a) low-level behaviors like walking and reflexes, (b) learning regularities in sensorimotor spaces, and (c) human-like cognition. We also show that robotic based research is not only a productive path to deepening our understanding of cognition, but that robots can strongly benefit from human-like cognition in order to become more autonomous, robust, resilient, and safe.△ Less"
The Størmer problem for an aligned rotator,"Authors:V. Epp,O. N. Pervukhina",Abstract:The effective potential energy of the particles in the field of rotating uniformly magnetized celestial body is investigated. The axis of rotation coincides with the axis of the magnetic field. Electromagnetic field of the body is composed of a dipole magnetic and quadrupole electric fields. The geometry of the trapping regions is studied as a function of the magnetic field magnitude and the rotat…▽ MoreThe effective potential energy of the particles in the field of rotating uniformly magnetized celestial body is investigated. The axis of rotation coincides with the axis of the magnetic field. Electromagnetic field of the body is composed of a dipole magnetic and quadrupole electric fields. The geometry of the trapping regions is studied as a function of the magnetic field magnitude and the rotation speed of the body. Examples of the potential energy topology for different values of these parameters are given. The main difference from the classical Størmer problem is that the single toroidal trapping region predicted by Størmer is divided into equatorial and off-equatorial trapping regions. Applicability of the idealized model of a rotating uniformly magnetized sphere with a vacuum magnetosphere to real celestial bodies is discussed.△ Less
On the Semantics of Intensionality and Intensional Recursion,Authors:G. A. Kavvos,"Abstract:Intensionality is a phenomenon that occurs in logic and computation. In the most general sense, a function is intensional if it operates at a level finer than (extensional) equality. This is a familiar setting for computer scientists, who often study different programs or processes that are interchangeable, i.e. extensionally equal, even though they are not implemented in the same way, so intensio…▽ MoreIntensionality is a phenomenon that occurs in logic and computation. In the most general sense, a function is intensional if it operates at a level finer than (extensional) equality. This is a familiar setting for computer scientists, who often study different programs or processes that are interchangeable, i.e. extensionally equal, even though they are not implemented in the same way, so intensionally distinct. Concomitant with intensionality is the phenomenon of intensional recursion, which refers to the ability of a program to have access to its own code. In computability theory, intensional recursion is enabled by Kleene's Second Recursion Theorem. This thesis is concerned with the crafting of a logical toolkit through which these phenomena can be studied. Our main contribution is a framework in which mathematical and computational constructions can be considered either extensionally, i.e. as abstract values, or intensionally, i.e. as fine-grained descriptions of their construction. Once this is achieved, it may be used to analyse intensional recursion.△ Less"
Temporal logic control of general Markov decision processes by approximate policy refinement,"Authors:Sofie Haesaert,Sadegh Soudjani,Alessandro Abate","Abstract:The formal verification and controller synthesis for Markov decision processes that evolve over uncountable state spaces are computationally hard and thus generally rely on the use of approximations. In this work, we consider the correct-by-design control of general Markov decision processes (gMDPs) with respect to temporal logic properties by leveraging approximate probabilistic relations between…▽ MoreThe formal verification and controller synthesis for Markov decision processes that evolve over uncountable state spaces are computationally hard and thus generally rely on the use of approximations. In this work, we consider the correct-by-design control of general Markov decision processes (gMDPs) with respect to temporal logic properties by leveraging approximate probabilistic relations between the original model and its abstraction. We newly work with a robust satisfaction for the construction and verification of control strategies, which allows for both deviations in the outputs of the gMDPs and in the probabilistic transitions. The computation is done over the reduced or abstracted models, such that when a property is robustly satisfied on the abstract model, it is also satisfied on the original model with respect to a refined control strategy.△ Less"
Interfacial Phonon Scattering and Transmission Loss in >1 um Thick Silicon-on-insulator Thin Films,"Authors:Puqing Jiang,Lucas Lindsay,Xi Huang,Yee Kan Koh","Abstract:…of incidence, and the interface roughness, as historically evaluated using a specularity parameter p formulated by Ziman [J. M. Ziman, Electrons and Phonons (Clarendon Press,Oxford, 1960)]. This parameter was initially defined to determine the probability of a phonon specularly reflecting or diffusely scattering from the rough surface of a material. The val…▽ MoreScattering of phonons at boundaries of a crystal (grains, surfaces, or solid/solid interfaces) is characterized by the phonon wavelength, the angle of incidence, and the interface roughness, as historically evaluated using a specularity parameter p formulated by Ziman [J. M. Ziman, Electrons and Phonons (Clarendon Press,Oxford, 1960)]. This parameter was initially defined to determine the probability of a phonon specularly reflecting or diffusely scattering from the rough surface of a material. The validity of Ziman's theory as extended to solid/solid interfaces has not been previously validated. To better understand the interfacial scattering of phonons and to test the validity of Ziman's theory, we precisely measured the in-plane thermal conductivity of a series of Si films in silicon-on-insulator (SOI) wafers by time-domain thermoreflectance (TDTR) for a Si film thickness range of 1 - 10 μm and a temperature range of 100 - 300 K. The Si/SiO2 interface roughness was determined to be 0.11+/-0.04 nm using transmission electron microscopy (TEM). Furthermore, we compared our in-plane thermal conductivity measurements to theoretical calculations that combine first-principles phonon transport with Ziman's theory. Calculations using Ziman's specularity parameter significantly overestimate values from the TDTR measurements. We attribute this discrepancy to phonon transmission through the solid/solid interface into the substrate, which is not accounted for by Ziman's theory for surfaces. We derive a simple expression for the specularity parameter at solid/amorphous interfaces and achieve good agreement between calculations and measurement values.△ Less"
Geometry-Aware Learning of Maps for Camera Localization,"Authors:Samarth Brahmbhatt,Jinwei Gu,Kihwan Kim,James Hays,Jan Kautz","Abstract:…for camera rotation which is better suited for deep-learning based camera pose regression. Experimental results on both the indoor 7-Scenes dataset and the outdoorOxfordRobotCar dataset show significant performance improvement over prior work. The MapNet project webpage is https://goo.gl/mRB3Au.▽ MoreMaps are a key component in image-based camera localization and visual SLAM systems: they are used to establish geometric constraints between images, correct drift in relative pose estimation, and relocalize cameras after lost tracking. The exact definitions of maps, however, are often application-specific and hand-crafted for different scenarios (e.g. 3D landmarks, lines, planes, bags of visual words). We propose to represent maps as a deep neural net called MapNet, which enables learning a data-driven map representation. Unlike prior work on learning maps, MapNet exploits cheap and ubiquitous sensory inputs like visual odometry and GPS in addition to images and fuses them together for camera localization. Geometric constraints expressed by these inputs, which have traditionally been used in bundle adjustment or pose-graph optimization, are formulated as loss terms in MapNet training and also used during inference. In addition to directly improving localization accuracy, this allows us to update the MapNet (i.e., maps) in a self-supervised manner using additional unlabeled video sequences from the scene. We also propose a novel parameterization for camera rotation which is better suited for deep-learning based camera pose regression. Experimental results on both the indoor 7-Scenes dataset and the outdoorOxfordRobotCar dataset show significant performance improvement over prior work. The MapNet project webpage is https://goo.gl/mRB3Au.△ Less"
Saliency Weighted Convolutional Features for Instance Search,"Authors:Eva Mohedano,Kevin McGuinness,Xavier Giro-i-Nieto,Noel E. O'Connor","Abstract:…in instance search tasks. The proposed approach outperforms the state-of-the-art on the challenging INSTRE benchmark by a large margin, and provides similar performance on theOxfordand Paris benchmarks compared to more complex methods that use off-the-shelf representations. The source code used in this project is available at https://imatge-upc.github.io/s…▽ MoreThis work explores attention models to weight the contribution of local convolutional representations for the instance search task. We present a retrieval framework based on bags of local convolutional features (BLCF) that benefits from saliency weighting to build an efficient image representation. The use of human visual attention models (saliency) allows significant improvements in retrieval performance without the need to conduct region analysis or spatial verification, and without requiring any feature fine tuning. We investigate the impact of different saliency models, finding that higher performance on saliency benchmarks does not necessarily equate to improved performance when used in instance search tasks. The proposed approach outperforms the state-of-the-art on the challenging INSTRE benchmark by a large margin, and provides similar performance on theOxfordand Paris benchmarks compared to more complex methods that use off-the-shelf representations. The source code used in this project is available at https://imatge-upc.github.io/salbow/△ Less"
Magnetic fields driven by tidal mixing in radiative stars,"Authors:Jérémie Vidal,David Cébron,Nathanaël Schaeffer,Rainer Hollerbach","Abstract:Stellar magnetism plays an important role in stellar evolution theory. Approximatively 10% of observed main sequence (MS) and pre-main-sequence (PMS) radiative stars exhibit surface magnetic fields above the detection limit, raising the question of their origin. These stars host outer radiative envelopes, which are stably stratified. Therefore, they are assumed to be motionless in standard models…▽ MoreStellar magnetism plays an important role in stellar evolution theory. Approximatively 10% of observed main sequence (MS) and pre-main-sequence (PMS) radiative stars exhibit surface magnetic fields above the detection limit, raising the question of their origin. These stars host outer radiative envelopes, which are stably stratified. Therefore, they are assumed to be motionless in standard models of stellar structure and evolution. We focus on rapidly rotating, radiative stars which may be prone to the tidal instability, due to an orbital companion. Using direct numerical simulations in a sphere, we study the interplay between a stable stratification and the tidal instability, and assess its dynamo capability. We show that the tidal instability is triggered regardless of the strength of the stratification (Brunt-V{ä}is{ä}l{ä} frequency). Furthermore, the tidal instability can lead to both mixing and self-induced magnetic fields in stably stratified layers (provided that the Brunt-V{ä}is{ä}l{ä} frequency does not exceed the stellar spin rate in the simulations too much). The application to stars suggests that the resulting magnetic fields could be observable at the stellar surfaces. Indeed, we expect magnetic field strengths up to several Gauss.Consequently, tidally driven dynamos should be considered as a (complementary) dynamo mechanism, possibly operating in radiative MS and PMS stars hosting orbital companions. In particular, tidally driven dynamos may explain the observed magnetism of tidally deformed and rapidly rotating Vega-like stars.△ Less"
Applications and Challenges of Real-time Mobile DNA Analysis,"Authors:Steven Y. Ko,Lauren Sassoubre,Jaroslaw Zola","Abstract:The DNA sequencing is the process of identifying the exact order of nucleotides within a given DNA molecule. The new portable and relatively inexpensive DNA sequencers, such asOxfordNanopore MinION, have the potential to move DNA sequencing outside of laboratory, leading to faster and more accessible DNA-based diagnostics. However, portable DNA sequencing…▽ MoreThe DNA sequencing is the process of identifying the exact order of nucleotides within a given DNA molecule. The new portable and relatively inexpensive DNA sequencers, such asOxfordNanopore MinION, have the potential to move DNA sequencing outside of laboratory, leading to faster and more accessible DNA-based diagnostics. However, portable DNA sequencing and analysis are challenging for mobile systems, owing to high data throughputs and computationally intensive processing performed in environments with unreliable connectivity and power.
  In this paper, we provide an analysis of the challenges that mobile systems and mobile computing must address to maximize the potential of portable DNA sequencing, and in situ DNA analysis. We explain the DNA sequencing process and highlight the main differences between traditional and portable DNA sequencing in the context of the actual and envisioned applications. We look at the identified challenges from the perspective of both algorithms and systems design, showing the need for careful co-design.△ Less"
Driven to Distraction: Self-Supervised Distractor Learning for Robust Monocular Visual Odometry in Urban Environments,"Authors:Dan Barnes,Will Maddern,Geoffrey Pascoe,Ingmar Posner","Abstract:…the correct egomotion even when 90% of the image is obscured by dynamic, independently moving objects. We evaluate our robust VO methods on more than 400km of driving from theOxfordRobotCar Dataset and demonstrate reduced odometry drift and significantly improved egomotion estimation in the presence of large moving vehicles in urban traffic.▽ MoreWe present a self-supervised approach to ignoring ""distractors"" in camera images for the purposes of robustly estimating vehicle motion in cluttered urban environments. We leverage offline multi-session mapping approaches to automatically generate a per-pixel ephemerality mask and depth map for each input image, which we use to train a deep convolutional network. At run-time we use the predicted ephemerality and depth as an input to a monocular visual odometry (VO) pipeline, using either sparse features or dense photometric matching. Our approach yields metric-scale VO using only a single camera and can recover the correct egomotion even when 90% of the image is obscured by dynamic, independently moving objects. We evaluate our robust VO methods on more than 400km of driving from theOxfordRobotCar Dataset and demonstrate reduced odometry drift and significantly improved egomotion estimation in the presence of large moving vehicles in urban traffic.△ Less"
Language-Based Image Editing with Recurrent Attentive Models,"Authors:Jianbo Chen,Yelong Shen,Jianfeng Gao,Jingjing Liu,Xiaodong Liu","Abstract:…that the framework leads to state-of-the-art performance on image segmentation on the ReferIt dataset. Third, we present the first language-based colorization result on theOxford-102 Flowers dataset.▽ MoreWe investigate the problem of Language-Based Image Editing (LBIE). Given a source image and a natural language description, we want to generate a target image by editing the source image based on the description. We propose a generic modeling framework for two sub-tasks of LBIE: language-based image segmentation and image colorization. The framework uses recurrent attentive models to fuse image and language features. Instead of using a fixed step size, we introduce for each region of the image a termination gate to dynamically determine after each inference step whether to continue extrapolating additional information from the textual description. The effectiveness of the framework is validated on three datasets. First, we introduce a synthetic dataset, called CoSaL, to evaluate the end-to-end performance of our LBIE system. Second, we show that the framework leads to state-of-the-art performance on image segmentation on the ReferIt dataset. Third, we present the first language-based colorization result on theOxford-102 Flowers dataset.△ Less"
Fine-tuning CNN Image Retrieval with No Human Annotation,"Authors:Filip Radenović,Giorgos Tolias,Ondřej Chum","Abstract:…average pooling and show that it boosts retrieval performance. Applying the proposed method to the VGG network achieves state-of-the-art performance on the standard benchmarks:OxfordBuildings, Paris, and Holidays datasets.▽ MoreImage descriptors based on activations of Convolutional Neural Networks (CNNs) have become dominant in image retrieval due to their discriminative power, compactness of representation, and search efficiency. Training of CNNs, either from scratch or fine-tuning, requires a large amount of annotated data, where a high quality of annotation is often crucial. In this work, we propose to fine-tune CNNs for image retrieval on a large collection of unordered images in a fully automated manner. Reconstructed 3D models obtained by the state-of-the-art retrieval and structure-from-motion methods guide the selection of the training data. We show that both hard-positive and hard-negative examples, selected by exploiting the geometry and the camera positions available from the 3D models, enhance the performance of particular-object retrieval. CNN descriptor whitening discriminatively learned from the same training data outperforms commonly used PCA whitening. We propose a novel trainable Generalized-Mean (GeM) pooling layer that generalizes max and average pooling and show that it boosts retrieval performance. Applying the proposed method to the VGG network achieves state-of-the-art performance on the standard benchmarks:OxfordBuildings, Paris, and Holidays datasets.△ Less"
Efficient simulation techniques for biochemical reaction networks,Authors:Christopher Lester,"Abstract:Discrete-state, continuous-time Markov models are becoming commonplace in the modelling of biochemical processes. The mathematical formulations that such models lead to are opaque, and, due to their complexity, are often considered analytically intractable. As such, a variety of Monte Carlo simulation algorithms have been developed to explore model dynamics empirically. Whilst well-known methods,…▽ MoreDiscrete-state, continuous-time Markov models are becoming commonplace in the modelling of biochemical processes. The mathematical formulations that such models lead to are opaque, and, due to their complexity, are often considered analytically intractable. As such, a variety of Monte Carlo simulation algorithms have been developed to explore model dynamics empirically. Whilst well-known methods, such as the Gillespie Algorithm, can be implemented to investigate a given model, the computational demands of traditional simulation techniques remain a significant barrier to modern research.
  In order to further develop and explore biologically relevant stochastic models, new and efficient computational methods are required. In this thesis, high-performance simulation algorithms are developed to estimate summary statistics that characterise a chosen reaction network. The algorithms make use of variance reduction techniques, which exploit statistical properties of the model dynamics, to improve performance.
  The multi-level method is an example of a variance reduction technique. The method estimates summary statistics of well-mixed, spatially homogeneous models by using estimates from multiple ensembles of sample paths of different accuracies. In this thesis, the multi-level method is developed in three directions: firstly, a nuanced implementation framework is described; secondly, a reformulated method is applied to stiff reaction systems; and, finally, different approaches to variance reduction are implemented and compared.
  The variance reduction methods that underpin the multi-level method are then re-purposed to understand how the dynamics of a spatially-extended Markov model are affected by changes in its input parameters. By exploiting the inherent dynamics of spatially-extended models, an efficient finite difference scheme is used to estimate parametric sensitivities robustly.△ Less"
Preconditioners for Two-Phase Incompressible Navier-Stokes Flow,"Authors:Niall Bootland,Alistair Bentley,Christopher Kees,Andrew Wathen","Abstract:…(PCD) preconditioner [H. C. Elman, D. J. Silvester and A. J. Wathen, Finite Elements and Fast Iterative Solvers: with Applications in Incompressible Fluid Dynamics, second ed.,OxfordUniversity Press, 2014]. However, the original technique fails to give comparable performance in its given form when applied to variable coefficient Navier-Stokes systems such…▽ MoreWe consider iterative methods for solving the linearised Navier-Stokes equations arising from two-phase flow problems and the efficient preconditioning of such systems when using mixed finite element methods. Our target application is simulation within the Proteus toolkit; in particular, we will give results for a dynamic dam-break problem in 2D. We focus on a preconditioner motivated by approximate commutators which has proved effective, displaying mesh-independent convergence for the constant coefficient single-phase Navier-Stokes equations. This approach is known as the ""pressure convection-diffusion"" (PCD) preconditioner [H. C. Elman, D. J. Silvester and A. J. Wathen, Finite Elements and Fast Iterative Solvers: with Applications in Incompressible Fluid Dynamics, second ed.,OxfordUniversity Press, 2014]. However, the original technique fails to give comparable performance in its given form when applied to variable coefficient Navier-Stokes systems such as those arising in two-phase flow models. Here we develop a generalisation of this preconditioner appropriate for two-phase flow, requiring a new form for PCD. We omit considerations of boundary conditions to focus on the key features of two-phase flow. Before considering our target application, we present numerical results within the controlled setting of a simplified problem using a variety of different mixed elements. We compare these results with those for a straightforward extension to another commutator-based method known as the ""least-squares commutator"" (LSC) preconditioner, a technique also discussed in the aforementioned reference. We demonstrate that favourable properties of the original PCD and LSC preconditioners (without boundary adjustments) are retained with the new preconditioners in the two-phase situation.△ Less"
Evolution in Virtual Worlds,Authors:Tim Taylor,"Abstract:This chapter discusses the possibility of instilling a virtual world with mechanisms for evolution and natural selection in order to generate rich ecosystems of complex organisms in a process akin to biological evolution. Some previous work in the area is described, and successes and failures are discussed. The components of a more comprehensive framework for designing such worlds are mapped out,…▽ MoreThis chapter discusses the possibility of instilling a virtual world with mechanisms for evolution and natural selection in order to generate rich ecosystems of complex organisms in a process akin to biological evolution. Some previous work in the area is described, and successes and failures are discussed. The components of a more comprehensive framework for designing such worlds are mapped out, including the design of the individual organisms, the properties and dynamics of the environmental medium in which they are evolving, and the representational relationship between organism and environment. Some of the key issues discussed include how to allow organisms to evolve new structures and functions with few restrictions, and how to create an interconnectedness between organisms in order to generate drives for continuing evolutionary activity.△ Less"
Theory of one-dimensional Vlasov-Maxwell equilibria: with applications to collisionless current sheets and flux tubes,Authors:Oliver Allanson,"Abstract:We study the theory of Vlasov-Maxwell equilibria in one spatial dimension, as well as its application to current sheet and flux tube models. The 'inverse problem' is that of determining a Vlasov-Maxwell equilibrium distribution function self-consistent with a given magnetic field. We develop the theory of inversion using expansions in Hermite polynomials of the canonical momenta. Sufficient condit…▽ MoreWe study the theory of Vlasov-Maxwell equilibria in one spatial dimension, as well as its application to current sheet and flux tube models. The 'inverse problem' is that of determining a Vlasov-Maxwell equilibrium distribution function self-consistent with a given magnetic field. We develop the theory of inversion using expansions in Hermite polynomials of the canonical momenta. Sufficient conditions for the convergence of a Hermite expansion are found, given a pressure tensor. For large classes of DFs, we prove that non-negativity of the distribution function is contingent on the magnetisation of the plasma, and make conjectures for all classes.
  The inverse problem is considered for nonlinear 'force-free Harris sheets'. By applying the Hermite method, we construct new models that can describe sub-unity values of the plasma beta $(β_{pl})$ for the first time. Whilst analytical convergence is proven for all $β_{pl}$, numerical convergence is attained for $β_{pl}= 0.85$, and then $β_{pl} = 0.05$ after a 're-gauging' process.
  We consider the properties that a pressure tensor must satisfy to be consistent with 'asymmetric Harris sheets', and construct new examples. It is possible to analytically solve the inverse problem in some cases, others must be tackled numerically. We present new exact Vlasov-Maxwell equilibria for asymmetric current sheets, which can be written as a sum of shifted Maxwellian distributions, ideal for implementations in particle-in-cell simulations.
  We study the correspondence between the microscopic and macroscopic descriptions of equilibrium in cylindrical geometry, and then attempt to find Vlasov-Maxwell equilibria for the nonlinear force-free 'Gold-Hoyle' model. However, it is necessary to include a background field, which can be arbitrarily weak if desired. The equilibrium can be electrically non-neutral, depending on the bulk flows.△ Less"
Categorical Quantum Dynamics,Authors:Stefano Gogioso,"Abstract:We use strong complementarity to introduce dynamics and symmetries within the framework of CQM, which we also extend to infinite-dimensional separable Hilbert spaces: these were long-missing features, which open the way to a wealth of new applications. The coherent treatment presented in this work also provides a variety of novel insights into the dynamics and symmetries of quantum systems: exampl…▽ MoreWe use strong complementarity to introduce dynamics and symmetries within the framework of CQM, which we also extend to infinite-dimensional separable Hilbert spaces: these were long-missing features, which open the way to a wealth of new applications. The coherent treatment presented in this work also provides a variety of novel insights into the dynamics and symmetries of quantum systems: examples include the extremely simple characterisation of symmetry-observable duality, the connection of strong complementarity with the Weyl Canonical Commutation Relations, the generalisations of Feynman's clock construction, the existence of time observables and the emergence of quantum clocks.
  Furthermore, we show that strong complementarity is a key resource for quantum algorithms and protocols. We provide the first fully diagrammatic, theory-independent proof of correctness for the quantum algorithm solving the Hidden Subgroup Problem, and show that strong complementarity is the feature providing the quantum advantage. In quantum foundations, we use strong complementarity to derive the exact conditions relating non-locality to the structure of phase groups, within the context of Mermin-type non-locality arguments. Our non-locality results find further application to quantum cryptography, where we use them to define a quantum-classical secret sharing scheme with provable device-independent security guarantees.
  All in all, we argue that strong complementarity is a truly powerful and versatile building block for quantum theory and its applications, and one that should draw a lot more attention in the future.△ Less"
The algebra of entanglement and the geometry of composition,Authors:Amar Hadzihasanovic,"Abstract:String diagrams turn algebraic equations into topological moves that have recurring shapes, involving the sliding of one diagram past another. We individuate, at the root of this fact, the dual nature of polygraphs as presentations of higher algebraic theories, and as combinatorial descriptions of ""directed spaces"". Operations of polygraphs modelled on operations of topological spaces are used as…▽ MoreString diagrams turn algebraic equations into topological moves that have recurring shapes, involving the sliding of one diagram past another. We individuate, at the root of this fact, the dual nature of polygraphs as presentations of higher algebraic theories, and as combinatorial descriptions of ""directed spaces"". Operations of polygraphs modelled on operations of topological spaces are used as the foundation of a compositional universal algebra, where sliding moves arise from tensor products of polygraphs. We reconstruct several higher algebraic theories in this framework.
  In this regard, the standard formalism of polygraphs has some technical problems. We propose a notion of regular polygraph, barring cell boundaries that are not homeomorphic to a disk of the appropriate dimension. We define a category of non-degenerate shapes, and show how to calculate their tensor products. Then, we introduce a notion of weak unit to recover weakly degenerate boundaries in low dimensions, and prove that the existence of weak units is equivalent to a representability property.
  We then turn to applications of diagrammatic algebra to quantum theory. We re-evaluate the category of Hilbert spaces from the perspective of categorical universal algebra, which leads to a bicategorical refinement. Then, we focus on the axiomatics of fragments of quantum theory, and present the ZW calculus, the first complete diagrammatic axiomatisation of the theory of qubits.
  The ZW calculus has several advantages over ZX calculi, including a computationally meaningful normal form, and a fragment whose diagrams can be read as setups of fermionic oscillators. Moreover, its generators reflect an operational classification of entangled states of 3 qubits. We conclude with generalisations of the ZW calculus to higher-dimensional systems, including the definition of a universal set of generators in each dimension.△ Less"
The PDFLattice2017 workshop: a summary report,"Authors:Emanuele R. Nocera,Huey-Wen Lin,Fred Olness,Kostas Orginos,Juan Rojo","Abstract:The workshop on Parton Distributions and Lattice Calculations in the LHC era (PDFLattice2017) was hosted at Balliol College,Oxford(UK), from 22$^{\rm nd}$ to 24$^{\rm th}$ March 2017. The workshop brought together the lattice-QCD and the global-fit physicists who devote their efforts to determine the parton distribution functions (PDFs) of the proton. The…▽ MoreThe workshop on Parton Distributions and Lattice Calculations in the LHC era (PDFLattice2017) was hosted at Balliol College,Oxford(UK), from 22$^{\rm nd}$ to 24$^{\rm th}$ March 2017. The workshop brought together the lattice-QCD and the global-fit physicists who devote their efforts to determine the parton distribution functions (PDFs) of the proton. The goals were to make the two communities more familiar between each other, review developments from both sides, and set precision targets for lattice calculations so that they can contribute, together with the forthcoming experimental input, to the next generation of PDF determinations. This contribution summarises the relevant outcome of the workshop, in anticipation of a thorough white paper.△ Less"
Logical paradoxes in quantum computation,Authors:Nadish de Silva,"Abstract:While quantum computers are expected to yield considerable advantages over classical devices, the precise features of quantum theory enabling these advantages remain unclear. Contextuality--the denial of a notion of classical physical reality--has emerged as a promising hypothesis.
  Magic states are quantum resources critical for practically achieving universal quantum computation. They exhibit t…▽ MoreWhile quantum computers are expected to yield considerable advantages over classical devices, the precise features of quantum theory enabling these advantages remain unclear. Contextuality--the denial of a notion of classical physical reality--has emerged as a promising hypothesis.
  Magic states are quantum resources critical for practically achieving universal quantum computation. They exhibit the standard form of contextuality that is known to enable probabilistic advantages in a variety of computational and communicational tasks. Strong contextuality is an extremal form of contextuality describing systems that exhibit logically paradoxical behaviour.
  Here, we consider special magic states that deterministically enable quantum computation. After introducing number-theoretic techniques for constructing exotic quantum paradoxes, we present large classes of strongly contextual magic states that enable deterministic implementation of gates from the Clifford hierarchy. These surprising discoveries bolster a refinement of the resource theory of contextuality that emphasises the computational power of logical paradoxes.△ Less"
Improved ArtGAN for Conditional Synthesis of Natural Image and Artwork,"Authors:Wei Ren Tan,Chee Seng Chan,Hernan Aguirre,Kiyoshi Tanaka","Abstract:…outperforms the state-of-the-art results on CIFAR-10 in terms of Inception score. Qualitatively, we demonstrate that ArtGAN is able to generate plausible-looking images onOxford-102 and CUB-200, as well as able to draw realistic artworks based on style, artist, and genre. The source code and models are available at: https://github.com/cs-chan/ArtGAN▽ MoreThis paper proposes a series of new approaches to improve Generative Adversarial Network (GAN) for conditional image synthesis and we name the proposed model as ArtGAN. One of the key innovation of ArtGAN is that, the gradient of the loss function w.r.t. the label (randomly assigned to each generated image) is back-propagated from the categorical discriminator to the generator. With the feedback from the label information, the generator is able to learn more efficiently and generate image with better quality. Inspired by recent works, an autoencoder is incorporated into the categorical discriminator for additional complementary information. Last but not least, we introduce a novel strategy to improve the image quality. In the experiments, we evaluate ArtGAN on CIFAR-10 and STL-10 via ablation studies. The empirical results showed that our proposed model outperforms the state-of-the-art results on CIFAR-10 in terms of Inception score. Qualitatively, we demonstrate that ArtGAN is able to generate plausible-looking images onOxford-102 and CUB-200, as well as able to draw realistic artworks based on style, artist, and genre. The source code and models are available at: https://github.com/cs-chan/ArtGAN△ Less"
Type Safe Redis Queries: A Case Study of Type-Level Programming in Haskell,"Authors:Ting-Yan Lai,Tyng-Ruey Chuang,Shin-Cheng Mu","Abstract:Redis is an in-memory data structure store, often used as a database, with a Haskell interface Hedis. Redis is dynamically typed --- a key can be discarded and re-associated to a value of a different type, and a command, when fetching a value of a type it does not expect, signals a runtime error. We develop a domain-specific language that, by exploiting Haskell type-level programming techniques in…▽ MoreRedis is an in-memory data structure store, often used as a database, with a Haskell interface Hedis. Redis is dynamically typed --- a key can be discarded and re-associated to a value of a different type, and a command, when fetching a value of a type it does not expect, signals a runtime error. We develop a domain-specific language that, by exploiting Haskell type-level programming techniques including indexed monad, type-level literals and closed type families, keeps track of types of values in the database and statically guarantees that type errors cannot happen for a class of Redis programs.△ Less"
proFIA: A data preprocessing workflow for Flow Injection Analysis coupled to High-Resolution Mass Spectrometry,"Authors:Alexis Delabrière,Ulli Hohenester,Benoit Colsch,Christophe Junot,François Fenaille,Etienne Thévenot","Abstract:Motivation: Flow Injection Analysis coupled to High-Resolution Mass Spectrometry (FIA-HRMS) is a promising approach for high-throughput metabolomics. FIA-HRMS data, however, cannot be preprocessed with current software tools which rely on liquid chromatography separation, or handle low resolution data only. Results: We thus developed the proFIA package, which implements a suite of innovative algor…▽ MoreMotivation: Flow Injection Analysis coupled to High-Resolution Mass Spectrometry (FIA-HRMS) is a promising approach for high-throughput metabolomics. FIA-HRMS data, however, cannot be preprocessed with current software tools which rely on liquid chromatography separation, or handle low resolution data only. Results: We thus developed the proFIA package, which implements a suite of innovative algorithms to preprocess FIA-HRMS raw files, and generates the table of peak intensities. The workflow consists of 3 steps: i) noise estimation, peak detection and quantification, ii) peak grouping across samples, and iii) missing value imputation. In addition, we have implemented a new indicator to quantify the potential alteration of the feature peak shape due to matrix effect. The preprocessing is fast (less than 15 s per file), and the value of the main parameters (ppm and dmz) can be easily inferred from the mass resolution of the instrument. Application to two metabolomics datasets (including spiked serum samples) showed high precision (96%) and recall (98%) compared with manual integration. These results demonstrate that proFIA achieves very efficient and robust detection and quantification of FIA-HRMS data, and opens new opportunities for high-throughput phenotyping. Availability: The proFIA software (as well as the plasFIA data set) is available as an R package on the Bioconductor repository (http://bioconductor.org/packages/proFIA), and as a Galaxy module on the Main Toolshed (https://toolshed.g2.bx.psu.edu/) and on the Workflow4Metabolomics online infrastructure (http://workflow4metabolomics.org). Contacts: alexis.delabriere@cea.fr and etienne.thevenot@cea.fr.△ Less"
Flower Categorization using Deep Convolutional Neural Networks,"Authors:Ayesha Gurnani,Viraj Mavani,Vandit Gajjar,Yash Khandhediya","Abstract:…of different flowers. For this, we have used Visual Geometry Group's 102 category flower dataset having 8189 images of 102 different flowers from University ofOxford. The method is basically divided into two parts; Image segmentation and classification. We have compared the performance of two different Convolutional Neural Network architectures GoogLeNe…▽ MoreWe have developed a deep learning network for classification of different flowers. For this, we have used Visual Geometry Group's 102 category flower dataset having 8189 images of 102 different flowers from University ofOxford. The method is basically divided into two parts; Image segmentation and classification. We have compared the performance of two different Convolutional Neural Network architectures GoogLeNet and AlexNet for classification purpose. By keeping the hyper parameters same for both architectures, we have found that the top 1 and top 5 accuracies of GoogLeNet are 47.15% and 69.17% respectively whereas the top 1 and top 5 accuracies of AlexNet are 43.39% and 68.68% respectively. These results are extremely good when compared to random classification accuracy of 0.98%. This method for classification of flowers can be implemented in real time applications and can be used to help botanists for their research as well as camping enthusiasts.△ Less"
The Message or the Messenger? Inferring Virality and Diffusion Structure from Online Petition Signature Data,"Authors:Chi Ling Chan,Justin Lai,Bryan Hooi,Todd Davies","Abstract:Goel et al. (2016) examined diffusion data from Twitter to conclude that online petitions are shared more virally than other types of content. Their definition of structural virality, which measures the extent to which diffusion follows a broadcast model or is spread person to person (virally), depends on knowing the topology of the diffusion cascade. But often the diffusion structure cannot be ob…▽ MoreGoel et al. (2016) examined diffusion data from Twitter to conclude that online petitions are shared more virally than other types of content. Their definition of structural virality, which measures the extent to which diffusion follows a broadcast model or is spread person to person (virally), depends on knowing the topology of the diffusion cascade. But often the diffusion structure cannot be observed directly. We examined time-stamped signature data from the Obama White House's We the People petition platform. We developed measures based on temporal dynamics that, we argue, can be used to infer diffusion structure as well as the more intrinsic notion of virality sometimes known as infectiousness. These measures indicate that successful petitions are likely to be higher in both intrinsic and structural virality than unsuccessful petitions are. We also investigate threshold effects on petition signing that challenge simple contagion models, and report simulations for a theoretical model that are consistent with our data.△ Less"
Equality of the Jellium and Uniform Electron Gas next-order asymptotic terms for Coulomb and Riesz potentials,"Authors:Codina Cotar,Mircea Petrache","Abstract:…lower bound on the famous Lieb-Oxfordconstant. Our work also rigorously confirms some of the predictions formulated by physicists, regarding the optimal value of the Uniform Electron Gas second-order asymptotic term.
  We also show that on the whole range $s\in(0,d)$, the Uniform Electron Gas second-order constant is continuous in $s$.▽ MoreWe consider two sharp next-order asymptotics problems, namely the asymptotics for the minimum energy for optimal point configurations and the asymptotics for the many-marginals Optimal Transport, in both cases with Riesz costs with inverse power-law long range interactions. The first problem describes the ground state of a Coulomb or Riesz gas, while the second appears as a semiclassical limit of the Density Functional Theory energy modelling a quantum version of the same system. Recently the second-order term in these expansions was precisely described, and corresponds respectively to a Jellium and to a Uniform Electron Gas model.
  The present work shows that for inverse-power-law interactions with power $s\in [d-2,d)$ in $d$ dimensions, the two problems have the same minimum value asymptotically. For the Coulomb case in $d=3$, our result verifies the physicists' long-standing conjecture regarding the equality of the second-order terms for these two problems. Furthermore, our work implies that, whereas minimum values are equal, the minimizers may be different. Moreover, provided that the crystallization hypothesis in $d=3$ holds, which is analogous to Abrikosov's conjecture in $d=2$, then our result verifies the physicists' conjectured $\approx 1.4442$ lower bound on the famous Lieb-Oxfordconstant. Our work also rigorously confirms some of the predictions formulated by physicists, regarding the optimal value of the Uniform Electron Gas second-order asymptotic term.
  We also show that on the whole range $s\in(0,d)$, the Uniform Electron Gas second-order constant is continuous in $s$.△ Less"
Semantic Image Synthesis via Adversarial Learning,"Authors:Hao Dong,Simiao Yu,Chao Wu,Yike Guo","Abstract:…loss functions, which are optimized to fulfill the aforementioned two requirements. We have evaluated our model by conducting experiments on Caltech-200 bird dataset andOxford-102 flower dataset, and have demonstrated that our model is capable of synthesizing realistic images that match the given descriptions, while still maintain other features of original…▽ MoreIn this paper, we propose a way of synthesizing realistic images directly with natural language description, which has many useful applications, e.g. intelligent image manipulation. We attempt to accomplish such synthesis: given a source image and a target text description, our model synthesizes images to meet two requirements: 1) being realistic while matching the target text description; 2) maintaining other image features that are irrelevant to the text description. The model should be able to disentangle the semantic information from the two modalities (image and text), and generate new images from the combined semantics. To achieve this, we proposed an end-to-end neural architecture that leverages adversarial learning to automatically learn implicit loss functions, which are optimized to fulfill the aforementioned two requirements. We have evaluated our model by conducting experiments on Caltech-200 bird dataset andOxford-102 flower dataset, and have demonstrated that our model is capable of synthesizing realistic images that match the given descriptions, while still maintain other features of original images.△ Less"
Geospatial Semantics,Authors:Yingjie Hu,"Abstract:Geospatial semantics is a broad field that involves a variety of research areas. The term semantics refers to the meaning of things, and is in contrast with the term syntactics. Accordingly, studies on geospatial semantics usually focus on understanding the meaning of geographic entities as well as their counterparts in the cognitive and digital world, such as cognitive geographic concepts and dig…▽ MoreGeospatial semantics is a broad field that involves a variety of research areas. The term semantics refers to the meaning of things, and is in contrast with the term syntactics. Accordingly, studies on geospatial semantics usually focus on understanding the meaning of geographic entities as well as their counterparts in the cognitive and digital world, such as cognitive geographic concepts and digital gazetteers. Geospatial semantics can also facilitate the design of geographic information systems (GIS) by enhancing the interoperability of distributed systems and developing more intelligent interfaces for user interactions. During the past years, a lot of research has been conducted, approaching geospatial semantics from different perspectives, using a variety of methods, and targeting different problems. Meanwhile, the arrival of big geo data, especially the large amount of unstructured text data on the Web, and the fast development of natural language processing methods enable new research directions in geospatial semantics. This chapter, therefore, provides a systematic review on the existing geospatial semantic research. Six major research areas are identified and discussed, including semantic interoperability, digital gazetteers, geographic information retrieval, geospatial Semantic Web, place semantics, and cognitive geographic concepts.△ Less"
Mixing Flows on Moduli Spaces of Flat Bundles over Surfaces,"Authors:Giovanni Forni,William M. Goldman","Abstract:We extend Teichmueller dynamics to a flow on the total space of a flat bundle of deformation spaces of representations of the fundamental group of a fixed surface S in a Lie group G. The resulting dynamical system is a continuous version of the action of the mapping class group of S on the deformation space. We observe how ergodic properties of this action relate to this flow. When G is compact, t…▽ MoreWe extend Teichmueller dynamics to a flow on the total space of a flat bundle of deformation spaces of representations of the fundamental group of a fixed surface S in a Lie group G. The resulting dynamical system is a continuous version of the action of the mapping class group of S on the deformation space. We observe how ergodic properties of this action relate to this flow. When G is compact, this flow is strongly mixing over each component of the derormation space and of each stratum of the Teichmueller unit sphere bundle over the Riemann moduli space. We prove ergodicity for the analogous lift of the Weil-Petersson geodesic local. flow.△ Less"
The life cycles of Be viscous decretion discs: time-dependent modelling of infrared continuum observations,"Authors:Rodrigo G. Vieira,Alex C. Carciofi,Jon E. Bjorkman,Thomas Rivinius,Dietrich Baade,Leandro R. Rímulo","Abstract:We apply the viscous decretion disc (VDD) model to interpret the infrared disc continuum emission of 80 Be stars observed in different epochs. In this way, we determined 169 specific disc structures, namely their density scale, $ρ_0$, and exponent, $n$. We found that the $n$ values range mainly between $1.5$ and $3.5$, and $ρ_0$ varies between $10^{-12}$ and $10^{-10}\,\mathrm{g\,cm^{-3}}$, with a…▽ MoreWe apply the viscous decretion disc (VDD) model to interpret the infrared disc continuum emission of 80 Be stars observed in different epochs. In this way, we determined 169 specific disc structures, namely their density scale, $ρ_0$, and exponent, $n$. We found that the $n$ values range mainly between $1.5$ and $3.5$, and $ρ_0$ varies between $10^{-12}$ and $10^{-10}\,\mathrm{g\,cm^{-3}}$, with a peak close to the lower value. Our large sample also allowed us to firmly establish that the discs around early-type stars are denser than in late-type stars. Additionally, we estimated the disc mass decretion rates and found that they range between $10^{-12}$ and $10^{-9}\,\mathrm{M_{\odot}\,yr^{-1}}$. These values are compatible with recent stellar evolution models of fast-rotating stars. One of the main findings of this work is a correlation between the $ρ_0$ and $n$ values. In order to find out whether these relations can be traced back to the evolution of discs or have some other origin, we used the VDD model to calculate temporal sequences under different assumptions for the time profile of the disc mass injection. The results support the hypothesis that the observed distribution of disc properties is due to a common evolutionary path. In particular, our results suggest that the timescale for disc growth, during which the disc is being actively fed by mass injection episodes, is shorter than the timescale for disc dissipation, when the disc is no longer fed by the star and dissipates as a result of the viscous diffusion of the disc material.△ Less"
Deep Discrete Hashing with Self-supervised Pairwise Labels,"Authors:Jingkuan Song,Tao He,Hangbo Fan,Lianli Gao","Abstract:…function steering the learning process, which is based on the neighborhood structure in the original space. Experimental results on standard datasets (CIFAR-10, NUS-WIDE, andOxford-17) demonstrate that our DDH significantly outperforms existing hashing methods by large margin in terms of~mAP for image retrieval and object recognition. Code is available at \…▽ MoreHashing methods have been widely used for applications of large-scale image retrieval and classification. Non-deep hashing methods using handcrafted features have been significantly outperformed by deep hashing methods due to their better feature representation and end-to-end learning framework. However, the most striking successes in deep hashing have mostly involved discriminative models, which require labels. In this paper, we propose a novel unsupervised deep hashing method, named Deep Discrete Hashing (DDH), for large-scale image retrieval and classification. In the proposed framework, we address two main problems: 1) how to directly learn discrete binary codes? 2) how to equip the binary representation with the ability of accurate image retrieval and classification in an unsupervised way? We resolve these problems by introducing an intermediate variable and a loss function steering the learning process, which is based on the neighborhood structure in the original space. Experimental results on standard datasets (CIFAR-10, NUS-WIDE, andOxford-17) demonstrate that our DDH significantly outperforms existing hashing methods by large margin in terms of~mAP for image retrieval and object recognition. Code is available at \url{https://github.com/htconquer/ddh}.△ Less"
"Like trainer, like bot? Inheritance of bias in algorithmic content moderation","Authors:Reuben Binns,Michael Veale,Max Van Kleek,Nigel Shadbolt","Abstract:The internet has become a central medium through which `networked publics' express their opinions and engage in debate. Offensive comments and personal attacks can inhibit participation in these spaces. Automated content moderation aims to overcome this problem using machine learning classifiers trained on large corpora of texts manually annotated for offence. While such systems could help encoura…▽ MoreThe internet has become a central medium through which `networked publics' express their opinions and engage in debate. Offensive comments and personal attacks can inhibit participation in these spaces. Automated content moderation aims to overcome this problem using machine learning classifiers trained on large corpora of texts manually annotated for offence. While such systems could help encourage more civil debate, they must navigate inherently normatively contestable boundaries, and are subject to the idiosyncratic norms of the human raters who provide the training data. An important objective for platforms implementing such measures might be to ensure that they are not unduly biased towards or against particular norms of offence. This paper provides some exploratory methods by which the normative biases of algorithmic content moderation systems can be measured, by way of a case study using an existing dataset of comments labelled for offence. We train classifiers on comments labelled by different demographic subsets (men and women) to understand how differences in conceptions of offence between these groups might affect the performance of the resulting models on various test sets. We conclude by discussing some of the ethical choices facing the implementers of algorithmic moderation systems, given various desired levels of diversity of viewpoints amongst discussion participants.△ Less"
Classification of boundary Lefschetz fibrations over the disc,"Authors:Stefan Behrens,Gil R. Cavalcanti,Ralph L. Klaasse","Abstract:We show that a four-manifold admits a boundary Lefschetz fibration over the disc if and only if it is diffeomorphic to $S^1 \times S^3\# n \overline{\mathbb{C} P^2}$, $\# m\mathbb{C} P^2 \#n\overline{\mathbb{C} P^2}$ or $\# m (S^2 \times S^2)$. Given the relation between boundary Lefschetz fibrations and stable generalized complex structures, we conclude that the manifolds…▽ MoreWe show that a four-manifold admits a boundary Lefschetz fibration over the disc if and only if it is diffeomorphic to $S^1 \times S^3\# n \overline{\mathbb{C} P^2}$, $\# m\mathbb{C} P^2 \#n\overline{\mathbb{C} P^2}$ or $\# m (S^2 \times S^2)$. Given the relation between boundary Lefschetz fibrations and stable generalized complex structures, we conclude that the manifolds $S^1 \times S^3\# n \overline{\mathbb{C} P^2}$, $\#(2m+1)\mathbb{C} P^2 \#n\overline{\mathbb{C} P^2}$ and $\# (2m+1) S^2 \times S^2$ admit stable structures whose type change locus has a single component and are the only four-manifolds whose stable structure arise from boundary Lefschetz fibrations over the disc.△ Less"
"Mediated behavioural change in human-machine networks: exploring network characteristics, trust and motivation","Authors:Paul Walland,J. Brian Pickering","Abstract:Human-machine networks pervade much of contemporary life. Network change is the product of structural modifications along with differences in participant be-havior. If we assume that behavioural change in a human-machine network is the result of changing the attitudes of participants in the network, then the question arises whether network structure can affect participant attitude. Taking citizen…▽ MoreHuman-machine networks pervade much of contemporary life. Network change is the product of structural modifications along with differences in participant be-havior. If we assume that behavioural change in a human-machine network is the result of changing the attitudes of participants in the network, then the question arises whether network structure can affect participant attitude. Taking citizen par-ticipation as an example, engagement with relevant stakeholders reveals trust and motivation to be the major objectives for the network. Using a typology to de-scribe network state based on multiple characteristic or dimensions, we can pre-dict possible behavioural outcomes in the network. However, this has to be medi-ated via attitude change. Motivation for the citizen participation network can only increase in line with enhanced trust. The focus for changing network dynamics, therefore, shifts to the dimensional changes needed to encourage increased trust. It turns out that the coordinated manipulation of multiple dimensions is needed to bring about the desired shift in attitude.△ Less"
"Hyperfunctions, the Duistermaat-Heckman theorem, and Loop Groups","Authors:James A. Mracek,Lisa C. Jeffrey","Abstract:In this article we investigate the Duistermaat-Heckman theorem using the theory of hyperfunctions. In applications involving Hamiltonian torus actions on infinite dimensional manifolds, this more general theory seems to be necessary in order to accomodate the existence of the infinite order differential operators which arise from the isotropy representations on the tangent spaces to fixed points.…▽ MoreIn this article we investigate the Duistermaat-Heckman theorem using the theory of hyperfunctions. In applications involving Hamiltonian torus actions on infinite dimensional manifolds, this more general theory seems to be necessary in order to accomodate the existence of the infinite order differential operators which arise from the isotropy representations on the tangent spaces to fixed points. We will quickly review of the theory of hyperfunctions and their Fourier transforms. We will then apply this theory to construct a hyperfunction analogue of the Duistermaat-Heckman distribution. Our main goal will be to study the Duistermaat-Heckman hyperfunction of $ΩSU(2)$, but in getting to this goal we will also characterize the singular locus of the moment map for the Hamiltonian action of $T\times S^1$ on $ΩG$. The main goal of this paper is to present a Duistermaat-Heckman hyperfunction arising from a Hamiltonian action on an infinite dimensional manifold.△ Less"
Measurement of human activity using velocity GPS data obtained from mobile phones,"Authors:Yasuko Kawahata,Takayuki Mizuno,Akira Ishii","Abstract:Human movement is used as an indicator of human activity in modern society. The velocity of moving humans is calculated based on position information obtained from mobile phones. The level of human activity, as recorded by velocity, varies throughout the day. Therefore, velocity can be used to identify the intervals of highest and lowest activity. More specifically, we obtained mobile-phone GPS da…▽ MoreHuman movement is used as an indicator of human activity in modern society. The velocity of moving humans is calculated based on position information obtained from mobile phones. The level of human activity, as recorded by velocity, varies throughout the day. Therefore, velocity can be used to identify the intervals of highest and lowest activity. More specifically, we obtained mobile-phone GPS data from the people around Shibuya station in Tokyo, which has the highest population density in Japan. From these data, we observe that velocity tends to consistently increase with the changes in social activities. For example, during the earthquake in Kumamoto Prefecture in April 2016, the activity on that day was much lower than usual. In this research, we focus on natural disasters such as earthquakes owing to their significant effects on human activities in developed countries like Japan. In the event of a natural disaster in another developed country, considering the change in human behavior at the time of the disaster (e.g., the 2016 Kumamoto Great Earthquake) from the viewpoint of velocity allows us to improve our planning for mitigation measures. Thus, we analyze the changes in human activity through velocity calculations in Shibuya, Tokyo, and compare times of disasters with normal times.△ Less"
HiDi: An efficient reverse engineering schema for large scale dynamic regulatory network reconstruction using adaptive differentiation,"Authors:Yue Deng,Hector Zenil,Jesper Tégner,Narsis A. Kiani","Abstract:The use of differential equations (ODE) is one of the most promising approaches to network inference. The success of ODE-based approaches has, however, been limited, due to the difficulty in estimating parameters and by their lack of scalability. Here we introduce a novel method and pipeline to reverse engineer gene regulatory networks from gene expression of time series and perturbation data base…▽ MoreThe use of differential equations (ODE) is one of the most promising approaches to network inference. The success of ODE-based approaches has, however, been limited, due to the difficulty in estimating parameters and by their lack of scalability. Here we introduce a novel method and pipeline to reverse engineer gene regulatory networks from gene expression of time series and perturbation data based upon an improvement on the calculation scheme of the derivatives and a pre-filtration step to reduce the number of possible links. The method introduces a linear differential equation model with adaptive numerical differentiation that is scalable to extremely large regulatory networks. We demonstrate the ability of this method to outperform current state-of-the-art methods applied to experimental and synthetic data using test data from the DREAM4 and DREAM5 challenges. Our method displays greater accuracy and scalability. We benchmark the performance of the pipeline with respect to data set size and levels of noise. We show that the computation time is linear over various network sizes.△ Less"
Position-sensitive propagation of information on social media using social physics approach,"Authors:Akira Ishii,Takayuki Mizuno,Yasuko Kawahata","Abstract:The excitement and convergence of tweets on specific topics are well studied. However, by utilizing the position information of Tweet, it is also possible to analyze the position-sensitive tweet. In this research, we focus on bomb terrorist attacks and propose a method for separately analyzing the number of tweets at the place where the incident occurred, nearby, and far. We made measurements of p…▽ MoreThe excitement and convergence of tweets on specific topics are well studied. However, by utilizing the position information of Tweet, it is also possible to analyze the position-sensitive tweet. In this research, we focus on bomb terrorist attacks and propose a method for separately analyzing the number of tweets at the place where the incident occurred, nearby, and far. We made measurements of position-sensitive tweets and suggested a theory to explain it. This theory is an extension of the mathematical model of the hit phenomenon.△ Less"
Undergraduate algebra in nineteenth-centuryOxford,Authors:Joseph Gage,"Abstract:The nineteenth century was an important period for bothOxfordmathematics and algebra in general. While there is extensive documentation of mathematical research in…▽ MoreThe nineteenth century was an important period for bothOxfordmathematics and algebra in general. While there is extensive documentation of mathematical research inOxfordat this time, the same cannot be said of the teaching. The content of the course presents a different picture: it shows what those who set it felt was most valuable for a young mathematician to learn, perhaps indicating what direction they expected mathematics to take in the future. To find out what undergraduates were taught, I have looked through examination papers of the years between 1828 and 1912 with a focus on algebra, as well as supporting material. In this paper I will present my findings. I will give a picture of what anOxfordundergraduate's course in algebra looked like by the end of the nineteenth century and discuss my own conclusions as to why it took such a form.△ Less"
Episodic mass ejections from common-envelope objects,"Authors:Matthew Clayton,Philipp Podsiadlowski,Natasha Ivanova,Stephen Justham","Abstract:After the initial fast spiral-in phase experienced by a common-envelope binary, the system may enter a slow, self-regulated phase, possibly lasting 100s of years, in which all the energy released by orbital decay can be efficiently transported to the surface, where it is radiated away. If the remaining envelope is to be removed during this phase, this removal must occur through some as-yet-undeter…▽ MoreAfter the initial fast spiral-in phase experienced by a common-envelope binary, the system may enter a slow, self-regulated phase, possibly lasting 100s of years, in which all the energy released by orbital decay can be efficiently transported to the surface, where it is radiated away. If the remaining envelope is to be removed during this phase, this removal must occur through some as-yet-undetermined mechanism. We carried out 1-d hydrodynamic simulations of a low-mass red giant undergoing a synthetic common-envelope event in such a slow spiral-in phase, using the stellar evolutionary code MESA. We simulated the heating of the envelope due to frictional dissipation from a binary companion's orbit in multiple configurations and investigated the response of the giant's envelope. We find that our model envelopes become dynamically unstable and develop large-amplitude pulsations, with periods in the range 3-20 years and very short growth time-scales of similar order. The shocks and associated rebounds that emerge as these pulsations grow are in some cases strong enough to dynamically eject shells of matter of up to 0.1 $\mathrm{M}_{\odot}$, $\sim 10$ % of the mass of the envelope, from the stellar surface at above escape velocity. These ejections are seen to repeat within a few decades, leading to a time-averaged mass-loss rate of order $10^{-3}$ $\mathrm{M}_{\odot} \: \mathrm{yr}^{-1}$ which is sufficiently high to represent a candidate mechanism for removing the entire envelope over the duration of the slow spiral-in phase.△ Less"
Lecture Notes on the Statistical Mechanics of Disordered Systems,Authors:Patrick Charbonneau,"Abstract:This material complements David Chandler's Introduction to Modern Statistical Mechanics (OxfordUniversity Press, 1987) in a graduate-level, one-semester course I teach in the Department of Chemistry at Duke University. Students enter this course with some knowledge of statistical thermodynamics and quantum mechanics, usually acquired from undergraduate…▽ MoreThis material complements David Chandler's Introduction to Modern Statistical Mechanics (OxfordUniversity Press, 1987) in a graduate-level, one-semester course I teach in the Department of Chemistry at Duke University. Students enter this course with some knowledge of statistical thermodynamics and quantum mechanics, usually acquired from undergraduate physical chemistry at the level of D. A. McQuarrie & J. D. Simon's Physical Chemistry: A Molecular Approach (University Science Books, 1997). These notes, which introduce students to a modern treatment of glassiness and to the replica method, build on the material and problems contained in the eight chapters of Chandler's textbook.△ Less"
Beyond similarity assessment: Selecting the optimal model for sequence alignment via the Factorized Asymptotic Bayesian algorithm,"Authors:Taikai Takeda,Michiaki Hamada","Abstract:Pair Hidden Markov Models (PHMMs) are probabilistic models used for pairwise sequence alignment, a quintessential problem in bioinformatics. PHMMs include three types of hidden states: match, insertion and deletion. Most previous studies have used one or two hidden states for each PHMM state type. However, few studies have examined the number of states suitable for representing sequence data or im…▽ MorePair Hidden Markov Models (PHMMs) are probabilistic models used for pairwise sequence alignment, a quintessential problem in bioinformatics. PHMMs include three types of hidden states: match, insertion and deletion. Most previous studies have used one or two hidden states for each PHMM state type. However, few studies have examined the number of states suitable for representing sequence data or improving alignment accuracy.We developed a novel method to select superior models (including the number of hidden states) for PHMM. Our method selects models with the highest posterior probability using Factorized Information Criteria (FIC), which is widely utilised in model selection for probabilistic models with hidden variables. Our simulations indicated this method has excellent model selection capabilities with slightly improved alignment accuracy. We applied our method to DNA datasets from 5 and 28 species, ultimately selecting more complex models than those used in previous studies.△ Less"
Winning on the Merits: The Joint Effects of Content and Style on Debate Outcomes,"Authors:Lu Wang,Nick Beauchamp,Sarah Shugars,Kechen Qin","Abstract:…that estimates the effects of linguistic features and the latent persuasive strengths of different topics, as well as the interactions between the two. Using a dataset of 118Oxford-style debates, our model's combination of content (as latent topics) and style (as linguistic features) allows us to predict audience-adjudicated winners with 74% accuracy, s…▽ MoreDebate and deliberation play essential roles in politics and government, but most models presume that debates are won mainly via superior style or agenda control. Ideally, however, debates would be won on the merits, as a function of which side has the stronger arguments. We propose a predictive model of debate that estimates the effects of linguistic features and the latent persuasive strengths of different topics, as well as the interactions between the two. Using a dataset of 118Oxford-style debates, our model's combination of content (as latent topics) and style (as linguistic features) allows us to predict audience-adjudicated winners with 74% accuracy, significantly outperforming linguistic features alone (66%). Our model finds that winning sides employ stronger arguments, and allows us to identify the linguistic features associated with strong or weak arguments.△ Less"
"Show, Adapt and Tell: Adversarial Training of Cross-domain Image Captioner","Authors:Tseng-Hung Chen,Yuan-Hong Liao,Ching-Yao Chuang,Wan-Ting Hsu,Jianlong Fu,Min Sun","Abstract:…method to select high-quality sentences without additional supervision (e.g., tags). To evaluate, we use MSCOCO as the source domain and four other datasets (CUB-200-2011,Oxford-102, TGIF, and Flickr30k) as the target domains. Our method consistently performs well on all datasets. In particular, on CUB-200-2011, we achieve 21.8% CIDEr-D improvement after ad…▽ MoreImpressive image captioning results are achieved in domains with plenty of training image and sentence pairs (e.g., MSCOCO). However, transferring to a target domain with significant domain shifts but no paired training data (referred to as cross-domain image captioning) remains largely unexplored. We propose a novel adversarial training procedure to leverage unpaired data in the target domain. Two critic networks are introduced to guide the captioner, namely domain critic and multi-modal critic. The domain critic assesses whether the generated sentences are indistinguishable from sentences in the target domain. The multi-modal critic assesses whether an image and its generated sentence are a valid pair. During training, the critics and captioner act as adversaries -- captioner aims to generate indistinguishable sentences, whereas critics aim at distinguishing them. The assessment improves the captioner through policy gradient updates. During inference, we further propose a novel critic-based planning method to select high-quality sentences without additional supervision (e.g., tags). To evaluate, we use MSCOCO as the source domain and four other datasets (CUB-200-2011,Oxford-102, TGIF, and Flickr30k) as the target domains. Our method consistently performs well on all datasets. In particular, on CUB-200-2011, we achieve 21.8% CIDEr-D improvement after adaptation. Utilizing critics during inference further gives another 4.5% boost.△ Less"
SemEval-2017 Task 8: RumourEval: Determining rumour veracity and support for rumours,"Authors:Leon Derczynski,Kalina Bontcheva,Maria Liakata,Rob Procter,Geraldine Wong Sak Hoi,Arkaitz Zubiaga","Abstract:Media is full of false claims. EvenOxfordDictionaries named ""post-truth"" as the word of 2016. This makes it more important than ever to build systems that can identify the veracity of a story, and the kind of discourse there is around it. RumourEval is a SemEval shared task that aims to identify and handle rumours and reactions to them, in text. We…▽ MoreMedia is full of false claims. EvenOxfordDictionaries named ""post-truth"" as the word of 2016. This makes it more important than ever to build systems that can identify the veracity of a story, and the kind of discourse there is around it. RumourEval is a SemEval shared task that aims to identify and handle rumours and reactions to them, in text. We present an annotation scheme, a large dataset covering multiple topics - each having their own families of claims and replies - and use these to pose two concrete challenges as well as the results achieved by participants on these challenges.△ Less"
The Creative Process of Cultural Evolution,Authors:Liane Gabora,"Abstract:Even this saying itself is a variant of a similar statement attributed to Bernard of Chartres in the 12th Century, and inspired the title for a book by Steven Hawking and an album by Oasis. Creative ideas beget other creative ideas and, as a result, modifications accumulate, and we see an overall increase in the complexity of cultural novelty over time, a phenomenon sometimes referred to as the ra…▽ MoreEven this saying itself is a variant of a similar statement attributed to Bernard of Chartres in the 12th Century, and inspired the title for a book by Steven Hawking and an album by Oasis. Creative ideas beget other creative ideas and, as a result, modifications accumulate, and we see an overall increase in the complexity of cultural novelty over time, a phenomenon sometimes referred to as the ratchet effect (Tomasello, Kruger, & Ratner, 1993). Although we may never meet the people or objects that creatively influence us, by assimilating what we encounter around us and bringing to bear our own insights and perspectives, we all contribute in our own way, however small, to a second evolutionary process -- the evolution of culture. This chapter explores how we can better understand culture by understanding the creative processes that fuel it, and better understand creativity by examining it from its cultural context. First, we look at some theoretical frameworks for how culture evolves and what these frameworks imply for the role of creativity. Then we will see how questions about the relationship between creativity and cultural evolution have been addressed using an agent-based model. We will also discuss studies of how creative outputs are influenced, in perhaps unexpected ways, by other ideas and individuals, and how individual creative styles ""peek through"" cultural outputs in different domains.△ Less"
Characterizing Information Diets of Social Media Users,"Authors:Juhi Kulshrestha,Muhammad Bilal Zafar,Lisette Espin-Noboa,Krishna P. Gummadi,Saptarshi Ghosh","Abstract:With the widespread adoption of social media sites like Twitter and Facebook, there has been a shift in the way information is produced and consumed. Earlier, the only producers of information were traditional news organizations, which broadcast the same carefully-edited information to all consumers over mass media channels. Whereas, now, in online social media, any user can be a producer of infor…▽ MoreWith the widespread adoption of social media sites like Twitter and Facebook, there has been a shift in the way information is produced and consumed. Earlier, the only producers of information were traditional news organizations, which broadcast the same carefully-edited information to all consumers over mass media channels. Whereas, now, in online social media, any user can be a producer of information, and every user selects which other users she connects to, thereby choosing the information she consumes. Moreover, the personalized recommendations that most social media sites provide also contribute towards the information consumed by individual users. In this work, we define a concept of information diet -- which is the topical distribution of a given set of information items (e.g., tweets) -- to characterize the information produced and consumed by various types of users in the popular Twitter social media. At a high level, we find that (i) popular users mostly produce very specialized diets focusing on only a few topics; in fact, news organizations (e.g., NYTimes) produce much more focused diets on social media as compared to their mass media diets, (ii) most users' consumption diets are primarily focused towards one or two topics of their interest, and (iii) the personalized recommendations provided by Twitter help to mitigate some of the topical imbalances in the users' consumption diets, by adding information on diverse topics apart from the users' primary topics of interest.△ Less"
Introduction To The Monogenic Signal,Authors:Christopher P. Bridge,"Abstract:…research. In particular, it has been found to be useful in the analysis of ultrasound imagery in several research scenarios mostly in work done within the BioMedIA lab atOxford. However, the literature on the monogenic signal can be difficult to penetrate due to the lack of a single resource to explain the various principles from basics. The purpose of this…▽ MoreThe monogenic signal is an image analysis methodology that was introduced by Felsberg and Sommer in 2001 and has been employed for a variety of purposes in image processing and computer vision research. In particular, it has been found to be useful in the analysis of ultrasound imagery in several research scenarios mostly in work done within the BioMedIA lab atOxford. However, the literature on the monogenic signal can be difficult to penetrate due to the lack of a single resource to explain the various principles from basics. The purpose of this document is therefore to introduce the principles, purpose, applications, and limitations of the methodology. It assumes some background knowledge from the fields of image and signal processing, in particular a good knowledge of Fourier transforms as applied to signals and images. We will not attempt to provide a thorough math- ematical description or derivation of the monogenic signal, but rather focus on developing an intuition for understanding and using the methodology and refer the reader elsewhere for a more mathematical treatment.△ Less"
Structured Learning of Tree Potentials in CRF for Image Segmentation,"Authors:Fayao Liu,Guosheng Lin,Ruizhi Qiao,Chunhua Shen","Abstract:…optimization can be efficiently solved by combining a modified column generation and cutting-planes techniques. Experimental results on both binary (Graz-02, Weizmann horse,Oxfordflower) and multi-class (MSRC-21, PASCAL VOC 2012) segmentation datasets demonstrate the power of the learned nonlinear nonparametric potentials.▽ MoreWe propose a new approach to image segmentation, which exploits the advantages of both conditional random fields (CRFs) and decision trees. In the literature, the potential functions of CRFs are mostly defined as a linear combination of some pre-defined parametric models, and then methods like structured support vector machines (SSVMs) are applied to learn those linear coefficients. We instead formulate the unary and pairwise potentials as nonparametric forests---ensembles of decision trees, and learn the ensemble parameters and the trees in a unified optimization problem within the large-margin framework. In this fashion, we easily achieve nonlinear learning of potential functions on both unary and pairwise terms in CRFs. Moreover, we learn class-wise decision trees for each object that appears in the image. Due to the rich structure and flexibility of decision trees, our approach is powerful in modelling complex data likelihoods and label relationships. The resulting optimization problem is very challenging because it can have exponentially many variables and constraints. We show that this challenging optimization can be efficiently solved by combining a modified column generation and cutting-planes techniques. Experimental results on both binary (Graz-02, Weizmann horse,Oxfordflower) and multi-class (MSRC-21, PASCAL VOC 2012) segmentation datasets demonstrate the power of the learned nonlinear nonparametric potentials.△ Less"
TAC-GAN - Text Conditioned Auxiliary Classifier Generative Adversarial Network,"Authors:Ayushman Dash,John Cristian Borges Gamboa,Sheraz Ahmed,Marcus Liwicki,Muhammad Zeshan Afzal","Abstract:…of class information, known to diversify the generated samples and improve their structural coherence, has not been explored. We trained the presented TAC-GAN model on theOxford-102 dataset of flowers, and evaluated the discriminability of the generated images with Inception-Score, as well as their diversity using the Multi-Scale Structural Similarity Index…▽ MoreIn this work, we present the Text Conditioned Auxiliary Classifier Generative Adversarial Network, (TAC-GAN) a text to image Generative Adversarial Network (GAN) for synthesizing images from their text descriptions. Former approaches have tried to condition the generative process on the textual data; but allying it to the usage of class information, known to diversify the generated samples and improve their structural coherence, has not been explored. We trained the presented TAC-GAN model on theOxford-102 dataset of flowers, and evaluated the discriminability of the generated images with Inception-Score, as well as their diversity using the Multi-Scale Structural Similarity Index (MS-SSIM). Our approach outperforms the state-of-the-art models, i.e., its inception score is 3.45, corresponding to a relative increase of 7.8% compared to the recently introduced StackGan. A comparison of the mean MS-SSIM scores of the training and generated samples per class shows that our approach is able to generate highly diverse images with an average MS-SSIM of 0.14 over all generated classes.△ Less"
"Irreducibility of moduli of semistable Chains and applications to U(p,q)-Higgs bundles","Authors:Steven Bradlow,Oscar Garcia-Prada,Peter Gothen,Jochen Heinloth","Abstract:We give necessary and sufficient conditions for moduli spaces of semistable chains on a curve to be irreducible and non-empty. This gives information on the irreducible components of the nilpotent cone of GL_n-Higgs bundles and the irreducible components of moduli of systems of Hodge bundles on curves. As we do not impose coprimality restrictions, we can apply this to prove connectedness for modul…▽ MoreWe give necessary and sufficient conditions for moduli spaces of semistable chains on a curve to be irreducible and non-empty. This gives information on the irreducible components of the nilpotent cone of GL_n-Higgs bundles and the irreducible components of moduli of systems of Hodge bundles on curves. As we do not impose coprimality restrictions, we can apply this to prove connectedness for moduli spaces of U(p,q)-Higgs bundles.△ Less"
Subcritical Turbulence in the Mega Ampere Spherical Tokamak,Authors:Ferdinand van Wyk,"Abstract:The transport of heat out of tokamak plasmas by turbulence is the dominant mechanism limiting the performance of fusion reactors. Turbulence can be driven by the ion temperature gradient (ITG) and suppressed by toroidal sheared flows. Numerical simulations attempting to understand turbulence are crucial for guiding the design of future reactors. We investigate ion-scale turbulence via gyrokinetic…▽ MoreThe transport of heat out of tokamak plasmas by turbulence is the dominant mechanism limiting the performance of fusion reactors. Turbulence can be driven by the ion temperature gradient (ITG) and suppressed by toroidal sheared flows. Numerical simulations attempting to understand turbulence are crucial for guiding the design of future reactors. We investigate ion-scale turbulence via gyrokinetic simulations in the outer core of the Mega Ampere Spherical Tokamak (MAST). We perform a parameter scan in the values of the ITG and the flow shear. We show that nonlinear simulations reproduce the experimental ion heat flux and that the experimental values of the ITG and the flow shear lie close to the turbulence threshold. We demonstrate that the system is subcritical in the presence of flow shear, i.e., the system is formally stable, but transitions to a turbulent state given a large enough initial perturbation. We propose a scenario for the transition to turbulence previously unreported in tokamak plasmas: close to the threshold, the plasma is dominated by a low number of coherent long-lived structures; as the system is taken away from the threshold into the more unstable regime, the number of these structures increases until they fill the domain and a more conventional turbulence emerges. We make comparisons of correlation properties between our simulations and experimental measurements of density fluctuations from the MAST BES diagnostic. We apply a synthetic diagnostic to our simulation data and find reasonable agreement of the correlation properties of the simulated and experimental turbulence, most notably of the correlation time. We show that the properties of turbulence are essentially functions of the ion heat flux. We find that turbulence close to the threshold is strongly affected by flow shear, whereas far from threshold, the turbulence resembles a conventional ITG-driven regime.△ Less"
Statistical Inference in Political Networks Research,"Authors:Bruce A. Desmarais,Skyler J. Cranmer","Abstract:Researchers interested in statistically modeling network data have a well-established and quickly growing set of approaches from which to choose. Several of these methods have been regularly applied in research on political networks, while others have yet to permeate the field. Here, we review the most prominent methods of inferential network analysis---both for cross-sectionally and longitudinall…▽ MoreResearchers interested in statistically modeling network data have a well-established and quickly growing set of approaches from which to choose. Several of these methods have been regularly applied in research on political networks, while others have yet to permeate the field. Here, we review the most prominent methods of inferential network analysis---both for cross-sectionally and longitudinally observed networks including (temporal) exponential random graph models, latent space models, the quadratic assignment procedure, and stochastic actor oriented models. For each method, we summarize its analytic form, identify prominent published applications in political science and discuss computational considerations. We conclude with a set of guidelines for selecting a method for a given application.△ Less"
Borrowing Treasures from the Wealthy: Deep Transfer Learning through Selective Joint Fine-tuning,"Authors:Weifeng Ge,Yizhou Yu","Abstract:…achieves state-of-the-art performance on multiple visual classification tasks with insufficient training data for deep learning. Such tasks include Caltech 256, MIT Indoor 67,OxfordFlowers 102 and Stanford Dogs 120. In comparison to fine-tuning without a source domain, the proposed method can improve the classification accuracy by 2% - 10% using a single m…▽ MoreDeep neural networks require a large amount of labeled training data during supervised learning. However, collecting and labeling so much data might be infeasible in many cases. In this paper, we introduce a source-target selective joint fine-tuning scheme for improving the performance of deep learning tasks with insufficient training data. In this scheme, a target learning task with insufficient training data is carried out simultaneously with another source learning task with abundant training data. However, the source learning task does not use all existing training data. Our core idea is to identify and use a subset of training images from the original source learning task whose low-level characteristics are similar to those from the target learning task, and jointly fine-tune shared convolutional layers for both tasks. Specifically, we compute descriptors from linear or nonlinear filter bank responses on training images from both tasks, and use such descriptors to search for a desired subset of training samples for the source learning task.
  Experiments demonstrate that our selective joint fine-tuning scheme achieves state-of-the-art performance on multiple visual classification tasks with insufficient training data for deep learning. Such tasks include Caltech 256, MIT Indoor 67,OxfordFlowers 102 and Stanford Dogs 120. In comparison to fine-tuning without a source domain, the proposed method can improve the classification accuracy by 2% - 10% using a single model.△ Less"
Emergence and Reductionism: an awkward Baconian alliance,Authors:Piers Coleman,"Abstract:This article discusses the relationship between emergence and reductionism from the perspective of a condensed matter physicist. Reductionism and emergence play an intertwined role in the everyday life of the physicist, yet we rarely stop to contemplate their relationship: indeed, the two are often regarded as conflicting world-views of science. I argue that in practice, they compliment one-anothe…▽ MoreThis article discusses the relationship between emergence and reductionism from the perspective of a condensed matter physicist. Reductionism and emergence play an intertwined role in the everyday life of the physicist, yet we rarely stop to contemplate their relationship: indeed, the two are often regarded as conflicting world-views of science. I argue that in practice, they compliment one-another, forming an awkward alliance in a fashion envisioned by the Renaissance scientist, Francis Bacon. Looking at the historical record in classical and quantum physics, I discuss how emergence fits into a reductionist view of nature. Often, a deep understanding of reductionist physics depends on the understanding of its emergent consequences. Thus the concept of energy was unknown to Newton, Leibnitz, Lagrange or Hamilton, because they did not understand heat. Similarly, the understanding of the weak force awaited an understanding of the Meissner effect in superconductivity. Emergence can thus be likened to an encrypted consequence of reductionism. Taking examples from current research, including topological insulators and strange metals, I show that the convection between emergence and reductionism continues to provide a powerful driver for frontier scientific research, linking the lab with the cosmos.△ Less"
Experiments on Crowdsourcing Policy Assessment,"Authors:J. Prpic,A. Taeihagh,J. Melton","Abstract:Can Crowds serve as useful allies in policy design? How do non-expert Crowds perform relative to experts in the assessment of policy measures? Does the geographic location of non-expert Crowds, with relevance to the policy context, alter the performance of non-experts Crowds in the assessment of policy measures? In this work, we investigate these questions by undertaking experiments designed to re…▽ MoreCan Crowds serve as useful allies in policy design? How do non-expert Crowds perform relative to experts in the assessment of policy measures? Does the geographic location of non-expert Crowds, with relevance to the policy context, alter the performance of non-experts Crowds in the assessment of policy measures? In this work, we investigate these questions by undertaking experiments designed to replicate expert policy assessments with non-expert Crowds recruited from Virtual Labor Markets. We use a set of ninety-six climate change adaptation policy measures previously evaluated by experts in the Netherlands as our control condition to conduct experiments using two discrete sets of non-expert Crowds recruited from Virtual Labor Markets. We vary the composition of our non-expert Crowds along two conditions: participants recruited from a geographical location directly relevant to the policy context and participants recruited at-large. We discuss our research methods in detail and provide the findings of our experiments.△ Less"
Crowd Capital in Governance Contexts,"Authors:J. Prpic,P. Shukla","Abstract:To begin to understand the implications of the implementation of IT-mediated Crowds for Politics and Policy purposes, this research builds the first-known dataset of IT-mediated Crowd applications currently in use in the governance context. Using Crowd Capital theory and governance theory as frameworks to organize our data collection, we undertake an exploratory data analysis of some fundamental f…▽ MoreTo begin to understand the implications of the implementation of IT-mediated Crowds for Politics and Policy purposes, this research builds the first-known dataset of IT-mediated Crowd applications currently in use in the governance context. Using Crowd Capital theory and governance theory as frameworks to organize our data collection, we undertake an exploratory data analysis of some fundamental factors defining this emerging field. Specific factors outlined and discussed include the type of actors implementing IT-mediated Crowds in the governance context, the global geographic distribution of the applications, and the nature of the Crowd-derived resources being generated for governance purposes. The findings from our dataset of 209 on-going endeavours indicates that a wide-diversity of actors are engaging IT-mediated Crowds in the governance context, both jointly and severally, that these endeavours can be found to exist on all continents, and that said actors are generating Crowd-derived resources in at least ten distinct governance sectors. We discuss the ramifications of these and our other findings in comparison to the research literature on the private-sector use of IT-mediated Crowds, while highlighting some unique future research opportunities stemming from our work.△ Less"
A Framework for Policy Crowdsourcing,"Authors:J. Prpic,A. Taeihagh,J. Melton","Abstract:What is the state of the literature in respect to Crowdsourcing for policy making? This work attempts to answer this question by collecting, categorizing, and situating the extant research investigating Crowdsourcing for policy, within the broader Crowdsourcing literature. To do so, the work first extends the Crowdsourcing literature by introducing, defining, explaining, and using seven universal…▽ MoreWhat is the state of the literature in respect to Crowdsourcing for policy making? This work attempts to answer this question by collecting, categorizing, and situating the extant research investigating Crowdsourcing for policy, within the broader Crowdsourcing literature. To do so, the work first extends the Crowdsourcing literature by introducing, defining, explaining, and using seven universal characteristics of all general Crowdsourcing techniques, to vividly draw-out the relative trade-offs of each mode of Crowdsourcing. From this beginning, the work systematically and explicitly weds the three types of Crowdsourcing to the stages of the Policy cycle as a method of situating the extant literature spanning both domains. Thereafter, we discuss the trends, highlighting the research gaps, and outline the overlaps in the research on Crowdsourcing for policy, stemming from our analysis.△ Less"
Abstract bivariant Cuntz semigroups,"Authors:Ramon Antoine,Francesc Perera,Hannes Thiel","Abstract:We show that abstract Cuntz semigroups form a closed symmetric monoidal category. Thus, given Cuntz semigroups $S$ and $T$, there is another Cuntz semigroup $[[S,T]]$ playing the role of morphisms from $S$ to $T$. Applied to C$^*$-algebras $A$ and $B$, the semigroup $[[\mathrm{Cu}(A),\mathrm{Cu}(B)]]$ should be considered as the target in analogues of the UCT for bivariant theories of Cuntz semigr…▽ MoreWe show that abstract Cuntz semigroups form a closed symmetric monoidal category. Thus, given Cuntz semigroups $S$ and $T$, there is another Cuntz semigroup $[[S,T]]$ playing the role of morphisms from $S$ to $T$. Applied to C$^*$-algebras $A$ and $B$, the semigroup $[[\mathrm{Cu}(A),\mathrm{Cu}(B)]]$ should be considered as the target in analogues of the UCT for bivariant theories of Cuntz semigroups.
  Abstract bivariant Cuntz semigroups are computable in a number of interesting cases. We also show that order-zero maps between C$^*$-algebras naturally define elements in the respective bivariant Cuntz semigroup.△ Less"
Siamese Network of Deep Fisher-Vector Descriptors for Image Retrieval,"Authors:Eng-Jon Ong,Sameed Husain,Miroslaw Bober","Abstract:…of deep descriptors over the course of the learning process. We show that the proposed approach gives significant improvements over the state-of-the-art methods on theOxfordand Paris image retrieval datasets. Additionally, we provide a baseline performance measure for both these datasets with the inclusion of 1 million distractors.▽ MoreThis paper addresses the problem of large scale image retrieval, with the aim of accurately ranking the similarity of a large number of images to a given query image. To achieve this, we propose a novel Siamese network. This network consists of two computational strands, each comprising of a CNN component followed by a Fisher vector component. The CNN component produces dense, deep convolutional descriptors that are then aggregated by the Fisher Vector method. Crucially, we propose to simultaneously learn both the CNN filter weights and Fisher Vector model parameters. This allows us to account for the evolving distribution of deep descriptors over the course of the learning process. We show that the proposed approach gives significant improvements over the state-of-the-art methods on theOxfordand Paris image retrieval datasets. Additionally, we provide a baseline performance measure for both these datasets with the inclusion of 1 million distractors.△ Less"
Improved Descriptors for Patch Matching and Reconstruction,"Authors:Rahul Mitra,Jiakai Zhang,Sanath Narayan,Shuaib Ahmed,Sharat Chandran,Arjun Jain","Abstract:…has better coverage of the overall viewpoint, scale, and lighting changes in comparison to the MVS dataset. We evaluate our approach on publicly available datasets, such asOxfordAffine Covariant Regions Dataset (ACRD) [12], MVS [18], Synthetic [6] and Strecha [15] datasets to quantify the image descriptor performance. Scenes from the…▽ MoreWe propose a convolutional neural network (ConvNet) based approach for learning local image descriptors which can be used for significantly improved patch matching and 3D reconstructions. A multi-resolution ConvNet is used for learning keypoint descriptors. We also propose a new dataset consisting of an order of magnitude more number of scenes, images, and positive and negative correspondences compared to the currently available Multi-View Stereo (MVS) [18] dataset. The new dataset also has better coverage of the overall viewpoint, scale, and lighting changes in comparison to the MVS dataset. We evaluate our approach on publicly available datasets, such asOxfordAffine Covariant Regions Dataset (ACRD) [12], MVS [18], Synthetic [6] and Strecha [15] datasets to quantify the image descriptor performance. Scenes from theOxfordACRD, MVS and Synthetic datasets are used for evaluating the patch matching performance of the learnt descriptors while the Strecha dataset is used to evaluate the 3D reconstruction task. Experiments show that the proposed descriptor outperforms the current state-of-the-art descriptors in both the evaluation tasks.△ Less"
Detailed modeling of dust distribution in the disk of HD 142527,"Authors:Kang-Lou Soon,Tomoyuki Hanawa,Takayuki Muto,Takashi Tsukagoshi,Munetake Momose","Abstract:We investigate the dust distribution in the crescent disk around HD 142527 based on the continuum emission at $890 \mathrm{\ μm}$ obtained by ALMA Cycle 0. The map is divided into $18$ azimuthal sectors, and the radial intensity profile in each sector is reproduced with a 2D disk model. Our model takes account of scattering and inclination of the disk as well as the azimuthal dependence in intensi…▽ MoreWe investigate the dust distribution in the crescent disk around HD 142527 based on the continuum emission at $890 \mathrm{\ μm}$ obtained by ALMA Cycle 0. The map is divided into $18$ azimuthal sectors, and the radial intensity profile in each sector is reproduced with a 2D disk model. Our model takes account of scattering and inclination of the disk as well as the azimuthal dependence in intensity. When the dust is assumed to have the conventional composition and maximum size of $1\ \mathrm{mm}$, the northwestern region ($PA=329^{\circ}-29^{\circ}$) cannot be reproduced. This is because the model intensity gets insensitive to the increase in surface density due to heavy self-scattering, reaching its ceiling much lower than the observed intensity. The ceiling depends on the position angle. When the scattering opacity is reduced by a factor of $10$, the intensity distribution is reproduced successfully in all the sectors including those in the northwestern region. The best fit model parameters depend little on the scattering opacity in the southern region where the disk is optically thin. The contrast of dust surface density along $PA$ is derived to be about $40$, much smaller than the value for the cases of conventional opacities ($70-130$). These results strongly suggest that the albedo is lower than considered by some reasons at least in the northwestern region.△ Less"
The HIX galaxy survey I: Study of the most gas rich galaxies from HIPASS,"Authors:K. A. Lutz,V. A. Kilborn,B. Catinella,B. S. Koribalski,T. H. Brown,L. Cortese,H. Dénes,G. I. G. Józsa,O. I. Wong","Abstract:We present the HI eXtreme (HIX) galaxy survey targeting some of the most HI rich galaxies in the southern hemisphere. The 13 HIX galaxies have been selected to host the most massive HI discs at a given stellar luminosity. We compare these galaxies to a control sample of average galaxies detected in the HI Parkes All Sky Survey (HIPASS, Barnes et al. 2001). As the control sample is matched in stell…▽ MoreWe present the HI eXtreme (HIX) galaxy survey targeting some of the most HI rich galaxies in the southern hemisphere. The 13 HIX galaxies have been selected to host the most massive HI discs at a given stellar luminosity. We compare these galaxies to a control sample of average galaxies detected in the HI Parkes All Sky Survey (HIPASS, Barnes et al. 2001). As the control sample is matched in stellar luminosity, we find that the stellar properties of HIX galaxies are similar to the control sample. Furthermore, the specific star formation rate and optical morphology do not differ between HIX and control galaxies. We find, however, the HIX galaxies to be less efficient in forming stars. For the most HI massive galaxy in our sample (ESO075-G006, $\rm log\ M_{HI}\ [M_{\odot}] = 10.8$) the kinematic properties are the reason for inefficient star formation and HI excess. Examining the Australian Telescope Compact Array (ATCA) HI imaging and Wide Field Spectrograph (WiFeS) optical spectra of ESO075-G006 reveals an undisturbed galaxy without evidence for recent major, violent accretion events. A tilted-ring fit to the HI disc together with the gas-phase oxygen abundance distribution supports the scenario that gas has been constantly accreted onto ESO07-G006 but the high specific angular momentum makes ESO075-G006 very inefficient in forming stars. Thus a massive HI disc has been built up.△ Less"
Construction and implementation of asymptotic expansions for Laguerre-type orthogonal polynomials,"Authors:Daan Huybrechs,Peter Opsomer","Abstract:Laguerre and Laguerre-type polynomials are orthogonal polynomials on the interval $[0,\infty)$ with respect to a weight function of the form $w(x) = x^α e^{-Q(x)}, Q(x) = \sum_{k=0}^m q_k x^k, α> -1, q_m > 0$. The classical Laguerre polynomials correspond to $Q(x)=x$. The computation of higher-order terms of the asymptotic expansions of these polynomials for large degree becomes quite complicated,…▽ MoreLaguerre and Laguerre-type polynomials are orthogonal polynomials on the interval $[0,\infty)$ with respect to a weight function of the form $w(x) = x^α e^{-Q(x)}, Q(x) = \sum_{k=0}^m q_k x^k, α> -1, q_m > 0$. The classical Laguerre polynomials correspond to $Q(x)=x$. The computation of higher-order terms of the asymptotic expansions of these polynomials for large degree becomes quite complicated, and a full description seems to be lacking in literature. However, this information is implicitly available in the work of Vanlessen, based on a non-linear steepest descent analysis of an associated so-called Riemann--Hilbert problem. We will extend this work and show how to efficiently compute an arbitrary number of higher-order terms in the asymptotic expansions of Laguerre and Laguerre-type polynomials. This effort is similar to the case of Jacobi and Jacobi-type polynomials in a previous paper. We supply an implementation with explicit expansions in four different regions of the complex plane. These expansions can also be extended to Hermite-type weights of the form $\exp(-\sum_{k=0}^m q_k x^{2k})$ on $(-\infty,\infty)$, and to general non-polynomial functions $Q(x)$ using contour integrals. The expansions may be used, e.g., to compute Gauss-Laguerre quadrature rules in a lower computational complexity than based on the recurrence relation, and with improved accuracy for large degree. They are also of interest in random matrix theory.△ Less"
Joint Hand Detection and Rotation Estimation by Using CNN,"Authors:Xiaoming Deng,Ye Yuan,Yinda Zhang,Ping Tan,Liang Chang,Shuo Yang,Hongan Wang","Abstract:…detection and rotation estimation tasks. Experiments show that our method achieves better results than state-of-the-art detection models on widely-used benchmarks such asOxfordand Egohands database. We further show that rotation estimation and classification can mutually benefit each other.▽ MoreHand detection is essential for many hand related tasks, e.g. parsing hand pose, understanding gesture, which are extremely useful for robotics and human-computer interaction. However, hand detection in uncontrolled environments is challenging due to the flexibility of wrist joint and cluttered background. We propose a deep learning based approach which detects hands and calibrates in-plane rotation under supervision at the same time. To guarantee the recall, we propose a context aware proposal generation algorithm which significantly outperforms the selective search. We then design a convolutional neural network(CNN) which handles object rotation explicitly to jointly solve the object detection and rotation estimation tasks. Experiments show that our method achieves better results than state-of-the-art detection models on widely-used benchmarks such asOxfordand Egohands database. We further show that rotation estimation and classification can mutually benefit each other.△ Less"
Radial measurements of IMF-sensitive absorption features in two massive ETGs,"Authors:Sam P. Vaughan,Roger L. Davies,Simon Zieleniewski,Ryan C. W. Houghton","Abstract:We make radial measurements of stellar initial mass function (IMF) sensitive absorption features in the two massive early-type galaxies NGC 1277 and IC 843. Using theOxfordShort Wavelength Integral Field SpecTrogaph (SWIFT), we obtain resolved measurements of the NaI0.82 and FeH0.99 indices, among others, finding both galaxies show strong gradients in NaI…▽ MoreWe make radial measurements of stellar initial mass function (IMF) sensitive absorption features in the two massive early-type galaxies NGC 1277 and IC 843. Using theOxfordShort Wavelength Integral Field SpecTrogaph (SWIFT), we obtain resolved measurements of the NaI0.82 and FeH0.99 indices, among others, finding both galaxies show strong gradients in NaI absorption combined with flat FeH profiles at $\sim0.4$Å. We find these measurements may be explained by radial gradients in the IMF, appropriate abundance gradients in [Na/Fe] and [Fe/H], or a combination of the two, and our data is unable to break this degeneracy. We also use full spectral fitting to infer global properties from an integrated spectrum of each object, deriving a unimodal IMF slope consistent with Salpeter in IC 843 ($x=2.27\pm0.17$) but steeper than Salpeter in NGC 1277 ($x=2.69\pm0.11$), despite their similar FeH equivalent widths. Independently, we fit the strength of the FeH feature and compare to the E-MILES and CvD12 stellar population libraries, finding agreement between the models. The IMF values derived in this way are in close agreement with those from spectral fitting in NGC 1277 ($x_{\mathrm{CvD}}=2.59^{+0.25}_{-0.48}$ , $x_{\mathrm{E-MILES}}=2.77\pm0.31$), but are less consistent in IC 843, with the IMF derived from FeH alone leading to steeper slopes than when fitting the full spectrum ($x_{\mathrm{CvD}}=2.57^{+0.30}_{-0.41}$, $x_{\mathrm{E-MILES}}=2.72\pm0.25$). This work highlights the importance of a large wavelength coverage for breaking the degeneracy between abundance and IMF variations, and may bring into doubt the use of the Wing-Ford band as an IMF index if used without other spectral information.△ Less"
Up-down asymmetric tokamaks,Authors:Justin Ball,"Abstract:Bulk toroidal rotation has proven capable of stabilising both dangerous MHD modes and turbulence. In this thesis, we explore a method to drive rotation in large tokamaks: up-down asymmetry in the magnetic equilibrium. We seek to maximise this rotation by finding optimal up-down asymmetric flux surface shapes.
  First, we use the ideal MHD model to show that low order external shaping (e.g. elongat…▽ MoreBulk toroidal rotation has proven capable of stabilising both dangerous MHD modes and turbulence. In this thesis, we explore a method to drive rotation in large tokamaks: up-down asymmetry in the magnetic equilibrium. We seek to maximise this rotation by finding optimal up-down asymmetric flux surface shapes.
  First, we use the ideal MHD model to show that low order external shaping (e.g. elongation) is best for creating up-down asymmetric flux surfaces throughout the device. Then, we calculate realistic up-down asymmetric equilibria for input into nonlinear gyrokinetic turbulence analysis. Analytic gyrokinetics shows that, in the limit of fast shaping effects, a poloidal tilt of the flux surface shaping has little effect on turbulent transport. Since up-down symmetric surfaces do not transport momentum, this invariance to tilt implies that devices with mirror symmetry about any line in the poloidal plane will drive minimal rotation. Accordingly, further analytic investigation suggests that non-mirror symmetric flux surfaces with envelopes created by the beating of fast shaping effects may create significantly stronger momentum transport.
  Guided by these analytic results, we carry out local nonlinear gyrokinetic simulations of non-mirror symmetric flux surfaces created with the lowest possible shaping effects. First, we consider tilted elliptical flux surfaces with a Shafranov shift and find little increase in the momentum transport when the effect of the pressure profile on the equilibrium is included. We then simulate flux surfaces with independently-tilted elongation and triangularity. These two-mode configurations show a $60\%$ increase over configurations with just elongation or triangularity. A rough analytic estimate indicates that the optimal two-mode configuration can drive rotation with an on-axis Alfven Mach number of $1.5 \%$ in an ITER-like machine.△ Less"
PUMA: The Positional Update and Matching Algorithm,"Authors:J. L. B. Line,R. L. Webster,B. Pindor,D. A. Mitchell,C. M. Trott","Abstract:…able to recover ionospheric offsets. Finally, we use this sky model to calibrate and remove foreground sources from simulated interferometric data, generated using OSKAR (theOxfordUniversity visibility generator). We demonstrate that there is a substantial improvement in foreground source removal when using higher frequency and higher resolution source pos…▽ MoreWe present new software to cross-match low-frequency radio catalogues: the Positional Update and Matching Algorithm (PUMA). PUMA combines a positional Bayesian probabilistic approach with spectral matching criteria, allowing for confusing sources in the matching process. We go on to create a radio sky model using PUMA based on the Murchison Widefield Array Commissioning Survey, and are able to automatically cross-match 98.5% of sources. Using the characteristics of this sky model, we create simple simulated mock catalogues on which to test PUMA, and find that PUMA can reliably find the correct spectral indices of sources, along with being able to recover ionospheric offsets. Finally, we use this sky model to calibrate and remove foreground sources from simulated interferometric data, generated using OSKAR (theOxfordUniversity visibility generator). We demonstrate that there is a substantial improvement in foreground source removal when using higher frequency and higher resolution source positions, even when correcting positions by an average of 0.3 given a synthesized beam-width of 2.3.△ Less"
Tricks from Deep Learning,"Authors:Atılım Güneş Baydin,Barak A. Pearlmutter,Jeffrey Mark Siskind","Abstract:The deep learning community has devised a diverse set of methods to make gradient optimization, using large datasets, of large and highly complex models with deeply cascaded nonlinearities, practical. Taken as a whole, these methods constitute a breakthrough, allowing computational structures which are quite wide, very deep, and with an enormous number and variety of free parameters to be effectiv…▽ MoreThe deep learning community has devised a diverse set of methods to make gradient optimization, using large datasets, of large and highly complex models with deeply cascaded nonlinearities, practical. Taken as a whole, these methods constitute a breakthrough, allowing computational structures which are quite wide, very deep, and with an enormous number and variety of free parameters to be effectively optimized. The result now dominates much of practical machine learning, with applications in machine translation, computer vision, and speech recognition. Many of these methods, viewed through the lens of algorithmic differentiation (AD), can be seen as either addressing issues with the gradient itself, or finding ways of achieving increased efficiency using tricks that are AD-related, but not provided by current AD systems.
  The goal of this paper is to explain not just those methods of most relevance to AD, but also the technical constraints and mindset which led to their discovery. After explaining this context, we present a ""laundry list"" of methods developed by the deep learning community. Two of these are discussed in further mathematical detail: a way to dramatically reduce the size of the tape when performing reverse-mode AD on a (theoretically) time-reversible process like an ODE integrator; and a new mathematical insight that allows for the implementation of a stochastic Newton's method.△ Less"
Evolving the Incremental λ Calculus into a Model of Forward Automatic Differentiation (AD),"Authors:Robert Kelly,Barak A. Pearlmutter,Jeffrey Mark Siskind","Abstract:Formal transformations somehow resembling the usual derivative are surprisingly common in computer science, with two notable examples being derivatives of regular expressions and derivatives of types. A newcomer to this list is the incremental $λ$-calculus, or ILC, a ""theory of changes"" that deploys a formal apparatus allowing the automatic generation of efficient update functions which perform in…▽ MoreFormal transformations somehow resembling the usual derivative are surprisingly common in computer science, with two notable examples being derivatives of regular expressions and derivatives of types. A newcomer to this list is the incremental $λ$-calculus, or ILC, a ""theory of changes"" that deploys a formal apparatus allowing the automatic generation of efficient update functions which perform incremental computation. The ILC is not only defined, but given a formal machine-understandable definition---accompanied by mechanically verifiable proofs of various properties, including in particular correctness of various sorts. Here, we show how the ILC can be mutated into propagating tangents, thus serving as a model of Forward Accumulation Mode Automatic Differentiation. This mutation is done in several steps. These steps can also be applied to the proofs, resulting in machine-checked proofs of the correctness of this model of forward AD.△ Less"
DiffSharp: An AD Library for .NET Languages,"Authors:Atılım Güneş Baydin,Barak A. Pearlmutter,Jeffrey Mark Siskind","Abstract:DiffSharp is an algorithmic differentiation or automatic differentiation (AD) library for the .NET ecosystem, which is targeted by the C# and F# languages, among others. The library has been designed with machine learning applications in mind, allowing very succinct implementations of models and optimization routines. DiffSharp is implemented in F# and exposes forward and reverse AD operators as g…▽ MoreDiffSharp is an algorithmic differentiation or automatic differentiation (AD) library for the .NET ecosystem, which is targeted by the C# and F# languages, among others. The library has been designed with machine learning applications in mind, allowing very succinct implementations of models and optimization routines. DiffSharp is implemented in F# and exposes forward and reverse AD operators as general nestable higher-order functions, usable by any .NET language. It provides high-performance linear algebra primitives---scalars, vectors, and matrices, with a generalization to tensors underway---that are fully supported by all the AD operators, and which use a BLAS/LAPACK backend via the highly optimized OpenBLAS library. DiffSharp currently uses operator overloading, but we are developing a transformation-based version of the library using F#'s ""code quotation"" metaprogramming facility. Work on a CUDA-based GPU backend is also underway.△ Less"
Efficient Implementation of a Higher-Order Language with Built-In AD,"Authors:Jeffrey Mark Siskind,Barak A. Pearlmutter","Abstract:We show that Automatic Differentiation (AD) operators can be provided in a dynamic language without sacrificing numeric performance. To achieve this, general forward and reverse AD functions are added to a simple high-level dynamic language, and support for them is included in an aggressive optimizing compiler. Novel technical mechanisms are discussed, which have the ability to migrate the AD tran…▽ MoreWe show that Automatic Differentiation (AD) operators can be provided in a dynamic language without sacrificing numeric performance. To achieve this, general forward and reverse AD functions are added to a simple high-level dynamic language, and support for them is included in an aggressive optimizing compiler. Novel technical mechanisms are discussed, which have the ability to migrate the AD transformations from run-time to compile-time. The resulting system, although only a research prototype, exhibits startlingly good performance. In fact, despite the potential inefficiencies entailed by support of a functional-programming language and a first-class AD operator, performance is competitive with the fastest available preprocessor-based Fortran AD systems. On benchmarks involving nested use of the AD operators, it can even dramatically exceed their performance.△ Less"
Binomial Checkpointing for Arbitrary Programs with No User Annotation,"Authors:Jeffrey Mark Siskind,Barak A. Pearlmutter","Abstract:Heretofore, automatic checkpointing at procedure-call boundaries, to reduce the space complexity of reverse mode, has been provided by systems like Tapenade. However, binomial checkpointing, or treeverse, has only been provided in Automatic Differentiation (AD) systems in special cases, e.g., through user-provided pragmas on DO loops in Tapenade, or as the nested taping mechanism in adol-c for tim…▽ MoreHeretofore, automatic checkpointing at procedure-call boundaries, to reduce the space complexity of reverse mode, has been provided by systems like Tapenade. However, binomial checkpointing, or treeverse, has only been provided in Automatic Differentiation (AD) systems in special cases, e.g., through user-provided pragmas on DO loops in Tapenade, or as the nested taping mechanism in adol-c for time integration processes, which requires that user code be refactored. We present a framework for applying binomial checkpointing to arbitrary code with no special annotation or refactoring required. This is accomplished by applying binomial checkpointing directly to a program trace. This trace is produced by a general-purpose checkpointing mechanism that is orthogonal to AD.△ Less"
Radial gradients in initial mass function sensitive absorption features in the Coma brightest cluster galaxies,"Authors:Simon Zieleniewski,Ryan C. W. Houghton,Niranjan Thatte,Roger L. Davies,Sam P. Vaughan","Abstract:Using theOxfordShort Wavelength Integral Field specTrograph (SWIFT), we trace radial variations of initial mass function (IMF) sensitive absorption features of three galaxies in the Coma cluster. We obtain resolved spectroscopy of the central 5kpc for the two central brightest-cluster galaxies (BCGs) NGC4889, NGC4874, and the BCG in the south-west group NG…▽ MoreUsing theOxfordShort Wavelength Integral Field specTrograph (SWIFT), we trace radial variations of initial mass function (IMF) sensitive absorption features of three galaxies in the Coma cluster. We obtain resolved spectroscopy of the central 5kpc for the two central brightest-cluster galaxies (BCGs) NGC4889, NGC4874, and the BCG in the south-west group NGC4839, as well as unresolved data for NGC4873 as a low-$σ_*$ control. We present radial measurements of the IMF-sensitive features sodium NaI$_{\rm{SDSS}}$, calcium triplet CaT and iron-hydride FeH0.99, along with the magnesium MgI0.88 and titanium oxide TiO0.89 features. We employ two separate methods for both telluric correction and sky-subtraction around the faint FeH feature to verify our analysis. Within NGC4889 we find strong gradients of NaI$_{\rm{SDSS}}$ and CaT but a flat FeH profile, which from comparing to stellar population synthesis models, suggests an old, $α$-enhanced population with a Chabrier, or even bottom-light IMF. The age and abundance is in line with previous studies but the normal IMF is in contrast to recent results suggesting an increased IMF slope with increased velocity dispersion. We measure flat NaI$_{\rm{SDSS}}$ and FeH profiles within NGC4874 and determine an old, possibly slightly $α$-enhanced and Chabrier IMF population. We find an $α$-enhanced, Chabrier IMF population in NGC4873. Within NGC4839 we measure both strong NaI$_{\rm{SDSS}}$ and strong FeH, although with a large systematic uncertainty, suggesting a possible heavier IMF. The IMFs we infer for these galaxies are supported by published dynamical modelling. We stress that IMF constraints should be corroborated by further spectral coverage and independent methods on a galaxy-by-galaxy basis.△ Less"
Granular Impact: A Grain-scale Approach,"Authors:Abram H. Clark,Alec Petersen,Lou Kondic,Corey O'Hern,Robert P. Behringer","Abstract:This work summarizes a series of studies on two-dimensional granular impact, where an intruding object strikes a granular material at high speed. Many previous studies on granular impact have used a macroscopic force law, which is dominated by an inertial drag term proportional to the intruder velocity squared. The primary focus here is on the microscopic force response of the granular material, a…▽ MoreThis work summarizes a series of studies on two-dimensional granular impact, where an intruding object strikes a granular material at high speed. Many previous studies on granular impact have used a macroscopic force law, which is dominated by an inertial drag term proportional to the intruder velocity squared. The primary focus here is on the microscopic force response of the granular material, and how the grain-scale effects give rise to this inertial drag term. We show that the inertial drag arises from intermittent collisions with force-chain-like structures. We construct a simple collisional model to explain the inertial drag, as well as off-axis instability and rotations. Finally, we show how the granular response changes when the intruder speed approaches $d/t_c$, leading to a failure of the inertial drag description in this regime. Here, $d$ is the mean particle diameter and $t_c$ the characteristic momentum-transfer time between two grains.△ Less"
The Machine Learning Algorithm as Creative Musical Tool,"Authors:Rebecca Fiebrink,Baptiste Caramiaux","Abstract:Machine learning is the capacity of a computational system to learn structures from datasets in order to make predictions on newly seen data. Such an approach offers a significant advantage in music scenarios in which musicians can teach the system to learn an idiosyncratic style, or can break the rules to explore the system's capacity in unexpected ways. In this chapter we draw on music, machine…▽ MoreMachine learning is the capacity of a computational system to learn structures from datasets in order to make predictions on newly seen data. Such an approach offers a significant advantage in music scenarios in which musicians can teach the system to learn an idiosyncratic style, or can break the rules to explore the system's capacity in unexpected ways. In this chapter we draw on music, machine learning, and human-computer interaction to elucidate an understanding of machine learning algorithms as creative tools for music and the sonic arts. We motivate a new understanding of learning algorithms as human-computer interfaces. We show that, like other interfaces, learning algorithms can be characterised by the ways their affordances intersect with goals of human users. We also argue that the nature of interaction between users and algorithms impacts the usability and usefulness of those algorithms in profound ways. This human-centred view of machine learning motivates our concluding discussion of what it means to employ machine learning as a creative tool.△ Less"
A first thermodynamic interpretation of the technology transfer activities,"Authors:S. Ripandelli,U. Lucia","Abstract:…innovation and to ensure that scientific and technological developments could become accessible to a wider range of users. Starting from important Universities (MIT, Stanford,Oxford, etc) nowadays the TT is assuming a central role. It is called the third mission, together with education and research. The importance to provide new theories and tools to descr…▽ MoreIn the last years new interdisciplinary approaches to economics and social science have been developed. A Thermodynamic approach to socio-economics has brought to a new interdisciplinary scientific field called econophysics. Why thermodynamic? Thermodynamic is a statistical theory for large atomic system under constraints of energy[1] and the economy can be considered a large system governed by complex rules. The present job proposes a new application, starting from econophysic, passing throughout the thermodynamic laws to interpret and to described the Technology Transfer (TT) activities. Using the definition of economy (i.e. economy[dictionary def.] = the process or system by which goods and services are produced, sold, and bought in a country or region) the TT can be considered an important sub-domain of the economy and a transversal new area of the scientific research. The TT is the process of transferring knowledge, that uses the results from the research to produce innovation and to ensure that scientific and technological developments could become accessible to a wider range of users. Starting from important Universities (MIT, Stanford,Oxford, etc) nowadays the TT is assuming a central role. It is called the third mission, together with education and research. The importance to provide new theories and tools to describe the TT activities and their behavior, has been retained fundamental to support the social rapid evolution that is involving the TT offices. The presented work uses the thermodynamic theories applying them to Technology Transfer and starting from the concept of entropy, exergy and anergy. The output analysis should become an help to make decision to improve the TT activities and a better resources employment.△ Less"
End-to-end Learning of Deep Visual Representations for Image Retrieval,"Authors:Albert Gordo,Jon Almazan,Jerome Revaud,Diane Larlus","Abstract:…our approach significantly outperforms previous retrieval approaches, including state-of-the-art methods based on costly local descriptor indexing and spatial verification. OnOxford5k, Paris 6k and Holidays, we respectively report 94.7, 96.6, and 94.8 mean average precision. Our representations can also be heavily compressed using product quantization with…▽ MoreWhile deep learning has become a key ingredient in the top performing methods for many computer vision tasks, it has failed so far to bring similar improvements to instance-level image retrieval. In this article, we argue that reasons for the underwhelming results of deep methods on image retrieval are threefold: i) noisy training data, ii) inappropriate deep architecture, and iii) suboptimal training procedure. We address all three issues.
  First, we leverage a large-scale but noisy landmark dataset and develop an automatic cleaning method that produces a suitable training set for deep retrieval. Second, we build on the recent R-MAC descriptor, show that it can be interpreted as a deep and differentiable architecture, and present improvements to enhance it. Last, we train this network with a siamese architecture that combines three streams with a triplet loss. At the end of the training process, the proposed architecture produces a global image representation in a single forward pass that is well suited for image retrieval. Extensive experiments show that our approach significantly outperforms previous retrieval approaches, including state-of-the-art methods based on costly local descriptor indexing and spatial verification. OnOxford5k, Paris 6k and Holidays, we respectively report 94.7, 96.6, and 94.8 mean average precision. Our representations can also be heavily compressed using product quantization with little loss in accuracy. For additional material, please see www.xrce.xerox.com/Deep-Image-Retrieval.△ Less"
Interferometric computation beyond quantum theory,Authors:Andrew J. P. Garner,"Abstract:There are quantum solutions for computational problems that make use of interference at some stage in the algorithm. These stages can be mapped into the physical setting of a single particle travelling through a many-armed interferometer. There has been recent foundational interest in theories beyond quantum theory. Here, we present a generalized formulation of computation in the context of a many…▽ MoreThere are quantum solutions for computational problems that make use of interference at some stage in the algorithm. These stages can be mapped into the physical setting of a single particle travelling through a many-armed interferometer. There has been recent foundational interest in theories beyond quantum theory. Here, we present a generalized formulation of computation in the context of a many-armed interferometer, and explore how theories can differ from quantum theory and still perform distributed calculations in this set-up. We shall see that quaternionic quantum theory proves a suitable candidate, whereas box-world does not. We also find that a classical hidden variable model first presented by Spekkens [Phys. Rev. A, 75:3:32100, 2007] can also be used for this type of computation due to the epistemic restriction placed on the hidden variable.△ Less"
High-dynamic-range water window ptychography,"Authors:Max Rose,Dmitry Dzhigaev,Tobias Senkbeil,Andreas R. von Gundlach,Susan Stuhr,Christoph Rumancev,Ilya Besedin,Petr Skopintsev,Jens Viefhaus,Axel Rosenhahn,Ivan A. Vartanyants","Abstract:Ptychographic imaging with soft X-rays, especially in the water window energy range, suffers from limited detector dynamic range that directly influences the maximum spatial resolution achievable. High-dynamic-range data can be obtained by multiple exposures. By this approach we have increased the dynamic range of a ptychographic data set by a factor of 76 and obtained diffraction signal till the…▽ MorePtychographic imaging with soft X-rays, especially in the water window energy range, suffers from limited detector dynamic range that directly influences the maximum spatial resolution achievable. High-dynamic-range data can be obtained by multiple exposures. By this approach we have increased the dynamic range of a ptychographic data set by a factor of 76 and obtained diffraction signal till the corners of the detector. The real space half period resolution was improved from 50 nm for the single exposure data to 18 nm for the high-dynamic-range data.△ Less"
Initial analysis of extragalactic fields using a new AKARI/IRC analysis pipeline,"Authors:H. R. Davidge,S. Serjeant,C. P. Pearson","Abstract:We present the first results of a new data analysis pipeline for processing extragalactic AKARI/IRC images. The main improvements of the pipeline over the standard analysis are the removal of Earth shine and image distortion correction. We present the differential number counts of the AKARI/IRC S11 filter IRAC validation field. The differential number counts are consistent with S11 AKARI NEP deep…▽ MoreWe present the first results of a new data analysis pipeline for processing extragalactic AKARI/IRC images. The main improvements of the pipeline over the standard analysis are the removal of Earth shine and image distortion correction. We present the differential number counts of the AKARI/IRC S11 filter IRAC validation field. The differential number counts are consistent with S11 AKARI NEP deep and 12 microns WISE NEP number counts, and with a phenomenological backward evolution galaxy model, at brighter fluxes densities. There is a detection of deeper galaxies in the IRAC validation field.△ Less"
Find Your Own Way: Weakly-Supervised Segmentation of Path Proposals for Urban Autonomy,"Authors:Dan Barnes,Will Maddern,Ingmar Posner","Abstract:…using a vehicle equipped with only a monocular camera without relying on explicit modelling of road or lane markings. We evaluate our method on the large-scale KITTI andOxfordRobotCar datasets and demonstrate reliable path proposal and obstacle segmentation in a wide variety of environments under a range of lighting, weather and traffic conditions. We illu…▽ MoreWe present a weakly-supervised approach to segmenting proposed drivable paths in images with the goal of autonomous driving in complex urban environments. Using recorded routes from a data collection vehicle, our proposed method generates vast quantities of labelled images containing proposed paths and obstacles without requiring manual annotation, which we then use to train a deep semantic segmentation network. With the trained network we can segment proposed paths and obstacles at run-time using a vehicle equipped with only a monocular camera without relying on explicit modelling of road or lane markings. We evaluate our method on the large-scale KITTI andOxfordRobotCar datasets and demonstrate reliable path proposal and obstacle segmentation in a wide variety of environments under a range of lighting, weather and traffic conditions. We illustrate how the method can generalise to multiple path proposals at intersections and outline plans to incorporate the system into a framework for autonomous urban driving.△ Less"
Hyperbolic volume estimates via train tracks,Authors:Antonio De Capua,"Abstract:In this thesis we describe how to estimate the distance spanned in the pants graph by a train track splitting sequence on a surface, up to multiplicative and additive constants. If some moderate assumptions on a splitting sequence are satisfied, each vertex set of a train track in it will represent a vertex of a graph which is naturally quasi-isometric to the pants graph; moreover the splitting se…▽ MoreIn this thesis we describe how to estimate the distance spanned in the pants graph by a train track splitting sequence on a surface, up to multiplicative and additive constants. If some moderate assumptions on a splitting sequence are satisfied, each vertex set of a train track in it will represent a vertex of a graph which is naturally quasi-isometric to the pants graph; moreover the splitting sequence gives an edge-path in this graph so, more precisely, our distance estimate holds between the extreme points of this path. The present distance estimate is inspired by a result of Masur, Mosher and Schleimer for distances in the marking graph. However, we can apply their line of proof only after some manipulation of the splitting sequence: a rearrangement, changing the order the elementary moves are performed in, so that the ones producing Dehn twists are brought together; and then an untwisting, which suppresses the majority of these latter moves to give a new sequence, which does not end with the same track as before, but does not include any portion that is almost stationary in the pants graph. The required distance is then, up to constants, the number of splits occurring in the untwisted sequence. A consequence of our main theorem together with a result of Brock is that, given a pseudo-Anosov self-diffeomorphism $ψ$ of a surface $S$, the maximal splitting sequence introduced by Agol gives us an estimate for the hyperbolic volume of the mapping torus built from $S$ and $ψ$. There are also some interesting consequences for the hyperbolic volume of a solid torus minus a closed braid, via a machinery employed by Dynnikov and Wiest.△ Less"
A spectral algorithm for fast de novo layout of uncorrected long nanopore reads,"Authors:Antoine Recanati,Thomas Brüls,Alexandre d'Aspremont","Abstract:…to layout the uncorrected nanopore reads, and its seamless integration into a straightforward overlap/layout/consensus (OLC) assembly scheme. The method is shown to assembleOxfordNanopore reads from several bacterial genomes into good quality (~99% identity to the reference) genome-sized contigs, while yielding more fragmented assemblies from a Sacharomyce…▽ MoreMotivation: New long read sequencers promise to transform sequencing and genome assembly by producing reads tens of kilobases long. However their high error rate significantly complicates assembly and requires expensive correction steps to layout the reads using standard assembly engines.
  Results: We present an original and efficient spectral algorithm to layout the uncorrected nanopore reads, and its seamless integration into a straightforward overlap/layout/consensus (OLC) assembly scheme. The method is shown to assembleOxfordNanopore reads from several bacterial genomes into good quality (~99% identity to the reference) genome-sized contigs, while yielding more fragmented assemblies from a Sacharomyces cerevisiae reference strain.
  Availability and implementation: http://github.com/antrec/spectrassembler
  Contact: antoine.recanati@inria.fr△ Less"
A Low Order Finite Element Method for Poroelasticity with Applications to Lung Modelling,Authors:Lorenz Berger,"Abstract:In this thesis we develop a stabilised finite element method for solving the equations of poroelasticity to enable solving complex models of biological tissues such as the human lungs. For the proposed numerical scheme, we use the lowest possible approximation order: piecewise constant approximation for the pressure, and piecewise linear continuous elements for the displacements and fluid flux. Du…▽ MoreIn this thesis we develop a stabilised finite element method for solving the equations of poroelasticity to enable solving complex models of biological tissues such as the human lungs. For the proposed numerical scheme, we use the lowest possible approximation order: piecewise constant approximation for the pressure, and piecewise linear continuous elements for the displacements and fluid flux. Due to the discontinuous pressure approximation, sharp pressure gradients due to changes in material coefficients or boundary layer solutions can be captured reliably. We begin by developing theoretical results for approximating the linear poroelastic equations valid in small deformations. In particular, we prove existence and uniqueness, an energy estimate and an optimal a-priori error estimate for the discretised problem. We then extend this work and construct a stabilised finite element method to solve the poroelastic equations valid in large deformations. We present the linearisation and discretisation for this nonlinear problem, and give a detailed account of the implementation. We rigorously test both the linear and nonlinear finite element method using numerous test problems to verify theoretical stability and convergence results, and the method's ability to reliably capture steep pressure gradients. Finally, we derive a poroelastic model for lung parenchyma coupled to an airway fluid network model, and develop a stable method to solve the coupled model. Numerical simulations, on a realistic lung geometry, illustrate the coupling between the poroelastic medium and the network flow model, and simulations of tidal breathing are shown to reproduce global physiologically realistic measurements. We also investigate the effect of airway constriction and tissue weakening on the ventilation, tissue stress and alveolar pressure distribution.△ Less"
Indistinguishability,Authors:Simon Saunders,"Abstract:This is a systematic review of the concept of indistinguishability in both classical and quantum mechanics, with particular attention to Gibbs' paradox. Section 1 is on the Gibbs paradox; section 2 is a defense of the concept of classical indistinguishability, that addresses (and refutes) the view that classical particles can always be distinguished by their trajectories so are distinguishable. Se…▽ MoreThis is a systematic review of the concept of indistinguishability in both classical and quantum mechanics, with particular attention to Gibbs' paradox. Section 1 is on the Gibbs paradox; section 2 is a defense of the concept of classical indistinguishability, that addresses (and refutes) the view that classical particles can always be distinguished by their trajectories so are distinguishable. Section 3 is on the notion of object more generally, and on whether indistinguishables should be thought of as objects at all△ Less"
The Algebra of Open and Interconnected Systems,Authors:Brendan Fong,"Abstract:Herein we develop category-theoretic tools for understanding network-style diagrammatic languages. The archetypal network-style diagrammatic language is that of electric circuits; other examples include signal flow graphs, Markov processes, automata, Petri nets, chemical reaction networks, and so on. The key feature is that the language is comprised of a number of components with multiple (input/o…▽ MoreHerein we develop category-theoretic tools for understanding network-style diagrammatic languages. The archetypal network-style diagrammatic language is that of electric circuits; other examples include signal flow graphs, Markov processes, automata, Petri nets, chemical reaction networks, and so on. The key feature is that the language is comprised of a number of components with multiple (input/output) terminals, each possibly labelled with some type, that may then be connected together along these terminals to form a larger network. The components form hyperedges between labelled vertices, and so a diagram in this language forms a hypergraph. We formalise the compositional structure by introducing the notion of a hypergraph category. Network-style diagrammatic languages and their semantics thus form hypergraph categories, and semantic interpretation gives a hypergraph functor.
  The first part of this thesis develops the theory of hypergraph categories. In particular, we introduce the tools of decorated cospans and corelations. Decorated cospans allow straightforward construction of hypergraph categories from diagrammatic languages: the inputs, outputs, and their composition are modelled by the cospans, while the 'decorations' specify the components themselves. Not all hypergraph categories can be constructed, however, through decorated cospans. Decorated corelations are a more powerful version that permits construction of all hypergraph categories and hypergraph functors. These are often useful for constructing the semantic categories of diagrammatic languages and functors from diagrams to the semantics. To illustrate these principles, the second part of this thesis details applications to linear time-invariant dynamical systems and passive linear networks.△ Less"
Chance in the Everett interpretation,Authors:Simon Saunders,"Abstract:The notion of objective probability or chance, as a physical trait of the world, has proved elusive; the identification of chances with actual frequencies does not succeed. An adequate theory of chance should explain not only the connection of chance with statistics, but with degrees of belief, and more broadly the entire phenomenology of (seemingly) chance events and their measurement. Branching…▽ MoreThe notion of objective probability or chance, as a physical trait of the world, has proved elusive; the identification of chances with actual frequencies does not succeed. An adequate theory of chance should explain not only the connection of chance with statistics, but with degrees of belief, and more broadly the entire phenomenology of (seemingly) chance events and their measurement. Branching structure in the decoherence-based many worlds theory provides an account of what chance is that satisfies all these desiderata, including the requirement that chance involves uncertainty.△ Less"
Does Infrastructure Investment Lead to Economic Growth or Economic Fragility? Evidence from China,"Authors:Atif Ansar,Bent Flyvbjerg,Alexander Budzier,Daniel Lunn","Abstract:The prevalent view in the economics literature is that a high level of infrastructure investment is a precursor to economic growth. China is especially held up as a model to emulate. Based on the largest dataset of its kind, this paper punctures the twin myths that, first, infrastructure creates economic value, and, second, China has a distinct advantage in its delivery. Far from being an engine o…▽ MoreThe prevalent view in the economics literature is that a high level of infrastructure investment is a precursor to economic growth. China is especially held up as a model to emulate. Based on the largest dataset of its kind, this paper punctures the twin myths that, first, infrastructure creates economic value, and, second, China has a distinct advantage in its delivery. Far from being an engine of economic growth, the typical infrastructure investment fails to deliver a positive risk adjusted return. Moreover, China's track record in delivering infrastructure is no better than that of rich democracies. Where investments are debt-financed, overinvesting in unproductive projects results in the buildup of debt, monetary expansion, instability in financial markets, and economic fragility, exactly as we see in China today. We conclude that poorly managed infrastructure investments are a main explanation of surfacing economic and financial problems in China. We predict that, unless China shifts to a lower level of higher-quality infrastructure investments, the country is headed for an infrastructure-led national financial and economic crisis, which is likely also to be a crisis for the international economy. China's infrastructure investment model is not one to follow for other countries but one to avoid.△ Less"
Exploring quantum teleportation through unitary error bases,Authors:Benjamin Musto,"Abstract:Unitary error bases have a great number of applications across quantum information and quantum computation, and are fundamentally linked to quantum teleportation, dense coding and quantum error correction. Werner's combinatorial construction builds a unitary error basis from a family of Hadamard matrices and a Latin square. In this dissertation, I give a new categorical axiomatisation of Latin squ…▽ MoreUnitary error bases have a great number of applications across quantum information and quantum computation, and are fundamentally linked to quantum teleportation, dense coding and quantum error correction. Werner's combinatorial construction builds a unitary error basis from a family of Hadamard matrices and a Latin square. In this dissertation, I give a new categorical axiomatisation of Latin squares, and use this to give a fully graphical presentation and proof of the correctness of Werner's construction. The categorical approach makes clear that some of the Latin square axioms are unnecessary for the construction to go through, and I propose a generalised construction scheme with the potential to create new classes of unitary error bases.△ Less"
Android Malware Detection Using Parallel Machine Learning Classifiers,"Authors:Suleiman Y. Yerima,Sakir Sezer,Igor Muttik","Abstract:Mobile malware has continued to grow at an alarming rate despite on-going efforts towards mitigating the problem. This has been particularly noticeable on Android due to its being an open platform that has subsequently overtaken other platforms in the share of the mobile smart devices market. Hence, incentivizing a new wave of emerging Android malware sophisticated enough to evade most common dete…▽ MoreMobile malware has continued to grow at an alarming rate despite on-going efforts towards mitigating the problem. This has been particularly noticeable on Android due to its being an open platform that has subsequently overtaken other platforms in the share of the mobile smart devices market. Hence, incentivizing a new wave of emerging Android malware sophisticated enough to evade most common detection methods. This paper proposes and investigates a parallel machine learning based classification approach for early detection of Android malware. Using real malware samples and benign applications, a composite classification model is developed from parallel combination of heterogeneous classifiers. The empirical evaluation of the model under different combination schemes demonstrates its efficacy and potential to improve detection accuracy. More importantly, by utilizing several classifiers with diverse characteristics, their strengths can be harnessed not only for enhanced Android malware detection but also quicker white box analysis by means of the more interpretable constituent classifiers.△ Less"
"""What's (the) Matter?"", A Show on Elementary Particle Physics with 28 Demonstration Experiments","Authors:Herbi K. Dreiner,Max Becker,Mikolaj Borzyszkowski,Maxim Braun,Alexander Faßbender,Julia Hampel,Maike Hansen,Dustin Hebecker,Timo Heepenstrick,Sascha Heinz,Katharina Hortmanns,Christian Jost,Michael Kortmann,Matthias U. Kruckow,Till Leuteritz,Claudia Lütz,Philip Mahlberg,Johannes Müllers,Toby Opferkuch,Ewald Paul,Peter Pauli,Merlin Rossbach,Steffen Schaepe,Tobias Schiffer,Jan F. Schmidt, et al. (4 additional authors not shown)","Abstract:…Switzerland in 2012. Two experimentalists tell them about colliders and our heroes watch live as the Higgs boson is produced and decays. The show was presented in English atOxfordUniversity and University College London, as well as Padua University and ICTP Trieste. It was 1st performed in German at the Deutsche Museum, Bonn (5/'14). The show has elev…▽ MoreWe present the screenplay of a physics show on particle physics, by the Physikshow of Bonn University. The show is addressed at non-physicists aged 14+ and communicates basic concepts of elementary particle physics including the discovery of the Higgs boson in an entertaining fashion. It is also demonstrates a successful outreach activity heavily relying on the university physics students. This paper is addressed at anybody interested in particle physics and/or show physics. This paper is also addressed at fellow physicists working in outreach, maybe the experiments and our choice of simple explanations will be helpful. Furthermore, we are very interested in related activities elsewhere, in particular also demonstration experiments relevant to particle physics, as often little of this work is published.
  Our show involves 28 live demonstration experiments. These are presented in an extensive appendix, including photos and technical details. The show is set up as a quest, where 2 students from Bonn with the aid of a caretaker travel back in time to understand the fundamental nature of matter. They visit Rutherford and Geiger in Manchester around 1911, who recount their famous experiment on the nucleus and show how particle detectors work. They travel forward in time to meet Lawrence at Berkeley around 1950, teaching them about the how and why of accelerators. Next, they visit Wu at DESY, Hamburg, around 1980, who explains the strong force. They end up in the LHC tunnel at CERN, Geneva, Switzerland in 2012. Two experimentalists tell them about colliders and our heroes watch live as the Higgs boson is produced and decays. The show was presented in English atOxfordUniversity and University College London, as well as Padua University and ICTP Trieste. It was 1st performed in German at the Deutsche Museum, Bonn (5/'14). The show has eleven speaking parts and involves in total 20 people.△ Less"
Ghosts! A Location-Based Bluetooth LE Mobile Game for Museum Exploration,"Authors:Tommy Nilsson,Alan Blackwell,Carl Hogsden,David Scruton","Abstract:BLE (Bluetooth Low Energy) is a new wireless communication technology that, thanks to reduced power consumption, promises to facilitate communication between computing devices and help us harness their power in environments and contexts previously untouched by information technology. Museums and other facilities housing various cultural content are a particularly interesting area of application. T…▽ MoreBLE (Bluetooth Low Energy) is a new wireless communication technology that, thanks to reduced power consumption, promises to facilitate communication between computing devices and help us harness their power in environments and contexts previously untouched by information technology. Museums and other facilities housing various cultural content are a particularly interesting area of application. The University of Cambridge Museums consortium has put considerable effort into researching the potential uses of emerging technologies such as BLE to unlock new experiences enriching the way we engage with cultural information. As a part of this research initiative, our ambition has been to examine the challenges and opportunities introduced by the introduction of a BLE-centred system into the museum context. We present an assessment of the potential offered by this technology and of the design approaches that might yield the best results when developing BLE-centred experiences for museum environments. A pivotal part of our project consisted of designing, developing and evaluating a prototype mobile location-based BLE-centred game. A number of technical problems, such as unstable and fluctuating signal strength, were encountered throughout the project lifecycle. Instead of attempting to eliminate such problems, we argued in favour of embracing them and turning them into a cornerstone of the gameplay. Our study suggested that this alternative seamful design approach yields particularly good results when deploying the technology in public environments. The project outcome also demonstrated the potential of BLE-centred solutions to reach out and engage new demographics, especially children, extending their interest in museum visits.△ Less"
TheOxfordOlympics Study 2016: Cost and Cost Overrun at the Games,"Authors:Bent Flyvbjerg,Allison Stewart,Alexander Budzier","Abstract:Given that Olympic Games held over the past decade each have cost USD 8.9 billion on average, the size and financial risks of the Games warrant study. The objectives of theOxfordOlympics study are to (1) establish the actual outturn costs of previous Olympic Games in a manner where cost can consistently be compared across Games; (2) establish cost overruns…▽ MoreGiven that Olympic Games held over the past decade each have cost USD 8.9 billion on average, the size and financial risks of the Games warrant study. The objectives of theOxfordOlympics study are to (1) establish the actual outturn costs of previous Olympic Games in a manner where cost can consistently be compared across Games; (2) establish cost overruns for previous Games, i.e., the degree to which final outturn costs reflect projected budgets at the bid stage, again in a way that allows comparison across Games; (3) test whether the Olympic Games Knowledge Management Program has reduced cost risk for the Games, and, finally, (4) benchmark cost and cost overrun for the Rio 2016 Olympics against previous Games. The main contribution of theOxfordstudy is to establish a phenomenology of cost and cost overrun at the Olympics, which allows consistent and systematic comparison across Games. This has not been done before. The study concludes that for a city and nation to decide to stage the Olympic Games is to decide to take on one of the most costly and financially most risky type of megaproject that exists, something that many cities and nations have learned to their peril.△ Less"
On the Habitability of Our Universe,Authors:Abraham Loeb,"Abstract:Is life most likely to emerge at the present cosmic time near a star like the Sun? We consider the habitability of the Universe throughout cosmic history, and conservatively restrict our attention to the context of ""life as we know it"" and the standard cosmological model, LCDM. The habitable cosmic epoch started shortly after the first stars formed, about 30 Myr after the Big Bang, and will end ab…▽ MoreIs life most likely to emerge at the present cosmic time near a star like the Sun? We consider the habitability of the Universe throughout cosmic history, and conservatively restrict our attention to the context of ""life as we know it"" and the standard cosmological model, LCDM. The habitable cosmic epoch started shortly after the first stars formed, about 30 Myr after the Big Bang, and will end about 10 Tyr from now, when all stars will die. We review the formation history of habitable planets and find that unless habitability around low mass stars is suppressed, life is most likely to exist near 0.1 solar mass stars ten trillion years from now. Spectroscopic searches for biosignatures in the atmospheres of transiting Earth-mass planets around low mass stars will determine whether present-day life is indeed premature or typical from a cosmic perspective.△ Less"
Quench dynamics in integrable systems,Authors:Natan Andrei,"Abstract:These notes cover in some detail lectures I gave at the Les Houches Summer School 2012. I describe here work done with Deepak Iyer with important contributions from Hujie Guan. I discuss some aspects of the physics revealed by quantum quenches and present a formalism for studying the quench dynamics of integrable systems. The formalism presented generalizes an approach by Yudson and is applied to…▽ MoreThese notes cover in some detail lectures I gave at the Les Houches Summer School 2012. I describe here work done with Deepak Iyer with important contributions from Hujie Guan. I discuss some aspects of the physics revealed by quantum quenches and present a formalism for studying the quench dynamics of integrable systems. The formalism presented generalizes an approach by Yudson and is applied to Lieb-Liniger model which describes a gas of $N$ interacting bosons moving on the continuous infinite line while interacting via a short range potential. We carry out the quench from several initial states and for any number of particles and compute the evolution of the density and the noise correlations. In the long time limit the system dilutes and we find that for any value of repulsive coupling independently of the initial state the system asymptotes towards astrongly repulsive gas, while for any value of attractive coupling, the system forms a maximal bound state that dominates at longer times. In either case the system equilibrates but does not thermalize, an effect that is consistent with prethermalization. These results can be confronted with experiments. For much more detail see: Phys. Rev. A 87, 053628 (2013) on which these notes are based. Further applications of the approach to the Heisenberg model and to the Anderson model will be presented elsewhere.△ Less"
Using Sequence Ensembles for Seeding Alignments of MinION Sequencing Data,"Authors:Rastislav Rabatin,Broňa Brejová,Tomáš Vinař","Abstract:OxfordNanopore MinION sequencer is currently the smallest sequencing device available. While being able to produce very long reads (reads of up to 100~kbp were reported), it is prone to high sequencing error rates of up to 30%. Since most of these errors are insertions or deletions, it is very difficult to adapt popular seed-based algorithms designed for al…▽ MoreOxfordNanopore MinION sequencer is currently the smallest sequencing device available. While being able to produce very long reads (reads of up to 100~kbp were reported), it is prone to high sequencing error rates of up to 30%. Since most of these errors are insertions or deletions, it is very difficult to adapt popular seed-based algorithms designed for aligning data sets with much lower error rates.
  Base calling of MinION reads is typically done using hidden Markov models. In this paper, we propose to represent each sequencing read by an ensemble of sequences sampled from such a probabilistic model. This approach can improve the sensitivity and false positive rate of seeding an alignment compared to using a single representative base call sequence for each read.△ Less"
Founded Semantics and Constraint Semantics of Logic Rules,"Authors:Yanhong A. Liu,Scott D. Stoller","Abstract:Logic rules and inference are fundamental in computer science and have been studied extensively. However, prior semantics of logic languages can have subtle implications and can disagree significantly, on even very simple programs, including in attempting to solve the well-known Russell's paradox. These semantics are often non-intuitive and hard-to-understand when unrestricted negation is used in…▽ MoreLogic rules and inference are fundamental in computer science and have been studied extensively. However, prior semantics of logic languages can have subtle implications and can disagree significantly, on even very simple programs, including in attempting to solve the well-known Russell's paradox. These semantics are often non-intuitive and hard-to-understand when unrestricted negation is used in recursion.
  This paper describes a simple new semantics for logic rules, founded semantics, and its straightforward extension to another simple new semantics, constraint semantics, that unify the core of different prior semantics. The new semantics support unrestricted negation, as well as unrestricted existential and universal quantifications. They are uniquely expressive and intuitive by allowing assumptions about the predicates, rules, and reasoning to be specified explicitly, as simple and precise binary choices. They are completely declarative and relate cleanly to prior semantics. In addition, founded semantics can be computed in linear time in the size of the ground program.△ Less"
Stable polarized del Pezzo surfaces,"Authors:Ivan Cheltsov,Jesus Martinez-Garcia",Abstract:We give a simple sufficient condition for K-stability of polarized del Pezzo surfaces and for the existence of a constant scalar curvature Kahler metric in the Kahler class corresponding to the polarization.We give a simple sufficient condition for K-stability of polarized del Pezzo surfaces and for the existence of a constant scalar curvature Kahler metric in the Kahler class corresponding to the polarization.△ Less
Drug response prediction by inferring pathway-response associations with Kernelized Bayesian Matrix Factorization,"Authors:Muhammad Ammad-ud-din,Suleiman A. Khan,Disha Malani,Astrid Murumägi,Olli Kallioniemi,Tero Aittokallio,Samuel Kaski","Abstract:A key goal of computational personalized medicine is to systematically utilize genomic and other molecular features of samples to predict drug responses for a previously unseen sample. Such predictions are valuable for developing hypotheses for selecting therapies tailored for individual patients. This is especially valuable in oncology, where molecular and genetic heterogeneity of the cells has a…▽ MoreA key goal of computational personalized medicine is to systematically utilize genomic and other molecular features of samples to predict drug responses for a previously unseen sample. Such predictions are valuable for developing hypotheses for selecting therapies tailored for individual patients. This is especially valuable in oncology, where molecular and genetic heterogeneity of the cells has a major impact on the response. However, the prediction task is extremely challenging, raising the need for methods that can effectively model and predict drug responses. In this study, we propose a novel formulation of multi-task matrix factorization that allows selective data integration for predicting drug responses. To solve the modeling task, we extend the state-of-the-art kernelized Bayesian matrix factorization (KBMF) method with component-wise multiple kernel learning. In addition, our approach exploits the known pathway information in a novel and biologically meaningful fashion to learn the drug response associations. Our method quantitatively outperforms the state of the art on predicting drug responses in two publicly available cancer data sets as well as on a synthetic data set. In addition, we validated our model predictions with lab experiments using an in-house cancer cell line panel. We finally show the practical applicability of the proposed method by utilizing prior knowledge to infer pathway-drug response associations, opening up the opportunity for elucidating drug action mechanisms. We demonstrate that pathway-response associations can be learned by the proposed model for the well known EGFR and MEK inhibitors.△ Less"
When Does a Boltzmannian Equilibrium Exist?,"Authors:Charlotte Werndl,Roman Frigg","Abstract:The received wisdom in statistical mechanics is that isolated systems, when left to themselves, approach equilibrium. But under what circumstances does an equilibrium state exist and an approach to equilibrium take place? In this paper we address these questions from the vantage point of the long-run fraction of time definition of Boltzmannian equilibrium that we developed in two recent papers (We…▽ MoreThe received wisdom in statistical mechanics is that isolated systems, when left to themselves, approach equilibrium. But under what circumstances does an equilibrium state exist and an approach to equilibrium take place? In this paper we address these questions from the vantage point of the long-run fraction of time definition of Boltzmannian equilibrium that we developed in two recent papers (Werndl and Frigg 2015a, 2015b). After a short summary of Boltzmannian statistical mechanics and our definition of equilibrium, we state an existence theorem which provides general criteria for the existence of an equilibrium state. We first illustrate how the theorem works with a toy example, which allows us to illustrate the various elements of the theorem in a simple setting. After commenting on the ergodic programme, we discuss equilibria in a number of different gas systems: the ideal gas, the dilute gas, the Kac gas, the stadium gas, the mushroom gas and the multi-mushroom gas. In the conclusion we briefly summarise the main points and highlight open questions.△ Less"
Tripartite Unions,Authors:Nachum Dershowitz,Abstract:This note provides conditions under which the union of three well-founded binary relations is also well-founded.This note provides conditions under which the union of three well-founded binary relations is also well-founded.△ Less
Extraction of clinical information from the non-invasive fetal electrocardiogram,Authors:Joachim Behar,"Abstract:Estimation of the fetal heart rate (FHR) has gained interest in the last century, low heart rate variability has been studied to identify intrauterine growth restricted fetuses (prepartum), and abnormal FHR patterns have been associated with fetal distress during delivery (intrapartum). Several monitoring techniques have been proposed for FHR estimation, including auscultation and Doppler ultrasou…▽ MoreEstimation of the fetal heart rate (FHR) has gained interest in the last century, low heart rate variability has been studied to identify intrauterine growth restricted fetuses (prepartum), and abnormal FHR patterns have been associated with fetal distress during delivery (intrapartum). Several monitoring techniques have been proposed for FHR estimation, including auscultation and Doppler ultrasound. This thesis focuses on the extraction of the non-invasive fetal electrocardiogram (NI-FECG) recorded from a limited set of abdominal sensors. The main challenge with NI-FECG extraction techniques is the low signal-to-noise ratio of the FECG signal on the abdominal mixture signal which consists of a dominant maternal ECG component, FECG and noise. However the NI-FECG offers many advantages over the alternative fetal monitoring techniques, the most important one being the opportunity to enable morphological analysis of the FECG which is vital for determining whether an observed FHR event is normal or pathological. In order to advance the field of NI-FECG signal processing, the development of standardised public databases and benchmarking of a number of published and novel algorithms was necessary.△ Less"
"Applications of Probabilistic Programming (Master's thesis, 2015)",Authors:Yura N Perov,"Abstract:This thesis describes work on two applications of probabilistic programming: the learning of probabilistic program code given specifications, in particular program code of one-dimensional samplers; and the facilitation of sequential Monte Carlo inference with help of data-driven proposals. The latter is presented with experimental results on a linear Gaussian model and a non-parametric dependent D…▽ MoreThis thesis describes work on two applications of probabilistic programming: the learning of probabilistic program code given specifications, in particular program code of one-dimensional samplers; and the facilitation of sequential Monte Carlo inference with help of data-driven proposals. The latter is presented with experimental results on a linear Gaussian model and a non-parametric dependent Dirichlet process mixture of objects model for object recognition and tracking.
  In Chapter 1 we provide a brief introduction to probabilistic programming.
  In Chapter 2 we present an approach to automatic discovery of samplers in the form of probabilistic programs. We formulate a Bayesian approach to this problem by specifying a grammar-based prior over probabilistic program code. We use an approximate Bayesian computation method to learn the programs, whose executions generate samples that statistically match observed data or analytical characteristics of distributions of interest. In our experiments we leverage different probabilistic programming systems to perform Markov chain Monte Carlo sampling over the space of programs. Experimental results have demonstrated that, using the proposed methodology, we can learn approximate and even some exact samplers. Finally, we show that our results are competitive with regard to genetic programming methods.
  In Chapter 3, we describe a way to facilitate sequential Monte Carlo inference in probabilistic programming using data-driven proposals. In particular, we develop a distance-based proposal for the non-parametric dependent Dirichlet process mixture of objects model. We implement this approach in the probabilistic programming system Anglican, and show that for that model data-driven proposals provide significant performance improvements. We also explore the possibility of using neural networks to improve data-driven proposals.△ Less"
"Riemann hypothesis equivalences, Robin inequality, Lagarias criterion, and Riemann hypothesis",Authors:Ahmad Sabihi,"Abstract:In this paper, we briefly review most of accomplished research in Riemann Zeta function and Riemann hypothesis since Riemann's age including Riemann hypothesis equivalences as well. We then make use of Robin and Lagarias' criteria to prove Riemann hypothesis. The goal is, using Lagarias criterion for $n\geq 1$ since Lagarias criterion states that Riemann hypothesis holds if and only if the inequal…▽ MoreIn this paper, we briefly review most of accomplished research in Riemann Zeta function and Riemann hypothesis since Riemann's age including Riemann hypothesis equivalences as well. We then make use of Robin and Lagarias' criteria to prove Riemann hypothesis. The goal is, using Lagarias criterion for $n\geq 1$ since Lagarias criterion states that Riemann hypothesis holds if and only if the inequality $\sum_{d|n}d\leq H_{n}+\exp(H_{n})\log(H_{n})$ holds for all $n\geq 1$. Although, Robin's criterion is used as well. Our approach breaks up the set of the natural numbers into three main subsets. The first subset is $\{n\in \mathbb{N}| ~ 1\leq n\leq 5040\}$. The second one is $\{n\in \mathbb{N}| ~ 5041\leq n\leq 19685\}$ and the third one is $\{n\in \mathbb{N}| ~ n\geq 19686\}$. In our proof, the third subset for even integers is broken up into odd integer class number sets. Then, mathematical arguments are stated for each odd integer class number set. Odd integer class number set is introduced in this paper. Since the Lagarias criterion holds for the first subset regarding computer aided computations, we do prove it using both Lagarias and Robin's criteria for the second and third subsets and mathematical arguments accompanied by a large volume of computer language programs. It then follows that Riemann hypothesis holds as well.△ Less"
Evidence for Large-Scale Subsurface Convection in the Sun,Authors:M. F. Woodard,"Abstract:A helioseismic statistical waveform analysis of subsurface flow was performed on two 720-day time series of SOHO/MDI Medium-l spherical-harmonic coefficients. The time series coincide with epochs of high and low solar activity. Time-dependent coupling-strength coefficients b(s,t;n,l) of modes of the same radial order n and degree l, but different azimuthal order m, were inferred from the waveform…▽ MoreA helioseismic statistical waveform analysis of subsurface flow was performed on two 720-day time series of SOHO/MDI Medium-l spherical-harmonic coefficients. The time series coincide with epochs of high and low solar activity. Time-dependent coupling-strength coefficients b(s,t;n,l) of modes of the same radial order n and degree l, but different azimuthal order m, were inferred from the waveform analysis. These coefficients are sensitive to flows and general aspherical structure. For odd values of s << l, the coefficient b(s,t;n,l) measures an average over depth of the amplitude of one spherical-harmonic (s,t) component of the toroidal flow velocity field. The depth-dependent weighting function defining the average velocity is the fractional kinetic energy density in radius of modes of the (n,l) multiplet. A mean-square (n,l)-dependent flow velocity was inferred from the b-coefficients for s in the range 5 through 35 for each n and l in the respective ranges 1 through 5 and 120 through 149 for the epochs of high and low activity. A further averaging, over l, yielded a root mean square flow velocity as a function of n for each epoch, which average increases from about 20 m/s at n=1 to 35 m/s at n=5. The inferred velocities are consistent with (though perhaps do not demand) a cellular pattern of flow extending over the vertical range of mode sensitivity, estimated to be about four percent of the solar radius below the photosphere.△ Less"
Fourier Transform Ultrasound Spectroscopy For the Determination of Wave Propagation Parameters,Authors:Barnana Pal,"Abstract:…are captured. The Fast Fourier Transform (FFT) components of the echo signals are taken to compute k, α and r, the reflection constant at the boundary, using Oak Ridge andOxfordmethod. The results are compared with existing literature values.▽ MoreThe reported results for ultrasonic wave attenuation constant (α) in pure water show noticeable inconsistency in magnitude. A ""Propagating-Wave"" model analysis of the most popular pulse-echo technique indicates that this is a consequence of the inherent wave propagation characteristics in a bounded medium. In the present work Fourier Transform Ultrasound Spectroscopy (FTUS) is adopted to determine ultrasonic wave propagation parameters, the wave number (k) and attenuation constant (α) at 1MHz frequency in tri-distilled water at room temperature (25oC). Pulse-echo signals obtained under same experimental conditions regarding the exciting input signal and reflecting boundary wall of the water container for various lengths of water columns are captured. The Fast Fourier Transform (FFT) components of the echo signals are taken to compute k, α and r, the reflection constant at the boundary, using Oak Ridge andOxfordmethod. The results are compared with existing literature values.△ Less"
Faster R-CNN Features for Instance Search,"Authors:Amaia Salvador,Xavier Giro-i-Nieto,Ferran Marques,Shin'ichi Satoh","Abstract:…the suitability of Faster R-CNN features when the network is fine-tuned for the same objects one wants to retrieve. We assess the performance of our proposed system with theOxfordBuildings 5k, Paris Buildings 6k and a subset of TRECVid Instance Search 2013, achieving competitive results.▽ MoreImage representations derived from pre-trained Convolutional Neural Networks (CNNs) have become the new state of the art in computer vision tasks such as instance retrieval. This work explores the suitability for instance retrieval of image- and region-wise representations pooled from an object detection CNN such as Faster R-CNN. We take advantage of the object proposals learned by a Region Proposal Network (RPN) and their associated CNN features to build an instance search pipeline composed of a first filtering stage followed by a spatial reranking. We further investigate the suitability of Faster R-CNN features when the network is fine-tuned for the same objects one wants to retrieve. We assess the performance of our proposed system with theOxfordBuildings 5k, Paris Buildings 6k and a subset of TRECVid Instance Search 2013, achieving competitive results.△ Less"
Bags of Local Convolutional Features for Scalable Instance Search,"Authors:Eva Mohedano,Amaia Salvador,Kevin McGuinness,Ferran Marques,Noel E. O'Connor,Xavier Giro-i-Nieto","Abstract:…used for query expansion. We demonstrate the suitability of the BoW representation based on local CNN features for instance retrieval, achieving competitive performance on theOxfordand Paris buildings benchmarks. We show that our proposed system for CNN feature aggregation with BoW outperforms state-of-the-art techniques using sum pooling at a subset of th…▽ MoreThis work proposes a simple instance retrieval pipeline based on encoding the convolutional features of CNN using the bag of words aggregation scheme (BoW). Assigning each local array of activations in a convolutional layer to a visual word produces an \textit{assignment map}, a compact representation that relates regions of an image with a visual word. We use the assignment map for fast spatial reranking, obtaining object localizations that are used for query expansion. We demonstrate the suitability of the BoW representation based on local CNN features for instance retrieval, achieving competitive performance on theOxfordand Paris buildings benchmarks. We show that our proposed system for CNN feature aggregation with BoW outperforms state-of-the-art techniques using sum pooling at a subset of the challenging TRECVid INS benchmark.△ Less"
Conversational flow inOxford-style debates,"Authors:Justine Zhang,Ravi Kumar,Sujith Ravi,Cristian Danescu-Niculescu-Mizil","Abstract:…views on important issues. In this work we propose a methodology for tracking how ideas flow between participants throughout a debate. We use this approach in a case study ofOxford-style debates---a competitive format where the winner is determined by audience votes---and show how the outcome of a debate depends on aspects of conversational flow. In particu…▽ MorePublic debates are a common platform for presenting and juxtaposing diverging views on important issues. In this work we propose a methodology for tracking how ideas flow between participants throughout a debate. We use this approach in a case study ofOxford-style debates---a competitive format where the winner is determined by audience votes---and show how the outcome of a debate depends on aspects of conversational flow. In particular, we find that winners tend to make better use of a debate's interactive component than losers, by actively pursuing their opponents' points rather than promoting their own ideas over the course of the conversation.△ Less"
Accurate selfcorrection of errors in long reads using de Bruijn graphs,"Authors:Leena Salmela,Riku Walve,Eric Rivals,Esko Ukkonen","Abstract:New long read sequencing technologies, like PacBio SMRT andOxfordNanoPore, can produce sequencing reads up to 50,000 bp long but with an error rate of at least 15%. Reducing the error rate is necessary for subsequent utilisation of the reads in, e.g., de novo genome assembly. The error correction problem has been tackled either by aligning the long reads a…▽ MoreNew long read sequencing technologies, like PacBio SMRT andOxfordNanoPore, can produce sequencing reads up to 50,000 bp long but with an error rate of at least 15%. Reducing the error rate is necessary for subsequent utilisation of the reads in, e.g., de novo genome assembly. The error correction problem has been tackled either by aligning the long reads against each other or by a hybrid approach that uses the more accurate short reads produced by second generation sequencing technologies to correct the long reads. We present an error correction method that uses long reads only. The method consists of two phases: first we use an iterative alignment-free correction method based on de Bruijn graphs with increasing length of k-mers, and second, the corrected reads are further polished using long-distance dependencies that are found using multiple alignments. According to our experiments the proposed method is the most accurate one relying on long reads only for read sets with high coverage. Furthermore, when the coverage of the read set is at least 75x, the throughput of the new method is at least 20% higher. LoRMA is freely available at http://www.cs.helsinki.fi/u/lmsalmel/LoRMA/.△ Less"
Fine's Theorem on First-Order Complete Modal Logics,Authors:Robert Goldblatt,"Abstract:Fine's influential Canonicity Theorem states that if a modal logic is determined by a first-order definable class of Kripke frames, then it is valid in its canonical frames. This article reviews the background and context of this result, and the history of its impact on further research. It then develops a new characterisation of when a logic is canonically valid, providing a precise point of dist…▽ MoreFine's influential Canonicity Theorem states that if a modal logic is determined by a first-order definable class of Kripke frames, then it is valid in its canonical frames. This article reviews the background and context of this result, and the history of its impact on further research. It then develops a new characterisation of when a logic is canonically valid, providing a precise point of distinction with the property of first-order completeness. The ultimate point is that the construction of the canonical frame of a modal algebra does not commute with the ultrapower construction.△ Less"
Marr Revisited: 2D-3D Alignment via Surface Normal Prediction,"Authors:Aayush Bansal,Bryan Russell,Abhinav Gupta","Abstract:…to the success of our approach is the ability to recover accurate surface normals for objects in the depicted scene. We introduce a skip-network model built on the pre-trainedOxfordVGG convolutional neural network (CNN) for surface normal prediction. Our model achieves state-of-the-art accuracy on the NYUv2 RGB-D dataset for surface normal prediction, and…▽ MoreWe introduce an approach that leverages surface normal predictions, along with appearance cues, to retrieve 3D models for objects depicted in 2D still images from a large CAD object library. Critical to the success of our approach is the ability to recover accurate surface normals for objects in the depicted scene. We introduce a skip-network model built on the pre-trainedOxfordVGG convolutional neural network (CNN) for surface normal prediction. Our model achieves state-of-the-art accuracy on the NYUv2 RGB-D dataset for surface normal prediction, and recovers fine object detail compared to previous methods. Furthermore, we develop a two-stream network over the input image and predicted surface normals that jointly learns pose and style for CAD model retrieval. When using the predicted surface normals, our two-stream network matches prior work using surface normals computed from RGB-D images on the task of pose prediction, and achieves state of the art when using RGB-D input. Finally, our two-stream network allows us to retrieve CAD models that better match the style and pose of a depicted object compared with baseline approaches.△ Less"
"Strong Gravitational Lenses and Multi-Wavelength Galaxy Surveys with AKARI, Herschel, SPICA and Euclid",Authors:Stephen Serjeant,"Abstract:Submillimetre and millimetre-wave surveys with Herschel and the South Pole Telescope have revolutionised the discovery of strong gravitational lenses. Their follow-ups have been greatly facilitated by the multi-wavelength supplementary data in the survey fields. The forthcoming Euclid optical/near-infrared space telescope will also detect strong gravitational lenses in large numbers, and orbital c…▽ MoreSubmillimetre and millimetre-wave surveys with Herschel and the South Pole Telescope have revolutionised the discovery of strong gravitational lenses. Their follow-ups have been greatly facilitated by the multi-wavelength supplementary data in the survey fields. The forthcoming Euclid optical/near-infrared space telescope will also detect strong gravitational lenses in large numbers, and orbital constraints are likely to require placing its deep survey at the North Ecliptic Pole (the natural deep field for a wide class of ground-based and space-based observatories including AKARI, JWST and SPICA). In this paper I review the current status of the multi-wavelength survey coverage in the NEP, and discuss the prospects for the detection of strong gravitational lenses in forthcoming or proposed facilities such as Euclid, FIRSPEX and SPICA.△ Less"
DeepNano: Deep Recurrent Neural Networks for Base Calling in MinION Nanopore Reads,"Authors:Vladimír Boža,Broňa Brejová,Tomáš Vinař","Abstract:Motivation: The MinION device byOxfordNanopore is the first portable sequencing device. MinION is able to produce very long reads (reads over 100~kBp were reported), however it suffers from high sequencing error rate. In this paper, we show that the error rate can be reduced by improving the base calling process.
  Results: We present the first open-source…▽ MoreMotivation: The MinION device byOxfordNanopore is the first portable sequencing device. MinION is able to produce very long reads (reads over 100~kBp were reported), however it suffers from high sequencing error rate. In this paper, we show that the error rate can be reduced by improving the base calling process.
  Results: We present the first open-source DNA base caller for the MinION sequencing platform byOxfordNanopore. By employing carefully crafted recurrent neural networks, our tool improves the base calling accuracy compared to the default base caller supplied by the manufacturer. This advance may further enhance applicability of MinION for genome sequencing and various clinical applications.
  Availability: DeepNano can be downloaded at http://compbio.fmph.uniba.sk/deepnano/.
  Contact: boza@fmph.uniba.sk△ Less"
Reduced Perplexity: A simplified perspective on assessing probabilistic forecasts,Authors:Kenric P. Nelson,"Abstract:A simple, intuitive approach to the assessment of probabilistic inferences is introduced. The Shannon information metrics are translated to the probability domain. The translation shows that the negative logarithmic score and the geometric mean are equivalent measures of the accuracy of a probabilistic inference. Thus there is both a quantitative reduction in perplexity, which is the inverse of th…▽ MoreA simple, intuitive approach to the assessment of probabilistic inferences is introduced. The Shannon information metrics are translated to the probability domain. The translation shows that the negative logarithmic score and the geometric mean are equivalent measures of the accuracy of a probabilistic inference. Thus there is both a quantitative reduction in perplexity, which is the inverse of the geometric mean of the probabilities, as good inference algorithms reduce the uncertainty and a qualitative reduction due to the increased clarity between the original set of probabilistic forecasts and their central tendency, the geometric mean. Further insight is provided by showing that the Rényi and Tsallis entropy functions translated to the probability domain are both the weighted generalized mean of the distribution. The generalized mean of probabilistic forecasts forms a spectrum of performance metrics referred to as a Risk Profile. The arithmetic mean is used to measure the decisiveness, while the -2/3 mean is used to measure the robustness.△ Less"
Gyrokinetic simulations of fusion plasmas using a spectral velocity space representation,Authors:Joseph Thomas Parker,"Abstract:Magnetic confinement fusion reactors suffer severely from heat and particle losses through turbulent transport, which has inspired the construction of ever larger and more expensive reactors. Numerical simulations are vital to their design and operation, but particle collisions are too infrequent for fluid descriptions to be valid. Instead, strongly magnetised fusion plasmas are described by the g…▽ MoreMagnetic confinement fusion reactors suffer severely from heat and particle losses through turbulent transport, which has inspired the construction of ever larger and more expensive reactors. Numerical simulations are vital to their design and operation, but particle collisions are too infrequent for fluid descriptions to be valid. Instead, strongly magnetised fusion plasmas are described by the gyrokinetic equations, a nonlinear integro-differential system for evolving the particle distribution functions in a five-dimensional position and velocity space, and the consequent electromagnetic field. Due to the high dimensionality, simulations of small reactor sections require hundreds of thousands of CPU hours on High Performance Computing platforms.
  We develop a Hankel-Hermite spectral representation for velocity space that exploits structural features of the gyrokinetic system. The representation exactly conserves discrete free energy in the absence of explicit dissipation, while our Hermite hypercollision operator captures Landau damping with few variables. Calculation of the electromagnetic fields becomes purely local, eliminating inter-processor communication in, and vastly accelerating, searches for linear instabilities. We implement these ideas in SpectroGK, an efficient parallel code.
  Turbulent fusion plasmas may dissipate free energy through linear phase mixing to fine scales in velocity space, as in Landau damping, or through a nonlinear cascade to fine scales in physical space, as in hydrodynamic turbulence. Using SpectroGK to study saturated electrostatic drift-kinetic turbulence, we find that the nonlinear cascade suppresses linear phase mixing at energetically-dominant scales, so the turbulence is fluid-like. We use this observation to derive Fourier-Hermite spectra for the electrostatic potential and distribution function, and confirm these spectra with simulations.△ Less"
A modified implementation of MINRES to monitor residual subvector norms for block systems,"Authors:Roland Herzog,Kirk M. Soodhalter","Abstract:…MINRES is derived which allows to monitor the norms of the subvectors individually. Compared to the implementation from the textbook of [Elman, Sylvester and Wathen,OxfordUniversity Press, 2014], our method requires one extra vector of storage and no additional applications of the preconditioner. Numerical experiments are included.▽ MoreSaddle-point systems, i.e., structured linear systems with symmetric matrices are considered. A modified implementation of (preconditioned) MINRES is derived which allows to monitor the norms of the subvectors individually. Compared to the implementation from the textbook of [Elman, Sylvester and Wathen,OxfordUniversity Press, 2014], our method requires one extra vector of storage and no additional applications of the preconditioner. Numerical experiments are included.△ Less"
"Random Matrices, Boundaries and Branes",Authors:Benjamin Niedner,"Abstract:This thesis is devoted to the application of random matrix theory to the study of random surfaces, both discrete and continuous; special emphasis is placed on surface boundaries and the associated boundary conditions in this formalism. In particular, using a multi-matrix integral with permutation symmetry, we are able to calculate the partition function of the Potts model on a random planar lattic…▽ MoreThis thesis is devoted to the application of random matrix theory to the study of random surfaces, both discrete and continuous; special emphasis is placed on surface boundaries and the associated boundary conditions in this formalism. In particular, using a multi-matrix integral with permutation symmetry, we are able to calculate the partition function of the Potts model on a random planar lattice with various boundary conditions imposed. We proceed to investigate the correspondence between the critical points in the phase diagram of this model and two-dimensional Liouville theory coupled to conformal field theories with global $\mathcal{W}$-symmetry. In this context, each boundary condition can be interpreted as the description of a brane in a family of bosonic string backgrounds. This investigation suggests that a spectrum of initially distinct boundary conditions of a given system may become degenerate when the latter is placed on a random surface of bounded genus, effectively leaving a smaller set of independent boundary conditions. This curious and much-debated feature is then further scrutinised by considering the double scaling limit of a two-matrix integral. For this model, we can show explicitly how this apparent degeneracy is in fact resolved by accounting for contributions invisible in string perturbation theory. Altogether, these developments provide novel descriptions of hitherto unexplored boundary conditions as well as new insights into the non-perturbative physics of boundaries and branes.△ Less"
Crowdsourcing On-street Parking Space Detection,"Authors:Ruizhi Liao,Cristian Roman,Peter Ball,Shumao Ou,Liping Chen","Abstract:…It is mounted on the passenger side of a car to measure the distance from the vehicle to the nearest roadside obstacle. Multiple road tests are conducted around Wheatley,Oxfordto gather results and emulate the crowdsourcing approach. By extracting parked vehicles' features from the collected trace, a supervised learning algorithm is developed to estim…▽ MoreAs the number of vehicles continues to grow, parking spaces are at a premium in city streets. Additionally, due to the lack of knowledge about street parking spaces, heuristic circling the blocks not only costs drivers' time and fuel, but also increases city congestion. In the wake of recent trend to build convenient, green and energy-efficient smart cities, we rethink common techniques adopted by high-profile smart parking systems, and present a user-engaged (crowdsourcing) and sonar-based prototype to identify urban on-street parking spaces. The prototype includes an ultrasonic sensor, a GPS receiver and associated Arduino micro-controllers. It is mounted on the passenger side of a car to measure the distance from the vehicle to the nearest roadside obstacle. Multiple road tests are conducted around Wheatley,Oxfordto gather results and emulate the crowdsourcing approach. By extracting parked vehicles' features from the collected trace, a supervised learning algorithm is developed to estimate roadside parking occupancy and spot illegal parking vehicles. A quantity estimation model is derived to calculate the required number of sensing units to cover urban streets. The estimation is quantitatively compared to a fixed sensing solution. The results show that the crowdsourcing way would need substantially fewer sensors compared to the fixed sensing system.△ Less"
Completeness and the ZX-calculus,Authors:Miriam Backens,"Abstract:Graphical languages offer intuitive and rigorous formalisms for quantum physics. They can be used to simplify expressions, derive equalities, and do computations. Yet in order to replace conventional formalisms, rigour alone is not sufficient: the new formalisms also need to have equivalent deductive power. This requirement is captured by the property of completeness, which means that any equality…▽ MoreGraphical languages offer intuitive and rigorous formalisms for quantum physics. They can be used to simplify expressions, derive equalities, and do computations. Yet in order to replace conventional formalisms, rigour alone is not sufficient: the new formalisms also need to have equivalent deductive power. This requirement is captured by the property of completeness, which means that any equality that can be derived using some standard formalism can also be derived graphically.
  In this thesis, I consider the ZX-calculus, a graphical language for pure state qubit quantum mechanics. I show that it is complete for pure state stabilizer quantum mechanics, so any problem within this fragment of quantum theory can be fully analysed using graphical methods. This includes questions of central importance in areas such as error-correcting codes or measurement-based quantum computation. Furthermore, I show that the ZX-calculus is complete for the single-qubit Clifford+T group, which is approximately universal: any single-qubit unitary can be approximated to arbitrary accuracy using only Clifford gates and the T-gate. [...] Lastly, I extend the use of rigorous graphical languages outside quantum theory to Spekkens' toy theory, a local hidden variable model that nevertheless exhibits some features commonly associated with quantum mechanics. [...] I develop a graphical calculus similar to the ZX-calculus that fully describes Spekkens' toy theory, and show that it is complete. Hence, stabilizer quantum mechanics and Spekkens' toy theory can be fully analysed and compared using graphical formalisms.
  Intuitive graphical languages can replace conventional formalisms for the analysis of many questions in quantum computation and foundations without loss of mathematical rigour or deductive power.△ Less"
Systematic reviews in paediatric multiple sclerosis and Creutzfeldt-Jakob disease exemplify shortcomings in methods used to evaluate therapies in rare conditions,"Authors:Steffen Unkel,Christian Röver,Nigel Stallard,Norbert Benda,Martin Posch,Sarah Zohar,Tim Friede","Abstract:…The observational studies are before-after studies or controlled studies. Three of the seven selected studies on CJD are RCTs, of which two received the maximum mark on theOxfordQuality Scale. Four trials are controlled observational studies.
  CONCLUSIONS: Evidence from double-blind RCTs on the efficacy of treatments appears to be variable between rare d…▽ MoreBACKGROUND: Randomized controlled trials (RCTs) are the gold standard design of clinical research to assess interventions. However, RCTs cannot always be applied for practical or ethical reasons. To investigate the current practices in rare diseases, we review evaluations of therapeutic interventions in paediatric multiple sclerosis (MS) and Creutzfeldt-Jakob disease (CJD). In particular, we shed light on the endpoints used, the study designs implemented and the statistical methodologies applied.
  METHODS: We conducted literature searches to identify relevant primary studies. Data on study design, objectives, endpoints, patient characteristics, randomization and masking, type of intervention, control, withdrawals and statistical methodology were extracted from the selected studies. The risk of bias and the quality of the studies were assessed.
  RESULTS: Twelve (seven) primary studies on paediatric MS (CJD) were included in the qualitative synthesis. No double-blind, randomized placebo-controlled trial for evaluating interventions in paediatric MS has been published yet. Evidence from one open-label RCT is available. The observational studies are before-after studies or controlled studies. Three of the seven selected studies on CJD are RCTs, of which two received the maximum mark on theOxfordQuality Scale. Four trials are controlled observational studies.
  CONCLUSIONS: Evidence from double-blind RCTs on the efficacy of treatments appears to be variable between rare diseases. With regard to paediatric conditions it remains to be seen what impact regulators will have through e.g., paediatric investigation plans. Overall, there is space for improvement by using innovative trial designs and data analysis techniques.△ Less"
On the validity of power functionals for the homogeneous electron gas in reduced.density-matrix-functional theory,"Authors:A. Putaja,F. G. Eich,T. Baldsiefen,E. Rasanen","Abstract:…to yield physically sound solutions that satisfy the Lieb-Oxfordlower bound for the exchange-correlation energy and exclude pinned states with the condition $n({\mathbf k})<1$ for all wave vectors ${\mathbf k}$. The results refine the constraints previously obtained from trial momentum distributions. We also compute the values for $α$ that yield the exa…▽ MorePhysically valid and numerically efficient approximations for the exchange and correlation energy are critical for reduced density-matrix functional theory to become a widely used method in electronic structure calculations. Here we examine the physical limits of power functionals of the form $f(n,n')=(n n')^α$ for the scaling function in the exchange-correlation energy. To this end we obtain numerically the minimizing momentum distributions for the three- and two-dimensional homogeneous electron gas, respectively. In particular, we examine the limiting values for the power $α$ to yield physically sound solutions that satisfy the Lieb-Oxfordlower bound for the exchange-correlation energy and exclude pinned states with the condition $n({\mathbf k})<1$ for all wave vectors ${\mathbf k}$. The results refine the constraints previously obtained from trial momentum distributions. We also compute the values for $α$ that yield the exact correlation energy and its kinetic part for both the three- and two-dimensional electron gas. In both systems, narrow regimes of validity and accuracy are found at $α\gtrsim 0.6$ and at $r_s\gtrsim 10$ for the density parameter, corresponding to relatively low densities.△ Less"
"Bachelor's thesis on generative probabilistic programming (in Russian language, June 2014)",Authors:Yura N Perov,"Abstract:…Krasnoyarsk, Russia. The work, which is described in this thesis, has been performing in 2012-2014 in the Massachusetts Institute of Technology and in the University ofOxfordby the colleagues of the author and by himself.▽ MoreThis Bachelor's thesis, written in Russian, is devoted to a relatively new direction in the field of machine learning and artificial intelligence, namely probabilistic programming. The thesis gives a brief overview to the already existing probabilistic programming languages: Church, Venture, and Anglican. It also describes the results of the first experiments on the automatic induction of probabilistic programs. The thesis was submitted, in June 2014, in partial fulfilment of the requirements for the degree of Bachelor of Science in Mathematics in the Department of Mathematics and Computer Science, Siberian Federal University, Krasnoyarsk, Russia. The work, which is described in this thesis, has been performing in 2012-2014 in the Massachusetts Institute of Technology and in the University ofOxfordby the colleagues of the author and by himself.△ Less"
Habits vs Environment: What really causes asthma?,"Authors:Mengfan Tang,Pranav Agrawal,Ramesh Jain","Abstract:Despite considerable number of studies on risk factors for asthma onset, very little is known about their relative importance. To have a full picture of these factors, both categories, personal and environmental data, have to be taken into account simultaneously, which is missing in previous studies. We propose a framework to rank the risk factors from heterogeneous data sources of the two categor…▽ MoreDespite considerable number of studies on risk factors for asthma onset, very little is known about their relative importance. To have a full picture of these factors, both categories, personal and environmental data, have to be taken into account simultaneously, which is missing in previous studies. We propose a framework to rank the risk factors from heterogeneous data sources of the two categories. Established on top of EventShop and Personal EventShop, this framework extracts about 400 features, and analyzes them by employing a gradient boosting tree. The features come from sources including personal profile and life-event data, and environmental data on air pollution, weather and PM2.5 emission sources. The top ranked risk factors derived from our framework agree well with the general medical consensus. Thus, our framework is a reliable approach, and the discovered rankings of relative importance of risk factors can provide insights for the prevention of asthma.△ Less"
Super-Coulombic atom-atom interactions in hyperbolic media,"Authors:Cristian L. Cortes,Zubin Jacob","Abstract:Dipole-dipole interactions which govern phenomena like cooperative Lamb shifts, superradiant decay rates, Van der Waals forces, as well as resonance energy transfer rates are conventionally limited to the Coulombic near-field. Here, we reveal a class of real-photon and virtual-photon long-range quantum electrodynamic (QED) interactions that have a singularity in media with hyperbolic dispersion. T…▽ MoreDipole-dipole interactions which govern phenomena like cooperative Lamb shifts, superradiant decay rates, Van der Waals forces, as well as resonance energy transfer rates are conventionally limited to the Coulombic near-field. Here, we reveal a class of real-photon and virtual-photon long-range quantum electrodynamic (QED) interactions that have a singularity in media with hyperbolic dispersion. The singularity in the dipole-dipole coupling, referred to as a Super-Coulombic interaction, is a result of an effective interaction distance that goes to zero in the ideal limit irrespective of the physical distance. We investigate the entire landscape of atom-atom interactions in hyperbolic media and propose practical implementations with phonon-polaritonic hexagonal boron nitride in the infrared spectral range and plasmonic super-lattice structures in the visible range. Our work paves the way for the control of cold atoms in hyperbolic media and the study of many-body atomic states where optical phonons mediate quantum interactions.△ Less"
Theory of Superconductivity in Graphite Intercalation Compounds,Authors:Yasutami Takada,"Abstract:On the basis of the model that was successfully applied to KC8, RbC8, and CsC8 in 1982, we have calculated the superconducting transition temperature Tc for CaC6 and YbC6 to find that the same model reproduces the observed Tc in those compounds as well, indicating that it is a standard model for superconductivity in the graphite intercalation compounds with Tc ranging over three orders of magnitud…▽ MoreOn the basis of the model that was successfully applied to KC8, RbC8, and CsC8 in 1982, we have calculated the superconducting transition temperature Tc for CaC6 and YbC6 to find that the same model reproduces the observed Tc in those compounds as well, indicating that it is a standard model for superconductivity in the graphite intercalation compounds with Tc ranging over three orders of magnitude. Further enhancement of Tc well beyond 10 K is also predicted. The present method for calculating Tc from first principles is compared with that in the density functional theory for superconductors, with paying attention to the feature of determining Tc without resort to the concept of the Coulomb pseudopotential.△ Less"
The Kardar-Parisi-Zhang equation - a statistical physics perspective,Authors:Herbert Spohn,"Abstract:The article covers the one-dimensional Kardar-Parisi-Zhang equation, weak drive limit, universality, directed polymers in a random medium, replica solutions, statistical mechanics of line ensembles, and its generalization to several components which is used to study equilibrium time correlations of anharmonic chains and of the discrete nonlinear Schrödinger equation. The notes are an extended vers…▽ MoreThe article covers the one-dimensional Kardar-Parisi-Zhang equation, weak drive limit, universality, directed polymers in a random medium, replica solutions, statistical mechanics of line ensembles, and its generalization to several components which is used to study equilibrium time correlations of anharmonic chains and of the discrete nonlinear Schrödinger equation. The notes are an extended version of my lectures at Les Houches July 2015.△ Less"
Modelling the Role of Nitric Oxide in Cerebral Autoregulation,Authors:Mark Catherall,"Abstract:Malfunction of the system which regulates the bloodflow in the brain is a major cause of stroke and dementia, costing many lives and many billions of pounds each year in the UK alone. This regulatory system, known as cerebral autoregulation, has been the subject of much experimental and mathematical investigation yet our understanding of it is still quite limited. One area in which our understandi…▽ MoreMalfunction of the system which regulates the bloodflow in the brain is a major cause of stroke and dementia, costing many lives and many billions of pounds each year in the UK alone. This regulatory system, known as cerebral autoregulation, has been the subject of much experimental and mathematical investigation yet our understanding of it is still quite limited. One area in which our understanding is particularly lacking is that of the role of nitric oxide, understood to be a potent vasodilator. The interactions of nitric oxide with the better understood myogenic response remain un-modelled and poorly understood. In this thesis we present a novel model of the arteriolar control mechanism, comprising a mixture of well-established and new models of individual processes, brought together for the first time. We show that this model is capable of reproducing experimentally observed behaviour very closely and go on to investigate its stability in the context of the vasculature of the whole brain. In conclusion we find that nitric oxide, although it plays a central role in determining equilibrium vessel radius, is unimportant to the dynamics of the system and its responses to variation in arterial blood pressure. We also find that the stability of the system is very sensitive to the dynamics of Ca$^{2+}$ within the muscle cell, and that self-sustaining Ca$^{2+}$ waves are not necessary to cause whole-vessel radius oscillations consistent with vasomotion.△ Less"
GELATO and SAGE: An Integrated Framework for MS Annotation,"Authors:Khalifeh AlJadda,Rene Ranzinger,Melody Porterfield,Brent Weatherly,Mohammed Korayem,John A. Miller,Khaled Rasheed,Krys J. Kochut,William S. York","Abstract:Several algorithms and tools have been developed to (semi) automate the process of glycan identification by interpreting Mass Spectrometric data. However, each has limitations when annotating MSn data with thousands of MS spectra using uncurated public databases. Moreover, the existing tools are not designed to manage MSn data where n > 2. We propose a novel software package to automate the annota…▽ MoreSeveral algorithms and tools have been developed to (semi) automate the process of glycan identification by interpreting Mass Spectrometric data. However, each has limitations when annotating MSn data with thousands of MS spectra using uncurated public databases. Moreover, the existing tools are not designed to manage MSn data where n > 2. We propose a novel software package to automate the annotation of tandem MS data. This software consists of two major components. The first, is a free, semi-automated MSn data interpreter called the Glycomic Elucidation and Annotation Tool (GELATO). This tool extends and automates the functionality of existing open source projects, namely, GlycoWorkbench (GWB) and GlycomeDB. The second is a machine learning model called Smart Anotation Enhancement Graph (SAGE), which learns the behavior of glycoanalysts to select annotations generated by GELATO that emulate human interpretation of the spectra.△ Less"
The Abstract Structure of Quantum Algorithms,Authors:William Zeng,"Abstract:Quantum information brings together theories of physics and computer science. This synthesis challenges the basic intuitions of both fields. In this thesis, we show that adopting a unified and general language for process theories advances foundations and practical applications of quantum information. Our first set of results analyze quantum algorithms with a process theoretic structure. We contri…▽ MoreQuantum information brings together theories of physics and computer science. This synthesis challenges the basic intuitions of both fields. In this thesis, we show that adopting a unified and general language for process theories advances foundations and practical applications of quantum information. Our first set of results analyze quantum algorithms with a process theoretic structure. We contribute new constructions of the Fourier transform and Pontryagin duality in dagger symmetric monoidal categories. We then use this setting to study generalized unitary oracles and give a new quantum blackbox algorithm for the identification of group homomorphisms, solving the GROUPHOMID problem. In the remaining section, we construct a novel model of quantum blackbox algorithms in non-deterministic classical computation. Our second set of results concerns quantum foundations. We complete work begun by Coecke et al., definitively connecting the Mermin non-locality of a process theory with a simple algebraic condition on that theory's phase groups. This result allows us to offer new experimental tests for Mermin non-locality and new protocols for quantum secret sharing. In our final chapter, we exploit the shared process theoretic structure of quantum information and distributional compositional linguistics. We propose a quantum algorithm adapted from Weibe et al. to classify sentences by meaning. The clarity of the process theoretic setting allows us to recover a speedup that is lost in the naive application of the algorithm. The main mathematical tools used in this thesis are group theory (esp. Fourier theory on finite groups), monoidal category theory, and categorical algebra.△ Less"
Gindikin-Karpelevich finiteness for Kac-Moody groups over local fields,Authors:Auguste Hébert,"Abstract:In this paper, we prove some finiteness results about split Kac-Moody groups over local non-archimedean fields. Our results generalize those of ""An affine Gindikin-Karpelevich formula"" by Alexander Braverman, Howard Garland, David Kazhdan and Manish Patnaik. We do not require our groups to be affine. We use the hovel I associated to this situation, which is the analogue of the Bruhat-Tits building…▽ MoreIn this paper, we prove some finiteness results about split Kac-Moody groups over local non-archimedean fields. Our results generalize those of ""An affine Gindikin-Karpelevich formula"" by Alexander Braverman, Howard Garland, David Kazhdan and Manish Patnaik. We do not require our groups to be affine. We use the hovel I associated to this situation, which is the analogue of the Bruhat-Tits building for a reductive group.△ Less"
The risk of divergence,Authors:Pierre Lescanne,"Abstract:We present infinite extensive strategy profiles with perfect information and we show that replacing finite by infinite changes the notions and the reasoning tools. The presentation uses a formalism recently developed by logicians and computer science theoreticians, called coinduction. This builds a bridge between economic game theory and the most recent advance in theoretical computer science and…▽ MoreWe present infinite extensive strategy profiles with perfect information and we show that replacing finite by infinite changes the notions and the reasoning tools. The presentation uses a formalism recently developed by logicians and computer science theoreticians, called coinduction. This builds a bridge between economic game theory and the most recent advance in theoretical computer science and logic. The key result is that rational agents may have strategy leading to divergence .△ Less"
Minimap and miniasm: fast mapping and de novo assembly for noisy long sequences,Authors:Heng Li,"Abstract:Motivation: Single Molecule Real-Time (SMRT) sequencing technology andOxfordNanopore technologies (ONT) produce reads over 10kbp in length, which have enabled high-quality genome assembly at an affordable cost. However, at present, long reads have an error rate as high as 10-15%. Complex and computationally intensive pipelines are required to assemble such…▽ MoreMotivation: Single Molecule Real-Time (SMRT) sequencing technology andOxfordNanopore technologies (ONT) produce reads over 10kbp in length, which have enabled high-quality genome assembly at an affordable cost. However, at present, long reads have an error rate as high as 10-15%. Complex and computationally intensive pipelines are required to assemble such reads.
  Results: We present a new mapper, minimap, and a de novo assembler, miniasm, for efficiently mapping and assembling SMRT and ONT reads without an error correction stage. They can often assemble a sequencing run of bacterial data into a single contig in a few minutes, and assemble 45-fold C. elegans data in 9 minutes, orders of magnitude faster than the existing pipelines. We also introduce a pairwise read mapping format (PAF) and a graphical fragment assembly format (GFA), and demonstrate the interoperability between ours and current tools.
  Availability and implementation: https://github.com/lh3/minimap and https://github.com/lh3/miniasm
  Contact: hengli@broadinstitute.org△ Less"
Non equilibrium quantum dynamics in ultra-cold quantum gases,Authors:Ehud Altman,"Abstract:Advances in controlling and measuring systems of ultra-cold atoms provided strong motivation to theoretical investigations of quantum dynamics in closed many-body systems. Fundamental questions on quantum dynamics and statistical mechanics are now within experimental reach: How is thermalization achieved in a closed quantum system? How does quantum dynamics cross over to effective classical physic…▽ MoreAdvances in controlling and measuring systems of ultra-cold atoms provided strong motivation to theoretical investigations of quantum dynamics in closed many-body systems. Fundamental questions on quantum dynamics and statistical mechanics are now within experimental reach: How is thermalization achieved in a closed quantum system? How does quantum dynamics cross over to effective classical physics? Can such a thermal or classical fate be evaded? In these lectures, given at the Les Houches Summer School of Physics ""Strongly Interacting Quantum Systems Out of Equilibrium"", I introduce the students to the novel properties that make ultra-cold atomic systems a unique platform for study of non equilibrium quantum dynamics. I review a selection of recent experimental and theoretical work in which universal features and emergent phenomena in quantum dynamics are highlighted.△ Less"
Pulsar Acceleration Searches on the GPU for the Square Kilometre Array,"Authors:Sofia Dimoudi,Wesley Armour","Abstract:…algorithm on Graphics Processor Units (GPUs) in the context of the SKA, as part of the Astro-Accelerate real-time data processing library, currently under development at theOxforde-Research Centre (OeRC), University ofOxford.▽ MorePulsar acceleration searches are methods for recovering signals from radio telescopes, that may otherwise be lost due to the effect of orbital acceleration in binary systems. The vast amount of data that will be produced by next generation instruments such as the Square Kilometre Array (SKA) necessitates real-time acceleration searches, which in turn requires the use of HPC platforms. We present our implementation of the Fourier Domain Acceleration Search (FDAS) algorithm on Graphics Processor Units (GPUs) in the context of the SKA, as part of the Astro-Accelerate real-time data processing library, currently under development at theOxforde-Research Centre (OeRC), University ofOxford.△ Less"
ReSeg: A Recurrent Neural Network-based Model for Semantic Segmentation,"Authors:Francesco Visin,Marco Ciccone,Adriana Romero,Kyle Kastner,Kyunghyun Cho,Yoshua Bengio,Matteo Matteucci,Aaron Courville","Abstract:…is efficient, flexible and suitable for a variety of semantic segmentation tasks. We evaluate ReSeg on several widely-used semantic segmentation datasets: Weizmann Horse,OxfordFlower, and CamVid; achieving state-of-the-art performance. Results show that ReSeg can act as a suitable architecture for semantic segmentation tasks, and may have further applicati…▽ MoreWe propose a structured prediction architecture, which exploits the local generic features extracted by Convolutional Neural Networks and the capacity of Recurrent Neural Networks (RNN) to retrieve distant dependencies. The proposed architecture, called ReSeg, is based on the recently introduced ReNet model for image classification. We modify and extend it to perform the more challenging task of semantic segmentation. Each ReNet layer is composed of four RNN that sweep the image horizontally and vertically in both directions, encoding patches or activations, and providing relevant global information. Moreover, ReNet layers are stacked on top of pre-trained convolutional layers, benefiting from generic local features. Upsampling layers follow ReNet layers to recover the original image resolution in the final predictions. The proposed ReSeg architecture is efficient, flexible and suitable for a variety of semantic segmentation tasks. We evaluate ReSeg on several widely-used semantic segmentation datasets: Weizmann Horse,OxfordFlower, and CamVid; achieving state-of-the-art performance. Results show that ReSeg can act as a suitable architecture for semantic segmentation tasks, and may have further applications in other structured prediction problems. The source code and model hyperparameters are available on https://github.com/fvisin/reseg.△ Less"
Stability Analysis of Magnetised Neutron Stars - A Semi-analytic Approach,"Authors:Marlene Herbrik,Kostas Kokkotas","Abstract:We implement a semi-analytic approach for stability analysis, addressing the ongoing uncertainty about stability and structure of neutron star magnetic fields. Applying the energy variational principle, a model system is displaced from its equilibrium state. The related energy density variation is set up analytically, whereas its volume integration is carried out numerically. This facilitates the…▽ MoreWe implement a semi-analytic approach for stability analysis, addressing the ongoing uncertainty about stability and structure of neutron star magnetic fields. Applying the energy variational principle, a model system is displaced from its equilibrium state. The related energy density variation is set up analytically, whereas its volume integration is carried out numerically. This facilitates the consideration of more realistic neutron star characteristics within the model compared to analytical treatments. At the same time, our method retains the possibility to yield general information about neutron star magnetic field and composition structures that are likely to be stable. In contrast to numerical studies, classes of parametrized systems can be studied at once, finally constraining realistic configurations for interior neutron star magnetic fields. We apply the stability analysis scheme on polytropic and non-barotropic neutron stars with toroidal, poloidal and mixed fields testing their stability in a Newtonian framework. Furthermore, we provide the analytical scheme for dropping the Cowling approximation in an axisymmetric system and investigate its impact. Our results confirm the instability of simple magnetised neutron star models as well as a stabilisation tendency in the case of mixed fields and stratification. These findings agree with analytical studies whose spectrum of model systems we extend by lifting former simplifications.△ Less"
Proceedings 12th International Workshop on Quantum Physics and Logic,"Authors:Chris Heunen,Peter Selinger,Jamie Vicary","Abstract:This volume contains the proceedings of the 12th International Workshop on Quantum Physics and Logic (QPL 2015), which was held July 15-17, 2015 atOxfordUniversity. The goal of this workshop series is to bring together researchers working on mathematical foundations of quantum physics, quantum computing, spatio-temporal causal structures, and related areas…▽ MoreThis volume contains the proceedings of the 12th International Workshop on Quantum Physics and Logic (QPL 2015), which was held July 15-17, 2015 atOxfordUniversity. The goal of this workshop series is to bring together researchers working on mathematical foundations of quantum physics, quantum computing, spatio-temporal causal structures, and related areas such as computational linguistics. Of particular interest are topics that use logical tools, ordered algebraic and category-theoretic structures, formal languages, semantical methods and other computer science methods for the study of physical behaviour in general.△ Less"
Computational models: Bottom-up and top-down aspects,"Authors:Laurent Itti,Ali Borji","Abstract:Computational models of visual attention have become popular over the past decade, we believe primarily for two reasons: First, models make testable predictions that can be explored by experimentalists as well as theoreticians, second, models have practical and technological applications of interest to the applied science and engineering communities. In this chapter, we take a critical look at rec…▽ MoreComputational models of visual attention have become popular over the past decade, we believe primarily for two reasons: First, models make testable predictions that can be explored by experimentalists as well as theoreticians, second, models have practical and technological applications of interest to the applied science and engineering communities. In this chapter, we take a critical look at recent attention modeling efforts. We focus on {\em computational models of attention} as defined by Tsotsos \& Rothenstein \shortcite{Tsotsos_Rothenstein11}: Models which can process any visual stimulus (typically, an image or video clip), which can possibly also be given some task definition, and which make predictions that can be compared to human or animal behavioral or physiological responses elicited by the same stimulus and task. Thus, we here place less emphasis on abstract models, phenomenological models, purely data-driven fitting or extrapolation models, or models specifically designed for a single task or for a restricted class of stimuli. For theoretical models, we refer the reader to a number of previous reviews that address attention theories and models more generally \cite{Itti_Koch01nrn,Paletta_etal05,Frintrop_etal10,Rothenstein_Tsotsos08,Gottlieb_Balan10,Toet11,Borji_Itti12pami}.△ Less"
Astronomy and Astrophysics in the Philosophy of Science,Authors:Sibylle Anderl,"Abstract:This article looks at philosophical aspects and questions that modern astrophysical research gives rise to. Other than cosmology, astrophysics particularly deals with understanding phenomena and processes operating at ""intermediate"" cosmic scales, which has rarely aroused philosophical interest so far. Being confronted with the attribution of antirealism by Ian Hacking because of its observational…▽ MoreThis article looks at philosophical aspects and questions that modern astrophysical research gives rise to. Other than cosmology, astrophysics particularly deals with understanding phenomena and processes operating at ""intermediate"" cosmic scales, which has rarely aroused philosophical interest so far. Being confronted with the attribution of antirealism by Ian Hacking because of its observational nature, astrophysics is equipped with a characteristic methodology that can cope with the missing possibility of direct interaction with most objects of research. In its attempt to understand the causal history of singular phenomena it resembles the historical sciences, while the search for general causal relations with respect to classes of processes or objects can rely on the ""cosmic laboratory"": the multitude of different phenomena and environments, naturally provided by the universe. Furthermore, the epistemology of astrophysics is strongly based on the use of models and simulations and a complex treatment of large amounts of data.△ Less"
Sheaf-Theoretic Methods in Quantum Mechanics and Quantum Information Theory,Authors:Carmen Maria Constantin,"Abstract:In this thesis we use the language of sheaf theory in order to develop a deeper understanding of some of the fundamental differences - such as entanglement, contextuality and non-locality - between quantum and classical physics. We first present, based on the work of Abramsky and Brandenburger, how sheaves, defined over certain posets of physically meaningful contexts, give a natural setting for c…▽ MoreIn this thesis we use the language of sheaf theory in order to develop a deeper understanding of some of the fundamental differences - such as entanglement, contextuality and non-locality - between quantum and classical physics. We first present, based on the work of Abramsky and Brandenburger, how sheaves, defined over certain posets of physically meaningful contexts, give a natural setting for capturing and analysing important quantum mechanical phenomena, such as quantum non-locality and contextuality. We also describe how this setting naturally leads to a three level hierarchy of quantum contextuality: weak contextuality, logical non-locality and strong contextuality. We use these insights in order to classify certain multipartite entangled states. Almost all of these turn out to be at least logically non-local, and a number of them even turn out to be strongly contextual. We further extend this result by showing that all n-qubit entangled states, with the exception of tensor products of single-qubit and bipartite maximally-entangled states, are logically non-local. Our proof is constructive: given any n-qubit state, we present an algorithm which produces n+2 local observables witnessing its logical non-locality. In the second half of the thesis we use the same basic principle of sheaves defined over physically meaningful contexts, in order to present an elegant mathematical language, known under the name of the Topos Approach, in which many quantum mechanical concepts, such as states, observables, and propositions about these, can be expressed. We then show that the language of the Topos Approach is as least as expressive, in logical terms, as traditional quantum logic. Finally, starting from a topos-theoretic perspective, we develop the construction of contextual entropy in order to give a unified treatment of classical and quantum notions of information theoretic entropy.△ Less"
The pleasures and pains of studying the two-type Richardson model,"Authors:Maria Deijfen,Olle Häggström","Abstract:This paper provides a survey of known results and open problems for the two-type Richardson model, which is a stochastic model for competition on $\mathbb{Z}^d$. In its simplest formulation, the Richardson model describes the evolution of a single infectious entity on $\mathbb{Z}^d$, but more recently the dynamics have been extended to comprise two competing growing entities. For this version of t…▽ MoreThis paper provides a survey of known results and open problems for the two-type Richardson model, which is a stochastic model for competition on $\mathbb{Z}^d$. In its simplest formulation, the Richardson model describes the evolution of a single infectious entity on $\mathbb{Z}^d$, but more recently the dynamics have been extended to comprise two competing growing entities. For this version of the model, the main question is whether there is a positive probability for both entities to simultaneously grow to occupy infinite parts of the lattice, the conjecture being that the answer is yes if and only if the entities have the same intensity. In this paper attention focuses on the two-type model, but the most important results for the one-type version are also described.△ Less"
Hairpins Participating in Folding of Human Telomeric Sequence Quadruplexes Studied by Standard and T-REMD Simulations,"Authors:Petr Stadlbauer,Petra Kührová,Pavel Banáš,Jaroslav Koča,Giovanni Bussi,Lukáš Trantírek,Michal Otyepka,Jiří Šponer","Abstract:DNA G-hairpins are potential key structures participating in folding of human telomeric guanine quadruplexes (GQ). We examined their properties by standard MD simulations starting from the folded state and long T-REMD starting from the unfolded state, accumulating ~130 μs of atomistic simulations. Antiparallel G-hairpins should spontaneously form in all stages of the folding to support lateral and…▽ MoreDNA G-hairpins are potential key structures participating in folding of human telomeric guanine quadruplexes (GQ). We examined their properties by standard MD simulations starting from the folded state and long T-REMD starting from the unfolded state, accumulating ~130 μs of atomistic simulations. Antiparallel G-hairpins should spontaneously form in all stages of the folding to support lateral and diagonal loops, with sub-μs scale rearrangements between them. We found no clear predisposition for direct folding into specific GQ topologies with specific syn/anti patterns. Our key prediction stemming from the T-REMD is that an ideal unfolded ensemble of the full GQ sequence populates all 4096 syn/anti combinations of its four G-stretches. The simulations can propose idealized folding pathways but we explain that such few-state pathways may be misleading. In the context of the available experimental data, the simulations strongly suggest that the GQ folding could be best understood by the kinetic partitioning mechanism with a set of deep competing minima on the folding landscape, with only a small fraction of molecules directly folding to the native fold. The landscape should further include nonspecific collapse processes where the molecules move via diffusion and consecutive random rare transitions, which could, e.g., structure the propeller loops.△ Less"
Path Integral Methods in Index Theorems,Authors:Mark van Loon,"Abstract:This paper provides a pedagogical introduction to the quantum mechanical path integral and its use in proving index theorems in geometry, specifically the Gauss-Bonnet-Chern theorem and Lefschetz fixed point theorem. It also touches on some other important concepts in mathematical physics, such as that of stationary phase, supersymmetry and localization. It is aimed at advanced undergraduates and…▽ MoreThis paper provides a pedagogical introduction to the quantum mechanical path integral and its use in proving index theorems in geometry, specifically the Gauss-Bonnet-Chern theorem and Lefschetz fixed point theorem. It also touches on some other important concepts in mathematical physics, such as that of stationary phase, supersymmetry and localization. It is aimed at advanced undergraduates and beginning graduates, with no previous knowledge beyond undergraduate quantum mechanics assumed. The necessary mathematical background in differential geometry is reviewed, though a familiarity with this material is undoubtedly helpful.△ Less"
ALMA images of discs: are all gaps carved by planets?,"Authors:Jean-François Gonzalez,Guillaume Laibe,Sarah T. Maddison,Christophe Pinte,François Ménard","Abstract:Protoplanetary discs are now routinely observed and exoplanets, after the numerous indirect discoveries, are starting to be directly imaged. To better understand the planet formation process, the next step is the detection of forming planets or of signposts of young planets still in their disc, such as gaps. A spectacular example is the ALMA science verification image of HL Tau showing numerous ga…▽ MoreProtoplanetary discs are now routinely observed and exoplanets, after the numerous indirect discoveries, are starting to be directly imaged. To better understand the planet formation process, the next step is the detection of forming planets or of signposts of young planets still in their disc, such as gaps. A spectacular example is the ALMA science verification image of HL Tau showing numerous gaps and rings in its disc. To study the observability of planet gaps, we ran 3D hydrodynamical simulations of a gas and dust disc containing a 5 M J gap-opening planet and characterised the spatial distribution of migrating, growing and fragmenting dust grains. We then computed the corresponding synthetic images for ALMA. For a value of the dust fragmentation threshold of 15 m s --1 for the collisional velocity, we identify for the first time a self-induced dust pile up in simulations taking fragmentation into account. This feature, in addition to the easily detected planet gap, causes a second apparent gap that could be mistaken for the signature of a second planet. It is therefore essential to be cautious in the interpretation of gap detections.△ Less"
On the higher order exterior and interior Whitehead products,"Authors:Marek Golasiński,Thiago de Melo","Abstract:…as well. The main result stated in Theorem 3.10 generalizes Theorem 1.10 in K.\ A.\ Hardie, \textit{A generalization of the Hopf construction}, Quart.\ J.\ Math.\OxfordSer.\ (2) \textbf{12} (1961), 196--204. and concerns to the Hopf invariant of the generalized Hopf construction.
  We close the paper applying the Gray's construction $\circ$ (called th…▽ MoreWe extend the notion of the exterior Whitehead product for maps $α_i:ΣA_i \to X_i$ for $i=1,\dots,n$, where $ΣA_i$ is the reduced suspension of $A_i$ and then, for the interior product with $X_i=J_{m_i}(X)$ as well. The main result stated in Theorem 3.10 generalizes Theorem 1.10 in K.\ A.\ Hardie, \textit{A generalization of the Hopf construction}, Quart.\ J.\ Math.\OxfordSer.\ (2) \textbf{12} (1961), 196--204. and concerns to the Hopf invariant of the generalized Hopf construction.
  We close the paper applying the Gray's construction $\circ$ (called the Theriault product) to a sequence $X_1,\dots,X_n$ of simply connected co-$H$-spaces to obtain a higher Gray--Whitehead product map \[w_n:Σ^{n-2}(X_1\circ\dots\circ X_n)\to T_1(X_1,\dots,X_n),\] where $T_1(X_1,\dots,X_n)$ is the fat wedge of $X_1,\dots,X_n$.△ Less"
Law on the Market? Abnormal Stock Returns and Supreme Court Decision-Making,"Authors:Daniel Martin Katz,Michael J Bommarito II,Tyler Soellinger,James Ming Chen","Abstract:What happens when the Supreme Court of the United States decides a case impacting one or more publicly-traded firms? While many have observed anecdotal evidence linking decisions or oral arguments to abnormal stock returns, few have rigorously or systematically investigated the behavior of equities around Supreme Court actions. In this research, we present the first comprehensive, longitudinal stu…▽ MoreWhat happens when the Supreme Court of the United States decides a case impacting one or more publicly-traded firms? While many have observed anecdotal evidence linking decisions or oral arguments to abnormal stock returns, few have rigorously or systematically investigated the behavior of equities around Supreme Court actions. In this research, we present the first comprehensive, longitudinal study on the topic, spanning over 15 years and hundreds of cases and firms. Using both intra- and interday data around decisions and oral arguments, we evaluate the frequency and magnitude of statistically-significant abnormal return events after Supreme Court action. On a per-term basis, we find 5.3 cases and 7.8 stocks that exhibit abnormal returns after decision. In total, across the cases we examined, we find 79 out of the 211 cases (37%) exhibit an average abnormal return of 4.4% over a two-session window with an average $|t|$-statistic of 2.9. Finally, we observe that abnormal returns following Supreme Court decisions materialize over the span of hours and days, not minutes, yielding strong implications for market efficiency in this context. While we cannot causally separate substantive legal impact from mere revision of beliefs, we do find strong evidence that there is indeed a ""law on the market"" effect as measured by the frequency of abnormal return events, and that these abnormal returns are not immediately incorporated into prices.△ Less"
ATP dependent NS3 helicase interaction with RNA: insights from molecular simulations,"Authors:Andrea Pérez-Villa,Maria Darvas,Giovanni Bussi","Abstract:Non structural protein 3 (NS3) helicase from hepatitis C virus is an enzyme that unwinds and translocates along nucleic acids with an ATP-dependent mechanism and has a key role in the replication of the viral RNA. An inchworm-like mechanism for translocation has been proposed based on crystal structures and single molecule experiments. We here perform atomistic molecular dynamics in explicit solve…▽ MoreNon structural protein 3 (NS3) helicase from hepatitis C virus is an enzyme that unwinds and translocates along nucleic acids with an ATP-dependent mechanism and has a key role in the replication of the viral RNA. An inchworm-like mechanism for translocation has been proposed based on crystal structures and single molecule experiments. We here perform atomistic molecular dynamics in explicit solvent on the microsecond time scale of the available experimental structures. We also construct and simulate putative intermediates for the translocation process, and we perform non-equilibrium targeted simulations to estimate their relative stability. For each of the simulated structures we carefully characterize the available conformational space, the ligand binding pocket, and the RNA binding cleft. The analysis of the hydrogen bond network and of the non-equilibrium trajectories indicates an ATP-dependent stabilization of one of the protein conformers. Additionally, enthalpy calculations suggest that entropic effects might be crucial for the stabilization of the experimentally observed structures.△ Less"
Probabilistic Modelling of Morphologically Rich Languages,Authors:Jan A. Botha,"Abstract:This thesis investigates how the sub-structure of words can be accounted for in probabilistic models of language. Such models play an important role in natural language processing tasks such as translation or speech recognition, but often rely on the simplistic assumption that words are opaque symbols. This assumption does not fit morphologically complex language well, where words can have rich in…▽ MoreThis thesis investigates how the sub-structure of words can be accounted for in probabilistic models of language. Such models play an important role in natural language processing tasks such as translation or speech recognition, but often rely on the simplistic assumption that words are opaque symbols. This assumption does not fit morphologically complex language well, where words can have rich internal structure and sub-word elements are shared across distinct word forms.
  Our approach is to encode basic notions of morphology into the assumptions of three different types of language models, with the intention that leveraging shared sub-word structure can improve model performance and help overcome data sparsity that arises from morphological processes.
  In the context of n-gram language modelling, we formulate a new Bayesian model that relies on the decomposition of compound words to attain better smoothing, and we develop a new distributed language model that learns vector representations of morphemes and leverages them to link together morphologically related words. In both cases, we show that accounting for word sub-structure improves the models' intrinsic performance and provides benefits when applied to other tasks, including machine translation.
  We then shift the focus beyond the modelling of word sequences and consider models that automatically learn what the sub-word elements of a given language are, given an unannotated list of words. We formulate a novel model that can learn discontiguous morphemes in addition to the more conventional contiguous morphemes that most previous models are limited to. This approach is demonstrated on Semitic languages, and we find that modelling discontiguous sub-word structures leads to improvements in the task of segmenting words into their contiguous morphemes.△ Less"
Challenging the Lieb-OxfordBound in a systematic way,"Authors:Michael Seidl,Stefan Vuckovic,Paola Gori-Giorgi","Abstract:The Lieb-Oxfordbound, a nontrivial inequality for the indirect part of the many-body Coulomb repulsion in an electronic system, plays an important role in the construction of approximations in density functional theory. Using the wavefunction for strictly-correlated electrons of a given density, we turn the search over wavefunctions appearing in the origina…▽ MoreThe Lieb-Oxfordbound, a nontrivial inequality for the indirect part of the many-body Coulomb repulsion in an electronic system, plays an important role in the construction of approximations in density functional theory. Using the wavefunction for strictly-correlated electrons of a given density, we turn the search over wavefunctions appearing in the original bound into a more manageable search over electron densities. This allows us to challenge the bound in a systematic way. We find that a maximizing density for the bound, if it exists, must have compact support. We also find that, at least for particle numbers $N\le 60$, a uniform density profile is not the most challenging for the bound. With our construction we improve the bound for $N=2$ electrons that was originally found by Lieb andOxford, we give a new lower bound to the constant appearing in the Lieb-Oxfordinequality valid for any $N$, and we provide an improved upper bound for the low-density uniform electron gas indirect energy.△ Less"
Slow Relaxations and Non-Equilibrium Dynamics in Classical and Quantum Systems,Authors:Giulio Biroli,"Abstract:The aim of these lectures, given at the Les Houches Summer School of Physics ""Strongly Interacting Quantum Systems Out of Equilibrium"", is providing an introduction to several important and interesting facets of out of equilibrium dynamics. In recent years, there has been a boost in the research on quantum systems out of equilibrium. If fifteen years ago hard condensed matter and classical statist…▽ MoreThe aim of these lectures, given at the Les Houches Summer School of Physics ""Strongly Interacting Quantum Systems Out of Equilibrium"", is providing an introduction to several important and interesting facets of out of equilibrium dynamics. In recent years, there has been a boost in the research on quantum systems out of equilibrium. If fifteen years ago hard condensed matter and classical statistical physics remained rather separate research fields, now the focus on several kinds of out of equilibrium dynamics is making them closer and closer. The aim of my lectures was to present to the students the richness of this topic, insisting on the common concepts and showing that there is much to gain in considering and learning out of equilibrium dynamics as a whole research field.△ Less"
Classify Sina Weibo users into High or Low happiness Groups Using Linguistic and Behavior Features,"Authors:Jingying Wang,Tianli Liu,Tingshao Zhu,Lei Zhang,Bibo Hao,Zhenxiang Chen","Abstract:…costs and low efficiency. This paper aims at identifying social network users' happiness level based on their Web behavior. We recruited 548 participants to fill in theOxfordHappiness Inventory (OHI) and divided them into two groups with high/low OHI score. We downloaded each Weibo user's data by calling API, and extracted 103 linguistic and behavi…▽ MoreIt's of great importance to measure happiness of social network users, but the existing method based on questionnaires suffers from high costs and low efficiency. This paper aims at identifying social network users' happiness level based on their Web behavior. We recruited 548 participants to fill in theOxfordHappiness Inventory (OHI) and divided them into two groups with high/low OHI score. We downloaded each Weibo user's data by calling API, and extracted 103 linguistic and behavior features. 24 features are identified with significant difference between high and low happiness groups. We trained a Decision Tree on these 24 features to make the prediction of high/low happiness group. The decision tree can be used to identify happiness level of any new social network user based on linguistic and behavior features. The Decision Tree can achieve 67.7% on precision. Although the capability of our Decision Tree is not ideal, classifying happiness via linguistic and behavior features on the Internet is proved to be feasible.△ Less"
Hyperfine transitions of 13CN from pre-protostellar sources,"Authors:David Flower,Pierre Hily-Blant","Abstract:Recent quantum mechanical calculations of rate coefficients for collisional transfer of population between the hyperfine states of 13CN enable their population densities to be determined. We have computed the relative populations of the hyperfine states of the N = 0, 1, 2 rotational states for kinetic temperatures 5 $\le$ T $\le$ 20 K and molecular hydrogen densities 1 $\le$ n(H2) $\le$10 10 cm --…▽ MoreRecent quantum mechanical calculations of rate coefficients for collisional transfer of population between the hyperfine states of 13CN enable their population densities to be determined. We have computed the relative populations of the hyperfine states of the N = 0, 1, 2 rotational states for kinetic temperatures 5 $\le$ T $\le$ 20 K and molecular hydrogen densities 1 $\le$ n(H2) $\le$10 10 cm --3. Spontaneous and induced radiative transitions were taken into account. Our calculations show that, if the lines are optically thin, the populations of the hyperfine states, F, within a given rotational manifold are proportional to their statistical weights, (2F + 1) -- i.e. in local thermodynamic equilibrium -- over the entire range of densities. We have re-analysed IRAM 30 m telescope observations of 13CN hyperfine transitions (N = 1 $\rightarrow$ 0) in four starless cores. A comparison of these observations with our calculations confirms that the hyperfine states are statistically populated in these sources.△ Less"
Elastic network models for RNA: a comparative assessment with molecular dynamics and SHAPE experiments,"Authors:Giovanni Pinamonti,Sandro Bottaro,Cristian Micheletti,Giovanni Bussi","Abstract:Elastic network models (ENMs) are valuable and efficient tools for characterizing the collective internal dynamics of proteins based on the knowledge of their native structures. The increasing evidence that the biological functionality of RNAs is often linked to their innate internal motions, poses the question of whether ENM approaches can be successfully extended to this class of biomolecules. T…▽ MoreElastic network models (ENMs) are valuable and efficient tools for characterizing the collective internal dynamics of proteins based on the knowledge of their native structures. The increasing evidence that the biological functionality of RNAs is often linked to their innate internal motions, poses the question of whether ENM approaches can be successfully extended to this class of biomolecules. This issue is tackled here by considering various families of elastic networks of increasing complexity applied to a representative set of RNAs. The fluctuations predicted by the alternative ENMs are stringently validated by comparison against extensive molecular dynamics simulations and SHAPE experiments. We find that simulations and experimental data are systematically best reproduced by either an all-atom or a three-beads-per-nucleotide representation (sugar-base-phosphate), with the latter arguably providing the best balance of accuracy and computational complexity.△ Less"
Optimal Seed Solver: Optimizing Seed Selection in Read Mapping,"Authors:Hongyi Xin,Richard Zhu,Sunny Nahar,John Emmons,Gennady Pekhimenko,Carl Kingsford,Can Alkan,Onur Mutlu","Abstract:Motivation: Optimizing seed selection is an important problem in read mapping. The number of non-overlapping seeds a mapper selects determines the sensitivity of the mapper while the total frequency of all selected seeds determines the speed of the mapper. Modern seed-and-extend mappers usually select seeds with either an equal and fixed-length scheme or with an inflexible placement scheme, both o…▽ MoreMotivation: Optimizing seed selection is an important problem in read mapping. The number of non-overlapping seeds a mapper selects determines the sensitivity of the mapper while the total frequency of all selected seeds determines the speed of the mapper. Modern seed-and-extend mappers usually select seeds with either an equal and fixed-length scheme or with an inflexible placement scheme, both of which limit the potential of the mapper to select less frequent seeds to speed up the mapping process. Therefore, it is crucial to develop a new algorithm that can adjust both the individual seed length and the seed placement, as well as derive less frequent seeds.
  Results: We present the Optimal Seed Solver (OSS), a dynamic programming algorithm that discovers the least frequently-occurring set of x seeds in an L-bp read in $O(x \times L)$ operations on average and in $O(x \times L^{2})$ operations in the worst case. We compared OSS against four state-of-the-art seed selection schemes and observed that OSS provides a 3-fold reduction of average seed frequency over the best previous seed selection optimizations.△ Less"
Amplifying the Impact of Open Access: Wikipedia and the Diffusion of Science,"Authors:Misha Teplitskiy,Grace Lu,Eamon Duede","Abstract:With the rise of Wikipedia as a first-stop source for scientific knowledge, it is important to compare its representation of that knowledge to that of the academic literature. Here we identify the 250 most heavily used journals in each of 26 research fields (4,721 journals, 19.4M articles in total) indexed by the Scopus database, and test whether topic, academic status, and accessibility make arti…▽ MoreWith the rise of Wikipedia as a first-stop source for scientific knowledge, it is important to compare its representation of that knowledge to that of the academic literature. Here we identify the 250 most heavily used journals in each of 26 research fields (4,721 journals, 19.4M articles in total) indexed by the Scopus database, and test whether topic, academic status, and accessibility make articles from these journals more or less likely to be referenced on Wikipedia. We find that a journal's academic status (impact factor) and accessibility (open access policy) both strongly increase the probability of it being referenced on Wikipedia. Controlling for field and impact factor, the odds that an open access journal is referenced on the English Wikipedia are 47% higher compared to paywall journals. One of the implications of this study is that a major consequence of open access policies is to significantly amplify the diffusion of science, through an intermediary like Wikipedia, to a broad audience.△ Less"
Stochastic modelling of regional archaeomagnetic series,"Authors:G. Hellio,N. Gillet,C. Bouligand,D. Jault","Abstract:SUMMARY We report a new method to infer continuous time series of the declination, inclination and  intensity of the magnetic field from archeomagnetic data. Adopting a Bayesian perspective, we need to specify a priori knowledge about the time evolution of the magnetic field. It consists in a time correlation function that we choose to be compatible with present knowledge about the geomagnetic tim…▽ MoreSUMMARY We report a new method to infer continuous time series of the declination, inclination and  intensity of the magnetic field from archeomagnetic data. Adopting a Bayesian perspective, we need to specify a priori knowledge about the time evolution of the magnetic field. It consists in a time correlation function that we choose to be compatible with present knowledge about the geomagnetic time spectra. The results are presented as distributions of possible values for the declination, inclination or intensity. We find that the methodology can be adapted to account for the age uncertainties of archeological artefacts and we use Markov Chain Monte Carlo to explore the possible dates of observations. We apply the method to intensity datasets from Mari, Syria and to intensity and directional datasets from Paris, France. Our reconstructions display more rapid variations than previous studies and we find that the possible values of geomagnetic field elements are not necessarily normally distributed. Another output of the model is better age estimates of archeological artefacts.△ Less"
The initial mass functions of M31 and M32 through far red stellar absorption features,"Authors:Simon Zieleniewski,Ryan C. W. Houghton,Niranjan Thatte,Roger L. Davies","Abstract:Using theOxfordShort Wavelength Integral Field specTrograph (SWIFT), we investigate radial variations of several initial mass function (IMF) dependent absorption features in M31 and M32. We obtain high signal-to-noise spectra at six pointings along the major axis of M31 out to ~ 700"" (2.7 kpc) and a single pointing of the central 10 pc for M32. In M31…▽ MoreUsing theOxfordShort Wavelength Integral Field specTrograph (SWIFT), we investigate radial variations of several initial mass function (IMF) dependent absorption features in M31 and M32. We obtain high signal-to-noise spectra at six pointings along the major axis of M31 out to ~ 700"" (2.7 kpc) and a single pointing of the central 10 pc for M32. In M31 the sodium NaI λ8190 index shows a flat equivalent width profile at ~ 0.4 Å through the majority of the bulge, with a strong gradient up to 0.8 Å in the central 10"" (38 pc); the Wing-Ford FeH λ9916 index is measured to be constant at 0.4 Å for all radii; and calcium triplet CaT λ8498, 8542, 8662 shows a gradual increase through the bulge towards the centre. M32 displays flat profiles for all three indices, with FeH at ~ 0.5 Å, very high CaT at ~ 0.8 Å and low NaI at ~ 0.1 Å. We analyse these data using stellar population models. We find that M31 is well described on all scales by a Chabrier IMF, with a gradient in sodium enhancement of [Na/Fe] ~ +0.3 dex in the outer bulge, rising within the central 10"" to perhaps [Na/Fe] ~ +1.0 dex in the nuclear region. We find M32 is described by a Chabrier IMF and young stellar age in line with other studies. Models show that CaT is much more sensitive to metallicity and [α/Fe] than to IMF. We note that the centres of M31 and M32 have very high stellar densities and yet we measure Chabrier IMFs in these regions.△ Less"
Understanding deep features with computer-generated imagery,"Authors:Mathieu Aubry,Bryan Russell","Abstract:…visualize them using principal component analysis. We show qualitative and quantitative results of our study on three CNNs trained on large image datasets: AlexNet, Places, andOxfordVGG. We observe important differences across the networks and CNN layers for different scene factors and object categories. Finally, we demonstrate that our analysis based on c…▽ MoreWe introduce an approach for analyzing the variation of features generated by convolutional neural networks (CNNs) with respect to scene factors that occur in natural images. Such factors may include object style, 3D viewpoint, color, and scene lighting configuration. Our approach analyzes CNN feature responses corresponding to different scene factors by controlling for them via rendering using a large database of 3D CAD models. The rendered images are presented to a trained CNN and responses for different layers are studied with respect to the input scene factors. We perform a decomposition of the responses based on knowledge of the input scene factors and analyze the resulting components. In particular, we quantify their relative importance in the CNN responses and visualize them using principal component analysis. We show qualitative and quantitative results of our study on three CNNs trained on large image datasets: AlexNet, Places, andOxfordVGG. We observe important differences across the networks and CNN layers for different scene factors and object categories. Finally, we demonstrate that our analysis based on computer-generated imagery translates to the network representation of natural images.△ Less"
Who are we now?,Authors:Robert Massey,"Abstract:In 2014 the Royal Astronomical Society carried out a survey of its membership, finding that we are both more and less diverse than UK society as a whole. Robert Massey summarizes the findings and what they mean for the Society in future.In 2014 the Royal Astronomical Society carried out a survey of its membership, finding that we are both more and less diverse than UK society as a whole. Robert Massey summarizes the findings and what they mean for the Society in future.△ Less"
Stellar populations in $ω$ Centauri: a multivariate analysis,"Authors:Didier Fraix-Burnet,E. Davoust","Abstract:We have performed multivariate statistical analyses of photometric and chemical abundance parameters of three large samples of stars in the globular cluster $ω$ Centauri. The statistical analysis of a sample of 735 stars based on seven chemical abundances with the method of Maximum Parsimony (cladistics) yields the most promising results: seven groups are found, distributed along three branches wi…▽ MoreWe have performed multivariate statistical analyses of photometric and chemical abundance parameters of three large samples of stars in the globular cluster $ω$ Centauri. The statistical analysis of a sample of 735 stars based on seven chemical abundances with the method of Maximum Parsimony (cladistics) yields the most promising results: seven groups are found, distributed along three branches with distinct chemical, spatial and kinematical properties. A progressive chemical evolution can be traced from one group to the next, but also within groups, suggestive of an inhomogeneous chemical enrichment of the initial interstellar matter. The adjustment of stellar evolution models shows that the groups with metallicities [Fe/H]\textgreater{}-1.5 are Helium-enriched, thus presumably of second generation. The spatial concentration of the groups increases with chemical evolution, except for two groups, which stand out in their other properties as well. The amplitude of rotation decreases with chemical evolution, except for two of the three metal-rich groups, which rotate fastest, as predicted by recent hydrodynamical simulations. The properties of the groups are interpreted in terms of star formation in gas clouds of different origins. In conclusion, our multivariate analysis has shown that metallicity alone cannot segregate the different populations of $ω$ Centauri.△ Less"
An Empirical Evaluation of Current Convolutional Architectures' Ability to Manage Nuisance Location and Scale Variability,"Authors:Nikolaos Karianakis,Jingming Dong,Stefano Soatto","Abstract:…performance to state-of-the-art levels. We test our hypothesis on a classification task using the ImageNet Challenge benchmark and on a wide-baseline matching task using theOxfordand Fischer's datasets.▽ MoreWe conduct an empirical study to test the ability of Convolutional Neural Networks (CNNs) to reduce the effects of nuisance transformations of the input data, such as location, scale and aspect ratio. We isolate factors by adopting a common convolutional architecture either deployed globally on the image to compute class posterior distributions, or restricted locally to compute class conditional distributions given location, scale and aspect ratios of bounding boxes determined by proposal heuristics. In theory, averaging the latter should yield inferior performance compared to proper marginalization. Yet empirical evidence suggests the converse, leading us to conclude that - at the current level of complexity of convolutional architectures and scale of the data sets used to train them - CNNs are not very effective at marginalizing nuisance variability. We also quantify the effects of context on the overall classification task and its impact on the performance of CNNs, and propose improved sampling techniques for heuristic proposal schemes that improve end-to-end performance to state-of-the-art levels. We test our hypothesis on a classification task using the ImageNet Challenge benchmark and on a wide-baseline matching task using theOxfordand Fischer's datasets.△ Less"
Stochastic network formation and homophily,"Authors:Paolo Pin,Brian Rogers",Abstract:This is a chapter of the forthcomingOxfordHandbook on the Economics of Networks.This is a chapter of the forthcomingOxfordHandbook on the Economics of Networks.△ Less
Visual Understanding via Multi-Feature Shared Learning with Global Consistency,"Authors:Lei Zhang,David Zhang","Abstract:…method is introduced as a fast solver owing to the convex sub-problems. Experiments on several benchmark visual datasets for multimedia understanding, such as the 17-categoryOxfordFlower dataset, the challenging 101-category Caltech dataset, the YouTube & Consumer Videos dataset and the large-scale NUS-WIDE dataset, demonstrate that the proposed approa…▽ MoreImage/video data is usually represented with multiple visual features. Fusion of multi-source information for establishing the attributes has been widely recognized. Multi-feature visual recognition has recently received much attention in multimedia applications. This paper studies visual understanding via a newly proposed l_2-norm based multi-feature shared learning framework, which can simultaneously learn a global label matrix and multiple sub-classifiers with the labeled multi-feature data. Additionally, a group graph manifold regularizer composed of the Laplacian and Hessian graph is proposed for better preserving the manifold structure of each feature, such that the label prediction power is much improved through the semi-supervised learning with global label consistency. For convenience, we call the proposed approach Global-Label-Consistent Classifier (GLCC). The merits of the proposed method include: 1) the manifold structure information of each feature is exploited in learning, resulting in a more faithful classification owing to the global label consistency; 2) a group graph manifold regularizer based on the Laplacian and Hessian regularization is constructed; 3) an efficient alternative optimization method is introduced as a fast solver owing to the convex sub-problems. Experiments on several benchmark visual datasets for multimedia understanding, such as the 17-categoryOxfordFlower dataset, the challenging 101-category Caltech dataset, the YouTube & Consumer Videos dataset and the large-scale NUS-WIDE dataset, demonstrate that the proposed approach compares favorably with the state-of-the-art algorithms. An extensive experiment on the deep convolutional activation features also show the effectiveness of the proposed approach. The code is available on http://www.escience.cn/people/lei/index.html△ Less"
"Far infrared and submillimetre surveys: from IRAS to Akari, Herschel and Planck","Authors:Michael Rowan-Robinson,Lingyu Wang","Abstract:We discuss a new IRAS Faint Source Catalog galaxy redshift catalogue (RIFSCz) which incorporates data from Galex, SDSS, 2MASS, WISE, Akari and Planck. Akari fluxes are consistent with photometry from other far infrared and submillimetre missions provided an aperture correction is applied. Results from the Hermes-SWIRE survey in Lockman are also discussed briefly, and the strong contrast between th…▽ MoreWe discuss a new IRAS Faint Source Catalog galaxy redshift catalogue (RIFSCz) which incorporates data from Galex, SDSS, 2MASS, WISE, Akari and Planck. Akari fluxes are consistent with photometry from other far infrared and submillimetre missions provided an aperture correction is applied. Results from the Hermes-SWIRE survey in Lockman are also discussed briefly, and the strong contrast between the galaxy populations selected at 60 and 500 mu is summarized.△ Less"
A Two-Layer Local Constrained Sparse Coding Method for Fine-Grained Visual Categorization,"Authors:Guo Lihua,Guo Chenggan","Abstract:…Moreover, a quick dictionary updating process is derived to further improve the training speed. Two experimental results show that our method achieves 85.29% accuracy on theOxford102 flowers dataset and 67.8% accuracy on the CUB-200-2011 bird dataset, and the performance of our framework is highly competitive with existing literatures.▽ MoreFine-grained categories are more difficulty distinguished than generic categories due to the similarity of inter-class and the diversity of intra-class. Therefore, the fine-grained visual categorization (FGVC) is considered as one of challenge problems in computer vision recently. A new feature learning framework, which is based on a two-layer local constrained sparse coding architecture, is proposed in this paper. The two-layer architecture is introduced for learning intermediate-level features, and the local constrained term is applied to guarantee the local smooth of coding coefficients. For extracting more discriminative information, local orientation histograms are the input of sparse coding instead of raw pixels. Moreover, a quick dictionary updating process is derived to further improve the training speed. Two experimental results show that our method achieves 85.29% accuracy on theOxford102 flowers dataset and 67.8% accuracy on the CUB-200-2011 bird dataset, and the performance of our framework is highly competitive with existing literatures.△ Less"
Penrose's quasi-local mass for asymptotically anti-de Sitter space-times,"Authors:Ron Kelly,Paul Tod","Abstract:…space-times. A modification of the Witten argument is given to prove a positivity property of the resulting conserved quantities.
  [This work formed part of Ron Kelly'sOxfordD.Phil. thesis, and the first person pronoun refers to him. It appeared in hand-written form as `Asymptotically anti-de Sitter space-times' in Twistor Newsletter 20 (1985) pp1…▽ MorePenrose's quasi-local mass construction is carried through for two-surfaces at infinity in asymptotically anti-de Sitter space-times. A modification of the Witten argument is given to prove a positivity property of the resulting conserved quantities.
  [This work formed part of Ron Kelly'sOxfordD.Phil. thesis, and the first person pronoun refers to him. It appeared in hand-written form as `Asymptotically anti-de Sitter space-times' in Twistor Newsletter 20 (1985) pp11-23 (available at http://people.maths.ox.ac.uk/lmason/Tn/TN1-25), but is appearing type-set for the first time here. Footnotes marked ${\bf{PT}}$ have been added for this version by Paul Tod, in the hope of making this work available to a wider audience.]△ Less"
Compositional Distributional Semantics with Compact Closed Categories and Frobenius Algebras,Authors:Dimitri Kartsaklis,"Abstract:This thesis contributes to ongoing research related to the categorical compositional model for natural language of Coecke, Sadrzadeh and Clark in three ways: Firstly, I propose a concrete instantiation of the abstract framework based on Frobenius algebras (joint work with Sadrzadeh). The theory improves shortcomings of previous proposals, extends the coverage of the language, and is supported by e…▽ MoreThis thesis contributes to ongoing research related to the categorical compositional model for natural language of Coecke, Sadrzadeh and Clark in three ways: Firstly, I propose a concrete instantiation of the abstract framework based on Frobenius algebras (joint work with Sadrzadeh). The theory improves shortcomings of previous proposals, extends the coverage of the language, and is supported by experimental work that improves existing results. The proposed framework describes a new class of compositional models that find intuitive interpretations for a number of linguistic phenomena. Secondly, I propose and evaluate in practice a new compositional methodology which explicitly deals with the different levels of lexical ambiguity (joint work with Pulman). A concrete algorithm is presented, based on the separation of vector disambiguation from composition in an explicit prior step. Extensive experimental work shows that the proposed methodology indeed results in more accurate composite representations for the framework of Coecke et al. in particular and every other class of compositional models in general. As a last contribution, I formalize the explicit treatment of lexical ambiguity in the context of the categorical framework by resorting to categorical quantum mechanics (joint work with Coecke). In the proposed extension, the concept of a distributional vector is replaced with that of a density matrix, which compactly represents a probability distribution over the potential different meanings of the specific word. Composition takes the form of quantum measurements, leading to interesting analogies between quantum physics and linguistics.△ Less"
Neural Activation Constellations: Unsupervised Part Model Discovery with Convolutional Networks,"Authors:Marcel Simon,Erik Rodner","Abstract:…patterns computed using convolutional neural networks. In our experiments, we outperform existing approaches for fine-grained recognition on the CUB200-2011, NA birds,OxfordPETS, andOxfordFlowers dataset in case no part or bounding box annotations are available and achieve state-of-the-art performance for the Stanf…▽ MorePart models of object categories are essential for challenging recognition tasks, where differences in categories are subtle and only reflected in appearances of small parts of the object. We present an approach that is able to learn part models in a completely unsupervised manner, without part annotations and even without given bounding boxes during learning. The key idea is to find constellations of neural activation patterns computed using convolutional neural networks. In our experiments, we outperform existing approaches for fine-grained recognition on the CUB200-2011, NA birds,OxfordPETS, andOxfordFlowers dataset in case no part or bounding box annotations are available and achieve state-of-the-art performance for the Stanford Dog dataset. We also show the benefits of neural constellation models as a data augmentation technique for fine-tuning. Furthermore, our paper unites the areas of generic and fine-grained classification, since our approach is suitable for both scenarios. The source code of our method is available online at http://www.inf-cv.uni-jena.de/part_discovery△ Less"
Global Existence of Weak Solutions to the Barotropic Compressible Navier-Stokes Flows with Degenerate Viscosities,"Authors:Jing Li,Zhouping Xin","Abstract:…domains or whole space for large initial data. This, in particular, solved an open problem in [P. L. Lions. Mathematical topics in fluid mechanics. Vol. 2. Compressible models.OxfordUniversity Press, 1998].▽ MoreThis paper concerns the existence of global weak solutions to the barotropic compressible Navier-Stokes equations with degenerate viscosity coefficients. We construct suitable approximate system which has smooth solutions satisfying the energy inequality, the BD entropy one, and the Mellet-Vasseur type estimate. Then, after adapting the compactness results due to Mellet-Vasseur [Comm. Partial Differential Equations 32 (2007)], we obtain the global existence of weak solutions to the barotropic compressible Navier-Stokes equations with degenerate viscosity coefficients in two or three dimensional periodic domains or whole space for large initial data. This, in particular, solved an open problem in [P. L. Lions. Mathematical topics in fluid mechanics. Vol. 2. Compressible models.OxfordUniversity Press, 1998].△ Less"
The Problem of Confirmation in the Everett Interpretation,Authors:Emily Adlam,"Abstract:I argue that theOxfordschool Everett interpretation is internally incoherent, because we cannot claim that in an Everettian universe the kinds of reasoning we have used to arrive at our beliefs about quantum mechanics would lead us to form true beliefs. I show that in an Everettian context, the experimental evidence that we have available could not provide…▽ MoreI argue that theOxfordschool Everett interpretation is internally incoherent, because we cannot claim that in an Everettian universe the kinds of reasoning we have used to arrive at our beliefs about quantum mechanics would lead us to form true beliefs. I show that in an Everettian context, the experimental evidence that we have available could not provide empirical confirmation for quantum mechanics, and moreover that we would not even be able to establish reference to the theoretical entities of quantum mechanics. I then consider a range of existing Everettian approaches to the probability problem and show that they do not succeed in overcoming this incoherence.△ Less"
Ideological and Temporal Components of Network Polarization in Online Political Participatory Media,"Authors:David Garcia,Adiya Abisheva,Simon Schweighofer,Uwe Serdült,Frank Schweitzer","Abstract:Political polarization is traditionally analyzed through the ideological stances of groups and parties, but it also has a behavioral component that manifests in the interactions between individuals. We present an empirical analysis of the digital traces of politicians in politnetz.ch, a Swiss online platform focused on political activity, in which politicians interact by creating support links, co…▽ MorePolitical polarization is traditionally analyzed through the ideological stances of groups and parties, but it also has a behavioral component that manifests in the interactions between individuals. We present an empirical analysis of the digital traces of politicians in politnetz.ch, a Swiss online platform focused on political activity, in which politicians interact by creating support links, comments, and likes. We analyze network polarization as the level of intra- party cohesion with respect to inter-party connectivity, finding that supports show a very strongly polarized structure with respect to party alignment. The analysis of this multiplex network shows that each layer of interaction contains relevant information, where comment groups follow topics related to Swiss politics. Our analysis reveals that polarization in the layer of likes evolves in time, increasing close to the federal elections of 2011. Furthermore, we analyze the internal social network of each party through metrics related to hierarchical structures, information efficiency, and social resilience. Our results suggest that the online social structure of a party is related to its ideology, and reveal that the degree of connectivity across two parties increases when they are close in the ideological space of a multi-party system.△ Less"
The Kinematic Image of 2R Dyads and Exact Synthesis of 5R Linkages,"Authors:Tudor-Dan Rad,Hans-Peter Schröcker","Abstract:We characterise the kinematic image of the constraint variety of a 2R dyad as a regular ruled quadric in a 3-space that contains a ""null quadrilateral"". Three prescribed poses determine, in general, two such quadrics. This allows us to modify a recent algorithm for the synthesis of 6R linkages in such a way that two consecutive revolute axes coincide, thus producing a 5R linkage. Using the classic…▽ MoreWe characterise the kinematic image of the constraint variety of a 2R dyad as a regular ruled quadric in a 3-space that contains a ""null quadrilateral"". Three prescribed poses determine, in general, two such quadrics. This allows us to modify a recent algorithm for the synthesis of 6R linkages in such a way that two consecutive revolute axes coincide, thus producing a 5R linkage. Using the classical geometry of twisted cubics on a quadric, we explain some of the peculiar properties of the the resulting synthesis procedure for 5R linkages.△ Less"
A Theoretical Investigation Into Energy Transfer In Photosynthetic Open Quantum Systems,Authors:David M. Wilkins,"Abstract:This thesis looks at the electronic energy transfer in the Fenna-Matthews-Olson complex, in which evidence of long-lived coherence has been observed in 2-dimensional infrared experiments. I use three techniques: the numerically exact Hierarchical Equations of Motion, and the perturbative Redfield and Foerster theories, the latter of which ignores quantum coherence in the transfer. Both of the appr…▽ MoreThis thesis looks at the electronic energy transfer in the Fenna-Matthews-Olson complex, in which evidence of long-lived coherence has been observed in 2-dimensional infrared experiments. I use three techniques: the numerically exact Hierarchical Equations of Motion, and the perturbative Redfield and Foerster theories, the latter of which ignores quantum coherence in the transfer. Both of the approximate methods perform very well - and while oscillations in site populations (a hallmark of coherence) are present in the exact transfer dynamics and absent in the dynamics of Foerster theory, the latter gives a reasonable prediction of transfer rates and steady-state populations, despite being incoherent - suggesting that coherence is not vital for the dynamics of transfer. Since Foerster theory is very inexpensive to run and performs so well, I then apply it to calculate the effects of static disorder in bacteriochlorophyll site energies and of a more structured spectral density. Ultimately, the energy transfer in the complex is found to be very robust to changes in its environment, which is advantageous for its biological function.△ Less"
Frozen states and active-absorbing phase transitions of the Ising model on networks,"Authors:Abdul Khaleque,Parongama Sen","Abstract:A zero temperature quench of the Ising model is known to lead to a frozen steady state on random and small world networks. We study such quenches on random scale free networks (RSF) and compare the scenario with that in the Barabási-Albert network (BA) and the Watts Strogatz (WS) addition type network. While frozen states are present in all the cases, the RSF shows an order-disorder phase transiti…▽ MoreA zero temperature quench of the Ising model is known to lead to a frozen steady state on random and small world networks. We study such quenches on random scale free networks (RSF) and compare the scenario with that in the Barabási-Albert network (BA) and the Watts Strogatz (WS) addition type network. While frozen states are present in all the cases, the RSF shows an order-disorder phase transition of mean field nature as in the WS model as well as the existence of two absorbing phases separated by an active phase. The WS network also shows an active-absorbing (A-A) phase transition occurring at the known order-disorder transition point. The comparison of the RSF and the BA network results show interesting difference in finite size dependence.△ Less"
It's a Man's Wikipedia? Assessing Gender Inequality in an Online Encyclopedia,"Authors:Claudia Wagner,David Garcia,Mohsen Jadidi,Markus Strohmaier","Abstract:Wikipedia is a community-created encyclopedia that contains information about notable people from different countries, epochs and disciplines and aims to document the world's knowledge from a neutral point of view. However, the narrow diversity of the Wikipedia editor community has the potential to introduce systemic biases such as gender biases into the content of Wikipedia. In this paper we aim…▽ MoreWikipedia is a community-created encyclopedia that contains information about notable people from different countries, epochs and disciplines and aims to document the world's knowledge from a neutral point of view. However, the narrow diversity of the Wikipedia editor community has the potential to introduce systemic biases such as gender biases into the content of Wikipedia. In this paper we aim to tackle a sub problem of this larger challenge by presenting and applying a computational method for assessing gender bias on Wikipedia along multiple dimensions. We find that while women on Wikipedia are covered and featured well in many Wikipedia language editions, the way women are portrayed starkly differs from the way men are portrayed. We hope our work contributes to increasing awareness about gender biases online, and in particular to raising attention to the different levels in which gender biases can manifest themselves on the web.△ Less"
Second-Order Belief Hidden Markov Models,"Authors:Jungyeul Park,Mouna Chebbah,Siwar Jendoubi,Arnaud Martin","Abstract:Hidden Markov Models (HMMs) are learning methods for pattern recognition. The probabilistic HMMs have been one of the most used techniques based on the Bayesian model. First-order probabilistic HMMs were adapted to the theory of belief functions such that Bayesian probabilities were replaced with mass functions. In this paper, we present a second-order Hidden Markov Model using belief functions. P…▽ MoreHidden Markov Models (HMMs) are learning methods for pattern recognition. The probabilistic HMMs have been one of the most used techniques based on the Bayesian model. First-order probabilistic HMMs were adapted to the theory of belief functions such that Bayesian probabilities were replaced with mass functions. In this paper, we present a second-order Hidden Markov Model using belief functions. Previous works in belief HMMs have been focused on the first-order HMMs. We extend them to the second-order model.△ Less"
Belief Approach for Social Networks,"Authors:Salma Ben Dhaou,Mouloud Kharoune,Arnaud Martin,Boutheina Ben Yaghlane","Abstract:Nowadays, social networks became essential in information exchange between individuals. Indeed, as users of these networks, we can send messages to other people according to the links connecting us. Moreover, given the large volume of exchanged messages, detecting the true nature of the received message becomes a challenge. For this purpose, it is interesting to consider this new tendency with rea…▽ MoreNowadays, social networks became essential in information exchange between individuals. Indeed, as users of these networks, we can send messages to other people according to the links connecting us. Moreover, given the large volume of exchanged messages, detecting the true nature of the received message becomes a challenge. For this purpose, it is interesting to consider this new tendency with reasoning under uncertainty by using the theory of belief functions. In this paper, we tried to model a social network as being a network of fusion of information and determine the true nature of the received message in a well-defined node by proposing a new model: the belief social network.△ Less"
Designing a Belief Function-Based Accessibility Indicator to Improve Web Browsing for Disabled People,"Authors:Jean-Christophe Dubois,Yolande Le Gall,Arnaud Martin","Abstract:The purpose of this study is to provide an accessibility measure of web-pages, in order to draw disabled users to the pages that have been designed to be ac-cessible to them. Our approach is based on the theory of belief functions, using data which are supplied by reports produced by automatic web content assessors that test the validity of criteria defined by the WCAG 2.0 guidelines proposed by t…▽ MoreThe purpose of this study is to provide an accessibility measure of web-pages, in order to draw disabled users to the pages that have been designed to be ac-cessible to them. Our approach is based on the theory of belief functions, using data which are supplied by reports produced by automatic web content assessors that test the validity of criteria defined by the WCAG 2.0 guidelines proposed by the World Wide Web Consortium (W3C) organization. These tools detect errors with gradual degrees of certainty and their results do not always converge. For these reasons, to fuse information coming from the reports, we choose to use an information fusion framework which can take into account the uncertainty and imprecision of infor-mation as well as divergences between sources. Our accessibility indicator covers four categories of deficiencies. To validate the theoretical approach in this context, we propose an evaluation completed on a corpus of 100 most visited French news websites, and 2 evaluation tools. The results obtained illustrate the interest of our accessibility indicator.△ Less"
Belief Hierarchical Clustering,"Authors:Wiem Maalel,Kuang Zhou,Arnaud Martin,Zied Elouedi","Abstract:In the data mining field many clustering methods have been proposed, yet standard versions do not take into account uncertain databases. This paper deals with a new approach to cluster uncertain data by using a hierarchical clustering defined within the belief function framework. The main objective of the belief hierarchical clustering is to allow an object to belong to one or several clusters. To…▽ MoreIn the data mining field many clustering methods have been proposed, yet standard versions do not take into account uncertain databases. This paper deals with a new approach to cluster uncertain data by using a hierarchical clustering defined within the belief function framework. The main objective of the belief hierarchical clustering is to allow an object to belong to one or several clusters. To each belonging, a degree of belief is associated, and clusters are combined based on the pignistic properties. Experiments with real uncertain data show that our proposed method can be considered as a propitious tool.△ Less"
A regularized representation of the fractional Laplacian in n dimensions and its relation to Weierstrass-Mandelbrot type fractal functions,"Authors:Thomas Michelitsch,Gérard Maugin,Shahram Derogar,Rahman Mujibur","Abstract:We demonstrate that the fractional Laplacian (FL) is the principal characteristic operator of harmonic systems with {\it self-similar}
interparticle interactions. We show that the FL represents the ""{\it fractional continuum limit}"" of a discrete ""self-similar Laplacian"" which is obtained by Hamilton's variational principle from a discrete spring model.
We deduce from generalized self-similar elas…▽ MoreWe demonstrate that the fractional Laplacian (FL) is the principal characteristic operator of harmonic systems with {\it self-similar}
interparticle interactions. We show that the FL represents the ""{\it fractional continuum limit}"" of a discrete ""self-similar Laplacian"" which is obtained by Hamilton's variational principle from a discrete spring model.
We deduce from generalized self-similar elastic potentials regular representations for the FL which involve convolutions of symmetric finite difference operators of even orders extending the standard representation of the FL.
Further we deduce a regularized representation for the FL $-(-Δ)^{\fracα{2}}$ holding for $α\in \R \geq 0$.
We give an explicit proof that the regularized representation of the FL gives for integer powers $\fracα{2} \in \N\_0$ a distributional representation of the standard Laplacian operator $Δ$ 
including the trivial unity operator for $α\rightarrow 0$. 
We demonstrate that self-similar {\it harmonic} systems are {\it all} governed in a distributional sense by this {\it
regularized representation of the FL} which therefore can be conceived as characteristic footprint of self-similarity.△ Less"
Proceedings of the 11th workshop on Quantum Physics and Logic,"Authors:Bob Coecke,Ichiro Hasuo,Prakash Panangaden","Abstract:…Earlier workshops in this series, with the same acronym under the name ""Quantum Programming Languages"", were held in Ottawa (2003), Turku (2004), Chicago (2005), andOxford(2006). The first QPL under the new name Quantum Physics and Logic was held in Reykjavik (2008), followed byOxford(2009 and 2010), Nijme…▽ MoreThis volume contains the proceedings of the 11th International Workshop on Quantum Physics and Logic (QPL 2014), which was held from the 4th to the 6th of June, 2014, at Kyoto University, Japan.
  The goal of the QPL workshop series is to bring together researchers working on mathematical foundations of quantum physics, quantum computing and spatio-temporal causal structures, and in particular those that use logical tools, ordered algebraic and category-theoretic structures, formal languages, semantic methods and other computer science methods for the study of physical behavior in general. Over the past few years, there has been growing activity in these foundational approaches, together with a renewed interest in the foundations of quantum theory, which complement the more mainstream research in quantum computation. Earlier workshops in this series, with the same acronym under the name ""Quantum Programming Languages"", were held in Ottawa (2003), Turku (2004), Chicago (2005), andOxford(2006). The first QPL under the new name Quantum Physics and Logic was held in Reykjavik (2008), followed byOxford(2009 and 2010), Nijmegen (2011), Brussels (2012) and Barcelona (2013).△ Less"
On C*-algebras which cannot be decomposed into tensor products with both factors infinite-dimensional,Authors:Tomasz Kania,"Abstract:We prove that C*-algebras which, as Banach spaces, are Grothendieck cannot be decomposed into a tensor product of two infinite-dimensional C*-algebras. By a result of Pfitzner, this class contains all von Neumann algebras and their norm-quotients. We thus complement a recent result of Ghasemi who established a similar conclusion for the class of SAW*-algebras.We prove that C*-algebras which, as Banach spaces, are Grothendieck cannot be decomposed into a tensor product of two infinite-dimensional C*-algebras. By a result of Pfitzner, this class contains all von Neumann algebras and their norm-quotients. We thus complement a recent result of Ghasemi who established a similar conclusion for the class of SAW*-algebras.△ Less"
Two-velocity hydrodynamics in fluid mechanics: Part I Well posedness for zero Mach number systems,"Authors:Didier Bresch,Vincent Giovangigli,Ewelina Zatorska","Abstract:…and the conductivity introduced in [D. Bresch, E.H. Essoufi, and M. Sy, J. Math. Fluid Mech. 2007] gives a more complete answer to an open question formulated in [P.-L. Lions,Oxford1998]. A new mathematical entropy shows clearly the existence of two-velocity hydrodynamics with a fixed mixture ratio. As an application of our result we first discuss a model…▽ MoreIn this paper we prove global in time existence of weak solutions to zero Mach number systems arising in fluid mechanics. Relaxing a certain algebraic constraint between the viscosity and the conductivity introduced in [D. Bresch, E.H. Essoufi, and M. Sy, J. Math. Fluid Mech. 2007] gives a more complete answer to an open question formulated in [P.-L. Lions,Oxford1998]. A new mathematical entropy shows clearly the existence of two-velocity hydrodynamics with a fixed mixture ratio. As an application of our result we first discuss a model of gaseous mixture extending the results of [P. Embid, Comm. Partial Diff. Eqs. 1987] to the global weak solutions framework. Second, we present the ghost effect system studied by [C.D. Levermore, W. Sun, K. Trivisa, SIAM J. Math. Anal. 2012] and discuss a contribution of the density-dependent heat-conductivity coefficient to the issue of existence of weak solutions.△ Less"
TILDE: A Temporally Invariant Learned DEtector,"Authors:Yannick Verdie,Kwang Moo Yi,Pascal Fua,Vincent Lepetit","Abstract:…our method significantly outperforms the state-of-the-art methods in such challenging conditions, while still achieving state-of-the-art performance on the untrained standardOxforddataset.▽ MoreWe introduce a learning-based approach to detect repeatable keypoints under drastic imaging changes of weather and lighting conditions to which state-of-the-art keypoint detectors are surprisingly sensitive. We first identify good keypoint candidates in multiple training images taken from the same viewpoint. We then train a regressor to predict a score map whose maxima are those points so that they can be found by simple non-maximum suppression. As there are no standard datasets to test the influence of these kinds of changes, we created our own, which we will make publicly available. We will show that our method significantly outperforms the state-of-the-art methods in such challenging conditions, while still achieving state-of-the-art performance on the untrained standardOxforddataset.△ Less"
Temporal and spectral properties of quantum light,"Authors:Birgit Stiller,Ulrich Seyfarth,Gerd Leuchs","Abstract:The modes of the electromagnetic field are solutions of Maxwell's equations taking into account the material boundary conditions. The field modes of classical optics - properly normalized - are also the mode functions of quantum optics. Quantum physics adds that the excitation within each mode is quantized in close analogy to the harmonic oscillator. A complete set of mode functions forms a basis…▽ MoreThe modes of the electromagnetic field are solutions of Maxwell's equations taking into account the material boundary conditions. The field modes of classical optics - properly normalized - are also the mode functions of quantum optics. Quantum physics adds that the excitation within each mode is quantized in close analogy to the harmonic oscillator. A complete set of mode functions forms a basis with which any new modes can be reconstructed. In full generality each electromagnetic mode function in the four dimensional space-time is mathematically equivalent to a harmonic oscillator. The quantization of the electromagnetic field defines the excitation per mode and the correlation between modes. In classical optics there can be oscillations and stochastic fluctuations of amplitude, phase, polarization et cetera. In quantum optics there are in addition uncertain quantum field components, quantum correlations and quantized energies. Here, we present selected topics from classical to quantum optics. We start in the second chapter with the classical optics description of a light field and its spectral densities, their measurement and their interpretation. In the third chapter the quantum properties of a single light mode are reviewed as well as ways to measure these quantum properties. Gaussian states of a light mode are emphasized, i. e. states for which the Wigner function has a two dimensional Gaussian shape. The fourth chapter will be concerned with more than one mode presenting a unifying approach to quadratic Hamiltonians including phase conjugation which is related to time reversal.△ Less"
Distributed Representations for Compositional Semantics,Authors:Karl Moritz Hermann,"Abstract:The mathematical representation of semantics is a key issue for Natural Language Processing (NLP). A lot of research has been devoted to finding ways of representing the semantics of individual words in vector spaces. Distributional approaches --- meaning distributed representations that exploit co-occurrence statistics of large corpora --- have proved popular and successful across a number of tas…▽ MoreThe mathematical representation of semantics is a key issue for Natural Language Processing (NLP). A lot of research has been devoted to finding ways of representing the semantics of individual words in vector spaces. Distributional approaches --- meaning distributed representations that exploit co-occurrence statistics of large corpora --- have proved popular and successful across a number of tasks. However, natural language usually comes in structures beyond the word level, with meaning arising not only from the individual words but also the structure they are contained in at the phrasal or sentential level. Modelling the compositional process by which the meaning of an utterance arises from the meaning of its parts is an equally fundamental task of NLP.
  This dissertation explores methods for learning distributed semantic representations and models for composing these into representations for larger linguistic units. Our underlying hypothesis is that neural models are a suitable vehicle for learning semantically rich representations and that such representations in turn are suitable vehicles for solving important tasks in natural language processing. The contribution of this thesis is a thorough evaluation of our hypothesis, as part of which we introduce several new approaches to representation learning and compositional semantics, as well as multiple state-of-the-art models which apply distributed semantic representations to various tasks in NLP.△ Less"
Unifying Visual-Semantic Embeddings with Multimodal Neural Language Models,"Authors:Ryan Kiros,Ruslan Salakhutdinov,Richard S. Zemel","Abstract:…to encode sentences, we match the state-of-the-art performance on Flickr8K and Flickr30K without using object detections. We also set new best results when using the 19-layerOxfordconvolutional network. Furthermore we show that with linear encoders, the learned embedding space captures multimodal regularities in terms of vector space arithmetic e.g. *image…▽ MoreInspired by recent advances in multimodal learning and machine translation, we introduce an encoder-decoder pipeline that learns (a): a multimodal joint embedding space with images and text and (b): a novel language model for decoding distributed representations from our space. Our pipeline effectively unifies joint image-text embedding models with multimodal neural language models. We introduce the structure-content neural language model that disentangles the structure of a sentence to its content, conditioned on representations produced by the encoder. The encoder allows one to rank images and sentences while the decoder can generate novel descriptions from scratch. Using LSTM to encode sentences, we match the state-of-the-art performance on Flickr8K and Flickr30K without using object detections. We also set new best results when using the 19-layerOxfordconvolutional network. Furthermore we show that with linear encoders, the learned embedding space captures multimodal regularities in terms of vector space arithmetic e.g. *image of a blue car* - ""blue"" + ""red"" is near images of red cars. Sample captions generated for 800 images are made available for comparison.△ Less"
Boundary scattering of phonons: specularity of a randomly rough surface in the small perturbation limit,Authors:A. A. Maznev,"Abstract:…which is in agreement with the well-known Kirchhoff approximation result often referred to as Ziman's equation [J. M. Ziman, Electrons and Phonons (Clarendon Press,Oxford, 1960)]. In the opposite limiting case of a small correlation length, the specularity reduction is found to be proportional to $η^2k^4L^2$, with the fourth power dependence on frequen…▽ MoreScattering of normally incident longitudinal and transverse acoustic waves by a randomly rough surface of an elastically isotropic solid is analyzed within the small perturbation approach. In the limiting case of a large correlation length $L$ compared with the acoustic wavelength, the specularity reduction is given by $4η^2k^2$, where $η$ is the RMS roughness and $k$ is the acoustic wavevector, which is in agreement with the well-known Kirchhoff approximation result often referred to as Ziman's equation [J. M. Ziman, Electrons and Phonons (Clarendon Press,Oxford, 1960)]. In the opposite limiting case of a small correlation length, the specularity reduction is found to be proportional to $η^2k^4L^2$, with the fourth power dependence on frequency as in Rayleigh scattering. Numerical calculations for a Gaussian autocorrelation function of surface roughness connect these limiting cases and reveal a maximum of diffuse scattering at an intermediate value of $L$. This maximum becomes increasingly pronounced for the incident longitudinal wave as the Poisson's ratio of the medium approaches 1/2 as a result of increased scattering into transverse and Rayleigh surface waves. The results indicate that thermal transport models using Ziman's formula are likely to overestimate the heat flux dissipation due to boundary scattering, whereas modeling interface roughness as atomic disorder is likely to underestimate scattering.△ Less"
Gradient-dependent upper bound for the exchange-correlation energy and application to density functional theory,"Authors:L. A. Constantin,E. Fabiano,A. Terentjevs,F. Della Sala","Abstract:…bound for the exchange-correlation energy (sLL), based on the recent non-local bound derived by Lewin and Lieb. We show that sLL is equivalent to the original Lieb-Oxfordbound in rapidly-varying density cases but it is tighter for slowly-varying density systems. To show the utility of the sLL bound we apply it to the construction of simple semilocal and non…▽ MoreWe propose a simple gradient-dependent bound for the exchange-correlation energy (sLL), based on the recent non-local bound derived by Lewin and Lieb. We show that sLL is equivalent to the original Lieb-Oxfordbound in rapidly-varying density cases but it is tighter for slowly-varying density systems. To show the utility of the sLL bound we apply it to the construction of simple semilocal and non-local exchange and correlation functionals. In both cases improved results, with respect to the use of Lieb-Oxfordbound, are obtained showing the power of the sLL bound.△ Less"
Testing and Using the Lewin-Lieb Bound in Density Functional Theory,"Authors:David V. Feinblum,John Kenison,Kieron Burke","Abstract:Lewin and Lieb have recently proven several new bounds on the exchange-correlation energy that complement the Lieb-Oxfordbound. We test these bounds for atoms, for slowly-varying gases, and for Hooke's atom, finding them usually less strict than the Lieb-Oxfordbound. However, we also show that, if a generalized g…▽ MoreLewin and Lieb have recently proven several new bounds on the exchange-correlation energy that complement the Lieb-Oxfordbound. We test these bounds for atoms, for slowly-varying gases, and for Hooke's atom, finding them usually less strict than the Lieb-Oxfordbound. However, we also show that, if a generalized gradient approximation (GGA) is to guarantee satisfaction of the new bounds for all densities, new restrictions on the the exchange-correlation enhancement factor are implied.△ Less"
Gas around galaxy haloes: methodology comparisons using hydrodynamical simulations of the intergalactic medium,"Authors:Avery Meiksin,James S. Bolton,Eric R. Tittley","Abstract:We perform cosmological simulations of the intergalactic medium (IGM) at redshift z ~ 3 using the numerical gravity-hydrodynamics codes GADGET-3 and Enzo for the purpose of modelling the gaseous environments of galaxies. We identify haloes in the simulations using three different algorithms. Different rank orderings of the haloes by mass result, introducing a limiting factor in identifying haloes…▽ MoreWe perform cosmological simulations of the intergalactic medium (IGM) at redshift z ~ 3 using the numerical gravity-hydrodynamics codes GADGET-3 and Enzo for the purpose of modelling the gaseous environments of galaxies. We identify haloes in the simulations using three different algorithms. Different rank orderings of the haloes by mass result, introducing a limiting factor in identifying haloes with observed galaxies. We also compare the physical properties of the gas between the two codes, focussing primarily on the gas outside the virial radius, motivated by recent HI absorption measurements of the gas around z ~ 2 - 3 galaxies. The internal dispersion velocities of the gas in the haloes have converged for a box size of 30 comoving Mpc, but the centre-of-mass peculiar velocities of the haloes have not up to a box size of 60 comoving Mpc. The density and temperature of the gas within the instantaneous turn-around radii of the haloes are adequately captured for box sizes 30 Mpc on a side, but the results are highly sensitive to the treatment of unresolved, rapidly cooling gas, with the gas mass fraction within the virial radius severely depleted by star formation in the GADGET-3 simulations. Convergence of the gas peculiar velocity field on large scales requires a box size of at least 60 Mpc. Outside the turn-around radius, the physical state of the gas agrees to 30 percent or better both with box size and between simulation methods. We conclude that generic IGM simulations make accurate predictions for the intergalactic gas properties beyond the halo turn-around radii, but the gas properties on smaller scales are highly dependent on star formation and feedback implementations.△ Less"
Improvement of PSO algorithm by memory based gradient search - application in inventory management,"Authors:Tamás Varga,András Király,János Abonyi",Abstract:Advanced inventory management in complex supply chains requires effective and robust nonlinear optimization due to the stochastic nature of supply and demand variations. Application of estimated gradients can boost up the convergence of Particle Swarm Optimization (PSO) algorithm but classical gradient calculation cannot be applied to stochastic and uncertain systems. In these situations Monte-Car…▽ MoreAdvanced inventory management in complex supply chains requires effective and robust nonlinear optimization due to the stochastic nature of supply and demand variations. Application of estimated gradients can boost up the convergence of Particle Swarm Optimization (PSO) algorithm but classical gradient calculation cannot be applied to stochastic and uncertain systems. In these situations Monte-Carlo (MC) simulation can be applied to determine the gradient. We developed a memory based algorithm where instead of generating and evaluating new simulated samples the stored and shared former function evaluations of the particles are sampled to estimate the gradients by local weighted least squares regression. The performance of the resulted regional gradient-based PSO is verified by several benchmark problems and in a complex application example where optimal reorder points of a supply chain are determined.△ Less
Initial Mass Function for Massive Galaxies at z$\sim$1,"Authors:Shravan Shetty,Michele Cappellari","Abstract:We present the results on the stellar Initial Mass Function (IMF) normalisation of 68 massive ($M_\ast=10^{11}-10^{12}$$M_\odot$) Early-Type Galaxies (ETGs) at redshift of $\sim$1. This was achieved by deriving the stellar Mass-to-Light ratio (M/L) of the galaxies through axis-symmetric dynamical modelling and comparing it to the same derived via stellar population modelling through full spectrum…▽ MoreWe present the results on the stellar Initial Mass Function (IMF) normalisation of 68 massive ($M_\ast=10^{11}-10^{12}$$M_\odot$) Early-Type Galaxies (ETGs) at redshift of $\sim$1. This was achieved by deriving the stellar Mass-to-Light ratio (M/L) of the galaxies through axis-symmetric dynamical modelling and comparing it to the same derived via stellar population modelling through full spectrum fitting. The study also employs an Abundance Matching technique to account for the dark matter within the galaxies. The results demonstrate that massive ETGs at high redshifts on average have a Salpeter-like IMF normalisation, while providing observational evidence supporting previous predictions of low dark matter fraction in the inner regions ($\lesssim$ 1R$_{\rm e}$) of galaxies at higher redshift.△ Less"
Asymptotic behavior of splitting schemes involving time-subcycling techniques,"Authors:Guillaume Dujardin,Pauline Lafitte","Abstract:This paper deals with the numerical integration of well-posed multiscale systems of ODEs or evolutionary PDEs. As these systems appear naturally in engineering problems, time-subcycling techniques are widely used every day to improve computational efficiency. These methods rely on a decomposition of the vector field in a fast part and a slow part and take advantage of that decomposition. This way,…▽ MoreThis paper deals with the numerical integration of well-posed multiscale systems of ODEs or evolutionary PDEs. As these systems appear naturally in engineering problems, time-subcycling techniques are widely used every day to improve computational efficiency. These methods rely on a decomposition of the vector field in a fast part and a slow part and take advantage of that decomposition. This way, if an unconditionnally stable (semi-)implicit scheme cannot be easily implemented, one can integrate the fast equations with a much smaller time step than that of the slow equations, instead of having to integrate the whole system with a very small time-step to ensure stability. Then, one can build a numerical integrator using a standard composition method, such as a Lie or a Strang formula for example. Such methods are primarily designed to be convergent in short-time to the solution of the original problems. However, their longtime behavior rises interesting questions, the answers to which are not very well known. In particular, when the solutions of the problems converge in time to an asymptotic equilibrium state, the question of the asymptotic accuracy of the numerical longtime limit of the schemes as well as that of the rate of convergence is certainly of interest. In this context, the asymptotic error is defined as the difference between the exact and numerical asymptotic states. The goal of this paper is to apply that kind of numerical methods based on splitting schemes with subcycling to some simple examples of evolutionary ODEs and PDEs that have attractive equilibrium states, to address the aforementioned questions of asymptotic accuracy, to perform a rigorous analysis, and to compare them with their counterparts without subcycling. Our analysis is developed on simple linear ODE and PDE toy-models and is illustrated with several numerical experiments on these toy-models as well as on more complex systems. Lie and△ Less"
Casimir forces,"Authors:S. Reynaud,A. Lambrecht","Abstract:The present notes are organized as the lectures given at the Les Houches Summer School ""Quantum Optics and Nanophotonics"" in August 2013. The first section contains an introduction and a description of the current state-of-the-art for Casimir force measurements and their comparison with theory. The second and third sections are a pedagogical presentation of the main features of the theory of Casim…▽ MoreThe present notes are organized as the lectures given at the Les Houches Summer School ""Quantum Optics and Nanophotonics"" in August 2013. The first section contains an introduction and a description of the current state-of-the-art for Casimir force measurements and their comparison with theory. The second and third sections are a pedagogical presentation of the main features of the theory of Casimir forces for 1-dimensional model systems and for mirrors in 3-dimensional space.△ Less"
Angular Momentum across the Hubble sequence from the CALIFA survey,"Authors:Jesús Falcón-Barroso,Mariya Lyubenova,Glenn van de Ven,the CALIFA collaboration","Abstract:We investigate the stellar angular momentum of galaxies across the Hubble sequence from the CALIFA survey. The distribution of CALIFA elliptical and lenticular galaxies in the $λ_{\rm Re}-ε_{\rm e}$ diagram is consistent with that shown by the Atlas$^\mathrm{3D}$ survey. Our data, however, show that the location of spiral galaxies in this diagram is significantly different. We have found two famil…▽ MoreWe investigate the stellar angular momentum of galaxies across the Hubble sequence from the CALIFA survey. The distribution of CALIFA elliptical and lenticular galaxies in the $λ_{\rm Re}-ε_{\rm e}$ diagram is consistent with that shown by the Atlas$^\mathrm{3D}$ survey. Our data, however, show that the location of spiral galaxies in this diagram is significantly different. We have found two families of spiral galaxies with particularly peculiar properties: (a) spiral galaxies with much higher $λ_{\rm Re}$ values than any elliptical and lenticular galaxy; (b) low-mass spiral galaxies with observed $λ_{\rm Re}$ values much lower than expected for their apparent flattening. We use these two families of objects to argue that (1) fading alone cannot explain the transformation of spiral to lenticular galaxies, and (2) that those low-mass spiral galaxies are in fact dark matter dominated, which explains the unusually low angular momentum.△ Less"
Error correcting codes and spatial coupling,"Authors:Rafah El-Khatib,Jean Barbier,Ayaka Sakata,Rüdiger Urbanke","Abstract:These are notes from the lecture of Rüdiger Urbanke given at the autumn school ""Statistical Physics, Optimization, Inference, and Message-Passing Algorithms"", that took place in Les Houches, France from Monday September 30th, 2013, till Friday October 11th, 2013. The school was organized by Florent Krzakala from UPMC and ENS Paris, Federico Ricci-Tersenghi from La Sapienza Roma, Lenka Zdeborovà f…▽ MoreThese are notes from the lecture of Rüdiger Urbanke given at the autumn school ""Statistical Physics, Optimization, Inference, and Message-Passing Algorithms"", that took place in Les Houches, France from Monday September 30th, 2013, till Friday October 11th, 2013. The school was organized by Florent Krzakala from UPMC and ENS Paris, Federico Ricci-Tersenghi from La Sapienza Roma, Lenka Zdeborovà from CEA Saclay and CNRS, and Riccardo Zecchina from Politecnico Torino. The first three sections cover the basics of polar codes and low density parity check codes. In the last three sections, we see how the spatial coupling helps belief propagation decoding.△ Less"
MEGAHIT: An ultra-fast single-node solution for large and complex metagenomics assembly via succinct de Bruijn graph,"Authors:Dinghua Li,Chi-Man Liu,Ruibang Luo,Kunihiko Sadakane,Tak-Wah Lam","Abstract:MEGAHIT is a NGS de novo assembler for assembling large and complex metagenomics data in a time- and cost-efficient manner. It finished assembling a soil metagenomics dataset with 252Gbps in 44.1 hours and 99.6 hours on a single computing node with and without a GPU, respectively. MEGAHIT assembles the data as a whole, i.e., it avoids pre-processing like partitioning and normalization, which might…▽ MoreMEGAHIT is a NGS de novo assembler for assembling large and complex metagenomics data in a time- and cost-efficient manner. It finished assembling a soil metagenomics dataset with 252Gbps in 44.1 hours and 99.6 hours on a single computing node with and without a GPU, respectively. MEGAHIT assembles the data as a whole, i.e., it avoids pre-processing like partitioning and normalization, which might compromise on result integrity. MEGAHIT generates 3 times larger assembly, with longer contig N50 and average contig length than the previous assembly. 55.8% of the reads were aligned to the assembly, which is 4 times higher than the previous. The source code of MEGAHIT is freely available at https://github.com/voutcn/megahit under GPLv3 license.△ Less"
Steric engineering of metal-halide perovskites with tunable optical band gaps,"Authors:Marina R. Filip,Giles E. Eperon,Henry J. Snaith,Feliciano Giustino","Abstract:Owing to their high energy-conversion efficiency and inexpensive fabrication routes, solar cells based on metal-organic halide perovskites have rapidly gained prominence as a disruptive technology. An attractive feature of perovskite absorbers is the possibility of tailoring their properties by changing the elemental composition through the chemical precursors. In this context, rational in silico…▽ MoreOwing to their high energy-conversion efficiency and inexpensive fabrication routes, solar cells based on metal-organic halide perovskites have rapidly gained prominence as a disruptive technology. An attractive feature of perovskite absorbers is the possibility of tailoring their properties by changing the elemental composition through the chemical precursors. In this context, rational in silico design represents a powerful tool for mapping the vast materials landscape and accelerating discovery. Here we show that the optical band gap of metal-halide perovskites, a key design parameter for solar cells, strongly correlates with a simple structural feature, the largest metal-halide-metal bond angle. Using this descriptor we suggest continuous tunability of the optical gap from the mid-infrared to the visible. Precise band gap engineering is achieved by controlling the bond angles through the steric size of the molecular cation. Based on these design principles we predict novel low-gap perovskites for optimum photovoltaic efficiency, and we demonstrate the concept of band gap modulation by synthesising and characterising novel mixed-cation perovskites.△ Less"
Expectation Propagation,"Authors:Jack Raymond,Andre Manoel,Manfred Opper","Abstract:Variational inference is a powerful concept that underlies many iterative approximation algorithms; expectation propagation, mean-field methods and belief propagations were all central themes at the school that can be perceived from this unifying framework. The lectures of Manfred Opper introduce the archetypal example of Expectation Propagation, before establishing the connection with the other a…▽ MoreVariational inference is a powerful concept that underlies many iterative approximation algorithms; expectation propagation, mean-field methods and belief propagations were all central themes at the school that can be perceived from this unifying framework. The lectures of Manfred Opper introduce the archetypal example of Expectation Propagation, before establishing the connection with the other approximation methods. Corrections by expansion about the expectation propagation are then explained. Finally some advanced inference topics and applications are explored in the final sections.△ Less"
Statistical Estimation: From Denoising to Sparse Regression and Hidden Cliques,"Authors:Eric W. Tramel,Santhosh Kumar,Andrei Giurgiu,Andrea Montanari","Abstract:These notes review six lectures given by Prof. Andrea Montanari on the topic of statistical estimation for linear models. The first two lectures cover the principles of signal recovery from linear measurements in terms of minimax risk. Subsequent lectures demonstrate the application of these principles to several practical problems in science and engineering. Specifically, these topics include den…▽ MoreThese notes review six lectures given by Prof. Andrea Montanari on the topic of statistical estimation for linear models. The first two lectures cover the principles of signal recovery from linear measurements in terms of minimax risk. Subsequent lectures demonstrate the application of these principles to several practical problems in science and engineering. Specifically, these topics include denoising of error-laden signals, recovery of compressively sensed signals, reconstruction of low-rank matrices, and also the discovery of hidden cliques within large networks.△ Less"
Local Algorithms for Graphs,"Authors:David Gamarnik,Mathieu Hemery,Samuel Hetterich",Abstract:We are going to analyze local algorithms over sparse random graphs. These algorithms are based on local information where local regards to a decision made by the exploration of a small neighbourhood of a certain vertex plus a believe of the structure of the whole graph and maybe added some randomness. This kind of algorithms can be a natural response to the given problem or an efficient approximat…▽ MoreWe are going to analyze local algorithms over sparse random graphs. These algorithms are based on local information where local regards to a decision made by the exploration of a small neighbourhood of a certain vertex plus a believe of the structure of the whole graph and maybe added some randomness. This kind of algorithms can be a natural response to the given problem or an efficient approximation such as the Belief Propagation Algorithm.△ Less
Statistical inference with probabilistic graphical models,"Authors:Angélique Drémeau,Christophe Schülke,Yingying Xu,Devavrat Shah","Abstract:These are notes from the lecture of Devavrat Shah given at the autumn school ""Statistical Physics, Optimization, Inference, and Message-Passing Algorithms"", that took place in Les Houches, France from Monday September 30th, 2013, till Friday October 11th, 2013. The school was organized by Florent Krzakala from UPMC & ENS Paris, Federico Ricci-Tersenghi from La Sapienza Roma, Lenka Zdeborova from C…▽ MoreThese are notes from the lecture of Devavrat Shah given at the autumn school ""Statistical Physics, Optimization, Inference, and Message-Passing Algorithms"", that took place in Les Houches, France from Monday September 30th, 2013, till Friday October 11th, 2013. The school was organized by Florent Krzakala from UPMC & ENS Paris, Federico Ricci-Tersenghi from La Sapienza Roma, Lenka Zdeborova from CEA Saclay & CNRS, and Riccardo Zecchina from Politecnico Torino. This lecture of Devavrat Shah (MIT) covers the basics of inference and learning. It explains how inference problems are represented within structures known as graphical models. The theoretical basis of the belief propagation algorithm is then explained and derived. This lecture sets the stage for generalizations and applications of message passing algorithms.△ Less"
"Commons at the Intersection of Peer Production, Citizen Science, and Big Data: Galaxy Zoo",Authors:Michael J. Madison,"Abstract:The knowledge commons research framework is applied to a case of commons governance grounded in research in modern astronomy. The case, Galaxy Zoo, is a leading example of at least three different contemporary phenomena. In the first place Galaxy Zoo is a global citizen science project, in which volunteer non-scientists have been recruited to participate in large-scale data analysis via the Intern…▽ MoreThe knowledge commons research framework is applied to a case of commons governance grounded in research in modern astronomy. The case, Galaxy Zoo, is a leading example of at least three different contemporary phenomena. In the first place Galaxy Zoo is a global citizen science project, in which volunteer non-scientists have been recruited to participate in large-scale data analysis via the Internet. In the second place Galaxy Zoo is a highly successful example of peer production, sometimes known colloquially as crowdsourcing, by which data are gathered, supplied, and/or analyzed by very large numbers of anonymous and pseudonymous contributors to an enterprise that is centrally coordinated or managed. In the third place Galaxy Zoo is a highly visible example of data-intensive science, sometimes referred to as e-science or Big Data science, by which scientific researchers develop methods to grapple with the massive volumes of digital data now available to them via modern sensing and imaging technologies. This chapter synthesizes these three perspectives on Galaxy Zoo via the knowledge commons framework.△ Less"
Cavity Method: Message Passing from a Physics Perspective,"Authors:Gino Del Ferraro,Chuang Wang,Dani Martí,Marc Mézard","Abstract:In this three-sections lecture cavity method is introduced as heuristic framework from a Physics perspective to solve probabilistic graphical models and it is presented both at the replica symmetric (RS) and 1-step replica symmetry breaking (1RSB) level. This technique has been applied with success on a wide range of models and problems such as spin glasses, random constrain satisfaction problems…▽ MoreIn this three-sections lecture cavity method is introduced as heuristic framework from a Physics perspective to solve probabilistic graphical models and it is presented both at the replica symmetric (RS) and 1-step replica symmetry breaking (1RSB) level. This technique has been applied with success on a wide range of models and problems such as spin glasses, random constrain satisfaction problems (rCSP), error correcting codes etc. Firstly, the RS cavity solution for Sherrington-Kirkpatrick model---a fully connected spin glass model---is derived and its equivalence to the RS solution obtained using replicas is discussed. Then, the general cavity method for diluted graphs is illustrated both at RS and 1RSB level. The latter was a significant breakthrough in the last decade and has direct applications to rCSP. Finally, as example of an actual problem, K-SAT is investigated using belief and survey propagation.△ Less"
Replica Theory and Spin Glasses,"Authors:Flaviano Morone,Francesco Caltagirone,Elizabeth Harrison,Giorgio Parisi","Abstract:These are notes from the lectures of Giorgio Parisi given at the autumn school ""Statistical Physics, Optimization, Inference, and Message-Passing Algorithm"", that took place in Les Houches, France from Monday September 30th, 2013, till Friday October 11th, 2013. The school was organized by Florent Krzakala from UPMC and ENS Paris, Federico Ricci-Tersenghi from ""La Sapienza"" Roma, Lenka Zdeborová f…▽ MoreThese are notes from the lectures of Giorgio Parisi given at the autumn school ""Statistical Physics, Optimization, Inference, and Message-Passing Algorithm"", that took place in Les Houches, France from Monday September 30th, 2013, till Friday October 11th, 2013. The school was organized by Florent Krzakala from UPMC and ENS Paris, Federico Ricci-Tersenghi from ""La Sapienza"" Roma, Lenka Zdeborová from CEA Saclay and CNRS, and Riccardo Zecchina from Politecnico Torino. The first lecture contains an introduction to the replica method, along with a concrete application to the computation of the eigenvalue distribution of random matrices in the GOE. In the second lecture, the solution of the SK model is derived, along with the phenomenon of replica symmetry breaking (RSB). In the third part, the physical meaning of the RSB is explained. The ultrametricity of the space of pure states emerges as a consequence of the hierarchical RSB scheme. Moreover, it is shown how some low temperature properties of physical observables can be derived by invoking the stochastic stability principle. Lecture four contains some rigorous results on the SK model: the existence of the thermodynamic limit, and the proof of the exactness of the hierarchical RSB solution.△ Less"
Evolving Delta-oriented Software Product Line Architectures,"Authors:Arne Haber,Holger Renel,Bernhard Rumpe,Ina Schaefer","Abstract:Diversity is prevalent in modern software systems. Several system variants exist at the same time in order to adapt to changing user requirements. Additionally, software systems evolve over time in order to adjust to unanticipated changes in their application environment. In modern software development, software architecture modeling is an important means to deal with system complexity by architec…▽ MoreDiversity is prevalent in modern software systems. Several system variants exist at the same time in order to adapt to changing user requirements. Additionally, software systems evolve over time in order to adjust to unanticipated changes in their application environment. In modern software development, software architecture modeling is an important means to deal with system complexity by architectural decomposition. This leads to the need of architectural description languages that can represent spatial and temporal variability. In this paper, we present delta modeling of software architectures as a uniform modeling formalism for architectural variability in space and in time. In order to avoid degeneration of the product line model under system evolution, we present refactoring techniques to maintain and improve the quality of the variability model. Using a running example from the automotive domain, we evaluate our approach by carrying out a case study that compares delta modeling with annotative variability modeling.△ Less"
"Computational Complexity, Phase Transitions, and Message-Passing for Community Detection","Authors:Aurélien Decelle,Janina Hüttel,Alaa Saade,Cristopher Moore","Abstract:We take a whirlwind tour of problems and techniques at the boundary of computer science and statistical physics. We start with a brief description of P, NP, and NP-completeness. We then discuss random graphs, including the emergence of the giant component and the k-core, using techniques from branching processes and differential equations. Using these tools as well as the second moment method, we…▽ MoreWe take a whirlwind tour of problems and techniques at the boundary of computer science and statistical physics. We start with a brief description of P, NP, and NP-completeness. We then discuss random graphs, including the emergence of the giant component and the k-core, using techniques from branching processes and differential equations. Using these tools as well as the second moment method, we give upper and lower bounds on the critical clause density for random k-SAT. We end with community detection in networks, variational methods, the Bethe free energy, belief propagation, the detectability transition, and the non-backtracking matrix.△ Less"
Improved Lieb-Oxfordexchange-correlation inequality with gradient correction,"Authors:Mathieu Lewin,Elliott H. Lieb","Abstract:We prove a Lieb-Oxford-type inequality on the indirect part of the Coulomb energy of a general many-particle quantum state, with a lower constant than the original statement but involving an additional gradient correction. The result is similar to a recent inequality of Benguria, Bley and Loss, except that the correction term is purely local, which is more u…▽ MoreWe prove a Lieb-Oxford-type inequality on the indirect part of the Coulomb energy of a general many-particle quantum state, with a lower constant than the original statement but involving an additional gradient correction. The result is similar to a recent inequality of Benguria, Bley and Loss, except that the correction term is purely local, which is more usual in density functional theory. In an appendix, we discuss the connection between the indirect energy and the classical Jellium energy for constant densities. We show that they differ by an explicit shift due to the long range of the Coulomb potential.△ Less"
Complexity Bounds for Ordinal-Based Termination,Authors:Sylvain Schmitz,"Abstract:`What more than its truth do we know if we have a proof of a theorem in a given formal system?' We examine Kreisel's question in the particular context of program termination proofs, with an eye to deriving complexity bounds on program running times.
  Our main tool for this are length function theorems, which provide complexity bounds on the use of well quasi orders. We illustrate how to prove su…▽ More`What more than its truth do we know if we have a proof of a theorem in a given formal system?' We examine Kreisel's question in the particular context of program termination proofs, with an eye to deriving complexity bounds on program running times.
  Our main tool for this are length function theorems, which provide complexity bounds on the use of well quasi orders. We illustrate how to prove such theorems in the simple yet until now untreated case of ordinals. We show how to apply this new theorem to derive complexity bounds on programs when they are proven to terminate thanks to a ranking function into some ordinal.△ Less"
Orientation covariant aggregation of local descriptors with embeddings,"Authors:Giorgos Tolias,Teddy Furon,Hervé Jégou","Abstract:…aggregation strategy is effective for image search, as shown by experiments performed on standard benchmarks for image and particular object retrieval, namely Holidays andOxfordbuildings.▽ MoreImage search systems based on local descriptors typically achieve orientation invariance by aligning the patches on their dominant orientations. Albeit successful, this choice introduces too much invariance because it does not guarantee that the patches are rotated consistently. This paper introduces an aggregation strategy of local descriptors that achieves this covariance property by jointly encoding the angle in the aggregation stage in a continuous manner. It is combined with an efficient monomial embedding to provide a codebook-free method to aggregate local descriptors into a single vector representation. Our strategy is also compatible and employed with several popular encoding methods, in particular bag-of-words, VLAD and the Fisher vector. Our geometric-aware aggregation strategy is effective for image search, as shown by experiments performed on standard benchmarks for image and particular object retrieval, namely Holidays andOxfordbuildings.△ Less"
A new method for the determination of action integrals in the study of galactic dynamics,Authors:Michael F. J. Fox,"Abstract:Action-angle coordinates are an essential tool for understanding the properties of the six dimensional phase space involved in orbits of stars in galactic potentials. A new method, which does not require specific knowledge of a generating function, is described, implemented and tested that calculates the actions of an orbit in an arbitrary potential of an integrable Hamiltonian given a set of Cart…▽ MoreAction-angle coordinates are an essential tool for understanding the properties of the six dimensional phase space involved in orbits of stars in galactic potentials. A new method, which does not require specific knowledge of a generating function, is described, implemented and tested that calculates the actions of an orbit in an arbitrary potential of an integrable Hamiltonian given a set of Cartesian phase space points. The method chooses between the simple harmonic oscillator and isochrone potentials to fit the data using a Levenberg-Marquardt routine. An average is taken over the angle coordinates by calculating volumes in phase space using the metric free FiEstAS algorithm. The perfect ellipsoidal potential, with actions chosen a priori, is used to test the output of the algorithm, giving some results that agree within 1%. Minimisation of a sampling error is discussed along with an identification of a source of noise in the data.△ Less"
Mean-field stochastic differential equations and associated PDEs,"Authors:Rainer Buckdahn,Juan Li,Shige Peng,Catherine Rainer","Abstract:In this paper we consider a mean-field stochastic differential equation, also called Mc Kean-Vlasov equation, with initial data $(t,x)\in[0,T]\times R^d,$ which coefficients depend on both the solution $X^{t,x}_s$ but also its law. By considering square integrable random variables $ξ$ as initial condition for this equation, we can easily show the flow property of the solution $X^{t,ξ}_s$ of this n…▽ MoreIn this paper we consider a mean-field stochastic differential equation, also called Mc Kean-Vlasov equation, with initial data $(t,x)\in[0,T]\times R^d,$ which coefficients depend on both the solution $X^{t,x}_s$ but also its law. By considering square integrable random variables $ξ$ as initial condition for this equation, we can easily show the flow property of the solution $X^{t,ξ}_s$ of this new equation. Associating it with a process $X^{t,x,P_ξ}_s$ which coincides with $X^{t,ξ}_s$, when one substitutes $ξ$ for $x$, but which has the advantage to depend only on the law $P_ξ$ of $ξ$, we characterise the function $V(t,x,P_ξ)=E[Φ(X^{t,x,P_ξ}_T,P_{X^{t,ξ}_T})]$ under appropriate regularity conditions on the coefficients of the stochastic differential equation as the unique classical solution of a non local PDE of mean-field type, involving the first and second order derivatives of $V$ with respect to its space variable and the probability law. The proof bases heavily on a preliminary study of the first and second order derivatives of the solution of the mean-field stochastic differential equation with respect to the probability law and a corresponding Itô formula. In our approach we use the notion of derivative with respect to a square integrable probability measure introduced in \cite{PL} and we extend it in a direct way to second order derivatives.△ Less"
Wavefront sensing from the image domain with theOxford-SWIFT integral field spectrograph,"Authors:Benjamin Pope,Niranjan Thatte,Rick Burruss,Matthias Tecza,Fraser Clarke,Garret Cotter","Abstract:…AO, where an asymmetric pupil mask and a single image are sufficient to map aberrations up to high order. We push toward internally diffraction-limited performance with theOxford-SWIFT integral field spectrograph coupled with the PALM-3000 extreme AO system on the Palomar 200-inch telescope. This represents the first observation in which the PALM-3000 + SWI…▽ MoreThe limits for adaptive-optics (AO) imaging at high contrast and high resolution are determined by residual phase errors from non-common-path aberrations not sensed by the wavefront sensor, especially for integral field spectrographs, where phase diversity techniques are complicated by the image slicer. We present the first application of kernel phase-based wavefront sensing to ground-based AO, where an asymmetric pupil mask and a single image are sufficient to map aberrations up to high order. We push toward internally diffraction-limited performance with theOxford-SWIFT integral field spectrograph coupled with the PALM-3000 extreme AO system on the Palomar 200-inch telescope. This represents the first observation in which the PALM-3000 + SWIFT internal point-spread-function has closely approached the Airy pattern. While this can only be used on SWIFT with an internal stimulus source, as at short wavelengths the uncorrected atmospheric wavefront errors are still > 1 radian, this nevertheless demonstrates the feasibility of detecting non-common-path errors with this method as an active optics paradigm for near-infrared, AO-corrected instruments at Palomar. We note that this is a particularly promising approach for correcting integral field spectrographs, as the diversity of many narrowband images provides strong constraints on the wavefront error estimate, and the average of reconstructions from many narrow bands can be used to improve overall reconstruction quality.△ Less"
Pairing and superfluidity of nucleons in neutron stars,"Authors:A. Gezerlis,C. J. Pethick,A. Schwenk","Abstract:We survey the current status of understanding of pairing and superfluidity of neutrons and protons in neutron stars from a theoretical perspective, with emphasis on basic physical properties. During the past two decades, the blossoming of the field of ultracold atomic gases and the development of quantum Monte Carlo methods for solving the many-body problem have been two important sources of inspi…▽ MoreWe survey the current status of understanding of pairing and superfluidity of neutrons and protons in neutron stars from a theoretical perspective, with emphasis on basic physical properties. During the past two decades, the blossoming of the field of ultracold atomic gases and the development of quantum Monte Carlo methods for solving the many-body problem have been two important sources of inspiration, and we shall describe how these have given insight into neutron pairing gaps. The equilibrium properties and collective oscillations of the inner crust of neutron stars, where neutrons paired in a $^1$S$_0$ state coexist with a lattice of neutron-rich nuclei, are also described. While pairing gaps are well understood at densities less than one tenth of the nuclear saturation density, significant uncertainties exist at higher densities due to the complicated nature of nucleon-nucleon interactions, the difficulty of solving the many-body problem under these conditions, and the increasing importance of many-nucleon interactions. We also touch more briefly on the subject of pairing of neutrons in other angular momentum states, specifically the $^3$P$_2$ state, as well as pairing of protons.△ Less"
Quantum Measurements: a modern view for quantum optics experimentalists,Authors:Aephraim M. Steinberg,"Abstract:In these notes, based on lectures given as part of the Les Houches summer school on Quantum Optics and Nanophotonics in August, 2013, I have tried to give a brief survey of some important approaches and modern tendencies in quantum measurement. I wish it to be clear from the outset that I shy explicitly away from the ""quantum measurement problem,"" and that the present treatment aims to elucidate t…▽ MoreIn these notes, based on lectures given as part of the Les Houches summer school on Quantum Optics and Nanophotonics in August, 2013, I have tried to give a brief survey of some important approaches and modern tendencies in quantum measurement. I wish it to be clear from the outset that I shy explicitly away from the ""quantum measurement problem,"" and that the present treatment aims to elucidate the theory and practice of various ways in which measurements can, in light of quantum mechanics, be carried out; and various formalisms for describing them. While the treatment is by necessity largely theoretical, the emphasis is meant to be on an experimental ""perspective"" on measurement -- that is, to place the priority on the possibility of gaining information through some process, and then attempting to model that process mathematically and consider its ramifications, rather than stressing a particular mathematical definition as the {\it sine qua non} of measurement. The textbook definition of measurement as being a particular set of mathematical operations carried out on particular sorts of operators has been so well drilled into us that many have the unfortunate tendency of saying ""that experiment can't be described by projections onto the eigenstates of a Hermitian operator, so it is not really a measurement,"" when of course any practitioner of an experimental science such as physics should instead say ""that experiment allowed us to measure something, and if the standard theory of measurement does not describe it, the standard theory of measurement is incomplete."" Idealisations are important, but when the real world breaks the approximations made in the theory, it is the theory which must be fixed, and not the real world.△ Less"
Discrepancies between isochrone fitting and gyrochronology for exoplanet host stars?,Authors:D. J. A. Brown,"Abstract:Using a sample of 68 planet-hosting stars I carry out a comparison of isochrone fitting and gyrochronology to investigate whether tidal interactions between the stars and their planets are leading to underestimated ages using the latter method. I find a slight tendency for isochrones to produce older age estimates but find no correlation with tidal time-scale, although for some individual systems…▽ MoreUsing a sample of 68 planet-hosting stars I carry out a comparison of isochrone fitting and gyrochronology to investigate whether tidal interactions between the stars and their planets are leading to underestimated ages using the latter method. I find a slight tendency for isochrones to produce older age estimates but find no correlation with tidal time-scale, although for some individual systems the effect of tides might be leading to more rapid rotation than expected from the stars' isochronal age, and therefore an underestimated gyrochronology age. By comparing to planetary systems in stellar clusters, I also find that in some cases isochrone fitting can overestimate the age of the star. The evidence for any bias on a sample-wide level is inconclusive.
  I also consider the subset of my sample for which the sky-projected alignment angle between the stellar rotation axis and the planet's orbital axis has been measured, finding similar patterns to those identified in the full sample. However, small sample sizes for both the misaligned and aligned systems prevent strong conclusions from being drawn.△ Less"
Heat-kernel and resolvent asymptotics for Schroedinger operators on metric graphs,"Authors:Jens Bolte,Sebastian Egger,Ralf Rueckriemen","Abstract:We consider Schroedinger operators on compact and non-compact (finite) metric graphs. For such operators we analyse their spectra, prove that their resolvents can be represented as integral operators and introduce trace-class regularisations of the resolvents. Our main result is a complete asymptotic expansion of the trace of the (regularised) heat-semigroup generated by the Schroedinger operator.…▽ MoreWe consider Schroedinger operators on compact and non-compact (finite) metric graphs. For such operators we analyse their spectra, prove that their resolvents can be represented as integral operators and introduce trace-class regularisations of the resolvents. Our main result is a complete asymptotic expansion of the trace of the (regularised) heat-semigroup generated by the Schroedinger operator. We also determine the leading coefficients in the expansion explicitly.△ Less"
Newton-Type Iterative Solver for Multiple View $L2$ Triangulation,"Authors:F. Lu,Z. Chen","Abstract:…obtained by the two-stage iterative bundle adjustment approach proposed here are also the L2 optimal solutions to all the calibrated data sets available online by theOxfordvisual geometry group. Extensive numerical experiments indicate the bundle adjustment approach solves more than 99% the real triangulation problems optimally. An IEEE 754 double precisio…▽ MoreIn this note, we show that the L2 optimal solutions to most real multiple view L2 triangulation problems can be efficiently obtained by two-stage Newton-like iterative methods, while the difficulty of such problems mainly lies in how to verify the L2 optimality. Such a working two-stage bundle adjustment approach features: first, the algorithm is initialized by symmedian point triangulation, a multiple-view generalization of the mid-point method; second, a symbolic-numeric method is employed to compute derivatives accurately; third, globalizing strategy such as line search or trust region is smoothly applied to the underlying iteration which assures algorithm robustness in general cases.
  Numerical comparison with tfml method shows that the local minimizers obtained by the two-stage iterative bundle adjustment approach proposed here are also the L2 optimal solutions to all the calibrated data sets available online by theOxfordvisual geometry group. Extensive numerical experiments indicate the bundle adjustment approach solves more than 99% the real triangulation problems optimally. An IEEE 754 double precision C++ implementation shows that it takes only about 0.205 second tocompute allthe 4983 points in theOxforddinosaur data setvia Gauss-Newton iteration hybrid with a line search strategy on a computer with a 3.4GHz Intel i7 CPU.△ Less"
A Compact Linear Programming Relaxation for Binary Sub-modular MRF,"Authors:Junyan Wang,Sai-Kit Yeung","Abstract:…the task of interactive object segmentation. Our LP model outperforms QP when converting the continuous labels to binary labels using different threshold values on the entireOxfordinteractive segmentation dataset. The computational complexity of our LP is of the same order as that of the QP, and it is significantly lower than the conventional LP relaxation…▽ MoreWe propose a novel compact linear programming (LP) relaxation for binary sub-modular MRF in the context of object segmentation. Our model is obtained by linearizing an $l_1^+$-norm derived from the quadratic programming (QP) form of the MRF energy. The resultant LP model contains significantly fewer variables and constraints compared to the conventional LP relaxation of the MRF energy. In addition, unlike QP which can produce ambiguous labels, our model can be viewed as a quasi-total-variation minimization problem, and it can therefore preserve the discontinuities in the labels. We further establish a relaxation bound between our LP model and the conventional LP model. In the experiments, we demonstrate our method for the task of interactive object segmentation. Our LP model outperforms QP when converting the continuous labels to binary labels using different threshold values on the entireOxfordinteractive segmentation dataset. The computational complexity of our LP is of the same order as that of the QP, and it is significantly lower than the conventional LP relaxation.△ Less"
A geometric study of Wasserstein spaces: isometric rigidity in negative curvature,"Authors:Jérôme Bertrand,Benoît Kloeckner","Abstract:Given a metric space X, one defines its Wasserstein space W2(X) as a set of sufficiently decaying probability measures on X endowed with a metric defined from optimal transportation. In this article, we continue the geometric study of W2(X) when X is a simply connected, nonpositively curved metric spaces by considering its isometry group. When X is Euclidean, the second named author proved that th…▽ MoreGiven a metric space X, one defines its Wasserstein space W2(X) as a set of sufficiently decaying probability measures on X endowed with a metric defined from optimal transportation. In this article, we continue the geometric study of W2(X) when X is a simply connected, nonpositively curved metric spaces by considering its isometry group. When X is Euclidean, the second named author proved that this isometry group is larger than the isometry group of X. In contrast, we prove here a rigidity result: when X is negatively curved, any isometry of W2(X) comes from an isometry of X.△ Less"
Reasoning with !-Graphs,Authors:Alexander Merry,"Abstract:The aim of this thesis is to present an extension to the string graphs of Dixon, Duncan and Kissinger that allows the finite representation of certain infinite families of graphs and graph rewrite rules, and to demonstrate that a logic can be built on this to allow the formalisation of inductive proofs in the string diagrams of compact closed and traced symmetric monoidal categories.
  String diag…▽ MoreThe aim of this thesis is to present an extension to the string graphs of Dixon, Duncan and Kissinger that allows the finite representation of certain infinite families of graphs and graph rewrite rules, and to demonstrate that a logic can be built on this to allow the formalisation of inductive proofs in the string diagrams of compact closed and traced symmetric monoidal categories.
  String diagrams provide an intuitive method for reasoning about monoidal categories. However, this does not negate the ability for those using them to make mistakes in proofs. To this end, there is a project (Quantomatic) to build a proof assistant for string diagrams, at least for those based on categories with a notion of trace. The development of string graphs has provided a combinatorial formalisation of string diagrams, laying the foundations for this project.
  The prevalence of commutative Frobenius algebras (CFAs) in quantum information theory, a major application area of these diagrams, has led to the use of variable-arity nodes as a shorthand for normalised networks of Frobenius algebra morphisms, so-called ""spider notation"". This notation greatly eases reasoning with CFAs, but string graphs are inadequate to properly encode this reasoning.
  This dissertation extends string graphs to allow for variable-arity nodes to be represented at all, and then introduces !-box notation (and structures to encode it) to represent string graph equations containing repeated subgraphs, where the number of repetitions is abitrary. It then demonstrates how we can reason directly about !-graphs, viewed as (typically infinite) families of string graphs. Of particular note is the presentation of a form of graph-based induction, allowing the formal encoding of proofs that previously could only be represented as a mix of string diagrams and explanatory text.△ Less"
Gedanken Densities and Exact Constraints in Density Functional Theory,"Authors:John P. Perdew,Adrienn Ruzsinszky,Jianwei Sun,Kieron Burke","Abstract:…propose a spherical two-electron gedanken density in which the dimensionless density gradient can be an arbitrary positive constant wherever the density is non-zero. The Lieb-Oxfordlower bound on the exchange energy can be satisfied within a generalized gradient approximation (GGA) by bounding its enhancement factor or simplest GGA exchange-energy density.…▽ MoreApproximations to the exact density functional for the exchange-correlation energy of a many-electron ground state can be constructed by satisfying constraints that are universal, i.e., valid for all electron densities. Gedanken densities are designed for the purpose of this construction, but need not be realistic. The uniform electron gas is an old gedanken density. Here, we propose a spherical two-electron gedanken density in which the dimensionless density gradient can be an arbitrary positive constant wherever the density is non-zero. The Lieb-Oxfordlower bound on the exchange energy can be satisfied within a generalized gradient approximation (GGA) by bounding its enhancement factor or simplest GGA exchange-energy density. This enhancement-factor bound is well known to be sufficient, but our gedanken density shows that it is also necessary. The conventional exact exchange-energy density satisfies no such local bound, but energy densities are not unique, and the simplest GGA exchange-energy density is not an approximation to it. We further derive a strongly and optimally tightened bound on the exchange enhancement factor of a two-electron density, which is satisfied by the local density approximation but is violated by all published GGA's or meta-GGA's. Finally, some consequences of the non-uniform density-scaling behavior for the asymptotics of the exchange enhancement factor of a GGA or meta-GGA are given.△ Less"
Performance evaluation of DNA copy number segmentation methods,"Authors:Morgane Pierre-Jean,Guillem Rigaill,Pierre Neuvial","Abstract:A number of bioinformatic or biostatistical methods are available for analyzing DNA copy number profiles measured from microarray or sequencing technologies. In the absence of rich enough gold standard data sets, the performance of these methods is generally assessed using unrealistic simulation studies, or based on small real data analyses. We have designed and implemented a framework to generate…▽ MoreA number of bioinformatic or biostatistical methods are available for analyzing DNA copy number profiles measured from microarray or sequencing technologies. In the absence of rich enough gold standard data sets, the performance of these methods is generally assessed using unrealistic simulation studies, or based on small real data analyses. We have designed and implemented a framework to generate realistic DNA copy number profiles of cancer samples with known truth. These profiles are generated by resampling real SNP microarray data from genomic regions with known copy-number state. The original real data have been extracted from dilutions series of tumor cell lines with matched blood samples at several concentrations. Therefore, the signal-to-noise ratio of the generated profiles can be controlled through the (known) percentage of tumor cells in the sample. In this paper, we describe this framework and illustrate some of the benefits of the proposed data generation approach on a practical use case: a comparison study between methods for segmenting DNA copy number profiles from SNP microarrays. This study indicates that no single method is uniformly better than all others. It also helps identifying pros and cons for the compared methods as a function of biologically informative parameters, such as the fraction of tumor cells in the sample and the proportion of heterozygous markers. Availability: R package jointSeg: http://r-forge.r-project.org/R/?group\_id=1562△ Less"
What Is It Like to Be a Brain Simulation?,Authors:Eray Özkural,"Abstract:We frame the question of what kind of subjective experience a brain simulation would have in contrast to a biological brain. We discuss the brain prosthesis thought experiment. We evaluate how the experience of the brain simulation might differ from the biological, according to a number of hypotheses about experience and the properties of simulation. Then, we identify finer questions relating to t…▽ MoreWe frame the question of what kind of subjective experience a brain simulation would have in contrast to a biological brain. We discuss the brain prosthesis thought experiment. We evaluate how the experience of the brain simulation might differ from the biological, according to a number of hypotheses about experience and the properties of simulation. Then, we identify finer questions relating to the original inquiry, and answer them from both a general physicalist, and panexperientialist perspective.△ Less"
Quantum optics and cavity QED with quantum dots in photonic crystals,Authors:Jelena Vuckovic,"Abstract:This chapter will primarily focus on the studies of quantum optics with semiconductor, epitaxially grown quantum dots embedded in photonic crystal cavities. We will start by giving brief introductions into photonic crystals and quantum dots, then proceed with the introduction to cavity quantum electrodynamics (QED) effects, with a particular emphasis on the demonstration of these effects on the qu…▽ MoreThis chapter will primarily focus on the studies of quantum optics with semiconductor, epitaxially grown quantum dots embedded in photonic crystal cavities. We will start by giving brief introductions into photonic crystals and quantum dots, then proceed with the introduction to cavity quantum electrodynamics (QED) effects, with a particular emphasis on the demonstration of these effects on the quantum dot-photonic crystal platform. Finally, we will focus on the applications of such cavity QED effects.△ Less"
Two Lectures On The Jones Polynomial And Khovanov Homology,Authors:Edward Witten,"Abstract:…dimensions. In the second lecture, I describe how Khovanov homology can emerge upon adding a fifth dimension. (Based on lectures presented at the Clay Research Conference atOxfordUniversity, and also at the Galileo Galilei Institute in Florence, the University of Milan, Harvard University, and the University of Pennsylvania.)▽ MoreIn the first of these two lectures, I describe a gauge theory approach to understanding quantum knot invariants as Laurent polynomials in a complex variable q. The two main steps are to reinterpret three-dimensional Chern-Simons gauge theory in four dimensional terms and then to apply electric-magnetic duality. The variable q is associated to instanton number in the dual description in four dimensions. In the second lecture, I describe how Khovanov homology can emerge upon adding a fifth dimension. (Based on lectures presented at the Clay Research Conference atOxfordUniversity, and also at the Galileo Galilei Institute in Florence, the University of Milan, Harvard University, and the University of Pennsylvania.)△ Less"
Effects of the Second Harmonic on the GAM in Electron Scale Turbulence,"Authors:Johan Anderson,Hans Nordman,Raghvendra Singh","Abstract:The effects higher order harmonics have been self-consistently included in the derivation of the electron branch of the electron Geodesic Acoustic Mode (el-GAM) in an Electron-Temperature-Gradient (ETG) turbulence background. The work is based on a two-fluid model including finite $β$-effects while retaining non-adiabatic ions. In solving the linear dispersion relation, it is found that the due to…▽ MoreThe effects higher order harmonics have been self-consistently included in the derivation of the electron branch of the electron Geodesic Acoustic Mode (el-GAM) in an Electron-Temperature-Gradient (ETG) turbulence background. The work is based on a two-fluid model including finite $β$-effects while retaining non-adiabatic ions. In solving the linear dispersion relation, it is found that the due to the coupling to the $m=2$ mode the real frequency may be significantly altered and yield higher values.△ Less"
Acoustic scattering by fractal screens: mathematical formulations and wavenumber-explicit continuity and coercivity estimates,"Authors:Simon N. Chandler-Wilde,David P. Hewett","Abstract:We consider time-harmonic acoustic scattering by planar sound-soft (Dirichlet) and sound-hard (Neumann) screens. In contrast to previous studies, in which the domain occupied by the screen is assumed to be Lipschitz or smoother, we consider screens occupying an arbitrary bounded open set in the plane. Thus our study includes cases where the closure of the domain occupied by the screen has larger p…▽ MoreWe consider time-harmonic acoustic scattering by planar sound-soft (Dirichlet) and sound-hard (Neumann) screens. In contrast to previous studies, in which the domain occupied by the screen is assumed to be Lipschitz or smoother, we consider screens occupying an arbitrary bounded open set in the plane. Thus our study includes cases where the closure of the domain occupied by the screen has larger planar Lebesgue measure than the screen, as can happen, for example, when the screen has a fractal boundary. We show how to formulate well-posed boundary value problems for such scattering problems, our arguments depending on results on the coercivity of the acoustic single-layer and hypersingular boundary integral operators, and on properties of Sobolev spaces on general open sets which appear to be new. Our analysis teases out the explicit wavenumber dependence of the continuity and coercivity constants of the boundary integral operators, viewed as mappings between fractional Sobolev spaces, this in part extending previous results of Ha-Duong. We also consider the complementary problem of propagation through a bounded aperture in an infinite planar screen.△ Less"
A frequency-independent boundary element method for scattering by two-dimensional screens and apertures,"Authors:David P. Hewett,Stephen Langdon,Simon N. Chandler-Wilde","Abstract:We propose and analyse a hybrid numerical-asymptotic $hp$ boundary element method for time-harmonic scattering of an incident plane wave by an arbitrary collinear array of sound-soft two-dimensional screens. Our method uses an approximation space enriched with oscillatory basis functions, chosen to capture the high frequency asymptotics of the solution. We provide a rigorous frequency-explicit err…▽ MoreWe propose and analyse a hybrid numerical-asymptotic $hp$ boundary element method for time-harmonic scattering of an incident plane wave by an arbitrary collinear array of sound-soft two-dimensional screens. Our method uses an approximation space enriched with oscillatory basis functions, chosen to capture the high frequency asymptotics of the solution. We provide a rigorous frequency-explicit error analysis which proves that the method converges exponentially as the number of degrees of freedom $N$ increases, and that to achieve any desired accuracy it is sufficient to increase $N$ in proportion to the square of the logarithm of the frequency as the frequency increases (standard boundary element methods require $N$ to increase at least linearly with frequency to retain accuracy). Our numerical results suggest that fixed accuracy can in fact be achieved at arbitrarily high frequencies with a frequency-independent computational cost, when the oscillatory integrals required for implementation are computed using Filon quadrature. We also show how our method can be applied to the complementary ""breakwater"" problem of propagation through an aperture in an infinite sound-hard screen.△ Less"
Finding More Relevance: Propagating Similarity on Markov Random Field for Image Retrieval,"Authors:Peng Lu,Xujun Peng,Xinshan Zhu,Xiaojie Wang","Abstract:…in the database and it can be easily integrated into the existing bag-of-visual-words(BoW) based systems to reduce the missing rate. We evaluate our method on the standardOxford-5K,Oxford-105K and Paris-6K dataset. The experiment results show that the proposed method significantly improves the retrieval accuracy on t…▽ MoreTo effectively retrieve objects from large corpus with high accuracy is a challenge task. In this paper, we propose a method that propagates visual feature level similarities on a Markov random field (MRF) to obtain a high level correspondence in image space for image pairs. The proposed correspondence between image pair reflects not only the similarity of low-level visual features but also the relations built through other images in the database and it can be easily integrated into the existing bag-of-visual-words(BoW) based systems to reduce the missing rate. We evaluate our method on the standardOxford-5K,Oxford-105K and Paris-6K dataset. The experiment results show that the proposed method significantly improves the retrieval accuracy on three datasets and exceeds the current state-of-the-art retrieval performance.△ Less"
Large Scale Structure Observations,Authors:Will J. Percival,"Abstract:Galaxy Surveys are enjoying a renaissance thanks to the advent of multi-object spectrographs on ground-based telescopes. The last 15 years have seen the fruits of this experimental advance, including the 2-degree Field Galaxy Redshift Survey (2dFGRS; Colless et al. 2003) and the Sloan Digital Sky Survey (SDSS; York et al. 2000). Most recently, the Baryon Oscillation Spectroscopic Survey (BOSS; Daw…▽ MoreGalaxy Surveys are enjoying a renaissance thanks to the advent of multi-object spectrographs on ground-based telescopes. The last 15 years have seen the fruits of this experimental advance, including the 2-degree Field Galaxy Redshift Survey (2dFGRS; Colless et al. 2003) and the Sloan Digital Sky Survey (SDSS; York et al. 2000). Most recently, the Baryon Oscillation Spectroscopic Survey (BOSS; Dawson et al. 2013), part of the SDSS-III project (Eisenstein et al. 2011), has provided the largest volume of the low-redshift Universe ever surveyed with a galaxy density useful for high-precision cosmology. This set of lecture notes looks at some of the physical processes that underpin these measurements, the evolution of measurements themselves, and looks ahead to the next 15 years and the advent of surveys such as the enhanced Baryon Oscillation Spectroscopic Survey (eBOSS), the Dark Energy Spectroscopic Instrument (DESI) and the ESA Euclid satellite mission.△ Less"
Logics for complexity classes,Authors:Vladimir Naidenko,"Abstract:A new syntactic characterization of problems complete via Turing reductions is presented. General canonical forms are developed in order to define such problems. One of these forms allows us to define complete problems on ordered structures, and another form to define them on unordered non-Aristotelian structures. Using the canonical forms, logics are developed for complete problems in various com…▽ MoreA new syntactic characterization of problems complete via Turing reductions is presented. General canonical forms are developed in order to define such problems. One of these forms allows us to define complete problems on ordered structures, and another form to define them on unordered non-Aristotelian structures. Using the canonical forms, logics are developed for complete problems in various complexity classes. Evidence is shown that there cannot be any complete problem on Aristotelian structures for several complexity classes. Our approach is extended beyond complete problems. Using a similar form, a logic is developed to capture the complexity class $NP\cap coNP$ which very likely contains no complete problem.△ Less"
Deformation induced changes in surface properties of polymers investigated by scanning force microscopy,"Authors:Sabine Hild,Armin Rosa,Othmar Marti","Abstract:In this study the possibility of combining commercial Scanning Force Microscopes (SFM) with stretching devices for the investigation of microscopic surface changes during stepwise elongation is investigated. Different types of stretching devices have been developed either for Scanning Platform-SFM or for Stand Alone-SFM. Their suitability for the investigation of deformation induced surface change…▽ MoreIn this study the possibility of combining commercial Scanning Force Microscopes (SFM) with stretching devices for the investigation of microscopic surface changes during stepwise elongation is investigated. Different types of stretching devices have been developed either for Scanning Platform-SFM or for Stand Alone-SFM. Their suitability for the investigation of deformation induced surface changes is demonstrated. A uniaxially oriented polypropylene film is stretched vertically to its extrusion direction. The reorientation of its microfibrillar structure is investigated and correlated to macroscopic structural changes determined by taking a force-elongation curve. Microtome cuts of natural rubber filled with 15 PHR carbon black are stretched. Changes in topography, local stiffness and adhesive force are simultaneously reported by using a new imaging method called Pulsed Force Mode (PFM).△ Less"
Einstein's cosmic model of 1931 revisited: an analysis and translation of a forgotten model of the universe,"Authors:C. O'Raifeartaigh,B. McCann","Abstract:…paper of the matter density and radius of the universe contain a numerical error, a finding that is supported by writing on a blackboard used by Einstein during a lecture atOxfordUniversity in May 1931. Our article concludes with a general discussion of his philosophy of cosmology.▽ MoreWe present a translation and analysis of a cosmic model published by Einstein in 1931. The paper, which is not widely known, features a model of a universe that undergoes an expansion followed by a contraction, quite different to his static model of 1917 or the monotonic Einstein-de Sitter model of 1932. The paper offers many insights into the cosmology of Albert Einstein in the light of the first evidence for an expanding universe, and we discuss his views of issues such as the curvature of space, the cosmological constant, the singularity and the timespan of the expansion. We argue that retrospective descriptions of this model as cyclic or periodic are not historically or mathematically accurate. We find that calculations in the paper of the matter density and radius of the universe contain a numerical error, a finding that is supported by writing on a blackboard used by Einstein during a lecture atOxfordUniversity in May 1931. Our article concludes with a general discussion of his philosophy of cosmology.△ Less"
Geochemical and planetary dynamical views on the origin of Earth's atmosphere and oceans,"Authors:Nicolas Dauphas,Alessandro Morbidelli","Abstract:Earth's volatile elements (H, C, and N) are essential to maintaining habitable conditions for metazoans and simpler life forms. However, identifying the sources (comets, meteorites, and trapped nebular gas) that supplied volatiles to Earth is not straightforward because secondary processes like mantle degassing, crustal recycling, and escape to space modified the composition of the atmosphere. Her…▽ MoreEarth's volatile elements (H, C, and N) are essential to maintaining habitable conditions for metazoans and simpler life forms. However, identifying the sources (comets, meteorites, and trapped nebular gas) that supplied volatiles to Earth is not straightforward because secondary processes like mantle degassing, crustal recycling, and escape to space modified the composition of the atmosphere. Here, we review two complementary approaches to investigate the origin of Earth's atmosphere and oceans. The geochemical approach uses volatile element abundances and isotopic compositions to identify the possible contributors to the atmosphere and to disentangle the processes that shaped it. In that respect, noble gases (He, Ne, Ar, Kr, and Xe), elements that are chemically inert and possess several isotopes produced by radioactivity, play a critical role. The dynamical approach uses our knowledge of planetary dynamics to track volatile delivery to the Earth, starting with dust transport in the disk to planet-building processes. The main conclusion is that Earth acquired most of its major volatile elements by accretion of planetesimals or embryos akin to volatile-rich meteorites. At the same time, solar/meteoritic noble gases were captured by embryos and some gases were lost to space, by hydrodynamic escape and large impacts. Comets did not contribute much H, C, and N but may have delivered significant noble gases, which could represent the only fingerprints of the bombardment of our planet with icy bodies. The processes that governed the delivery of volatile elements to the Earth are thought to be relatively common and it is likely that Earth-like planets covered with oceans exist in extra-solar systems.△ Less"
Galaxy formation,"Authors:Joseph Silk,Arianna Di Cintio,Irina Dvorkin","Abstract:Galaxy formation is at the forefront of observation and theory in cosmology. An improved understanding is essential for improving our knowledge both of the cosmological parameters, of the contents of the universe, and of our origins. In these lectures intended for graduate students, galaxy formation theory is reviewed and confronted with recent observational issues. In Lecture 1, the following top…▽ MoreGalaxy formation is at the forefront of observation and theory in cosmology. An improved understanding is essential for improving our knowledge both of the cosmological parameters, of the contents of the universe, and of our origins. In these lectures intended for graduate students, galaxy formation theory is reviewed and confronted with recent observational issues. In Lecture 1, the following topics are presented: star formation considerations, including IMF, star formation efficiency and star formation rate, the origin of the galaxy luminosity function, and feedback in dwarf galaxies. In Lecture 2, we describe formation of disks and massive spheroids, including the growth of supermassive black holes, negative feedback in spheroids, the AGN-star formation connection, star formation rates at high redshift and the baryon fraction in galaxies.△ Less"
Non-trivial Local Attractors of a Three-dimensional Dynamical System,Authors:Keying Guan,"Abstract:Based on both qualitative method and numerical tests for a series of particular cases in the parameter region, a=1, 0<b <1, it is shown that the three-dimensional system (2) may have a series of interesting phenomena on the non-trivial local attractors, such as the faint attractor (this term is suggested by the author), the local attractor with complex structure, twin spatial limit closed orbits,…▽ MoreBased on both qualitative method and numerical tests for a series of particular cases in the parameter region, a=1, 0<b <1, it is shown that the three-dimensional system (2) may have a series of interesting phenomena on the non-trivial local attractors, such as the faint attractor (this term is suggested by the author), the local attractor with complex structure, twin spatial limit closed orbits, the bifurcation of rotation numbers, and the spatial limit cycle, etc.. The system (2) is a very rich source in the study of dynamical system theory.△ Less"
Scanning wire beam position monitor for alignment of a high brightness inverse-Compton x-ray source,"Authors:Michael R. Hadmack,Eric B. Szarmes",Abstract:The Free-Electron Laser Laboratory at the University of Hawai`i has constructed and tested a scanning wire beam position monitor to aid the alignment and optimization of a high spectral brightness inverse-Compton scattering x-ray source. X-rays are produced by colliding the 40 MeV electron beam from a pulsed S-band linac with infrared laser pulses from a mode-locked free-electron laser driven by t…▽ MoreThe Free-Electron Laser Laboratory at the University of Hawai`i has constructed and tested a scanning wire beam position monitor to aid the alignment and optimization of a high spectral brightness inverse-Compton scattering x-ray source. X-rays are produced by colliding the 40 MeV electron beam from a pulsed S-band linac with infrared laser pulses from a mode-locked free-electron laser driven by the same electron beam. The electron and laser beams are focused to 60 μm diameters at the interaction point to achieve high scattering efficiency. This wire-scanner allows for high resolution measurements of the size and position of both the laser and electron beams at the interaction point to verify spatial coincidence. Time resolved measurements of secondary emission current allow us to monitor the transverse spatial evolution of the e-beam throughout the duration of a 4 μs macro-pulse while the laser is simultaneously profiled by pyrometer measurement of the occulted infrared beam. Using this apparatus we have demonstrated that the electron and laser beams can be co-aligned with a precision better than 10 μm as required to maximize x-ray yield.△ Less
Annotations regarding theOxfordQuestions,Authors:Spiridon Dumitru,"Abstract:The recently published ""OxfordQuestions"" are supplemented with annotations concerning: doctrine of wave packets collapse (and sub- sidiarily Schrodinger's cat thought experiment), description of quan- tum measurements respectively interpretation of uncertainty relations.The recently published ""OxfordQuestions"" are supplemented with annotations concerning: doctrine of wave packets collapse (and sub- sidiarily Schrodinger's cat thought experiment), description of quan- tum measurements respectively interpretation of uncertainty relations.△ Less"
Category-Theoretic Quantitative Compositional Distributional Models of Natural Language Semantics,Authors:Edward Grefenstette,"Abstract:This thesis is about the problem of compositionality in distributional semantics. Distributional semantics presupposes that the meanings of words are a function of their occurrences in textual contexts. It models words as distributions over these contexts and represents them as vectors in high dimensional spaces. The problem of compositionality for such models concerns itself with how to produce r…▽ MoreThis thesis is about the problem of compositionality in distributional semantics. Distributional semantics presupposes that the meanings of words are a function of their occurrences in textual contexts. It models words as distributions over these contexts and represents them as vectors in high dimensional spaces. The problem of compositionality for such models concerns itself with how to produce representations for larger units of text by composing the representations of smaller units of text.
  This thesis focuses on a particular approach to this compositionality problem, namely using the categorical framework developed by Coecke, Sadrzadeh, and Clark, which combines syntactic analysis formalisms with distributional semantic representations of meaning to produce syntactically motivated composition operations. This thesis shows how this approach can be theoretically extended and practically implemented to produce concrete compositional distributional models of natural language semantics. It furthermore demonstrates that such models can perform on par with, or better than, other competing approaches in the field of natural language processing.
  There are three principal contributions to computational linguistics in this thesis. The first is to extend the DisCoCat framework on the syntactic front and semantic front, incorporating a number of syntactic analysis formalisms and providing learning procedures allowing for the generation of concrete compositional distributional models. The second contribution is to evaluate the models developed from the procedures presented here, showing that they outperform other compositional distributional models present in the literature. The third contribution is to show how using category theory to solve linguistic problems forms a sound basis for research, illustrated by examples of work on this topic, that also suggest directions for future research.△ Less"
Spectral dimension in graph models of causal quantum gravity,Authors:Georgios Giasemidis,"Abstract:The phenomenon of scale dependent spectral dimension has attracted special interest in the quantum gravity community over the last eight years. It was first observed in computer simulations of the causal dynamical triangulation (CDT) approach to quantum gravity and refers to the reduction of the spectral dimension from 4 at classical scales to 2 at short distances. Thereafter several authors confi…▽ MoreThe phenomenon of scale dependent spectral dimension has attracted special interest in the quantum gravity community over the last eight years. It was first observed in computer simulations of the causal dynamical triangulation (CDT) approach to quantum gravity and refers to the reduction of the spectral dimension from 4 at classical scales to 2 at short distances. Thereafter several authors confirmed a similar result from different approaches to quantum gravity. Despite the contribution from different approaches, no analytical model was proposed to explain the numerical results as the continuum limit of CDT. In this thesis we introduce graph ensembles as toy models of CDT and show that both the continuum limit and a scale dependent spectral dimension can be defined rigorously. First we focus on a simple graph ensemble, the random comb. It does not have any dynamics from the gravity point of view, but serves as an instructive toy model to introduce the characteristic scale of the graph, study the continuum limit and define the scale dependent spectral dimension. Having defined the continuum limit, we study the reduction of the spectral dimension on more realistic toy models, the multigraph ensembles, which serve as a radial approximation of CDT. We focus on the (recurrent) multigraph approximation of the two-dimensional CDT whose ensemble measure is analytically controlled. The latter comes from the critical Galton-Watson process conditioned on non-extinction. Next we turn our attention to transient multigraph ensembles, corresponding to higher-dimensional CDT. Firstly we study their fractal properties and secondly calculate the scale dependent spectral dimension and compare it to computer simulations. We comment further on the relation between Horava-Lifshitz gravity, asymptotic safety, multifractional spacetimes and CDT-like models.△ Less"
The overlap in glassy systems,Authors:Giorgio Parisi,Abstract:In this paper I will consider many of the various definitions of the overlap and of its probability distribution that have been introduced in the literature starting from the original papers of Edwards and Anderson; I will present also some of the most recent results on the probability distribution of the local overlap in spin glasses. These quantities are related to the fluctuation dissipation re…▽ MoreIn this paper I will consider many of the various definitions of the overlap and of its probability distribution that have been introduced in the literature starting from the original papers of Edwards and Anderson; I will present also some of the most recent results on the probability distribution of the local overlap in spin glasses. These quantities are related to the fluctuation dissipation relations both in their local and in their global versions.△ Less
Operators on Banach Spaces of Bourgain-Delbaen Type,Authors:Matthew Tarbard,"Abstract:We begin by giving a detailed exposition of the original Bourgain-Delbaen construction and the generalised construction due to Argyros and Haydon. We show how these two constructions are related, and as a corollary, are able to prove that there exists some $δ> 0$ and an uncountable set of isometries on the original Bourgain-Delbaen spaces which are pairwise distance $δ$ apart.
  We subsequently ex…▽ MoreWe begin by giving a detailed exposition of the original Bourgain-Delbaen construction and the generalised construction due to Argyros and Haydon. We show how these two constructions are related, and as a corollary, are able to prove that there exists some $δ> 0$ and an uncountable set of isometries on the original Bourgain-Delbaen spaces which are pairwise distance $δ$ apart.
  We subsequently extend these ideas to obtain our main results. We construct new Banach spaces of Bourgain-Delbaen type, all of which have $\ell_1$ dual. The first class of spaces are HI and possess few, but not very few operators. We thus have a negative solution to the Argyros-Haydon question. We remark that all these spaces have finite dimensional Calkin algebra, and we investigate the corollaries of this result. We also construct a space with $\ell_1$ Calkin algebra and show that whilst this space is still of Bourgain-Delbaen type with $\ell_1$ dual, it behaves somewhat differently to the first class of spaces.
  Finally, we briefly consider shift-invariant $\ell_1$ preduals, and hint at how one might use the Bourgain-Delbaen construction to produce new, exotic examples.△ Less"
The equations of CCC,Authors:Paul Tod,Abstract:I review the equations of Conformal Cyclic Cosmology given by Penrose. I suggest a slight modification to Penrose's prescription and show how this works out for FRW cosmologies and for Class A Bianchi cosmologies.I review the equations of Conformal Cyclic Cosmology given by Penrose. I suggest a slight modification to Penrose's prescription and show how this works out for FRW cosmologies and for Class A Bianchi cosmologies.△ Less
Theoretical Studies of Superconductor-Insulator Transitions,"Authors:Yen Lee Loh,Nandini Trivedi","Abstract:In this article we study superconductor-insulator transitions within the general framework of an attractive Hubbard model. This is a well-defined model of s-wave superconductivity which permits different tuning parameters (disorder and field). Furthermore, it allows a comparison of various analytical and computational approaches in order to gain a complete understanding of the various effects of a…▽ MoreIn this article we study superconductor-insulator transitions within the general framework of an attractive Hubbard model. This is a well-defined model of s-wave superconductivity which permits different tuning parameters (disorder and field). Furthermore, it allows a comparison of various analytical and computational approaches in order to gain a complete understanding of the various effects of amplitude and phase fluctuations. We present a systematic pedagogical approach, aiming to equip the lay reader with enough apparatus to be able to understand the numerical calculations, reproduce some of the simpler results, and be able to tackle future problems related to inhomogeneous phases. We go into considerable detail on mean-field theory (MFT) and the Bogoliubov-de Gennes (BdG) approach, as these are a first line of attack which can capture much of the physics, but we also outline cases where this fails to capture phase fluctuations and more sophisticated Quantum Monte Carlo (QMC) calculations are necessary. We discuss the behavior of many observables, including densities of states, superfluid stiffness, and dynamical conductivity, for the disorder-tuned superconductor-insulator transition. We also discuss SITs tuned by parallel magnetic field, which are quite different due to pairbreaking.△ Less"
Gas Electron Multipliers versus Multiwire Proportional Chambers,"Authors:Serge Duarte Pinto,Jens Spanggaard","Abstract:Gas Electron Multiplication technology is finding more and more applications in beam instrumentation and at CERN these detectors have recently been adapted for use in transverse profile measurements at several of our facilities. In the experimental areas of CERN's Antiproton Decelerator, low energy Gas Electron Multipliers successfully replaced all Multwire Proportional Chambers in 2012 and anothe…▽ MoreGas Electron Multiplication technology is finding more and more applications in beam instrumentation and at CERN these detectors have recently been adapted for use in transverse profile measurements at several of our facilities. In the experimental areas of CERN's Antiproton Decelerator, low energy Gas Electron Multipliers successfully replaced all Multwire Proportional Chambers in 2012 and another detector type has now been developed for high energy applications in the experimental areas of the SPS, totaling a potential of more than a hundred profile detectors to be replaced by GEM detectors of different types. This paper aims to describe the historical evolution of GEM technology by covering the many different applications but with specific focus on its potential to replace Multiwire Proportional Chambers for standard transverse profile measurement.△ Less"
"Lorenz, Gödel and Penrose: New perspectives on determinism and causality in fundamental physics",Authors:T. N. Palmer,"Abstract:Despite being known for his pioneering work on chaotic unpredictability, the key discovery at the core of meteorologist Ed Lorenz's work is the link between space-time calculus and state-space fractal geometry. Indeed, properties of Lorenz's fractal invariant set relate space-time calculus to deep areas of mathematics such as Gödel's Incompleteness Theorem. These properties, combined with some rec…▽ MoreDespite being known for his pioneering work on chaotic unpredictability, the key discovery at the core of meteorologist Ed Lorenz's work is the link between space-time calculus and state-space fractal geometry. Indeed, properties of Lorenz's fractal invariant set relate space-time calculus to deep areas of mathematics such as Gödel's Incompleteness Theorem. These properties, combined with some recent developments in theoretical and observational cosmology, motivate what is referred to as the `cosmological invariant set postulate': that the universe $U$ can be considered a deterministic dynamical system evolving on a causal measure-zero fractal invariant set $I_U$ in its state space. Symbolic representations of $I_U$ are constructed explicitly based on permutation representations of quaternions. The resulting `invariant set theory' provides some new perspectives on determinism and causality in fundamental physics. For example, whilst the cosmological invariant set appears to have a rich enough structure to allow a description of quantum probability, its measure-zero character ensures it is sparse enough to prevent invariant set theory being constrained by the Bell inequality (consistent with a partial violation of the so-called measurement independence postulate). The primacy of geometry as embodied in the proposed theory extends the principles underpinning general relativity. As a result, the physical basis for contemporary programmes which apply standard field quantisation to some putative gravitational lagrangian is questioned. Consistent with Penrose's suggestion of a deterministic but non-computable theory of fundamental physics, a `gravitational theory of the quantum' is proposed based on the geometry of $I_U$, with potential observational consequences for the dark universe.△ Less"
Fast and Slow Rotators in the Densest Environments: a SWIFT IFS study of the Coma Cluster,"Authors:R. C. W. Houghton,Roger L. Davies,F. D'Eugenio,N. Scott,N. Thatte,F. Clarke,M. Tecza,G. S. Salter,L. M. R. Fogarty,T. Goodsall","Abstract:We present integral-field spectroscopy of 27 galaxies in the Coma cluster observed with theOxfordSWIFT spectrograph, exploring the kinematic morphology-density relationship in a cluster environment richer and denser than any in the ATLAS3D survey. Our new data enables comparison of the kinematic morphology relation in three very different clusters (Virgo,…▽ MoreWe present integral-field spectroscopy of 27 galaxies in the Coma cluster observed with theOxfordSWIFT spectrograph, exploring the kinematic morphology-density relationship in a cluster environment richer and denser than any in the ATLAS3D survey. Our new data enables comparison of the kinematic morphology relation in three very different clusters (Virgo, Coma and Abell 1689) as well as to the field/group environment. The Coma sample was selected to match the parent luminosity and ellipticity distributions of the early-type population within a radius 15' (0.43 Mpc) of the cluster centre, and is limited to r' = 16 mag (equivalent to M_K = -21.5 mag), sampling one third of that population. From analysis of the lambda-ellipticity diagram, we find 15+-6% of early-type galaxies are slow rotators; this is identical to the fraction found in the field and the average fraction in the Virgo cluster, based on the ATLAS3D data. It is also identical to the average fraction found recently in Abell 1689 by D'Eugenio et al.. Thus it appears that the average slow rotator fraction of early type galaxies remains remarkably constant across many different environments, spanning five orders of magnitude in galaxy number density. However, within each cluster the slow rotators are generally found in regions of higher projected density, possibly as a result of mass segregation by dynamical friction. These results provide firm constraints on the mechanisms that produce early-type galaxies: they must maintain a fixed ratio between the number of fast rotators and slow rotators while also allowing the total early-type fraction to increase in clusters relative to the field. A complete survey of Coma, sampling hundreds rather than tens of galaxies, could probe a more representative volume of Coma and provide significantly stronger constraints, particularly on how the slow rotator fraction varies at larger radii.△ Less"
Onsager reciprocity principle for kinetic models and kinetic schemes,"Authors:Ajit Kumar Mahendra,Ram Kumar Singh","Abstract:Boltzmann equation requires some alternative simpler kinetic model like BGK to replace the collision term. Such a kinetic model which replaces the Boltzmann collision integral should preserve the basic properties and characteristics of the Boltzmann equation and comply with the requirements of non equilibrium thermodynamics. Most of the research in development of kinetic theory based methods have…▽ MoreBoltzmann equation requires some alternative simpler kinetic model like BGK to replace the collision term. Such a kinetic model which replaces the Boltzmann collision integral should preserve the basic properties and characteristics of the Boltzmann equation and comply with the requirements of non equilibrium thermodynamics. Most of the research in development of kinetic theory based methods have focused more on entropy conditions, stability and ignored the crucial aspect of non equilibrium thermodynamics. The paper presents a new kinetic model formulated based on the principles of non equilibrium thermodynamics. The new kinetic model yields correct transport coefficients and satisfies Onsager's reciprocity relationship. The present work also describes a novel kinetic particle method and gas kinetic scheme based on this linkage of non-equilibrium thermodynamics and kinetic theory. The work also presents derivation of kinetic theory based wall boundary condition which complies with the principles of non-equilibrium thermodynamics, and can simulate both continuum and rarefied slip flow in order to avoid extremely costly multi-scale simulation.△ Less"
An Electronically Non-Adiabatic Generalization of Ring Polymer Molecular Dynamics,Authors:Timothy J. H. Hele,"Abstract:In this thesis I generalize Ring Polymer Molecular Dynamics (RPMD) rate theory to electronically non-adiabatic systems, followed by application to two one-dimensional curve crossing models and a multidimensional spin-boson model.In this thesis I generalize Ring Polymer Molecular Dynamics (RPMD) rate theory to electronically non-adiabatic systems, followed by application to two one-dimensional curve crossing models and a multidimensional spin-boson model.△ Less"
Laplacian-based generalized gradient approximations for the exchange energy,"Authors:Antonio C. Cancio,Chris E. Wagner","Abstract:…chosen to reproduce the SOGGA and the APBE variants of the GGA. The model reliably reproduces exchange energies of closed shell atoms, once constraints such the local Lieb-Oxfordbound, whose effects depend upon choice of energy-density gauge, are recast in invariant form.▽ MoreIt is well known that in the gradient expansion approximation to density functional theory (DFT) the gradient and Laplacian of the density make interchangeable contributions to the exchange correlation (XC) energy. This is an arbitrary ""gauge"" freedom for building DFT models, normally used to eliminate the Laplacian from the generalized gradient approximation (GGA) level of DFT development. We explore the implications of keeping the Laplacian at this level of DFT, to develop a model that fits the known behavior of the XC hole, which can only be described as a system average in conventional GGA. We generate a family of exchange models that obey the same constraints as conventional GGA's, but which in addition have a finite-valued potential at the atomic nucleus unlike GGA's. These are tested against exact densities and exchange potentials for small atoms, and for constraints chosen to reproduce the SOGGA and the APBE variants of the GGA. The model reliably reproduces exchange energies of closed shell atoms, once constraints such the local Lieb-Oxfordbound, whose effects depend upon choice of energy-density gauge, are recast in invariant form.△ Less"
Strongly correlated bosons and fermions in optical lattices,"Authors:Antoine Georges,Thierry Giamarchi",Abstract:These lectures are an introduction to the physics of strongly correlated fermions and bosons. They are specially targeted for the experimental realizations that have been provided by cold atomic gases in optical lattices.These lectures are an introduction to the physics of strongly correlated fermions and bosons. They are specially targeted for the experimental realizations that have been provided by cold atomic gases in optical lattices.△ Less
Biophysical modelling of the effects of inhaled radon progeny on the bronchial epithelium for the estimation of the relationships applied in the two stage clonal expansion model of carcinogenesis,"Authors:Balázs G. Madas,Katalin Varga","Abstract:There is a considerable debate between research groups applying the two stage clonal expansion model for lung cancer risk estimation, whether radon exposure affects initiation and transformation or promotion. The objective of the present study is to quantify the effects of radon progeny on these stages with biophysical models. For this purpose, numerical models of mutation induction and clonal gro…▽ MoreThere is a considerable debate between research groups applying the two stage clonal expansion model for lung cancer risk estimation, whether radon exposure affects initiation and transformation or promotion. The objective of the present study is to quantify the effects of radon progeny on these stages with biophysical models. For this purpose, numerical models of mutation induction and clonal growth were applied in order to estimate how initiation, transformation and promotion rates depend on tissue dose rate. It was found that rates of initiation and transformation increase monotonically with dose rate, while effective promotion rate decreases with time, but increases in a supralinear fashion with dose rate. Despite the uncertainty of the results due to the lack of experimental data, present study suggests that effects of radon exposure on both mutational events and clonal growth are significant, and should be considered in epidemiological analyses applying mathematical models of carcinogenesis.△ Less"
Uniform Random Planar Graphs with Degree Constraints,Authors:Chris Dowden,"Abstract:Random planar graphs have been the subject of much recent work. Many basic properties of the standard uniform random planar graph P_{n}, by which we mean a graph chosen uniformly at random from the set of all planar graphs with vertex set {1,2,...,n}, are now known, and variations on this standard random graph are also attracting interest.
  Prominent among the work on P_{n} have been asymptotic r…▽ MoreRandom planar graphs have been the subject of much recent work. Many basic properties of the standard uniform random planar graph P_{n}, by which we mean a graph chosen uniformly at random from the set of all planar graphs with vertex set {1,2,...,n}, are now known, and variations on this standard random graph are also attracting interest.
  Prominent among the work on P_{n} have been asymptotic results for the probability that P_{n} will be connected or contain given components/subgraphs. Such progress has been achieved through a combination of counting arguments and a generating function approach.
  More recently, attention has turned to P_{n,m}, the graph taken uniformly at random from the set of all planar graphs on {1,2,...,n} with exactly m(n) edges (this can be thought of as a uniform random planar graph with a constraint on the average degree). The case when m(n) = qn for fixed q in (1,3) has been investigated, and results obtained for the events that P_{n,qn} will be connected and that P_{n,qn} will contain given subgraphs.
  In Part I of this thesis, we use elementary counting arguments to extend the current knowledge of P_{n,m}. We investigate the probability that P_{n,m} will contain given components, the probability that P_{n,m} will contain given subgraphs, and the probability that P_{n,m} will be connected, all for general m(n), and show that there is different behaviour depending on which `region' the ratio m(n)/n falls into. In Part II, we investigate the same three topics for a uniform random planar graph with constraints on the maximum and minimum degrees.△ Less"
"An alternative Gospel of structure: order, composition, processes",Authors:Bob Coecke,"Abstract:We survey some basic mathematical structures, which arguably are more primitive than the structures taught at school. These structures are orders, with or without composition, and (symmetric) monoidal categories. We list several `real life' incarnations of each of these. This paper also serves as an introduction to these structures and their current and potentially future uses in linguistics, phys…▽ MoreWe survey some basic mathematical structures, which arguably are more primitive than the structures taught at school. These structures are orders, with or without composition, and (symmetric) monoidal categories. We list several `real life' incarnations of each of these. This paper also serves as an introduction to these structures and their current and potentially future uses in linguistics, physics and knowledge representation.△ Less"
"Map of Life: Measuring and Visualizing Species' Relatedness with ""Molecular Distance Maps""","Authors:Lila Kari,Kathleen A. Hill,Abu Sadat Sayem,Nathaniel Bryans,Katelyn Davis,Nikesh S. Dattani","Abstract:We propose a novel combination of methods that (i) portrays quantitative characteristics of a DNA sequence as an image, (ii) computes distances between these images, and (iii) uses these distances to output a map wherein each sequence is a point in a common Euclidean space. In the resulting ""Molecular Distance Map"" each point signifies a DNA sequence, and the geometric distance between any two poi…▽ MoreWe propose a novel combination of methods that (i) portrays quantitative characteristics of a DNA sequence as an image, (ii) computes distances between these images, and (iii) uses these distances to output a map wherein each sequence is a point in a common Euclidean space. In the resulting ""Molecular Distance Map"" each point signifies a DNA sequence, and the geometric distance between any two points reflects the degree of relatedness between the corresponding sequences and species.
  Molecular Distance Maps present compelling visual representations of relationships between species and could be used for taxonomic clarifications, for species identification, and for studies of evolutionary history. One of the advantages of this method is its general applicability since, as sequence alignment is not required, the DNA sequences chosen for comparison can be completely different regions in different genomes. In fact, this method can be used to compare any two DNA sequences. For example, in our dataset of 3,176 mitochondrial DNA sequences, it correctly finds the mtDNA sequences most closely related to that of the anatomically modern human (the Neanderthal, the Denisovan, and the chimp), and it finds that the sequence most different from it belongs to a cucumber. Furthermore, our method can be used to compare real sequences to artificial, computer-generated, DNA sequences. For example, it is used to determine that the distances between a Homo sapiens sapiens mtDNA and artificial sequences of the same length and same trinucleotide frequencies can be larger than the distance between the same human mtDNA and the mtDNA of a fruit-fly.
  We demonstrate this method's promising potential for taxonomical clarifications by applying it to a diverse variety of cases that have been historically controversial, such as the genus Polypterus, the family Tarsiidae, and the vast (super)kingdom Protista.△ Less"
TheOxfordQuestions on the foundations of quantum physics,"Authors:G. A. D. Briggs,J. N. Butterfield,A. Zeilinger","Abstract:The twentieth century saw two fundamental revolutions in physics -- relativity and quantum. Daily use of these theories can numb the sense of wonder at their immense empirical success. Does their instrumental effectiveness stand on the rock of secure concepts or the sand of unresolved fundamentals? Does measuring a quantum system probe, or even create, reality, or merely change belief? Must relati…▽ MoreThe twentieth century saw two fundamental revolutions in physics -- relativity and quantum. Daily use of these theories can numb the sense of wonder at their immense empirical success. Does their instrumental effectiveness stand on the rock of secure concepts or the sand of unresolved fundamentals? Does measuring a quantum system probe, or even create, reality, or merely change belief? Must relativity and quantum theory just co-exist or might we find a new theory which unifies the two? To bring such questions into sharper focus, we convened a conference on Quantum Physics and the Nature of Reality. Some issues remain as controversial as ever, but some are being nudged by theory's secret weapon of experiment.△ Less"
"Einstein, the reality of space, and the action-reaction principle","Authors:Harvey R. Brown,Dennis Lehmkuhl","Abstract:Einstein regarded as one of the triumphs of his 1915 theory of gravity --- the general theory of relativity --- that it vindicated the action--reaction principle, while Newtonian mechanics as well as his 1905 special theory of relativity supposedly violated it. In this paper we examine why Einstein came to emphasise this position several years after the development of general relativity. Several k…▽ MoreEinstein regarded as one of the triumphs of his 1915 theory of gravity --- the general theory of relativity --- that it vindicated the action--reaction principle, while Newtonian mechanics as well as his 1905 special theory of relativity supposedly violated it. In this paper we examine why Einstein came to emphasise this position several years after the development of general relativity. Several key considerations are relevant to the story: the connection Einstein originally saw between Mach's analysis of inertia and both the equivalence principle and the principle of general covariance, the waning of Mach's influence owing to de Sitter's 1917 results, and Einstein's detailed correspondence with Moritz Schlick in 1920.△ Less"
Live-wire 3D medical images segmentation,Authors:Ognjen Arandjelovic,"Abstract:This report describes the design, implementation, evaluation and original enhancements to the Live-Wire method for 2D and 3D image segmentation. Live-Wire 2D employs a semi-automatic paradigm; the user is asked to select a few boundary points of the object to segment, to steer the process in the right direction, while the result is displayed in real time. In our implementation segmentation is exte…▽ MoreThis report describes the design, implementation, evaluation and original enhancements to the Live-Wire method for 2D and 3D image segmentation. Live-Wire 2D employs a semi-automatic paradigm; the user is asked to select a few boundary points of the object to segment, to steer the process in the right direction, while the result is displayed in real time. In our implementation segmentation is extended to three dimensions by performing this process on a slice-by-slice basis. User's time and involvement is further reduced by allowing him to specify object contours in planes orthogonal to the slices. If these planes are chosen strategically, Live-Wire 3D can perform 2D segmentation in the plane of each slice automatically. This report also proposes two improvements to the original method, path heating and a new graph edge feature function based on variance of path properties along the boundary. We show that these improvements lead up to a 33% reduction in interaction with the user, and improved delineation in presence of strong interfering edges.△ Less"
An Adaptive Descriptor Design for Object Recognition in the Wild,"Authors:Zhenyu Guo,Z. Jane Wang","Abstract:…and multiple kernel learning, without estimating or specifying the styles of images used in training and testing. We conduct experiments on Domain Adaptation data set andOxfordFlower data set. The experiments also include several variants of the flower data set by processing the images with popular photo effects. The results demonstrate that our proposed m…▽ MoreDigital images nowadays have various styles of appearance, in the aspects of color tones, contrast, vignetting, and etc. These 'picture styles' are directly related to the scene radiance, image pipeline of the camera, and post processing functions. Due to the complexity and nonlinearity of these causes, popular gradient-based image descriptors won't be invariant to different picture styles, which will decline the performance of object recognition. Given that images shared online or created by individual users are taken with a wide range of devices and may be processed by various post processing functions, to find a robust object recognition system is useful and challenging. In this paper, we present the first study on the influence of picture styles for object recognition, and propose an adaptive approach based on the kernel view of gradient descriptors and multiple kernel learning, without estimating or specifying the styles of images used in training and testing. We conduct experiments on Domain Adaptation data set andOxfordFlower data set. The experiments also include several variants of the flower data set by processing the images with popular photo effects. The results demonstrate that our proposed method improve from standard descriptors in all cases.△ Less"
"Survival of the Unfittest: Why the Worst Infrastructure Gets Built, And What We Can Do about It",Authors:Bent Flyvbjerg,"Abstract:The article first describes characteristics of major infrastructure projects. Second, it documents a much neglected topic in economics: that ex ante estimates of costs and benefits are often very different from actual ex post costs and benefits. For large infrastructure projects the consequence is cost overruns, benefit shortfalls, and the systematic underestimation of risks. Third, implications f…▽ MoreThe article first describes characteristics of major infrastructure projects. Second, it documents a much neglected topic in economics: that ex ante estimates of costs and benefits are often very different from actual ex post costs and benefits. For large infrastructure projects the consequence is cost overruns, benefit shortfalls, and the systematic underestimation of risks. Third, implications for cost-benefit analysis are described, including that such analysis is not to be trusted for major infrastructure projects. Fourth, the article uncovers the causes of this state of affairs in terms of perverse incentives that encourage promoters to underestimate costs and overestimate benefits in the business cases for their projects. But the projects that are made to look best on paper are the projects that amass the highest cost overruns and benefit shortfalls in reality. The article depicts this situation as ""survival of the un-fittest."" Fifth, the article sets out to explain how the problem may be solved, with a view to arriving at more efficient and more democratic projects, and avoiding the scandals that often accompany major infrastructure investments. Finally, the article identifies current trends in major infrastructure development. It is argued that a rapid increase in stimulus spending combined with more investments in emerging economies combined with more spending on information technology is catapulting infrastructure investment from the frying pan into the fire.△ Less"
Types and forgetfulness in categorical linguistics and quantum mechanics,Authors:Peter Hines,"Abstract:The role of types in categorical models of meaning is investigated. A general scheme for how typed models of meaning may be used to compare sentences, regardless of their grammatical structure is described, and a toy example is used as an illustration. Taking as a starting point the question of whether the evaluation of such a type system 'loses information', we consider the parametrized typing as…▽ MoreThe role of types in categorical models of meaning is investigated. A general scheme for how typed models of meaning may be used to compare sentences, regardless of their grammatical structure is described, and a toy example is used as an illustration. Taking as a starting point the question of whether the evaluation of such a type system 'loses information', we consider the parametrized typing associated with connectives from this viewpoint.
  The answer to this question implies that, within full categorical models of meaning, the objects associated with types must exhibit a simple but subtle categorical property known as self-similarity. We investigate the category theory behind this, with explicit reference to typed systems, and their monoidal closed structure. We then demonstrate close connections between such self-similar structures and dagger Frobenius algebras. In particular, we demonstrate that the categorical structures implied by the polymorphically typed connectives give rise to a (lax unitless) form of the special forms of Frobenius algebras known as classical structures, used heavily in abstract categorical approaches to quantum mechanics.△ Less"
Explaining temporal trends in annualized relapse rates in placebo groups of randomized controlled trials in relapsing multiple sclerosis: systematic review and meta-regression,"Authors:Simon M. Steinvorth,Christian Röver,Simon Schneider,Richard Nicholas,Sebastian Straube,Tim Friede","Abstract:…We identified 56 studies. Patient age at baseline (p < 0.001), mean duration of multiple sclerosis (MS) at baseline (p = 0.048), size of treatment groups (p = 0.003),OxfordQuality Scale scores (p = 0.021), and the number of eligibility criteria (p<0.001) increased significantly, whereas pre-trial ARR (p = 0.001), the time span over which pre-trial AR…▽ MoreBackground: Recent studies have shown a decrease in annualised relapse rates (ARRs) in placebo groups of randomised controlled trials (RCTs) in relapsing multiple sclerosis (RMS).
  Methods: We conducted a systematic literature search of RCTs in RMS. Data on eligibility criteria and baseline characteristics were extracted and tested for significant trends over time. A meta-regression was conducted to estimate their contribution to the decrease of trial ARRs over time.
  Results: We identified 56 studies. Patient age at baseline (p < 0.001), mean duration of multiple sclerosis (MS) at baseline (p = 0.048), size of treatment groups (p = 0.003),OxfordQuality Scale scores (p = 0.021), and the number of eligibility criteria (p<0.001) increased significantly, whereas pre-trial ARR (p = 0.001), the time span over which pre-trial ARR was calculated (p < 0.001), and the duration of placebo-controlled follow-up (p = 0.006) decreased significantly over time. In meta-regression of trial placebo ARR, the temporal trend was found to be insignificant, with major factors explaining the variation: pre-trial ARR, the number of years used to calculate pre-trial ARR and study duration. Conclusion: The observed decline in trial ARRs may result from decreasing pre-trial ARRs and a shorter time period over which pre-trial ARRs were calculated. Increasing patient age and duration of illness may also contribute.△ Less"
Further analysis of the binary Euclidean algorithm,Authors:Richard P. Brent,"Abstract:The binary Euclidean algorithm is a variant of the classical Euclidean algorithm. It avoids multiplications and divisions, except by powers of two, so is potentially faster than the classical algorithm on a binary machine. We describe the binary algorithm and consider its average case behaviour. In particular, we correct some errors in the literature, discuss some results of Vallée, and describe a…▽ MoreThe binary Euclidean algorithm is a variant of the classical Euclidean algorithm. It avoids multiplications and divisions, except by powers of two, so is potentially faster than the classical algorithm on a binary machine. We describe the binary algorithm and consider its average case behaviour. In particular, we correct some errors in the literature, discuss some results of Vallée, and describe a numerical computation which supports a conjecture of Vallée.△ Less"
Stellar Superfluids,"Authors:Dany Page,James M. Lattimer,Madappa Prakash,Andrew W. Steiner","Abstract:Neutron stars provide a fertile environment for exploring superfluidity under extreme conditions. It is not surprising that Cooper pairing occurs in dense matter since nucleon pairing is observed in nuclei as energy differences between even-even and odd-even nuclei. Since superfluids and superconductors in neutron stars profoundly affect neutrino emissivities and specific heats, their presence can…▽ MoreNeutron stars provide a fertile environment for exploring superfluidity under extreme conditions. It is not surprising that Cooper pairing occurs in dense matter since nucleon pairing is observed in nuclei as energy differences between even-even and odd-even nuclei. Since superfluids and superconductors in neutron stars profoundly affect neutrino emissivities and specific heats, their presence can be observed in the thermal evolution of neutron stars. An ever-growing number of cooling neutron stars, now amounting to 13 thermal sources, and several additional objects from which upper limits to temperatures can be ascertained, can now be used to discriminate among theoretical scenarios and even to dramatically restrict properties of nucleon pairing at high densities. In addition, observations of pulsars, including their spin-downs and glitch histories, additionally support the conjecture that superfluidity and superconductivity are ubiquitous within, and important to our understanding of, neutron stars.△ Less"
Characteristics of Plasma Turbulence in the Mega Amp Spherical Tokamak,Authors:Young-chul Ghim,"Abstract:Turbulence is a major factor limiting the achievement of better tokamak performance as it enhances the transport of particles, momentum and heat which hinders the foremost objective of tokamaks. Hence, understanding and possibly being able to control turbulence in tokamaks is of paramount importance, not to mention our intellectual curiosity of it.Turbulence is a major factor limiting the achievement of better tokamak performance as it enhances the transport of particles, momentum and heat which hinders the foremost objective of tokamaks. Hence, understanding and possibly being able to control turbulence in tokamaks is of paramount importance, not to mention our intellectual curiosity of it.△ Less"
"Online Deliberation Design: Choices, Criteria, and Evidence","Authors:Todd Davies,Reid Chandler","Abstract:This chapter reviews empirical evidence bearing on the design of online forums for deliberative civic engagement. Dimensions of design are defined for different aspects of the deliberation: its purpose, the target population, the spatiotemporal distance separating participants, the communication medium, and the deliberative process to be followed. After a brief overview of criteria for evaluating…▽ MoreThis chapter reviews empirical evidence bearing on the design of online forums for deliberative civic engagement. Dimensions of design are defined for different aspects of the deliberation: its purpose, the target population, the spatiotemporal distance separating participants, the communication medium, and the deliberative process to be followed. After a brief overview of criteria for evaluating different design options, empirical findings are organized around design choices. Research has evolved away from treating technology for online deliberation dichotomously (either present or not) toward nuanced findings that differentiate between technological features, ways of using them, and cultural settings. The effectiveness of online deliberation depends on how well the communicative environment is matched to the deliberative task. Tradeoffs, e.g. between rich and lean media and between anonymous and identifiable participation, suggest different designs depending on the purpose and participants. Findings are limited by existing technologies, and may change as technologies and users co-evolve.△ Less"
Unconventional Superconductivity,Authors:M. R. Norman,"Abstract:A brief review of unconventional superconductivity is given, stretching from the halcyon days of helium-3 to the modern world of Majorana fermions. Along the way, we will encounter such strange beasts as heavy fermion superconductors, cuprates, and their iron-based cousins. Emphasis will be put on the fact that in almost all cases, an accepted microscopic theory has yet to emerge. This is attribut…▽ MoreA brief review of unconventional superconductivity is given, stretching from the halcyon days of helium-3 to the modern world of Majorana fermions. Along the way, we will encounter such strange beasts as heavy fermion superconductors, cuprates, and their iron-based cousins. Emphasis will be put on the fact that in almost all cases, an accepted microscopic theory has yet to emerge. This is attributed to the difficulty of constructing a theory of superconductivity outside the Migdal-Eliashberg framework.△ Less"
"Beyond the Betz Theory - Blockage, Wake Mixing and Turbulence",Authors:Takafumi Nishino,"Abstract:Recent analytical models concerning the limiting efficiency of marine hydrokinetic (MHK) turbines are reviewed with an emphasis on the significance of blockages (of local as well as global flow passages) and wake mixing. Also discussed is the efficiency of power generation from fully developed turbulent open channel flows. These issues are primarily concerned with the design/optimization of tidal…▽ MoreRecent analytical models concerning the limiting efficiency of marine hydrokinetic (MHK) turbines are reviewed with an emphasis on the significance of blockages (of local as well as global flow passages) and wake mixing. Also discussed is the efficiency of power generation from fully developed turbulent open channel flows. These issues are primarily concerned with the design/optimization of tidal turbine arrays; however, some of them are relevant to wind turbines as well.△ Less"
Two questions on polynomial decomposition,"Authors:Brian Wyman,Michael Zieve","Abstract:Given a univariate polynomial f(x) over a ring R, we examine when we can write f(x) as g(h(x)) where g and h are polynomials of degree at least 2. We answer two questions of Gusic regarding when the existence of such g and h over an extension of R implies the existence of such g and h over R. We also pose two new questions along these lines.Given a univariate polynomial f(x) over a ring R, we examine when we can write f(x) as g(h(x)) where g and h are polynomials of degree at least 2. We answer two questions of Gusic regarding when the existence of such g and h over an extension of R implies the existence of such g and h over R. We also pose two new questions along these lines.△ Less"
Spectral data for G-Higgs bundles,Authors:Laura P. Schaposnik,"Abstract:We develop a new geometric method of understanding principal G-Higgs bundles through their spectral data, for G a real form of a complex Lie group. In particular, we consider the case of G a split real form, as well as G = SL(2,R), U(p,p), SU(p,p), and Sp(2p,2p). Further, we give some applications of our results, and discuss open questions.We develop a new geometric method of understanding principal G-Higgs bundles through their spectral data, for G a real form of a complex Lie group. In particular, we consider the case of G a split real form, as well as G = SL(2,R), U(p,p), SU(p,p), and Sp(2p,2p). Further, we give some applications of our results, and discuss open questions.△ Less"
Coherent exciton transport in semiconductors,"Authors:Massimo Rontani,L. J. Sham","Abstract:We review the topic of Bose-Einstein condensation of excitons in semiconductors, focusing on the signatures of the macroscopic order of the exciton condensate.We review the topic of Bose-Einstein condensation of excitons in semiconductors, focusing on the signatures of the macroscopic order of the exciton condensate.△ Less"
Biological Database of Images and Genomes: tools for community annotations linking image and genomic information,"Authors:Andrew T. Oberlin,Dominika A. Jurkovic,Mitchell F. Balish,Iddo Friedberg","Abstract:Genomic data and biomedical imaging data are undergoing exponential growth. However, our understanding of the phenotype-genotype connection linking the two types of data is lagging behind. While there are many types of software that enable the manipulation and analysis of image data and genomic data as separate entities, there is no framework established for linking the two. We present a generic s…▽ MoreGenomic data and biomedical imaging data are undergoing exponential growth. However, our understanding of the phenotype-genotype connection linking the two types of data is lagging behind. While there are many types of software that enable the manipulation and analysis of image data and genomic data as separate entities, there is no framework established for linking the two. We present a generic set of software tools, BioDIG, that allows linking of image data to genomic data. BioDIG tools can be applied to a wide range of research problems that require linking images to genomes. BioDIG features the following: rapid construction of web-based workbenches, community-based annotation, user management, and web-services. By using BioDIG to create websites, researchers and curators can rapidly annotate large number of images with genomic information. Here we present the BioDIG software tools that include an image module, a genome module and a user management module. We also introduce a BioDIG-based website, MyDIG, which is being used to annotate images of Mycoplasma.△ Less"
Distribution of Aligned Letter Pairs in Optimal Alignments of Random Sequences,"Authors:Raphael Hauser,Heinrich Matzinger","Abstract:Considering the optimal alignment of two i.i.d. random sequences of length $n$, we show that when the scoring function is chosen randomly, almost surely the empirical distribution of aligned letter pairs in all optimal alignments converges to a unique limiting distribution as $n$ tends to infinity. This result is interesting because it helps understanding the microscopic path structure of a specia…▽ MoreConsidering the optimal alignment of two i.i.d. random sequences of length $n$, we show that when the scoring function is chosen randomly, almost surely the empirical distribution of aligned letter pairs in all optimal alignments converges to a unique limiting distribution as $n$ tends to infinity. This result is interesting because it helps understanding the microscopic path structure of a special type of last passage percolation problem with correlated weights, an area of long-standing open problems. Characterizing the microscopic path structure yields furthermore a robust alternative to optimal alignment scores for testing the relatedness of genetic sequences.△ Less"
A Monte Carlo Approach to the Fluctuation Problem in Optimal Alignments of Random Strings,"Authors:Saba Amsalu,Raphael Hauser,Heinrich Matzinger","Abstract:The problem of determining the correct order of fluctuation of the optimal alignment score of two random strings of length $n$ has been open for several decades. It is known that the biased expected effect of a random letter-change on the optimal score implies an order of fluctuation linear in $\sqrt{n}$. However, in many situations where such a biased effect is observed empirically, it has been i…▽ MoreThe problem of determining the correct order of fluctuation of the optimal alignment score of two random strings of length $n$ has been open for several decades. It is known that the biased expected effect of a random letter-change on the optimal score implies an order of fluctuation linear in $\sqrt{n}$. However, in many situations where such a biased effect is observed empirically, it has been impossible to prove analytically. The main result of this paper shows that when the rescaled-limit of the optimal alignment score increases in a certain direction, then the biased effect exists. On the basis of this result one can quantify a confidence level for the existence of such a biased effect and hence of an order $\sqrt{n}$ fluctuation based on simulation of optimal alignments scores.This is an important step forward, as the correct order of fluctuation was previously known only for certain special distributions. To illustrate the usefulness of our new methodology, we apply it to optimal alignments of strings written in the DNA-alphabet. As scoring function, we use the BLASTZ default-substitution matrix together with a realistic gap penalty. BLASTZ is one of the most widely used sequence alignment methodologies in bioinformatics. For this DNA-setting, we show that with a high level of confidence, the fluctuation of the optimal alignment score is of order $Θ(\sqrt{n})$. An important special case of optimal alignment score is the Longest Common Subsequence (LCS) of random strings. For binary sequences with equiprobable symbols, the question of the fluctuation of the LCS remains open. The symmetry in that case does not allow for our method. On the other hand, in real-life DNA sequences, it is not the case that all letters occur with the same frequency. Thus, for many real life situations, our method allows to determine the order of the fluctuation up to a high confidence level.△ Less"
Value sets of polynomial maps over finite fields,"Authors:Gary L. Mullen,Daqing Wan,Qiang Wang",Abstract:We provide upper bounds for the cardinality of the value set of a polynomial map in several variables over a finite field. These bounds generalize earlier bounds for univariate polynomials.We provide upper bounds for the cardinality of the value set of a polynomial map in several variables over a finite field. These bounds generalize earlier bounds for univariate polynomials.△ Less
The ALICE EMCal L1 trigger first year of operation experience,"Authors:O. Bourrion,N. Arbor,G. Conesa-Balbastre,C. Furget,R. Guernane,G. Marcotte","Abstract:The ALICE experiment at the LHC is equipped with an electromagnetic calorimeter (EMCal) designed to enhance its capabilities for jet, photon and electron measurement. In addition, the EMCal enables triggering on jets and photons with a centrality dependent energy threshold. After its commissioning in 2010, the EMCal Level 1 (L1) trigger was officially approved for physics data taking in 2011. Afte…▽ MoreThe ALICE experiment at the LHC is equipped with an electromagnetic calorimeter (EMCal) designed to enhance its capabilities for jet, photon and electron measurement. In addition, the EMCal enables triggering on jets and photons with a centrality dependent energy threshold. After its commissioning in 2010, the EMCal Level 1 (L1) trigger was officially approved for physics data taking in 2011. After describing the L1 hardware and trigger algorithms, the commissioning and the first year of running experience, both in proton and heavy ion beams, are reviewed. Additionally, the upgrades to the original L1 trigger design are detailed.△ Less"
"Development of a custom on-line ultrasonic vapour analyzer/flowmeter for the ATLAS inner detector, with application to gaseous tracking and Cherenkov detectors","Authors:R. Bates,M. Battistin,S. Berry,J. Berthoud,A. Bitadze,P. Bonneau,J. Botelho-Direito,N. Bousson,G. Boyd,G. Bozza,E. Da Riva,C. Degeorge,B. DiGirolamo,M. Doubek,J. Godlewski,G. Hallewell,S. Katunin,D. Lombard,M. Mathieu,S. McMahon,K. Nagai,E. Perez-Rodriguez,C. Rossi,A. Rozanov,V. Vacek, et al. (2 additional authors not shown)","Abstract:Precision sound velocity measurements can simultaneously determine binary gas composition and flow. We have developed an analyzer with custom electronics, currently in use in the ATLAS inner detector, with numerous potential applications. The instrument has demonstrated ~0.3% mixture precision for C3F8/C2F6 mixtures and < 10-4 resolution for N2/C3F8 mixtures. Moderate and high flow versions of the…▽ MorePrecision sound velocity measurements can simultaneously determine binary gas composition and flow. We have developed an analyzer with custom electronics, currently in use in the ATLAS inner detector, with numerous potential applications. The instrument has demonstrated ~0.3% mixture precision for C3F8/C2F6 mixtures and < 10-4 resolution for N2/C3F8 mixtures. Moderate and high flow versions of the instrument have demonstrated flow resolutions of +/- 2% F.S. for flows up to 250 l.min-1, and +/- 1.9% F.S. for linear flow velocities up to 15 ms-1; the latter flow approaching that expected in the vapour return of the thermosiphon fluorocarbon coolant recirculator being built for the ATLAS silicon tracker.△ Less"
Bose-Einstein condensation of photons,"Authors:Jan Klaers,Martin Weitz","Abstract:We review recent work on the Bose-Einstein condensation of photons in a dye microcavity environment. Other than for material particles, as e.g. cold atomic Bose gases, photons usually do not condense at low temperatures. For Planck's blackbody radiation, the most ubiquitous Bose gas, photon number and temperature are not independently tunable and at low temperatures the photons simply disappear in…▽ MoreWe review recent work on the Bose-Einstein condensation of photons in a dye microcavity environment. Other than for material particles, as e.g. cold atomic Bose gases, photons usually do not condense at low temperatures. For Planck's blackbody radiation, the most ubiquitous Bose gas, photon number and temperature are not independently tunable and at low temperatures the photons simply disappear in the system's walls, instead of massively occupying the cavity ground mode. In the here described approach, this obstacle is overcome by a fluorescence-induced thermalization mechanism in a dye-filled microcavity. Experimentally, both the thermalization of the photon gas and, at high photon densities, Bose-Einstein condensation has been observed. This article describes the thermalization mechanism of the photon gas in detail and summarizes so far performed experimental work.△ Less"
On proofs of the Farrell-Jones Conjecture,Authors:Arthur Bartels,"Abstract:These notes contain an introduction to proofs of Farrell-Jones Conjecture for some groups and are based on talks given in Ohio,Oxford, Berlin, Shanghai, Münster and Oberwolfach in 2011 and 2012.These notes contain an introduction to proofs of Farrell-Jones Conjecture for some groups and are based on talks given in Ohio,Oxford, Berlin, Shanghai, Münster and Oberwolfach in 2011 and 2012.△ Less"
Information requirements for enterprise systems,"Authors:Ian Sommerville,Russell Lock,Tim Storer","Abstract:In this paper, we discuss an approach to system requirements engineering, which is based on using models of the responsibilities assigned to agents in a multi-agency system of systems. The responsibility models serve as a basis for identifying the stakeholders that should be considered in establishing the requirements and provide a basis for a structured approach, described here, for information r…▽ MoreIn this paper, we discuss an approach to system requirements engineering, which is based on using models of the responsibilities assigned to agents in a multi-agency system of systems. The responsibility models serve as a basis for identifying the stakeholders that should be considered in establishing the requirements and provide a basis for a structured approach, described here, for information requirements elicitation. We illustrate this approach using a case study drawn from civil emergency management.△ Less"
On the supersingular loci of quaternionic Siegel space,Authors:Oliver Bueltel,"Abstract:The paper studies the supersingular locus of the characteristic p moduli space of principally polarized abelian 8-folds that are equipped with an action of a maximal order in a quaternion algebra, that is non-split at the infinite place, but split at p. The main result is that its irreducible components are Fermat surfaces of degree p+1.The paper studies the supersingular locus of the characteristic p moduli space of principally polarized abelian 8-folds that are equipped with an action of a maximal order in a quaternion algebra, that is non-split at the infinite place, but split at p. The main result is that its irreducible components are Fermat surfaces of degree p+1.△ Less"
Monoidal computer I: Basic computability by string diagrams,Authors:Dusko Pavlovic,"Abstract:We present a new model of computation, described in terms of monoidal categories. It conforms the Church-Turing Thesis, and captures the same computable functions as the standard models. It provides a succinct categorical interface to most of them, free of their diverse implementation details, using the ideas and structures that in the meantime emerged from research in semantics of computation and…▽ MoreWe present a new model of computation, described in terms of monoidal categories. It conforms the Church-Turing Thesis, and captures the same computable functions as the standard models. It provides a succinct categorical interface to most of them, free of their diverse implementation details, using the ideas and structures that in the meantime emerged from research in semantics of computation and programming. The salient feature of the language of monoidal categories is that it is supported by a sound and complete graphical formalism, string diagrams, which provide a concrete and intuitive interface for abstract reasoning about computation. The original motivation and the ultimate goal of this effort is to provide a convenient high level programming language for a theory of computational resources, such as one-way functions, and trapdoor functions, by adopting the methods for hiding the low level implementation details that emerged from practice. In the present paper, we make a first step towards this ambitious goal, and sketch a path to reach it. This path is pursued in three sequel papers, that are in preparation.△ Less"
Scientometrics,"Authors:Loet Leydesdorff,Staša Milojević","Abstract:The paper provides an overview of the field of scientometrics, that is: the study of science, technology, and innovation from a quantitative perspective. We cover major historical milestones in the development of this specialism from the 1960s to today and discuss its relationship with the sociology of scientific knowledge, the library and information sciences, and science policy issues such as in…▽ MoreThe paper provides an overview of the field of scientometrics, that is: the study of science, technology, and innovation from a quantitative perspective. We cover major historical milestones in the development of this specialism from the 1960s to today and discuss its relationship with the sociology of scientific knowledge, the library and information sciences, and science policy issues such as indicator development. The disciplinary organization of scientometrics is analyzed both conceptually and empirically, using a map of journals cited in the core journal of the field, entitled Scientometrics. A state-of-the-art review of five major research threads is provided: (1) the measurement of impact; (2) the delineation of reference sets; (3) theories of citation; (4) mapping science; and (5) the policy and management contexts of indicator developments.△ Less"
Nonlinear oscillators and high fidelity qubit state measurement in circuit quantum electrodynamics,"Authors:Eran Ginossar,Lev S. Bishop,S. M. Girvin","Abstract:In this book chapter we analyze the high excitation nonlinear response of the Jaynes-Cummings model in quantum optics when the qubit and cavity are strongly coupled. We focus on the parameter ranges appropriate for transmon qubits in the circuit quantum electrodynamics architecture, where the system behaves essentially as a nonlinear quantum oscillator and we analyze the quantum and semi-classical…▽ MoreIn this book chapter we analyze the high excitation nonlinear response of the Jaynes-Cummings model in quantum optics when the qubit and cavity are strongly coupled. We focus on the parameter ranges appropriate for transmon qubits in the circuit quantum electrodynamics architecture, where the system behaves essentially as a nonlinear quantum oscillator and we analyze the quantum and semi-classical dynamics. One of the central motivations is that under strong excitation tones, the nonlinear response can lead to qubit quantum state discrimination and we present initial results for the cases when the qubit and cavity are on resonance or far off-resonance (dispersive).△ Less"
The Zero Turbulence Manifold in Fusion Plasmas,Authors:E. G. Highcock,"Abstract:The transport of heat that results from turbulence is a major factor limiting the temperature gradient, and thus the performance, of fusion devices. We use nonlinear simulations to show that a toroidal equilibrium scale sheared flow can completely suppress the turbulence across a wide range of flow gradient and temperature gradient values. We demonstrate the existence of a bifurcation across this…▽ MoreThe transport of heat that results from turbulence is a major factor limiting the temperature gradient, and thus the performance, of fusion devices. We use nonlinear simulations to show that a toroidal equilibrium scale sheared flow can completely suppress the turbulence across a wide range of flow gradient and temperature gradient values. We demonstrate the existence of a bifurcation across this range whereby the plasma may transition from a low flow gradient and temperature gradient state to a higher flow gradient and temperature gra- dient state. We show further that the maximum temperature gradient that can be reached by such a transition is limited by the existence, at high flow gradient, of subcritical turbulence driven by the parallel velocity gradient (PVG). We use linear simulations and analytic calculations to examine the properties of the transiently growing modes which give rise to this subcritical turbulence, and conclude that there may be a critical value of the ratio of the PVG to the suppressing perpendicular gradient of the velocity (in a tokamak this ratio is equal to q/ε where q is the magnetic safety factor and ε the inverse aspect ratio) below which the PVG is unable to drive subcritical turbulence. In light of this, we use nonlinear simulations to calculate, as a function of three parameters (the perpendicular flow shear, q/ε and the temperature gradient), the surface within that parameter space which divides the regions where turbulence can and cannot be sustained: the zero- turbulence manifold. We are unable to conclude that there is in fact a critical value of q/ε below which PVG-driven turbulence is eliminated. Nevertheless, we demonstrate that at low values of q/ε, the maximum critical temperature gradient that can be reached without generating turbulence is dramatically increased.△ Less"
Twistor Theory of Higher-Dimensional Black Holes,Authors:Norman Metzner,"Abstract:The correspondence of stationary, axisymmetric, asymptotically flat space-times and bundles over a reduced twistor space has been established in four dimensions. The main impediment for an application of this correspondence to examples in higher dimensions is the lack of a higher-dimensional equivalent of the Ernst potential. This thesis will propose such a generalized Ernst potential, point out w…▽ MoreThe correspondence of stationary, axisymmetric, asymptotically flat space-times and bundles over a reduced twistor space has been established in four dimensions. The main impediment for an application of this correspondence to examples in higher dimensions is the lack of a higher-dimensional equivalent of the Ernst potential. This thesis will propose such a generalized Ernst potential, point out where the rod structure of the space-time can be found in the twistor picture and thereby provide a procedure for generating solutions to the Einstein field equations in higher dimensions from the rod structure, other asymptotic data, and the requirement of a regular axis. Examples in five dimensions are studied and necessary tools are developed, in particular rules for the transition between different adaptations of the patching matrix and rules for the elimination of conical singularities.△ Less"
Statistical mechanics of interfaces,Authors:Salvador Miracle-Sole,Abstract:Mathematical aspects of the theory of interfaces in statistical mechanics are discussed.Mathematical aspects of the theory of interfaces in statistical mechanics are discussed.△ Less
Ionic Interactions in Biological and Physical Systems: a Variational Treatment,Authors:Bob Eisenberg,"Abstract:Chemistry is about chemical reactions. Chemistry is about electrons changing their configurations as atoms and molecules react. Chemistry studies reactions as if they occurred in ideal infinitely dilute solutions. But most reactions occur in nonideal solutions. Then everything (charged) interacts with everything else (charged) through the electric field, which is short and long range extending to…▽ MoreChemistry is about chemical reactions. Chemistry is about electrons changing their configurations as atoms and molecules react. Chemistry studies reactions as if they occurred in ideal infinitely dilute solutions. But most reactions occur in nonideal solutions. Then everything (charged) interacts with everything else (charged) through the electric field, which is short and long range extending to boundaries of the system. Mathematics has recently been developed to deal with interacting systems of this sort. The variational theory of complex fluids has spawned the theory of liquid crystals. In my view, ionic solutions should be viewed as complex fluids. In both biology and electrochemistry ionic solutions are mixtures highly concentrated (~10M) where they are most important, near electrodes, nucleic acids, enzymes, and ion channels. Calcium is always involved in biological solutions because its concentration in a particular location is the signal that controls many biological functions. Such interacting systems are not simple fluids, and it is no wonder that analysis of interactions, such as the Hofmeister series, rooted in that tradition, has not succeeded as one would hope. We present a variational treatment of hard spheres in a frictional dielectric. The theory automatically extends to spatially nonuniform boundary conditions and the nonequilibrium systems and flows they produce. The theory is unavoidably self-consistent since differential equations are derived (not assumed) from models of (Helmholtz free) energy and dissipation of the electrolyte. The origin of the Hofmeister series is (in my view) an inverse problem that becomes well posed when enough data from disjoint experimental traditions are interpreted with a self-consistent theory.△ Less"
Nonembedding and nonextension results in special holonomy,Authors:Robert L. Bryant,"Abstract:Constructions of metrics with special holonomy by methods of exterior differential systems are reviewed and the interpretations of these construction as `flows' on hypersurface geometries are considered.
  It is shown that these hypersurface 'flows' are not generally well-posed for smooth initial data and counterexamples to existence are constructed.Constructions of metrics with special holonomy by methods of exterior differential systems are reviewed and the interpretations of these construction as `flows' on hypersurface geometries are considered.
  It is shown that these hypersurface 'flows' are not generally well-posed for smooth initial data and counterexamples to existence are constructed.△ Less"
Carbon nanotubes: Nonlinear high-Q resonators with strong coupling to single-electron tunneling,"Authors:Harold B. Meerwaldt,Gary A. Steele,Herre S. J. van der Zant","Abstract:Carbon nanotubes (CNTs) are nonlinear high-Q resonators with strong coupling to single-electron tunneling. We begin by describing several methods to detect the flexural motion of a CNT resonator. Next, we illustrate how single-electron tunneling in quantum dot CNT resonators leads to sharp dips in the mechanical resonance frequency and significant damping. We discuss four different contributions t…▽ MoreCarbon nanotubes (CNTs) are nonlinear high-Q resonators with strong coupling to single-electron tunneling. We begin by describing several methods to detect the flexural motion of a CNT resonator. Next, we illustrate how single-electron tunneling in quantum dot CNT resonators leads to sharp dips in the mechanical resonance frequency and significant damping. We discuss four different contributions to the nonlinear oscillation of a CNT resonator: beam-like mechanical nonlinearity, nonlinearity due to gate-induced mechanical tension, electrostatic nonlinearity, and nonlinearity due to single-electron tunneling, and provide quantitative estimates of their strengths. Finally, we show how the large response of the resonance frequency of a CNT resonator to a change in gate voltage or tension makes CNT resonators ideally suited for parametric excitation and for studying the coupling between different mechanical modes.△ Less"
Energy densities in the strong-interaction limit of density functional theory,"Authors:André Mirtschink,Michael Seidl,Paola Gori-Giorgi","Abstract:…discussed, comparing the energy densities of the Kohn-Sham, the physical, and the strong-interacting systems. We also use our results to analyze the local version of the Lieb-Oxfordbound, widely used in the construction of approximate exchange-correlation functionals.▽ MoreWe discuss energy densities in the strong-interaction limit of density functional theory, deriving an exact expression within the definition (gauge) of the electrostatic potential of the exchange-correlation hole. Exact results for small atoms and small model quantum dots are compared with available approximations defined in the same gauge. The idea of a local interpolation along the adiabatic connection is discussed, comparing the energy densities of the Kohn-Sham, the physical, and the strong-interacting systems. We also use our results to analyze the local version of the Lieb-Oxfordbound, widely used in the construction of approximate exchange-correlation functionals.△ Less"
AnOxfordSWIFT Integral Field Spectroscopy study of 14 early-type galaxies in the Coma cluster,"Authors:Nicholas Scott,Ryan C. W. Houghton,Roger L. Davies,Michele Cappellari,Niranjan Thatte,Fraser J. Clarke,Matthias Tecza","Abstract:As a demonstration of the capabilities of the newOxfordSWIFT integral field spectrograph, we present first observations for a set of 14 early-type galaxies in the core of the Coma cluster. Our data consist of I- and z-band spatially resolved spectroscopy obtained with theOxfordSWIFT spectrograph, combined with r-ba…▽ MoreAs a demonstration of the capabilities of the newOxfordSWIFT integral field spectrograph, we present first observations for a set of 14 early-type galaxies in the core of the Coma cluster. Our data consist of I- and z-band spatially resolved spectroscopy obtained with theOxfordSWIFT spectrograph, combined with r-band photometry from the SDSS archive for 14 early- type galaxies. We derive spatially resolved kinematics for all objects from observations of the calcium triplet absorption features at \sim 8500 {AA} . Using this kinematic information we classify galaxies as either Fast Rotators or Slow Rotators. We compare the fraction of fast and slow rotators in our sample, representing the densest environment in the nearby Universe, to results from the ATLAS3D survey, finding the slow rotator fraction is \sim 50 per cent larger in the core of the Coma cluster than in the Virgo cluster or field, a 1.2 σ increase given our selection criteria. Comparing our sample to the Virgo cluster core only (which is 24 times less dense than the Coma core) we find no evidence of an increase in the slow rotator fraction. Combining measurements of the effective velocity dispersion {σ_e} with the photometric data we determine the Fundamental Plane for our sample of galaxies. We find the use of the average velocity dispersion within 1 effective radius, {σ_e}, reduces the residuals by 13 per cent with respect to comparable studies using central velocity dispersions, consistent with other recent integral field Fundamental Plane determinations.△ Less"
On the violation of a local form of the Lieb-Oxfordbound,"Authors:J. G. Vilhena,E. Rasanen,L. Lehtovaara,M. A. L. Marques","Abstract:In the framework of density-functional theory, several popular density functionals for exchange and correlation have been constructed to satisfy a local form of the Lieb-Oxfordbound. In its original global expression, the bound represents a rigorous lower limit for the indirect Coulomb interaction energy. Here we employ exact-exchange calculations for the G…▽ MoreIn the framework of density-functional theory, several popular density functionals for exchange and correlation have been constructed to satisfy a local form of the Lieb-Oxfordbound. In its original global expression, the bound represents a rigorous lower limit for the indirect Coulomb interaction energy. Here we employ exact-exchange calculations for the G2 test set to show that the local form of the bound is violated in an extensive range of both the dimensionless gradient and the average electron density. Hence, the results demonstrate the severity in the usage of the local form of the bound in functional development. On the other hand, our results suggest alternative ways to construct accurate density functionals for the exchange energy.△ Less"
PageRank model of opinion formation on social networks,"Authors:Vivek Kandiah,Dima L. Shepelyansky","Abstract:We propose the PageRank model of opinion formation and investigate its rich properties on real directed networks of Universities of Cambridge andOxford, LiveJournal and Twitter. In this model the opinion formation of linked electors is weighted with their PageRank probability. We find that the society elite, corresponding to the top PageRank nodes, can impo…▽ MoreWe propose the PageRank model of opinion formation and investigate its rich properties on real directed networks of Universities of Cambridge andOxford, LiveJournal and Twitter. In this model the opinion formation of linked electors is weighted with their PageRank probability. We find that the society elite, corresponding to the top PageRank nodes, can impose its opinion to a significant fraction of the society. However, for a homogeneous distribution of two opinions there exists a bistability range of opinions which depends on a conformist parameter characterizing the opinion formation. We find that LiveJournal and Twitter networks have a stronger tendency to a totalitar opinion formation. We also analyze the Sznajd model generalized for scale-free networks with the weighted PageRank vote of electors.△ Less"
Eigenfunctions of the Cosine and Sine Transforms,Authors:Victor Katsnelson,"Abstract:…Our work can be considered a continuation and further development of results in \textit{Self-reciprocal functions} by Hardy and Titchmarsh:
  Quarterly Journ. of Math. (OxfordSer.) \textbf{1} (1930).▽ MoreA description of eigensubspaces of the cosine and sine operators is presented.
  The spectrum of each of these two operator consists of two eigenvalues (1,\,-1) and their eigensubspaces are infinite--dimensional. There are many possible bases for these subspaces, but most popular are bases constructed from the Hermite functions. We present other ""bases"" which are not discrete orthogonal sequences of vectors, but continuous orthogonal chains of vectors. Our work can be considered a continuation and further development of results in \textit{Self-reciprocal functions} by Hardy and Titchmarsh:
  Quarterly Journ. of Math. (OxfordSer.) \textbf{1} (1930).△ Less"
A model for the reversal of the toroidal rotation in tokamak,"Authors:Florin Spineanu,Madalina Vlad","Abstract:The transition from toroidal counter- to co- rotation in the core plasma has been observed at L to H transition in several tokamaks. Spontaneous reversal has also been observed in TCV beyond a threshold in the density. We develop a model based on the following phenomenology: (1) the increase of the gradient of the pressure triggers formation on a fast time scale of cells of convection (similar to…▽ MoreThe transition from toroidal counter- to co- rotation in the core plasma has been observed at L to H transition in several tokamaks. Spontaneous reversal has also been observed in TCV beyond a threshold in the density. We develop a model based on the following phenomenology: (1) the increase of the gradient of the pressure triggers formation on a fast time scale of cells of convection (similar to Rayleigh-Benard (RB), but with a single sign of vorticity); (2) poloidal rotation is induced by the envelope of the peripheric velocity of the convection cells; via the baroclinic term the gradients of temperature and density sustain the poloidal rotation against the decay due to the parallel viscosity; (3) the fast increase of poloidal flow induces a high time derivative of the radial electric field; (4) the neoclassical polarization creates a series of parallel accelerations (kiks on each bounce) of the trapped ions, leading to an increase of the toroidal precession or to its reversal; the source of energy is the work done by the radial electric field. (5) the diffusion transfers on resistive scale the toroidal momentum from the trapped ions to the untrapped ones. The correlated interactions are examined and the estimated time scales are found to be compatible with the observations.△ Less"
Hopf algebras---Variant notions and reconstruction theorems,Authors:Joost Vercruysse,"Abstract:Hopf algebras are closely related to monoidal categories. More precise, $k$-Hopf algebras can be characterized as those algebras whose category of finite dimensional representations is an autonomous monoidal category such that the forgetful functor to $k$-vectorspaces is a strict monoidal functor. This result is known as the Tannaka reconstruction theorem (for Hopf algebras). Because of the import…▽ MoreHopf algebras are closely related to monoidal categories. More precise, $k$-Hopf algebras can be characterized as those algebras whose category of finite dimensional representations is an autonomous monoidal category such that the forgetful functor to $k$-vectorspaces is a strict monoidal functor. This result is known as the Tannaka reconstruction theorem (for Hopf algebras). Because of the importance of both Hopf algebras in various fields, over the last last few decades, many generalizations have been defined. We will survey these different generalizations from the point of view of the Tannaka reconstruction theorem.△ Less"
Cosmic Explosions (Optical Transients),Authors:S. R. Kulkarni,"Abstract:This paper is an extended summary of the talk I gave at IAU Symposium ""New Horizons in Time Domain Astronomy"" (Oxford, 2011). I first review the history of transients (which is intimately related to the advent of wide-field telescopic imaging; I then summarize wide field imaging projects. The motivations that led to the design of the Palomar Transien…▽ MoreThis paper is an extended summary of the talk I gave at IAU Symposium ""New Horizons in Time Domain Astronomy"" (Oxford, 2011). I first review the history of transients (which is intimately related to the advent of wide-field telescopic imaging; I then summarize wide field imaging projects. The motivations that led to the design of the Palomar Transient Factory (PTF) followed by a summary of the astronomical returns from PTF. I review the lessons learnt from PTF. I conclude that, during this decade, optical transient searches will continue to flourish and may even accelerate as surveys at other wavelengths -- notably radio, UV and X-ray -- come on line. As a result, I venture to suggest that specialized searches for transients will continue -- even into the LSST era. I end the article by discussing the importance of follow-up telescopes for transient object studies -- a topical issue given that in the US the Portfolio Review is under away.△ Less"
Modular Categories,Authors:Michael Mueger,"Abstract:…is a review article on modular categories, extending an invited talk given at the workshop ""Categorical (co)algebraic methods in quantum informatics and linguistics"",Oxford, October 29-31, 2010. To appear in C. Heunen, M. Sadrzadeh, E. Grefenstette (eds.): Compositional methods in quantum physics and linguistics,…▽ MoreThis is a review article on modular categories, extending an invited talk given at the workshop ""Categorical (co)algebraic methods in quantum informatics and linguistics"",Oxford, October 29-31, 2010. To appear in C. Heunen, M. Sadrzadeh, E. Grefenstette (eds.): Compositional methods in quantum physics and linguistics,OxfordUniversity Press, 2012.△ Less"
The Future of X-ray Time Domain Surveys,"Authors:Daryl Haggard,Gregory R. Sivakoff","Abstract:Modern X-ray observatories yield unique insight into the astrophysical time domain. Each X-ray photon can be assigned an arrival time, an energy and a sky position, yielding sensitive, energy-dependent light curves and enabling time-resolved spectra down to millisecond time-scales. Combining those with multiple views of the same patch of sky (e.g., in the Chandra and XMM-Newton deep fields) so as…▽ MoreModern X-ray observatories yield unique insight into the astrophysical time domain. Each X-ray photon can be assigned an arrival time, an energy and a sky position, yielding sensitive, energy-dependent light curves and enabling time-resolved spectra down to millisecond time-scales. Combining those with multiple views of the same patch of sky (e.g., in the Chandra and XMM-Newton deep fields) so as to extend variability studies over longer baselines, the spectral timing capacity of X-ray observatories then stretch over 10 orders of magnitude at spatial resolutions of arcseconds, and 13 orders of magnitude at spatial resolutions of a degree. A wealth of high-energy time-domain data already exists, and indicates variability on timescales ranging from microseconds to years in a wide variety of objects, including numerous classes of AGN, high-energy phenomena at the Galactic centre, Galactic and extra-Galactic X-ray binaries, supernovae, gamma-ray bursts, stellar flares, tidal disruption flares, and as-yet unknown X-ray variables. This workshop explored the potential of strategic X-ray surveys to probe a broad range of astrophysical sources and phenomena. Here we present the highlights, with an emphasis on the science topics and mission designs that will drive future discovery in the X-ray time domain.△ Less"
Robust filtering: Correlated noise and multidimensional observation,"Authors:D. Crisan,J. Diehl,P. K. Friz,H. Oberhauser","Abstract:…Davis [In Stochastic Systems: The Mathematics of Filtering and Identification and Applications, Proc. NATO Adv. Study Inst. Les Arcs, Savoie, France 1980 505-528], [In TheOxfordHandbook of Nonlinear Filtering (2011) 403-424OxfordUniv. Press], this type of robust representation is also possible when the signal and…▽ MoreIn the late seventies, Clark [In Communication Systems and Random Process Theory (Proc. 2nd NATO Advanced Study Inst., Darlington, 1977) (1978) 721-734, Sijthoff & Noordhoff] pointed out that it would be natural for $π_t$, the solution of the stochastic filtering problem, to depend continuously on the observed data $Y=\{Y_s,s\in[0,t]\}$. Indeed, if the signal and the observation noise are independent one can show that, for any suitably chosen test function $f$, there exists a continuous map $θ^f_t$, defined on the space of continuous paths $C([0,t],\mathbb{R}^d)$ endowed with the uniform convergence topology such that $π_t(f)=θ^f_t(Y)$, almost surely; see, for example, Clark [In Communication Systems and Random Process Theory (Proc. 2nd NATO Advanced Study Inst., Darlington, 1977) (1978) 721-734, Sijthoff & Noordhoff], Clark and Crisan [Probab. Theory Related Fields 133 (2005) 43-56], Davis [Z. Wahrsch. Verw. Gebiete 54 (1980) 125-139], Davis [Teor. Veroyatn. Primen. 27 (1982) 160-167], Kushner [Stochastics 3 (1979) 75-83]. As shown by Davis and Spathopoulos [SIAM J. Control Optim. 25 (1987) 260-278], Davis [In Stochastic Systems: The Mathematics of Filtering and Identification and Applications, Proc. NATO Adv. Study Inst. Les Arcs, Savoie, France 1980 505-528], [In TheOxfordHandbook of Nonlinear Filtering (2011) 403-424OxfordUniv. Press], this type of robust representation is also possible when the signal and the observation noise are correlated, provided the observation process is scalar. For a general correlated noise and multidimensional observations such a representation does not exist. By using the theory of rough paths we provide a solution to this deficiency: the observation process $Y$ is ""lifted"" to the process $\mathbf{Y}$ that consists of $Y$ and its corresponding Lévy area process, and we show that there exists a continuous map $θ_t^f$, defined on a suitably chosen space of Hölder continuous paths such that $π_t(f)=θ_t^f(\mathbf{Y})$, almost surely.△ Less"
LOFT: Large Observatory For X-ray Timing,"Authors:R. P. Mignani,S. Zane,D. Walton,T. Kennedy,B. Winter,P. Smith,R. Cole,D. Kataria,A. Smith","Abstract:High-time-resolution X-ray observations of compact objects provide direct access to strong field gravity, black hole masses and spins, and the equation of state of ultra-dense matter. LOFT, the large observatory for X-ray timing, is specifically designed to study the very rapid X-ray flux and spectral variability that directly probe the motion of matter down to distances very close to black holes…▽ MoreHigh-time-resolution X-ray observations of compact objects provide direct access to strong field gravity, black hole masses and spins, and the equation of state of ultra-dense matter. LOFT, the large observatory for X-ray timing, is specifically designed to study the very rapid X-ray flux and spectral variability that directly probe the motion of matter down to distances very close to black holes and neutron stars. A 10 m^2-class instrument in combination with good spectral resolution (<260 eV @ 6 keV) is required to exploit the relevant diagnostics and holds the potential to revolutionise the study of collapsed objects in our Galaxy and of the brightest supermassive black holes in active galactic nuclei. LOFT will carry two main instruments: a Large Area Detector (LAD), to be built at MSSL/UCL with the collaboration of the Leicester Space Research Centre for the collimator) and a Wide Field Monitor (WFM). The ground-breaking characteristic of the LAD (that will work in the energy range 2-30 keV) is a mass per unit surface in the range of ~10 kg/m^2, enabling an effective area of ~10 m^2 (@10 keV) at a reasonable weight and improving by a factor of ~20 over all predecessors. This will allow timing measurements of unprecedented sensitivity, allowing the capability to measure the mass and radius of neutron stars with ~5% accuracy, or to reveal blobs orbiting close to the marginally stable orbit in active galactic nuclei. In this contribution we summarise the characteristics of the LOFT instruments and give an overview of the expectations for its capabilities.△ Less"
"The C++0x ""Concepts"" Effort",Authors:Jeremy G. Siek,"Abstract:…in functional programming and programming language theory. This article grew out of a lecture at the Spring School on Generic and Indexed Programming at the University ofOxford, March 2010.▽ MoreC++0x is the working title for the revision of the ISO standard of the C++ programming language that was originally planned for release in 2009 but that was delayed to 2011. The largest language extension in C++0x was ""concepts"", that is, a collection of features for constraining template parameters. In September of 2008, the C++ standards committee voted the concepts extension into C++0x, but then in July of 2009, the committee voted the concepts extension back out of C++0x.
  This article is my account of the technical challenges and debates within the ""concepts"" effort in the years 2003 to 2009. To provide some background, the article also describes the design space for constrained parametric polymorphism, or what is colloquially know as constrained generics. While this article is meant to be generally accessible, the writing is aimed toward readers with background in functional programming and programming language theory. This article grew out of a lecture at the Spring School on Generic and Indexed Programming at the University ofOxford, March 2010.△ Less"
Proof nets for the Lambek-Grishin calculus,"Authors:Michael Moortgat,Richard Moot","Abstract:Grishin's generalization of Lambek's Syntactic Calculus combines a non-commutative multiplicative conjunction and its residuals (product, left and right division) with a dual family: multiplicative disjunction, right and left difference. Interaction between these two families takes the form of linear distributivity principles. We study proof nets for the Lambek-Grishin calculus and the corresponde…▽ MoreGrishin's generalization of Lambek's Syntactic Calculus combines a non-commutative multiplicative conjunction and its residuals (product, left and right division) with a dual family: multiplicative disjunction, right and left difference. Interaction between these two families takes the form of linear distributivity principles. We study proof nets for the Lambek-Grishin calculus and the correspondence between these nets and unfocused and focused versions of its sequent calculus.△ Less"
Dynamical mean-field theories of correlation and disorder,"Authors:E. Miranda,V. Dobrosavljevic","Abstract:We provide a review of recently-develop dynamical mean-field theory (DMFT) approaches to the general problem of strongly correlated electronic systems with disorder. We first describe the standard DMFT approach, which is exact in the limit of large coordination, and explain why in its simplest form it cannot capture either Anderson localization or the glassy behavior of electrons. Various extensio…▽ MoreWe provide a review of recently-develop dynamical mean-field theory (DMFT) approaches to the general problem of strongly correlated electronic systems with disorder. We first describe the standard DMFT approach, which is exact in the limit of large coordination, and explain why in its simplest form it cannot capture either Anderson localization or the glassy behavior of electrons. Various extensions of DMFT are then described, including statistical DMFT, typical medium theory, and extended DMFT, methods specifically designed to overcome the limitations of the original formulation. We provide an overview of the results obtained using these approaches, including the formation of electronic Griffiths phases, the self-organized criticality of the Coulomb glass, and the two-fluid behavior near Mott-Anderson transitions. Finally, we outline research directions that may provide a route to bridge the gap between the DMFT-based theories and the complementary diffusion-mode approaches to the metal-insulator transition.△ Less"
Introduction to Metal-Insulator Transitions,Authors:V. Dobrosavljevic,"Abstract:In this overview we provide a general introduction to metal-insulator transitions, with focus on specific mechanisms that can localize the electrons in absence of magnetic or charge ordering, and produce well defined quantum critical behavior. We contrast the physical picture of Mott, who emphasized the role of electron-electron interactions, and that of Anderson, who stressed the possibility of i…▽ MoreIn this overview we provide a general introduction to metal-insulator transitions, with focus on specific mechanisms that can localize the electrons in absence of magnetic or charge ordering, and produce well defined quantum critical behavior. We contrast the physical picture of Mott, who emphasized the role of electron-electron interactions, and that of Anderson, who stressed the possibility of impurity-induced bound state formation, as alternative routes to arrest the electronic motion. We also describe more complicated situations when both phenomena play coexist, leading to meta-stability, slow relaxation, and glassy behavior of electrons. A critical overview of the available theoretical approaches is then presented, contrasting the weak-coupling perspective, which emphasizes diffusion-mode corrections, and the strong-coupling viewpoint, which stresses inhomogeneous phases and local correlation effects. We give specific examples of experimental systems, providing clues on what should be the most profitable path forward in unraveling the mystery of metal-insulator transitions.△ Less"
Periodically modulated quantum nonlinear oscillators,Authors:M. I. Dykman,"Abstract:This book chapter describes the dynamics of a modulated oscillator for resonant and nonresonant modulation. Two types of resonant modulation are considered: additive, with frequency close to the oscillator eigenfrequency, and parametric, with frequency close to twice the eigenfrequency. It is shown that relaxation of the oscillator is accompanied by quantum noise, which leads to a finite-width dis…▽ MoreThis book chapter describes the dynamics of a modulated oscillator for resonant and nonresonant modulation. Two types of resonant modulation are considered: additive, with frequency close to the oscillator eigenfrequency, and parametric, with frequency close to twice the eigenfrequency. It is shown that relaxation of the oscillator is accompanied by quantum noise, which leads to a finite-width distribution over quantum states even for T=0. The quantum noise also leads to switching between coexisting vibrational states via transitions over the barrier in phase space. The switching mechanism, quantum activation, has no analog in thermal equilibrium systems. The switching rates display characteristic scaling near bifurcation points. The power and absorption/amplification spectra of modulated oscillators are studied, including their fine structure. Nonresonant modulation can lead to cooling, heating, or self-sustained vibrations of an oscillator. The relation between the previously discussed direct nonresonant excitation of the oscillator and the excitation mechanism studied in optomechanics is analyzed.△ Less"
Around Gaia Alerts in 20 questions,"Authors:Lukasz Wyrzykowski,Simon Hodgkin","Abstract:Gaia is a European Space Agency (ESA) astrometry space mission, and a successor to the ESA Hipparcos mission. Gaia's main goal is to collect high-precision astrometric data (i.e. positions, parallaxes, and proper motions) for the brightest 1 billion objects in the sky. These data, complemented with multi-band, multi-epoch photometric and spectroscopic data collected from the same observing platfor…▽ MoreGaia is a European Space Agency (ESA) astrometry space mission, and a successor to the ESA Hipparcos mission. Gaia's main goal is to collect high-precision astrometric data (i.e. positions, parallaxes, and proper motions) for the brightest 1 billion objects in the sky. These data, complemented with multi-band, multi-epoch photometric and spectroscopic data collected from the same observing platform, will allow astronomers to reconstruct the formation history, structure, and evolution of the Galaxy.
  Gaia will observe the whole sky for 5 years, providing a unique opportunity for the discovery of large numbers of transient and anomalous events, e.g. supernovae, novae and microlensing events, GRB afterglows, fallback supernovae, and other theoretical or unexpected phenomena. The Photometric Science Alerts team has been tasked with the early detection, classification and prompt release of anomalous sources in the Gaia data stream. In this paper, we discuss the challenges we face in preparing to use Gaia to search for transient phenomena at optical wavelengths.△ Less"
Applications of Random Graphs to 2D Quantum Gravity,Authors:Max R. Atkin,"Abstract:The central topic of this thesis is two dimensional Quantum Gravity and its properties. The term Quantum Gravity itself is ambiguous as there are many proposals for its correct formulation and none of them have been verified experimentally. In this thesis we consider a number of closely related approaches to two dimensional quantum gravity that share the property that they may be formulated in ter…▽ MoreThe central topic of this thesis is two dimensional Quantum Gravity and its properties. The term Quantum Gravity itself is ambiguous as there are many proposals for its correct formulation and none of them have been verified experimentally. In this thesis we consider a number of closely related approaches to two dimensional quantum gravity that share the property that they may be formulated in terms of random graphs. In one such approach known as Causal Dynamical Triangulations, numerical computations suggest an interesting phenomenon in which the effective spacetime dimension is reduced in the UV. In this thesis we first address whether such a dynamical reduction in the number of dimensions may be understood in a simplified model. We introduce a continuum limit where this simplified model exhibits a reduction in the effective dimension of spacetime in the UV, in addition to having rich cross-over behaviour.
  In the second part of this thesis we consider an approach closely related to causal dynamical triangulation; namely dynamical triangulation. Although this theory is less well-behaved than causal dynamical triangulation, it is known how to couple it to matter, therefore allowing for potentially multiple boundary states to appear in the theory. We address the conjecture of Seiberg and Shih which states that all these boundary states are degenerate and may be constructed from a single principal boundary state. By use of the random graph formulation of the theory we compute the higher genus amplitudes with a single boundary and find that they violate the Seiberg-Shih conjecture. Finally we discuss whether this result prevents the replacement of boundary states by local operators as proposed by Seiberg.△ Less"
Time-Resolved Spectroscopy with SDSS,"Authors:Steven Bickerton,Carles Badenes,Thomas Hettinger,Timothy Beers,Sonya Huang","Abstract:We present a brief technical outline of the newly-formed 'Detection of Spectroscopic Differences over Time' (DS/DT) project. Our collaboration is using the individual exposures from the SDSS spectroscopic archive to produce a uniformly-processed set of time-resolved spectra. Here we provide an overview of the properties and processing of the available data, and highlight the wide range of time bas…▽ MoreWe present a brief technical outline of the newly-formed 'Detection of Spectroscopic Differences over Time' (DS/DT) project. Our collaboration is using the individual exposures from the SDSS spectroscopic archive to produce a uniformly-processed set of time-resolved spectra. Here we provide an overview of the properties and processing of the available data, and highlight the wide range of time baselines present in the archive.△ Less"
Optical pulsations from isolated neutron stars,Authors:R. P. Mignani,"Abstract:Being fast rotating objects, isolated neutron stars (INSs) are obvious targets for high-time resolution observations. With the number of optical/UV/IR INS detections now increased to 24, timing observations become more and more important in INS astrophysics.Being fast rotating objects, isolated neutron stars (INSs) are obvious targets for high-time resolution observations. With the number of optical/UV/IR INS detections now increased to 24, timing observations become more and more important in INS astrophysics.△ Less"
Real Time Classification of Transient Events in Synoptic Sky Surveys,"Authors:Ashish A. Mahabal,C. Donalek,S. G. Djorgovski,A. J. Drake,M. J. Graham,R. Williams,Y. Chen,B. Moghaddam,M. Turmon","Abstract:An automated, rapid classification of transient events detected in the modern synoptic sky surveys is essential for their scientific utility and effective follow-up using scarce resources. This problem will grow by orders of magnitude with the next generation of surveys. We are exploring a variety of novel automated classification techniques, mostly Bayesian, to respond to these challenges, using…▽ MoreAn automated, rapid classification of transient events detected in the modern synoptic sky surveys is essential for their scientific utility and effective follow-up using scarce resources. This problem will grow by orders of magnitude with the next generation of surveys. We are exploring a variety of novel automated classification techniques, mostly Bayesian, to respond to these challenges, using the ongoing CRTS sky survey as a testbed. We describe briefly some of the methods used.△ Less"
Collective Dynamics in Arrays of Coupled Nonlinear Resonators,"Authors:Ron Lifshitz,Eyal Kenig,M. C. Cross","Abstract:The study of collective nonlinear dynamics of coupled mechanical resonators is regaining attention in recent years thanks to rapid developments in the fields of microelectromechanical and nanoelectromechanical systems (MEMS and NEMS). We review a wide range of collective dynamical phenomena, while highlighting the common concepts and theoretical tools that we have developed for treating them. We p…▽ MoreThe study of collective nonlinear dynamics of coupled mechanical resonators is regaining attention in recent years thanks to rapid developments in the fields of microelectromechanical and nanoelectromechanical systems (MEMS and NEMS). We review a wide range of collective dynamical phenomena, while highlighting the common concepts and theoretical tools that we have developed for treating them. We provide detailed derivations of amplitude equations, which allow us to obtain reduced descriptions for the relevant dynamics of our complex systems. We apply these amplitude equations to study (a) resonant response to parametric excitation; (b) pattern selection, or the nonlinear competition between extended modes in situations of multistability; (c) formation and dynamics of intrinsically localized modes (ILM); and (d) spontaneous synchronization of oscillators with differing frequencies. All the predictions obtained from analyzing the different amplitude equations are in excellent agreement with numerical solutions of the underlying equations of motion, suggesting that the predicted effects can be observed in arrays of real micromechanical or nanomechanical resonators, thus motivating new experiments in these systems.△ Less"
The VAO Transient Facility,"Authors:Matthew J. Graham,S. G. Djorgovski,Andrew Drake,Ashish Mahabal,Roy Williams,Rob Seaman","Abstract:The time domain community wants robust and reliable tools to enable production of and subscription to community-endorsed event notification packets (VOEvent). The VAO Transient Facility (VTF) is being designed to be the premier brokering service for the community, both collecting and disseminating observations about time-critical astronomical transients but also supporting annotations and the appl…▽ MoreThe time domain community wants robust and reliable tools to enable production of and subscription to community-endorsed event notification packets (VOEvent). The VAO Transient Facility (VTF) is being designed to be the premier brokering service for the community, both collecting and disseminating observations about time-critical astronomical transients but also supporting annotations and the application of intelligent machine-learning to those observations. This distinguishes two types of activity associated with the facility: core infrastructure and user services. In this paper, we will review the prior art in both areas and describe the planned capabilities of the VTF. In particular, we will focus on scalability and quality-of-service issues required by the next generation of sky surveys, such as LSST and SKA.△ Less"
Gravitational Waves and Time Domain Astronomy,"Authors:Joan Centrella,Samaya Nissanke,Roy Williams","Abstract:The gravitational wave window onto the universe will open in roughly five years, when Advanced LIGO and Virgo achieve the first detections of high frequency gravitational waves, most likely coming from compact binary mergers. Electromagnetic follow-up of these triggers, using radio, optical, and high energy telescopes, promises exciting opportunities in multi-messenger time domain astronomy. In th…▽ MoreThe gravitational wave window onto the universe will open in roughly five years, when Advanced LIGO and Virgo achieve the first detections of high frequency gravitational waves, most likely coming from compact binary mergers. Electromagnetic follow-up of these triggers, using radio, optical, and high energy telescopes, promises exciting opportunities in multi-messenger time domain astronomy. In the next decade, space-based observations of low frequency gravitational waves from massive black hole mergers, and their electromagnetic counterparts, will open up further vistas for discovery. This two-part workshop at featured brief presentations and stimulating discussions on the challenges and opportunities presented by gravitational wave astronomy. Highlights from the workshop, with the emphasis on strategies for electromagnetic follow-up, are presented in this report.△ Less"
Algebras over a field and semantics for context based reasoning,Authors:Daoud Clarke,"Abstract:This paper introduces context algebras and demonstrates their application to combining logical and vector-based representations of meaning. Other approaches to this problem attempt to reproduce aspects of logical semantics within new frameworks. The approach we present here is different: We show how logical semantics can be embedded within a vector space framework, and use this to combine distribu…▽ MoreThis paper introduces context algebras and demonstrates their application to combining logical and vector-based representations of meaning. Other approaches to this problem attempt to reproduce aspects of logical semantics within new frameworks. The approach we present here is different: We show how logical semantics can be embedded within a vector space framework, and use this to combine distributional semantics, in which the meanings of words are represented as vectors, with logical semantics, in which the meaning of a sentence is represented as a logical form.△ Less"
Circuit quantum electrodynamics with a nonlinear resonator,"Authors:P. Bertet,F. R. Ong,M. Boissonneault,A. Bolduc,F. Mallet,A. C. Doherty,A. Blais,D. Vion,D. Esteve","Abstract:One of the most studied model systems in quantum optics is a two-level atom strongly coupled to a single mode of the electromagnetic field stored in a cavity, a research field named cavity quantum electrodynamics or CQED. CQED has recently received renewed attention due to its implementation with superconducting artificial atoms and coplanar resonators in the so-called circuit quantum electrodynam…▽ MoreOne of the most studied model systems in quantum optics is a two-level atom strongly coupled to a single mode of the electromagnetic field stored in a cavity, a research field named cavity quantum electrodynamics or CQED. CQED has recently received renewed attention due to its implementation with superconducting artificial atoms and coplanar resonators in the so-called circuit quantum electrodynamics (cQED) architecture. In cQED, the couplings can be much stronger than in CQED due to the design flexibility of superconducting circuits and to the enhanced field confinement in one-dimensional cavities. This enabled the realization of fundamental quantum physics and quantum information processing experiments with a degree of control comparable to that obtained in CQED.
  The purpose of this chapter is to investigate the situation where the resonator to which the atom is coupled is made nonlinear with a Kerr-type nonlinearity, causing its energy levels to be nonequidistant. The system is then described by a nonlinear Jaynes-Cummings Hamiltonian. This considerably enriches the physics since a pumped nonlinear resonator displays bistability, parametric amplification, and squeezing. The interplay of strong coupling and these nonlinear effects constitutes a novel model system for quantum optics that can be implemented experimentally with superconducting circuits.
  This chapter is organized as follows. In a first section we present the system consisting of a superconducting Kerr nonlinear resonator strongly coupled to a transmon qubit. In the second section, we describe the response of the sole nonlinear resonator to an external drive. In the third section, we show how the resonator bistability can be used to perform a high-fidelity readout of the transmon qubit. In the last section, we investigate the quantum backaction exerted by the intracavity field on the qubit.△ Less"
On Programs and Genomes,Authors:Eric Werner,"Abstract:We outline the global control architecture of genomes. A theory of genomic control information is presented. The concept of a developmental control network called a cene (for control gene) is introduced. We distinguish parts-genes from control genes or cenes. Cenes are interpreted and executed by the cell and, thereby, direct cell actions including communication, growth, division, differentiation…▽ MoreWe outline the global control architecture of genomes. A theory of genomic control information is presented. The concept of a developmental control network called a cene (for control gene) is introduced. We distinguish parts-genes from control genes or cenes. Cenes are interpreted and executed by the cell and, thereby, direct cell actions including communication, growth, division, differentiation and multi-cellular development. The cenome is the global developmental control network in the genome. The cenome is also a cene that consists of interlinked sub-cenes that guide the ontogeny of the organism. The complexity of organisms is linked to the complexity of the cenome. The relevance to ontogeny and evolution is mentioned. We introduce the concept of a universal cell and a universal genome.△ Less"
Renormalization Group and Problem of Radiation,Authors:I. M. Sigal,"Abstract:…given at the end of these lectures. This review will appear in ""Quantum Theory from Small to Large Scales"", Lecture Notes of the Les Houches Summer Schools, volume 95,OxfordUniversity Press, 2011. Key words: quantum electrodynamics, photons and electrons, renormalization group, quantum resonances, spectral theory, Schroedinger operators, ground sta…▽ MoreThe standard model of non-relativistic quantum electrodynamics describes non-relativistic quantum matter, such as atoms and molecules, coupled to the quantized electromagnetic field. Within this model, we review basic notions, results and techniques in the theory of radiation. We describe the key technique in this area - the spectral renormalization group. Our review is based on joint works with Volker Bach and Juerg Froehlich and with Walid Abou Salem, Thomas Chen, Jeremy Faupin and Marcel Griesemer. Brief discussion of related contributions is given at the end of these lectures. This review will appear in ""Quantum Theory from Small to Large Scales"", Lecture Notes of the Les Houches Summer Schools, volume 95,OxfordUniversity Press, 2011. Key words: quantum electrodynamics, photons and electrons, renormalization group, quantum resonances, spectral theory, Schroedinger operators, ground state, quantum dynamics, non-relativistic theory.△ Less"
Data formats for phonological corpora,"Authors:Laurent Romary,Andreas Witt","Abstract:The goal of the present chapter is to explore the possibility of providing the research (but also the industrial) community that commonly uses spoken corpora with a stable portfolio of well-documented standardised formats that allow a high re-use rate of annotated spoken resources and, as a consequence, better interoperability across tools used to produce or exploit such resources.The goal of the present chapter is to explore the possibility of providing the research (but also the industrial) community that commonly uses spoken corpora with a stable portfolio of well-documented standardised formats that allow a high re-use rate of annotated spoken resources and, as a consequence, better interoperability across tools used to produce or exploit such resources.△ Less"
Calabi-Yau Three-folds: Poincare Polynomials and Fractals,"Authors:Anthony Ashmore,Yang-Hui He","Abstract:We study the Poincare polynomials of all known Calabi-Yau three-folds as constrained polynomials of Littlewood type, thus generalising the well-known investigation into the distribution of the Euler characteristic and Hodge numbers. We find interesting fractal behaviour in the roots of these polynomials in relation to the existence of isometries, distribution versus typicality, and mirror symmetry…▽ MoreWe study the Poincare polynomials of all known Calabi-Yau three-folds as constrained polynomials of Littlewood type, thus generalising the well-known investigation into the distribution of the Euler characteristic and Hodge numbers. We find interesting fractal behaviour in the roots of these polynomials in relation to the existence of isometries, distribution versus typicality, and mirror symmetry.△ Less"
Some unsolved problems on cycles,"Authors:Chunhui Lai,Mingjing Liu","Abstract:…(see P. Erdos, Some unsolved problems in graph theory and combinatorial analysis, Combinatorial Mathematics and its Applications (Proc. Conf.,Oxford, 1969), pp. 97--109, Academic Press, London, 1971). This paper summarizes some results on these problems and the conjectures that relate to these. We do not think Hajós conjecture is true.▽ MoreHajos' conjecture that every simple even graph on $n$ vertices can be decomposed into at most $(n-1)/2$ cycles (see L. Lovasz, On covering of graphs, in: P. Erdos, G.O.H. Katona (Eds.), Theory of Graphs, Academic Press, New York, 1968, pp. 231 - 236). Let $f(n)$ be the maximum number of edges in a graph on $n$ vertices in which no two cycles have the same length. P. Erdos raised the problem of determining $f(n)$ (see J.A. Bondy and U.S.R. Murty, Graph Theory with Applications (Macmillan, New York, 1976), p.247, Problem 11).
  Given a graph $H$, what is the maximum number of edges of a graph with $n$ vertices not containing $H$ as a subgraph? This number is denoted $ex(n,H)$, and is known as the Turan number. P. Erdos conjectured that there exists a positive constant $c$ such that $ex(n,C_{2k})\geq cn^{1+1/k}$(see P. Erdos, Some unsolved problems in graph theory and combinatorial analysis, Combinatorial Mathematics and its Applications (Proc. Conf.,Oxford, 1969), pp. 97--109, Academic Press, London, 1971). This paper summarizes some results on these problems and the conjectures that relate to these. We do not think Hajós conjecture is true.△ Less"
The AdS/CFT Correspondence and Symmetry Breaking,Authors:Nessi Benishti,"Abstract:In the first part of this thesis we study baryonic U(1) symmetries dual to Betti multiplets in the AdS_4/CFT_3 correspondence for M2 branes at Calabi-Yau 4-fold singularities. We begin by focusing on isolated toric singularities without vanishing 6-cycles, which we classify, and propose for them field theory duals. We then study in detail the cone over Q^111 and find agreement between the spectrum…▽ MoreIn the first part of this thesis we study baryonic U(1) symmetries dual to Betti multiplets in the AdS_4/CFT_3 correspondence for M2 branes at Calabi-Yau 4-fold singularities. We begin by focusing on isolated toric singularities without vanishing 6-cycles, which we classify, and propose for them field theory duals. We then study in detail the cone over Q^111 and find agreement between the spectrum of baryonic operators in this theory and M5 branes wrapping 5-cycles in the Q^111 space. The physics of vacua in which these symmetries are spontaneously broken precisely matches a dual gravity analysis involving resolutions of the singularity, where we are able to match condensates, Goldstone bosons and global strings. We then study the implications of turning on a torsion 4-form flux. This flux non-trivially affects the supergravity dual of Higgsing, and we show that the supergravity and field theory analyses precisely match in an example based on Y^12(CP^2). We then explain how the choice of M-theory circle can result in exotic renormalization group flows. We also argue that theories where the resolutions have 6-cycles are expected to receive non-perturbative corrections from M5 instantons. We give a general formula relating the instanton action to normalizable harmonic 2-forms.
  In the second part of this thesis we study the breaking of baryonic symmetries in the AdS_5/CFT_4 correspondence. This leads, for particular vacuum expectation values, to the emergence of baryonic symmetries during the renormalization group flow. We identify these vacuum expectation values with critical values of the B-field moduli in the dual supergravity backgrounds. We study in detail the C^3/Z_3 orbifold theory and the dual supergravity backgrounds that correspond to the breaking of the emerging baryonic symmetries, and identify the expected Goldstone bosons and global strings in the IR.△ Less"
Magnetic field-induced novel insulating phase in 2D superconductors,"Authors:G. Sambandamurthy,N. Peter Armitage","Abstract:DC and finite frequency transport measurements of thin films of amorphous indium oxide that were driven through the critical point of superconductor-insulator transition by the application of perpendicular magnetic field are presented. The observation of non-monotonic dependence of resistance on magnetic field in the insulating phase, novel transport characteristics near the resistance peak and fi…▽ MoreDC and finite frequency transport measurements of thin films of amorphous indium oxide that were driven through the critical point of superconductor-insulator transition by the application of perpendicular magnetic field are presented. The observation of non-monotonic dependence of resistance on magnetic field in the insulating phase, novel transport characteristics near the resistance peak and finite superfluid stiffness in the insulating phase are all discussed from the point of view that suggests a possible relation between the conduction mechanisms in the superconducting and insulating phases. The results are summarized in the form of an experimental phase diagram for disordered superconductors in the disorder-magnetic field plane.△ Less"
"One, None and One Hundred Thousand Profiles: Re-imagining the Pirandellian Identity Dilemma in the Era of Online Social Networks","Authors:Alberto Pepe,Spencer Wolff,Karen Van Godtsenhoven","Abstract:Uno, Nessuno, Centomila (""One, No One and One Hundred Thousand"") is a classic novel by Italian playwright Luigi Pirandello. Published in 1925, it recounts the tragedy of Vitangelo Moscarda, a man who struggles to reclaim a coherent and unitary identity for himself in the face of an inherently social and multi-faceted world. What would Moscarda identity tragedy look like today? In this article we t…▽ MoreUno, Nessuno, Centomila (""One, No One and One Hundred Thousand"") is a classic novel by Italian playwright Luigi Pirandello. Published in 1925, it recounts the tragedy of Vitangelo Moscarda, a man who struggles to reclaim a coherent and unitary identity for himself in the face of an inherently social and multi-faceted world. What would Moscarda identity tragedy look like today? In this article we transplant Moscarda's identity play from its offline setting to the contemporary arena of social media and online social networks. With reference to established theories on identity construction, performance, and self-presentation, we re-imagine how Moscarda would go about defending the integrity of his selfhood in the face of the discountenancing influences of the online world.△ Less"
Using Inverse lambda and Generalization to Translate English to Formal Languages,"Authors:Chitta Baral,Juraj Dzifcak,Marcos Alvarez Gonzalez,Jiayu Zhou","Abstract:We present a system to translate natural language sentences to formulas in a formal or a knowledge representation language. Our system uses two inverse lambda-calculus operators and using them can take as input the semantic representation of some words, phrases and sentences and from that derive the semantic representation of other words and phrases. Our inverse lambda operator works on many forma…▽ MoreWe present a system to translate natural language sentences to formulas in a formal or a knowledge representation language. Our system uses two inverse lambda-calculus operators and using them can take as input the semantic representation of some words, phrases and sentences and from that derive the semantic representation of other words and phrases. Our inverse lambda operator works on many formal languages including first order logic, database query languages and answer set programming. Our system uses a syntactic combinatorial categorial parser to parse natural language sentences and also to construct the semantic meaning of the sentences as directed by their parsing. The same parser is used for both. In addition to the inverse lambda-calculus operators, our system uses a notion of generalization to learn semantic representation of words from the semantic representation of other words that are of the same category. Together with this, we use an existing statistical learning approach to assign weights to deal with multiple meanings of words. Our system produces improved results on standard corpora on natural language interfaces for robot command and control and database queries.△ Less"
Spin Pumping and Spin Transfer,"Authors:Arne Brataas,Yaroslav Tserkovnyak,Gerrit E. W. Bauer,Paul J. Kelly",Abstract:Spin pumping is the emission of a spin current by a magnetization dynamics while spin transfer stands for the excitation of magnetization by spin currents. Using Onsager's reciprocity relations we prove that spin pumping and spin-transfer torques are two fundamentally equivalent dynamic processes in magnetic structures with itinerant electrons. We review the theory of the coupled motion of the mag…▽ MoreSpin pumping is the emission of a spin current by a magnetization dynamics while spin transfer stands for the excitation of magnetization by spin currents. Using Onsager's reciprocity relations we prove that spin pumping and spin-transfer torques are two fundamentally equivalent dynamic processes in magnetic structures with itinerant electrons. We review the theory of the coupled motion of the magnetization order parameter and electron for textured bulk ferromagnets (e.g. containing domain walls) and heterostructures (such as spin valves). We present first-principles calculations for the material-dependent damping parameters of magnetic alloys. Theoretical and experimental results agree in general well.△ Less
Rotating Stars and Revolving Planets: Bayesian Exploration of the Pulsating Sky,Authors:Thomas J. Loredo,"Abstract:I describe ongoing work on development of Bayesian methods for exploring periodically varying phenomena in astronomy, addressing two classes of sources: pulsars, and extrasolar planets (exoplanets). For pulsars, the methods aim to detect and measure periodically varying signals in data consisting of photon arrival times, modeled as non-homogeneous Poisson point processes. For exoplanets, the metho…▽ MoreI describe ongoing work on development of Bayesian methods for exploring periodically varying phenomena in astronomy, addressing two classes of sources: pulsars, and extrasolar planets (exoplanets). For pulsars, the methods aim to detect and measure periodically varying signals in data consisting of photon arrival times, modeled as non-homogeneous Poisson point processes. For exoplanets, the methods address detection and estimation of planetary orbits using observations of the reflex motion ""wobble"" of a host star, including adaptive scheduling of observations to optimize inferences.△ Less"
The Tomaraho Conception of the Sky,"Authors:Guillermo Sequera,Alejandro Gangui","Abstract:The small community of the Tomaraho, an ethnic group culturally derived from the Zamucos, became known in the South American and world anthropological scenario in recent times. This group, far from the banks of the Paraguay river, remained concealed from organized modern societies for many years. Like any other groups of people in close contact with nature, the Tomaraho developed a profound and ri…▽ MoreThe small community of the Tomaraho, an ethnic group culturally derived from the Zamucos, became known in the South American and world anthropological scenario in recent times. This group, far from the banks of the Paraguay river, remained concealed from organized modern societies for many years. Like any other groups of people in close contact with nature, the Tomaraho developed a profound and rich world view which parallels other more widely researched aboriginal cultures as well as showing distinctive features of their own. This is also apparent in their imagery of the sky and of the characters that are closely connected with the celestial sphere. This paper is based on the lengthy anthropological studies of G. Sequera. We have recently undertaken a project to carry out a detailed analysis of the different astronomical elements present in the imagined sky of the Tomaraho and other Chamacoco ethnic groups. We will briefly review some aspects of this aboriginal culture: places where they live, regions of influence in the past, their linguistic family, their living habits and how the advancement of civilization affected their culture and survival. We will later mention the fieldwork carried out for decades and some of the existing studies and publications. We will also make a brief description of the methodology of this work and special anthropological practices. Last but not least, we will focus on the Tomaraho conception of the sky as well as describe the research work we have been doing in recent times.△ Less"
Naturalness of electroweak physics within minimal supergravity,Authors:S. Cassel,"Abstract:Low energy supersymmetry is motivated by its use as a solution to the hierarchy problem of the electroweak scale. Having motivated this model with naturalness arguments, it is then necessary to check whether the experimentally allowed parameter space permits realisations of the model with low fine tuning. The scope of this thesis is a study of naturalness of the electroweak physics in the minimal…▽ MoreLow energy supersymmetry is motivated by its use as a solution to the hierarchy problem of the electroweak scale. Having motivated this model with naturalness arguments, it is then necessary to check whether the experimentally allowed parameter space permits realisations of the model with low fine tuning. The scope of this thesis is a study of naturalness of the electroweak physics in the minimal supergravity model. The latest experimental constraints are applied, and the fine tuning is quantitatively evaluated for a scan across the parameter space. The fine tuning of the electroweak scale is evaluated at 2-loop order, and the fine tuning of the neutralino dark matter thermal relic energy density is also determined. The natural regions of the parameter space are identified and the associated phenomenology relevant for detection discussed. Naturalness limits are also found for the parameter space and spectrum. The minimum fine tuning found is 1 part in 9 when dark matter constraints are neglected, and 1 part in 15 when dark matter constraints are satisfied. For both cases, the minimum fine tuning is found for a Higgs mass of 115 GeV irrespective of whether the Higgs mass constraint is applied or not. The most natural spectrum includes light superpartner fermions, and heavy superpartner scalars. Minimal supergravity currently remains viable with respect to naturalness and a natural realisation may be discovered within the next couple of years.△ Less"
Spin Caloritronics,Authors:Gerrit E. W. Bauer,"Abstract:This is a brief overview of the state of the art of spin caloritronics, the science and technology of controlling heat currents by the electron spin degree of freedom (and vice versa).This is a brief overview of the state of the art of spin caloritronics, the science and technology of controlling heat currents by the electron spin degree of freedom (and vice versa).△ Less"
OxfordSWIFT IFS and multi-wavelength observations of the Eagle galaxy at z=0.77,"Authors:Susan A. Kassin,L. Fogarty,T. Goodsall,F. J. Clarke,R. W. C. Houghton,G. Salter,N. Thatte,M. Tecza,Roger L. Davies,Benjamin J. Weiner,C. N. A. Willmer,Samir Salim,Michael C. Cooper,Jeffrey A. Newman,Kevin Bundy,C. J. Conselice,A. M. Koekemoer,Lihwai Lin,Leonidas A. Moustakas,Tao Wang","Abstract:The `Eagle' galaxy at a redshift of 0.77 is studied with theOxfordShort Wavelength Integral Field Spectrograph (SWIFT) and multi-wavelength data from the All-wavelength Extended Groth strip International Survey (AEGIS). It was chosen from AEGIS because of the bright and extended emission in its slit spectrum. Three dimensional kinematic maps of the Eag…▽ MoreThe `Eagle' galaxy at a redshift of 0.77 is studied with theOxfordShort Wavelength Integral Field Spectrograph (SWIFT) and multi-wavelength data from the All-wavelength Extended Groth strip International Survey (AEGIS). It was chosen from AEGIS because of the bright and extended emission in its slit spectrum. Three dimensional kinematic maps of the Eagle reveal a gradient in velocity dispersion which spans 35-75 +/- 10 km/s and a rotation velocity of 25 +/- 5 km/s uncorrected for inclination. Hubble Space Telescope images suggest it is close to face-on. In comparison with galaxies from AEGIS at similar redshifts, the Eagle is extremely bright and blue in the rest-frame optical, highly star-forming, dominated by unobscured star-formation, and has a low metallicity for its size. This is consistent with its selection. The Eagle is likely undergoing a major merger and is caught in the early stage of a star-burst when it has not yet experienced metal enrichment or formed the mass of dust typically found in star-forming galaxies.△ Less"
Quantum-classical correspondence for a dc-biased cavity resonator--Cooper-pair transistor system,"Authors:Miles Blencowe,Andrew Armour,Alex Rimberg","Abstract:We investigate the quantum versus classical dynamics of a microwave cavity-coupled-Cooper pair transistor (CPT) system, where an applied dc bias causes the system to self-oscillate via the ac Josephson effect. Varying the dc bias allows the self-oscillation frequency to be tuned. An unusual feature of the system design is that the dc bias does not significantly affect the high quality factor of th…▽ MoreWe investigate the quantum versus classical dynamics of a microwave cavity-coupled-Cooper pair transistor (CPT) system, where an applied dc bias causes the system to self-oscillate via the ac Josephson effect. Varying the dc bias allows the self-oscillation frequency to be tuned. An unusual feature of the system design is that the dc bias does not significantly affect the high quality factor of the cavity mode to which the CPT predominantly couples. The CPT-cavity mode system has a mechanical analogue involving a driven coupled pendulum-oscillator system. The corresponding, nonlinear classical dynamical equations exhibit chaotic, as well as aperiodic motions depending on the initial conditions and the nature and strengths of the damping/noise forces. The quantum master equation exhibits such phenomena as dynamical tunneling and the generation of non-classical states from initial classical states. Obviating the need for an external ac-drive line, which typically is harder to noise filter than a dc bias line, the self-oscillating system described here has considerable promise for demonstrating macroscopic quantum dynamical behavior.△ Less"
Mid-IR-laser microscopy as a tool for defect investigation in bulk semiconductors,"Authors:O. V. Astafiev,V. P. Kalinushkin,V. A. Yuryev",Abstract:A non-destructive optical technique described in this paper is an effective new tool for the investigation of defects in semiconductors. The basic instrument for this technique---a mid-IR-laser microscope---being sensitive to accumulations of free carriers enables the study of both accumulations of electrically-active defects or impurities in bulk semiconductors and doped domains in semiconductor…▽ MoreA non-destructive optical technique described in this paper is an effective new tool for the investigation of defects in semiconductors. The basic instrument for this technique---a mid-IR-laser microscope---being sensitive to accumulations of free carriers enables the study of both accumulations of electrically-active defects or impurities in bulk semiconductors and doped domains in semiconductor structures. The optical beam induced scattering mode of this microscope is designed for the investigation of recombination-active defects but unlike EBIC it requires neither Schottky barrier or p--n junction nor special preparation of samples△ Less
General considerations of the electrostatic boundary conditions in oxide heterostructures,"Authors:Takuya Higuchi,Harold Y. Hwang",Abstract:This is a book chapter that covers general considerations of the electrostatic stability of oxide surfaces and interfaces.This is a book chapter that covers general considerations of the electrostatic stability of oxide surfaces and interfaces.△ Less
SWIFT Observations of the Arp 147 Ring galaxy system,"Authors:Lisa Fogarty,Niranjan Thatte,Matthias Tecza,Fraser Clarke,Timothy Goodsall,Ryan Houghton,Graeme Salter,Roger Davies,Susan Kassin","Abstract:We present observations of Arp 147, a galaxy system comprising a collisionally-created ring galaxy and an early-type galaxy, using theOxfordSWIFT integral field spectrograph (IFS) at the 200-inch Hale telescope. We derive spatially resolved kinematics from the IFS data and use these to study the interaction between the two galaxies. We find the edge-to-edg…▽ MoreWe present observations of Arp 147, a galaxy system comprising a collisionally-created ring galaxy and an early-type galaxy, using theOxfordSWIFT integral field spectrograph (IFS) at the 200-inch Hale telescope. We derive spatially resolved kinematics from the IFS data and use these to study the interaction between the two galaxies. We find the edge-to-edge expansion velocity of the ring is 225 +/- 8 km/s, implying an upper limit on the timescale for the collision of 50 Myrs. We also calculate that the angle of impact for the collision is between 33 degrees-54 degrees, where 0 degrees would imply a perpendicular collision. The ring galaxy is strongly star-forming with the star formation likely to have been triggered by the collision between the two galaxies. We measure some key physical parameters in an integrated and spatially resolved manner for the ring galaxy. Using observed B-I colours and the H-alpha equivalent widths, we conclude that two stellar components (a young and an old population) are required to simultaneously match both observed quantities. We constrain the age range, light and mass fractions of the young star formation in the ring, finding a modest age range, a light fraction of less than a third, and a negligible (<1%) mass fraction. We postulate that the redder colours observed in the SE corner of the ring galaxy could correspond to the nuclear bulge of the original disk galaxy from which the ring was created, consistent with the stellar mass in the SE quadrant being 30-50% of the total. The ring appears to have been a typical disk galaxy prior to the encounter. The ring shows electron densities consistent with typical values for star-forming HII regions. The eastern half of the ring exhibits a metallicity a factor of ~2 higher than the western half. The ionisation parameter, measured across the ring, roughly follows the previously observed trend with metallicity.△ Less"
MissForest - nonparametric missing value imputation for mixed-type data,"Authors:Daniel J. Stekhoven,Peter Bühlmann","Abstract:Modern data acquisition based on high-throughput technology is often facing the problem of missing data. Algorithms commonly used in the analysis of such large-scale data often depend on a complete set. Missing value imputation offers a solution to this problem. However, the majority of available imputation methods are restricted to one type of variable only: continuous or categorical. For mixed-t…▽ MoreModern data acquisition based on high-throughput technology is often facing the problem of missing data. Algorithms commonly used in the analysis of such large-scale data often depend on a complete set. Missing value imputation offers a solution to this problem. However, the majority of available imputation methods are restricted to one type of variable only: continuous or categorical. For mixed-type data the different types are usually handled separately. Therefore, these methods ignore possible relations between variable types. We propose a nonparametric method which can cope with different types of variables simultaneously. We compare several state of the art methods for the imputation of missing values. We propose and evaluate an iterative imputation method (missForest) based on a random forest. By averaging over many unpruned classification or regression trees random forest intrinsically constitutes a multiple imputation scheme. Using the built-in out-of-bag error estimates of random forest we are able to estimate the imputation error without the need of a test set. Evaluation is performed on multiple data sets coming from a diverse selection of biological fields with artificially introduced missing values ranging from 10% to 30%. We show that missForest can successfully handle missing values, particularly in data sets including different types of variables. In our comparative study missForest outperforms other methods of imputation especially in data settings where complex interactions and nonlinear relations are suspected. The out-of-bag imputation error estimates of missForest prove to be adequate in all settings. Additionally, missForest exhibits attractive computational efficiency and can cope with high-dimensional data.△ Less"
Matrix integrals and enumeration of maps,Authors:J. Bouttier,"Abstract:This chapter is an introduction to the connection between random matrices and maps, i.e graphs drawn on surfaces. We concentrate on the one-matrix model and explain how it encodes and allows to solve a map enumeration problem.This chapter is an introduction to the connection between random matrices and maps, i.e graphs drawn on surfaces. We concentrate on the one-matrix model and explain how it encodes and allows to solve a map enumeration problem.△ Less"
Networks,Authors:C. M. Roland,"Abstract:The two approaches to analyzing the large strain behavior of rubbery networks are phenomenologically, using strain energy functions drawn from continuum mechanics, and molecular models, which apply statistical mechanics to compute the effect of chain orientation on the entropy. The early rubber elasticity models ignored intermolecular interactions, whereas later developments (""constraint models"")…▽ MoreThe two approaches to analyzing the large strain behavior of rubbery networks are phenomenologically, using strain energy functions drawn from continuum mechanics, and molecular models, which apply statistical mechanics to compute the effect of chain orientation on the entropy. The early rubber elasticity models ignored intermolecular interactions, whereas later developments (""constraint models"") included the effect of entanglements or steric constraints on the mechanical stress. These constitutive equations for rubber elasticity are compared to experimental results, and the connection of network elasticity to the relaxation behaviour is discussed. For conventional elastomers there is a compromise between stiffness and strength. Different methods to circumvent this limitation are described. Examples are given of the properties obtained with novel network architectures, including interpenetrating networks, double networks, bimodal networks, miscible heterogeneous networks, and deswollen networks.△ Less"
Random permutations and related topics,Authors:Grigori Olshanski,Abstract:We present an overview of selected topics in random permutations and random partitions highlighting analogies with random matrix theory.We present an overview of selected topics in random permutations and random partitions highlighting analogies with random matrix theory.△ Less
Universality,Authors:A. B. J. Kuijlaars,"Abstract:Universality of eigenvalue spacings is one of the basic characteristics of random matrices. We give the precise meaning of universality and discuss the standard universality classes (sine, Airy, Bessel) and their appearance in unitary, orthogonal, and symplectic ensembles. The Riemann-Hilbert problem for orthogonal polynomials is one possible tool to derive universality in unitary random matrix en…▽ MoreUniversality of eigenvalue spacings is one of the basic characteristics of random matrices. We give the precise meaning of universality and discuss the standard universality classes (sine, Airy, Bessel) and their appearance in unitary, orthogonal, and symplectic ensembles. The Riemann-Hilbert problem for orthogonal polynomials is one possible tool to derive universality in unitary random matrix ensembles. An overview is presented of the Deift/Zhou steepest descent analysis of the Riemann-Hilbert problem in the one-cut regular case. Non-standard universality classes that arise at singular points in the spectrum are discussed at the end.△ Less"
"Constitutive modeling, non-linear behavior, and the stress-optic law",Authors:C. M. Roland,"Abstract:Constitutive modelling of the stress-strain response of rubbery polymers is described, with an emphasis on the limits to linearity for both neat and filled rubber (the latter due to the Payne effect). Deviations from the Boltzmann superposition principle (time-strain invariance) under reversing strain histories are reviewed, including a discussion of the Mullins effect. The chapter also reviews th…▽ MoreConstitutive modelling of the stress-strain response of rubbery polymers is described, with an emphasis on the limits to linearity for both neat and filled rubber (the latter due to the Payne effect). Deviations from the Boltzmann superposition principle (time-strain invariance) under reversing strain histories are reviewed, including a discussion of the Mullins effect. The chapter also reviews the application of the stress optic law and the conditions causing its breakdown: internal stress, the glass transition, orientational coupling, and creep recovery. Different empirical rules for nonlinear flow are considered, including the Cox-Merz rule and the relations of Laun and Gleissle.△ Less"
The Many Worlds of Hugh Everett III,Authors:Adrian Kent,"Abstract:…Byrne's biography of Hugh Everett III, ""The Many Worlds of Hugh Everett III: Multiple Universes, Mutual Assured Destruction, and the Meltdown of a Nuclear Family"", (OxfordUniversity Press, 2010).▽ MoreA review of Peter Byrne's biography of Hugh Everett III, ""The Many Worlds of Hugh Everett III: Multiple Universes, Mutual Assured Destruction, and the Meltdown of a Nuclear Family"", (OxfordUniversity Press, 2010).△ Less"
A new estimate on the indirect Coulomb Energy,"Authors:Rafael D. Benguria,Gonzalo Bley,Michael Loss","Abstract:…the indirect Coulomb energy in quantum mechanics in terms of the single particle density of the system. The new universal lower bound is an alternative to the classical Lieb--Oxfordbound (with a smaller constant, 1.45 < 1.68) but involving an additive kinetic energy term of the single particle density.▽ MoreWe prove a new lower bound on the indirect Coulomb energy in quantum mechanics in terms of the single particle density of the system. The new universal lower bound is an alternative to the classical Lieb--Oxfordbound (with a smaller constant, 1.45 < 1.68) but involving an additive kinetic energy term of the single particle density.△ Less"
"""Bridging the Gap"" through Australian Cultural Astronomy","Authors:Duane W. Hamacher,Ray P. Norris","Abstract:For more than 50,000 years, Indigenous Australians have incorporated celestial events into their oral traditions and used the motions of celestial bodies for navigation, time-keeping, food economics, and social structure. In this paper, we explore the ways in which Aboriginal people made careful observations of the sky, measurements of celestial bodies, and incorporated astronomical events into co…▽ MoreFor more than 50,000 years, Indigenous Australians have incorporated celestial events into their oral traditions and used the motions of celestial bodies for navigation, time-keeping, food economics, and social structure. In this paper, we explore the ways in which Aboriginal people made careful observations of the sky, measurements of celestial bodies, and incorporated astronomical events into complex oral traditions by searching for written records of time-keeping using celestial bodies, the use of rising and setting stars as indicators of special events, recorded observations of variable stars, the solar cycle, and lunar phases (including ocean tides and eclipses) in oral tradition, as well as astronomical measurements of the equinox, solstice, and cardinal points.△ Less"
ppiTrim: Constructing non-redundant and up-to-date interactomes,"Authors:Aleksandar Stojmirović,Yi-Kuo Yu","Abstract:Robust advances in interactome analysis demand comprehensive, non-redundant and consistently annotated datasets. By non-redundant, we mean that the accounting of evidence for every interaction should be faithful: each independent experimental support is counted exactly once, no more, no less. While many interactions are shared among public repositories, none of them contains the complete known int…▽ MoreRobust advances in interactome analysis demand comprehensive, non-redundant and consistently annotated datasets. By non-redundant, we mean that the accounting of evidence for every interaction should be faithful: each independent experimental support is counted exactly once, no more, no less. While many interactions are shared among public repositories, none of them contains the complete known interactome for any model organism. In addition, the annotations of the same experimental result by different repositories often disagree. This brings up the issue of which annotation to keep while consolidating evidences that are the same. The iRefIndex database, including interactions from most popular repositories with a standardized protein nomenclature, represents a significant advance in all aspects, especially in comprehensiveness. However, iRefIndex aims to maintain all information/annotation from original sources and requires users to perform additional processing to fully achieve the aforementioned goals.
  To address issues with iRefIndex and to achieve our goals, we have developed ppiTrim, a script that processes iRefIndex to produce non-redundant, consistently annotated datasets of physical interactions. Our script proceeds in three stages: mapping all interactants to gene identifiers and removing all undesired raw interactions, deflating potentially expanded complexes, and reconciling for each interaction the annotation labels among different source databases. As an illustration, we have processed the three largest organismal datasets: yeast, human and fruitfly. While ppiTrim can resolve most apparent conflicts between different labelings, we also discovered some unresolvable disagreements mostly resulting from different annotation policies among repositories.
  URL: http://www.ncbi.nlm.nih.gov/CBBresearch/Yu/downloads/ppiTrim.html△ Less"
The astronomical orientation of the urban plan of Alexandria,"Authors:Luisa Ferro,Giulio Magli","Abstract:Alexander the Great founded Alexandria in 331 BC. The newly founded town was conceived as an orthogonal grid based on a main longitudinal axis, later called Canopic Road. We analyse here the astronomical orientation of the project and propose that the main axis was deliberately oriented towards the rising sun on the day of birth of Alexander the Great. The argument is admittedly speculative as any…▽ MoreAlexander the Great founded Alexandria in 331 BC. The newly founded town was conceived as an orthogonal grid based on a main longitudinal axis, later called Canopic Road. We analyse here the astronomical orientation of the project and propose that the main axis was deliberately oriented towards the rising sun on the day of birth of Alexander the Great. The argument is admittedly speculative as any Archaeoastronomy argument not backed up by written sources. However, it is nested accurately into the archaeological records and into what is known on the foundation of the town. Further, a topographical analysis is given to sustain the thesis.△ Less"
Hypergraphs with many Kneser colorings (Extended Version),"Authors:Carlos Hoppen,Yoshiharu Kohayakawa,Hanno Lefmann","Abstract:…triangle, is related to the Erdős--Ko--Rado Theorem on intersecting systems of sets [Intersection Theorems for Systems of Finite Sets, Quarterly Journal of Mathematics,OxfordSeries, Series 2, {\bf 12} (1961), 313--320].▽ MoreFor fixed positive integers $r, k$ and $\ell$ with $1 \leq \ell < r$ and an $r$-uniform hypergraph $H$, let $κ(H, k,\ell)$ denote the number of $k$-colorings of the set of hyperedges of $H$ for which any two hyperedges in the same color class intersect in at least $\ell$ elements. Consider the function $\KC(n,r,k,\ell)=\max_{H\in{\mathcal H}_{n}} κ(H, k,\ell) $, where the maximum runs over the family ${\mathcal H}_n$ of all $r$-uniform hypergraphs on $n$ vertices. In this paper, we determine the asymptotic behavior of the function $\KC(n,r,k,\ell)$ for every fixed $r$, $k$ and $\ell$ and describe the extremal hypergraphs. This variant of a problem of Erdős and Rothschild, who considered edge colorings of graphs without a monochromatic triangle, is related to the Erdős--Ko--Rado Theorem on intersecting systems of sets [Intersection Theorems for Systems of Finite Sets, Quarterly Journal of Mathematics,OxfordSeries, Series 2, {\bf 12} (1961), 313--320].△ Less"
Observatories in Space,Authors:Catherine Turon,"Abstract:Space observatories are having major impacts on our knowledge of the Universe, from the Solar neighborhood to the cosmological background, opening many new windows out of reach to ground-based observatories. Celestial objects emit all over the electromagnetic spectrum, and the Earth's atmosphere blocks a large part of them. Moreover, space offers a very stable environment from where the whole sky…▽ MoreSpace observatories are having major impacts on our knowledge of the Universe, from the Solar neighborhood to the cosmological background, opening many new windows out of reach to ground-based observatories. Celestial objects emit all over the electromagnetic spectrum, and the Earth's atmosphere blocks a large part of them. Moreover, space offers a very stable environment from where the whole sky can be observed with no (or very little) perturbations, providing new observing possibilities. This chapter presents a few striking examples of astrophysics space observatories and of major results spanning from the Solar neighborhood and our Galaxy to external galaxies, quasars and the cosmological background.△ Less"
Glassy dynamics of electrons near the metal-insulator transition,Authors:Dragana Popović,Abstract:This review first describes the evidence that strongly suggests the existence of the metal-insulator transition (MIT) in a two-dimensional electron system in Si regardless of the amount of disorder. Extensive studies of the charge dynamics demonstrate that this transition is closely related to the glassy freezing of electrons as temperature T->0. Similarities to the behavior of three-dimensional m…▽ MoreThis review first describes the evidence that strongly suggests the existence of the metal-insulator transition (MIT) in a two-dimensional electron system in Si regardless of the amount of disorder. Extensive studies of the charge dynamics demonstrate that this transition is closely related to the glassy freezing of electrons as temperature T->0. Similarities to the behavior of three-dimensional materials raise the intriguing possibility that such correlated dynamics might be a universal feature of the MIT regardless of the dimensionality.△ Less
Wright's adaptive landscape versus Fisher's fundamental theorem,Authors:Steven A. Frank,"Abstract:Two giants of evolutionary theory, Sewall Wright and R. A. Fisher, fought bitterly for over thirty years. The Wright-Fisher controversy forms a cornerstone of the history and philosophy of biology. I argue that the standard interpretations of the Wright-Fisher controversy do not accurately represent the ideas and arguments of these two key historical figures. The usual account contrasts the major…▽ MoreTwo giants of evolutionary theory, Sewall Wright and R. A. Fisher, fought bitterly for over thirty years. The Wright-Fisher controversy forms a cornerstone of the history and philosophy of biology. I argue that the standard interpretations of the Wright-Fisher controversy do not accurately represent the ideas and arguments of these two key historical figures. The usual account contrasts the major slogans attached to each name: Wright's adaptive landscape and shifting balance theory of evolution versus Fisher's fundamental theorem of natural selection. These alternative theories are in fact incommensurable. Wright's theory is a detailed dynamical model of evolutionary change in actual populations. Fisher's theory is an abstract invariance and conservation law that, like all physical laws, captures essential features of a system but does not account for all aspects of dynamics in real examples. This key contrast between embodied theories of real cases and abstract laws is missing from prior analyses of Wright versus Fisher. They never argued about this contrast. Instead, the issue at stake in their arguments concerned the actual dynamics of real populations. Both agreed that fluctuations of nonadditive (epistatic) gene combinations play a central role in evolution. Wright emphasized stochastic fluctuations of gene combinations in small, isolated populations. By contrast, Fisher believed that fluctuating selection in large populations was the main cause of fluctuation in nonadditive gene combinations. Close reading shows that widely cited views attributed to Fisher mostly come from what Wright said about Fisher, whereas Fisher's own writings clearly do not support such views.△ Less"
Introduction to Categories and Categorical Logic,"Authors:Samson Abramsky,Nikos Tzevelekos","Abstract:…notes is to provide a succinct, accessible introduction to some of the basic ideas of category theory and categorical logic. The notes are based on a lecture course given atOxfordover the past few years. They contain numerous exercises, and hopefully will prove useful for self-study by those seeking a first introduction to the subject, with fairly minimal…▽ MoreThe aim of these notes is to provide a succinct, accessible introduction to some of the basic ideas of category theory and categorical logic. The notes are based on a lecture course given atOxfordover the past few years. They contain numerous exercises, and hopefully will prove useful for self-study by those seeking a first introduction to the subject, with fairly minimal prerequisites. The coverage is by no means comprehensive, but should provide a good basis for further study; a guide to further reading is included. The main prerequisite is a basic familiarity with the elements of discrete mathematics: sets, relations and functions. An Appendix contains a summary of what we will need, and it may be useful to review this first. In addition, some prior exposure to abstract algebra - vector spaces and linear maps, or groups and group homomorphisms - would be helpful.△ Less"
Strictly correlated uniform electron droplets,"Authors:E. Rasanen,M. Seidl,P. Gori-Giorgi","Abstract:…droplets in the strict-correlation limit. The indirect Coulomb interaction is found to increase as a function of the electron number, approaching the tighter forms of the Lieb-Oxfordbound recently proposed by Rasanen et al. [Phys. Rev. Lett. 102, 206406 (2009)]. The bound is satisfied in three-, two-, and one-dimensional droplets, and in the latter case it…▽ MoreWe study the energetic properties of finite but internally homogeneous D-dimensional electron droplets in the strict-correlation limit. The indirect Coulomb interaction is found to increase as a function of the electron number, approaching the tighter forms of the Lieb-Oxfordbound recently proposed by Rasanen et al. [Phys. Rev. Lett. 102, 206406 (2009)]. The bound is satisfied in three-, two-, and one-dimensional droplets, and in the latter case it is reached exactly - regardless of the type of interaction considered. Our results provide useful reference data for delocalized strongly correlated systems, and they can be used in the development and testing of exchange-correlation density functionals in the framework of density-functional theory.△ Less"
Transition Edge Sensor Thermometry for On-chip Materials Characterization,"Authors:D. J. Goldie,D. M. Glowacka,K. Rostem†,S. Withington","Abstract:The next generation of ultra-low-noise cryogenic detectors for space science applications require continued exploration of materials characteristics at low temperatures. The low noise and good energy sensitivity of current Transition Edge Sensors (TESs) permits measurements of thermal parameters of mesoscopic systems with unprecedented precision. We describe a radiometric technique for differentia…▽ MoreThe next generation of ultra-low-noise cryogenic detectors for space science applications require continued exploration of materials characteristics at low temperatures. The low noise and good energy sensitivity of current Transition Edge Sensors (TESs) permits measurements of thermal parameters of mesoscopic systems with unprecedented precision. We describe a radiometric technique for differential measurements of materials characteristics at low temperatures (below about 3K). The technique relies on the very broadband thermal radiation that couples between impedance-matched resistors that terminate a Nb superconducting microstrip and the power exchanged is measured using a TES. The capability of the TES to deliver fast, time-resolved thermometry further expands the parameter space: for example to investigate time-dependent heat capacity. Thermal properties of isolated structures can be measured in geometries that eliminate the need for complicating additional components such as the electrical wires of the thermometer itself. Differential measurements allow easy monitoring of temperature drifts in the cryogenic environment. The technique is rapid to use and easily calibrated. Preliminary results will be discussed.△ Less"
Boosted objects: a probe of beyond the Standard Model physics,"Authors:A. Abdesselam,E. Bergeaas Kuutmann,U. Bitenc,G. Brooijmans,J. Butterworth,P. Bruckman de Renstrom,D. Buarque Franzosi,R. Buckingham,B. Chapleau,M. Dasgupta,A. Davison,J. Dolen,S. Ellis,F. Fassi,J. Ferrando M. T. Frandsen,J. Frost,T. Gadfort,N. Glover,A. Haas,E. Halkiadakis,K. Hamilton,C. Hays,C. Hill,J. Jackson,C. Issever, et al. (37 additional authors not shown)","Abstract:We present the report of the hadronic working group of the BOOST2010 workshop held at the University ofOxfordin June 2010. The first part contains a review of the potential of hadronic decays of highly boosted particles as an aid for discovery at the LHC and a discussion of the status of tools developed to meet the challenge of reconstructing and isolating…▽ MoreWe present the report of the hadronic working group of the BOOST2010 workshop held at the University ofOxfordin June 2010. The first part contains a review of the potential of hadronic decays of highly boosted particles as an aid for discovery at the LHC and a discussion of the status of tools developed to meet the challenge of reconstructing and isolating these topologies. In the second part, we present new results comparing the performance of jet grooming techniques and top tagging algorithms on a common set of benchmark channels. We also study the sensitivity of jet substructure observables to the uncertainties in Monte Carlo predictions.△ Less"
Cosmic ray backgrounds for dark matter indirect detection,Authors:Philipp Mertsch,"Abstract:Recently, dark matter indirect searches have gained a lot of attention, mostly due to the possibility of recent anomalies in cosmic rays and microwave sky maps being due to the annihilation or decay of dark matter. In this thesis, we argue however that these signals are plagued by irreducible astrophysical backgrounds and show how plausible conventional physics can mimic the alleged dark matter si…▽ MoreRecently, dark matter indirect searches have gained a lot of attention, mostly due to the possibility of recent anomalies in cosmic rays and microwave sky maps being due to the annihilation or decay of dark matter. In this thesis, we argue however that these signals are plagued by irreducible astrophysical backgrounds and show how plausible conventional physics can mimic the alleged dark matter signals. In particular, we consider the possibility that the rise in the positron fraction observed by the PAMELA satellite is due to the production through (hadronic) cosmic ray spallation and subsequent acceleration of positrons, in the same sources as the primary cosmic rays. We present a new analytical estimate of the range of possible fluctuations in the high energy electron flux due to the discreteness of plausible cosmic ray sources. Fitting our result for the total electron-positron flux measured by the Fermi satellite allows us to fix the only free parameter of the model and make an independent prediction for the positron fraction. This model can be tested by considering similar effects expected for nuclear secondary-to-primary ratios such as B/C. A rise predicted above O(100) GeV/n would be an unique confirmation of our explanation for a rising positron fraction and rule out the dark matter explanation. Furthermore, we review the assumptions made in the extraction of the `WMAP haze' which has also been claimed to be due to electrons and positrons from dark matter annihilation in the Galactic centre region. We argue that the energy-dependence of their diffusion makes the extraction of the haze through template fitting unreliable. The systematic effects introduced by this can, under specific circumstances, reproduce the residual, suggesting that the `haze' may be just an artefact of the template subtraction.△ Less"
Is realism compatible with true randomness ?,Authors:Nicolas Gisin,"Abstract:It is argued that realism and true randomness are fully compatible. Realistic true random events are acts of pure creation that obey strict laws, but do not necessarily satisfy Kolmogorov's axioms of probabilities. Realistic true randomness is some sort of nondeterministic force, or propensity of physical systems to manifest such and such properties under such and such conditions. Realistic random…▽ MoreIt is argued that realism and true randomness are fully compatible. Realistic true random events are acts of pure creation that obey strict laws, but do not necessarily satisfy Kolmogorov's axioms of probabilities. Realistic true randomness is some sort of nondeterministic force, or propensity of physical systems to manifest such and such properties under such and such conditions. Realistic random events reflect preexisting properties, as required by realism, simply the reflection is not deterministic; still, the preexisting properties determine the propensities of the different possible events.
  It is argued that deterministic extensions of quantum physics are necessarily incompatible with special relativity. Hence, from today's violations of Bell's inequalities one can conclude that all future physics theories will display true randomness as does quantum physics.
  It is argued that accepting true randomness and realism leads to new questions with interesting answers, allowing one 1) to study nonlocality in configurations with many independent sources and 2) to bound how much free will is needed for a proper violation of Bell's inequality.△ Less"
Using Evolution Strategy with Meta-models for Well Placement Optimization,"Authors:Zyed Bouzarkouna,Didier Yu Ding,Anne Auger","Abstract:Optimum implementation of non-conventional wells allows us to increase considerably hydrocarbon recovery. By considering the high drilling cost and the potential improvement in well productivity, well placement decision is an important issue in field development. Considering complex reservoir geology and high reservoir heterogeneities, stochastic optimization methods are the most suitable approach…▽ MoreOptimum implementation of non-conventional wells allows us to increase considerably hydrocarbon recovery. By considering the high drilling cost and the potential improvement in well productivity, well placement decision is an important issue in field development. Considering complex reservoir geology and high reservoir heterogeneities, stochastic optimization methods are the most suitable approaches for optimum well placement. This paper proposes an optimization methodology to determine optimal well location and trajectory based upon the Covariance Matrix Adaptation - Evolution Strategy (CMA-ES) which is a variant of Evolution Strategies recognized as one of the most powerful derivative-free optimizers for continuous optimization. To improve the optimization procedure, two new techniques are investigated: (1). Adaptive penalization with rejection is developed to handle well placement constraints. (2). A meta-model, based on locally weighted regression, is incorporated into CMA-ES using an approximate ranking procedure. Therefore, we can reduce the number of reservoir simulations, which are computationally expensive. Several examples are presented. Our new approach is compared with a Genetic Algorithm incorporating the Genocop III technique. It is shown that our approach outperforms the genetic algorithm: it leads in general to both a higher NPV and a significant reduction of the number of reservoir simulations.△ Less"
LaAlO3/SrTiO3-Based Device Concepts,"Authors:Daniela F. Bogorin,Patrick Irvin,Cheng Cen,Jeremy Levy",Abstract:This is a book chapter that covers several device concepts based on the LaAlO3/SrTiO3 interface.This is a book chapter that covers several device concepts based on the LaAlO3/SrTiO3 interface.△ Less
The Disconnect Between Quantum Mechanics and Gravity,Authors:Daniel M. Greenberger,"Abstract:There is a serious disconnect between quantum theory and gravity. It occurs at the level of the very foundations of quantum theory, and is far deeper than just the matter of trying to quantize a non-linear theory. We shall examine some of the physical reasons for this disconnect and show how it manifests itself at the beginning, at the level of the equivalence principle.There is a serious disconnect between quantum theory and gravity. It occurs at the level of the very foundations of quantum theory, and is far deeper than just the matter of trying to quantize a non-linear theory. We shall examine some of the physical reasons for this disconnect and show how it manifests itself at the beginning, at the level of the equivalence principle.△ Less"
Conceptual Problems Related to Time and Mass in Quantum Theory,Authors:Daniel M. Greenberger,"Abstract:The concept of proper time cannot be just taken over from classical theory and applied to quantum theory. There are a number of serious ambiguities related to it. Similarly, the concept of mass has some inconsistencies attached to it. We argue that both of these concepts only make sense if they are included in the theory as dynamical variables, and subsequently as conjugate operators. We discuss t…▽ MoreThe concept of proper time cannot be just taken over from classical theory and applied to quantum theory. There are a number of serious ambiguities related to it. Similarly, the concept of mass has some inconsistencies attached to it. We argue that both of these concepts only make sense if they are included in the theory as dynamical variables, and subsequently as conjugate operators. We discuss the necessary uncertainty principle between them.△ Less"
The twistor space of a quaternionic contact manifold,"Authors:Johann Davidov,Stefan Ivanov,Ivan Minchev",Abstract:We show that the CR structure on the twistor space of a quaternionic contact structure described by Biquard is normal if and only if the Ricci curvature of the Biquard connection commutes with the endomorphisms in the quaternionic structure of the contact distribution.We show that the CR structure on the twistor space of a quaternionic contact structure described by Biquard is normal if and only if the Ricci curvature of the Biquard connection commutes with the endomorphisms in the quaternionic structure of the contact distribution.△ Less
Can quantum theory and special relativity peacefully coexist?,Authors:M. P. Seevinck,"Abstract:This white paper aims to identify an open problem in 'Quantum Physics and the Nature of Reality' --namely whether quantum theory and special relativity are formally compatible--, to indicate what the underlying issues are, and put forward ideas about how the problem might be addressed.This white paper aims to identify an open problem in 'Quantum Physics and the Nature of Reality' --namely whether quantum theory and special relativity are formally compatible--, to indicate what the underlying issues are, and put forward ideas about how the problem might be addressed.△ Less"
On the dynamics of tidal streams in the Milky Way galaxy,Authors:Andy Eyre,"Abstract:We present a brief history of Galactic astrophysics, and explain the origin of halo substructure in the Galaxy. We motivate our study of tidal streams by highlighting the tight constraints that analysis of the trajectories of tidal streams can place on the form of the Galactic potential.
  We address the reconstruction of orbits from observations of tidal streams. We upgrade the scheme reported by…▽ MoreWe present a brief history of Galactic astrophysics, and explain the origin of halo substructure in the Galaxy. We motivate our study of tidal streams by highlighting the tight constraints that analysis of the trajectories of tidal streams can place on the form of the Galactic potential.
  We address the reconstruction of orbits from observations of tidal streams. We upgrade the scheme reported by Binney (2008) and Jin & Lynden-Bell (2007), which reconstructs orbits from streams using radial-velocity measurements, to allow it to work with erroneous input data. The upgraded algorithm can correct for both statistical error on observations, and systematic error due to streams not delineating individual orbits, and given high-quality but realistic input data, it can diagnose the potential with considerable accuracy.
  We complement the work of Binney (2008) by deriving a new algorithm, which reconstructs orbits from streams using proper-motion data rather than radial velocities. We show that the new algorithm has a similar potency for diagnosing the Galactic potential.
  We explore the concept of Galactic parallax, which arises in connection with our proper-motion study. Galactic parallax allows trigonometric distance calculation to stars at 40 times the range of conventional parallax, although its applicability is limited to only those stars in tidal streams.
  We examine from first principles the mechanics of tidal stream formation and propagation. We find that the mechanics of tidal streams has a natural expression in terms of action-angle variables. We find that tidal streams in realistic galaxy potentials will generally not delineate orbits, and that attempting to constrain the Galactic potential by assuming that they do can lead to large systematic error. We show that we can accurately predict the real-space trajectories of streams, even when they differ significantly from orbits.△ Less"
Scientific interview,"Authors:J. Kurchan,J. S. Langer,T. A. Witten,P. G. Wolynes","Abstract:Four leading scientists in the field of the glass transition answer to a series of questions formulated by the Editors (L. Berthier, G. Biroli, J.-P. Bouchaud, L. Cipelletti, W. van Saarloos). No specific format has been imposed to their answers: the scientists were free to answer or not to any given question, using how much space they felt appropriate. After a first round of answers, each author…▽ MoreFour leading scientists in the field of the glass transition answer to a series of questions formulated by the Editors (L. Berthier, G. Biroli, J.-P. Bouchaud, L. Cipelletti, W. van Saarloos). No specific format has been imposed to their answers: the scientists were free to answer or not to any given question, using how much space they felt appropriate. After a first round of answers, each author was given the opportunity to read the answers by his colleagues and to adjust his own answers accordingly.△ Less"
An overview of the theories of the glass transition,Authors:Gilles Tarjus,"Abstract:The topic of the glass transition gives rise to a a wide diversity of views. It is, accordingly, characterized by a lack of agreement on which would be the most profitable theoretical perspective. In this chapter, I provide some elements that can help sorting out the many theoretical approaches, understanding their foundations, as well as discussing their validity and mutual compatibility. Along t…▽ MoreThe topic of the glass transition gives rise to a a wide diversity of views. It is, accordingly, characterized by a lack of agreement on which would be the most profitable theoretical perspective. In this chapter, I provide some elements that can help sorting out the many theoretical approaches, understanding their foundations, as well as discussing their validity and mutual compatibility. Along the way, I describe the progress made in the last twenty years, including new insights concerning the spatial heterogeneity of the dynamics and the characteristic length scales associated with the glass transition. An emphasis is put on those theories that associate glass formation with growing collective behavior and emerging universality.△ Less"
Modal quantum theory,"Authors:Benjamin Schumacher,Michael D. Westmoreland","Abstract:We present a discrete model theory similar in structure to ordinary quantum mechanics, but based on a finite field instead of complex amplitudes. The interpretation of this theory involves only the ""modal"" concepts of possibility and necessity rather than quantitative probability measures. Despite its simplicity, our model theory includes entangled states and has versions of both Bell's theorem an…▽ MoreWe present a discrete model theory similar in structure to ordinary quantum mechanics, but based on a finite field instead of complex amplitudes. The interpretation of this theory involves only the ""modal"" concepts of possibility and necessity rather than quantitative probability measures. Despite its simplicity, our model theory includes entangled states and has versions of both Bell's theorem and the no cloning theorem.△ Less"
Dynamical Heterogeneities in Grains and Foams,"Authors:Olivier Dauchot,Douglas J. Durian,Martin van Hecke","Abstract:Dynamical heterogeneities have been introduced in the context of the glass transition of molecular liquids and the lengthscale associated with them has been argued to be at the origin of the observed quasi-universal behaviour of glassy systems. Dense amorphous packings of granular media and foams also exhibit slow dynamics, intermittency and heterogeneities. We review a number of recent experiment…▽ MoreDynamical heterogeneities have been introduced in the context of the glass transition of molecular liquids and the lengthscale associated with them has been argued to be at the origin of the observed quasi-universal behaviour of glassy systems. Dense amorphous packings of granular media and foams also exhibit slow dynamics, intermittency and heterogeneities. We review a number of recent experimental studies of these systems, where one has direct access to the relevant space-time dynamics, allowing for direct visualisations of the dynamical heterogeneities. On one hand these visualisations provide a unique opportunity to access the microscopic mechanisms responsible for the growth of dynamical correlations. On the other hand focussing on the differences in these heterogeneities in microscopically different systems allows to discuss the range of the analogies between molecular thermal glasses and athermal glasses such as granular media and foams. Finally this review is the opportunity to discuss various approaches to actually extract quantitatively the dynamical lengthscale from experimental data.△ Less"
Growing length scales in aging systems,"Authors:Federico Corberi,Leticia F. Cugliandolo,Hajime Yoshino","Abstract:We summarize studies of growing lengths in different aging systems. The article is structured as follows. We recall the definition of a number of observables, typically correlations and susceptibilities, that give access to dynamic and static correlation lengths. We use a growing length perspective to review three out of equilibrium cases: domain growth phenomena; the evolution of Edwards-Wilkinso…▽ MoreWe summarize studies of growing lengths in different aging systems. The article is structured as follows. We recall the definition of a number of observables, typically correlations and susceptibilities, that give access to dynamic and static correlation lengths. We use a growing length perspective to review three out of equilibrium cases: domain growth phenomena; the evolution of Edwards-Wilkinson and Kardar-Parisi-Zhang manifolds and other directed elastic manifolds in random media; spin and structural glasses in relaxation and under an external drive. Finally, we briefly report on a mechanism for dynamic fluctuations in aging systems that is based on a time-reparametrization invariance scenario and may be at the origin of the dynamic growing length in glassy materials.△ Less"
Kinetically Constrained Models,"Authors:Juan P. Garrahan,Peter Sollich,Cristina Toninelli","Abstract:In this chapter we summarize recent developments in the study of kinetically constrained models (KCMs) as models for glass formers. After recalling the definition of the KCMs which we cover we study the possible occurrence of ergodicity breaking transitions and discuss in some detail how, before any such transition occurs, relaxation timescales depend on the relevant control parameter (density or…▽ MoreIn this chapter we summarize recent developments in the study of kinetically constrained models (KCMs) as models for glass formers. After recalling the definition of the KCMs which we cover we study the possible occurrence of ergodicity breaking transitions and discuss in some detail how, before any such transition occurs, relaxation timescales depend on the relevant control parameter (density or temperature). Then we turn to the main issue: the prediction of KCMs for dynamical heterogeneities. We focus in particular on multipoint correlation functions and susceptibilities, and decoupling in the transport coefficients. Finally we discuss the recent view of KCMs as being at first order coexistence between an active and an inactive space-time phase.△ Less"
Glassy dynamics and dynamical heterogeneity in colloids,"Authors:Luca Cipelletti,Eric R. Weeks","Abstract:Concentrated colloidal suspensions are a well-tested model system which has a glass transition. Colloids are suspensions of small solid particles in a liquid, and exhibit glassy behavior when the particle concentration is high; the particles are roughly analogous to individual molecules in a traditional glass. Because the particle size can be large (100 nm - 1000 nm), these samples can be studied…▽ MoreConcentrated colloidal suspensions are a well-tested model system which has a glass transition. Colloids are suspensions of small solid particles in a liquid, and exhibit glassy behavior when the particle concentration is high; the particles are roughly analogous to individual molecules in a traditional glass. Because the particle size can be large (100 nm - 1000 nm), these samples can be studied with a variety of optical techniques including microscopy and dynamic light scattering. Here we review the phenomena associated with the colloidal glass transition, and in particular discuss observations of spatial and temporally heterogeneous dynamics within colloidal samples near the glass transition. Although this Chapter focuses primarily on results from hard-sphere-like colloidal particles, we also discuss other colloidal systems with attractive or soft repulsive interactions.△ Less"
The Length Scales of Dynamic Heterogeneity: Results from Molecular Dynamics Simulations,Authors:Peter Harrowell,"Abstract:Over times shorter than that required for relaxation of enthalpy, a liquid can exhibit striking heterogeneities. The picture of these heterogeneities is complex with transient patches of rigidity, irregular yet persistent, intersected by tendrils of mobile particles, flickering intermittently into new spatial patterns of motion and arrest. The study of these dynamic heterogeneities has, over the l…▽ MoreOver times shorter than that required for relaxation of enthalpy, a liquid can exhibit striking heterogeneities. The picture of these heterogeneities is complex with transient patches of rigidity, irregular yet persistent, intersected by tendrils of mobile particles, flickering intermittently into new spatial patterns of motion and arrest. The study of these dynamic heterogeneities has, over the last 20 years, allowed us to characterize cooperative dynamics, to identify new strategies in controlling kinetics in glass-forming liquids and to begin to systematically explore the relationship between dynamics and structure that underpins the behaviour of amorphous materials. Computer simulations of the dynamics in atomic and molecular liquids have played a dominant role in all of this progress. While some may be uneasy about this reliance on modelling, it is unavoidable, given the amount of microscopic detail needed to characterize the dynamic heterogeneities. The complexities revealed by these simulations have called for new conceptual tools. In this essay, I have tried to provide the reader with a clear and complete account of how these tools have been developed in terms of the literature on kinetic length scales in molecular dynamics simulations. Through the `prism' of these length scales, this essay addresses the question what have we learnt about dynamic heterogeneities from computer simulations?△ Less"
Heterogeneities in amorphous systems under shear,"Authors:J. -L. Barrat,Anael Lemaitre","Abstract:The last decade has seen major progresses in studies of elementary mechanisms of deformation in amorphous materials. Here, we start with a review of physically-based theories of plasticity, going back to the identification of ""shear-transformations"" as early as the 70's. We show how constructive criticism of the theoretical models permits to formulate questions concerning the role of structural di…▽ MoreThe last decade has seen major progresses in studies of elementary mechanisms of deformation in amorphous materials. Here, we start with a review of physically-based theories of plasticity, going back to the identification of ""shear-transformations"" as early as the 70's. We show how constructive criticism of the theoretical models permits to formulate questions concerning the role of structural disorder, mechanical noise, and long-ranged elastic interactions. These questions provide the necessary context to understand what has motivated recent numerical studies. We then summarize their results, show why they had to focus on athermal systems, and point out the outstanding questions.△ Less"
Analytical approaches to time and length scales in models of glasses,"Authors:Silvio Franz,Guilhem Semerjian","Abstract:The goal of this chapter is to review recent analytical results about the growth of a (static) correlation length in glassy systems, and the connection that can be made between this length scale and the equilibrium correlation time of its dynamics. The definition of such a length scale is first given in a generic setting, including finite-dimensional models, along with rigorous bounds linking it t…▽ MoreThe goal of this chapter is to review recent analytical results about the growth of a (static) correlation length in glassy systems, and the connection that can be made between this length scale and the equilibrium correlation time of its dynamics. The definition of such a length scale is first given in a generic setting, including finite-dimensional models, along with rigorous bounds linking it to the correlation time. We then present some particular cases (finite connectivity mean-field models, and Kac limit of finite dimensional systems) where this length can be actually computed.△ Less"
Overview of different characterisations of dynamic heterogeneity,"Authors:Ludovic Berthier,Giulio Biroli,Jean-Philippe Bouchaud,Robert L Jack","Abstract:Dynamic heterogeneity is now recognised as a central aspect of structural relaxation in disordered materials with slow dynamics, and was the focus of intense research in the last decade. Here we describe how initial, indirect observations of dynamic heterogeneity have recently evolved into well-defined, quantitative, statistical characterisations, in particular through the use of high-order correl…▽ MoreDynamic heterogeneity is now recognised as a central aspect of structural relaxation in disordered materials with slow dynamics, and was the focus of intense research in the last decade. Here we describe how initial, indirect observations of dynamic heterogeneity have recently evolved into well-defined, quantitative, statistical characterisations, in particular through the use of high-order correlation and response functions. We highlight both recent progress and open questions about the characterisation of dynamic heterogeneity in glassy materials. We also discuss the limits of available tools and describe a few candidates for future research in order to gain deeper understanding of the origin and nature of glassiness in disordered systems.△ Less"
Optical NEP in Hot-Electron Nanobolometers,"Authors:Boris S. Karasik,Robin Cantor","Abstract:For the first time, we have measured the optical noise equivalent power (NEP) in titanium (Ti) superconducting hot-electron nanobolometers (nano-HEBs). The bolometers were 2μmx1μmx20nm and 1μmx1μmx20nm planar antenna-coupled devices. The measurements were done at λ = 460 μm using a cryogenic black body radiation source delivering optical power from a fraction of a femtowatt to a few 100s of femtow…▽ MoreFor the first time, we have measured the optical noise equivalent power (NEP) in titanium (Ti) superconducting hot-electron nanobolometers (nano-HEBs). The bolometers were 2μmx1μmx20nm and 1μmx1μmx20nm planar antenna-coupled devices. The measurements were done at λ = 460 μm using a cryogenic black body radiation source delivering optical power from a fraction of a femtowatt to a few 100s of femtowatts. A record low NEP = 3x10^{-19} W/Hz^{1/2} at 50 mK has been achieved. This sensitivity meets the requirements for SAFARI instrument on the SPICA telescope. The ways for further improvement of the nano-HEB detector sensitivity are discussed.△ Less"
"GYES, a multifibre spectrograph for the CFHT","Authors:P. Bonifacio,S. Mignot,J. -L. Dournaux,P. François,E. Caffau,F. Royer,C. Babusiaux,F. Arenou,C. Balkowski,O. Bienaymé,D. Briot,R. Carlberg,M. Cohen,G. B. Dalton,B. Famaey,G. Fasola,Y. Frémat,A. Gómez,M. Haywood,V. Hill,J. -M. Huet,D. Katz,D. Horville,R. Kudritzky,R. Lallement, et al. (16 additional authors not shown)","Abstract:…abundances for stars down to 16th magnitude and radial velocities, accurate to 1 km/s for fainter stars. The study is led by GEPI-Observatoire de Paris with a contribution fromOxfordfor the study of the positioner. The financing for the study comes from INSU CSAA and Observatoire de Paris. The conceptual study will be delivered to CFHT for review by Octobe…▽ MoreWe have chosen the name of GYES, one of the mythological giants with one hundred arms, offspring of Gaia and Uranus, for our instrument study of a multifibre spectrograph for the prime focus of the Canada-France-Hawaii Telescope. Such an instrument could provide an excellent ground-based complement for the Gaia mission and a northern complement to the HERMES project on the AAT. The CFHT is well known for providing a stable prime focus environment, with a large field of view, which has hosted several imaging instruments, but has never hosted a multifibre spectrograph. Building upon the experience gained at GEPI with FLAMES-Giraffe and X-Shooter, we are investigating the feasibility of a high multiplex spectrograph (about 500 fibres) over a field of view 1 degree in diameter. We are investigating an instrument with resolution in the range 15000 to 30000, which should provide accurate chemical abundances for stars down to 16th magnitude and radial velocities, accurate to 1 km/s for fainter stars. The study is led by GEPI-Observatoire de Paris with a contribution fromOxfordfor the study of the positioner. The financing for the study comes from INSU CSAA and Observatoire de Paris. The conceptual study will be delivered to CFHT for review by October 1st 2010.△ Less"
Piezoelectric rotator for studying quantum effects in semiconductor nanostructures at high magnetic fields and low temperatures,"Authors:L. A. Yeoh,A. Srinivasan,T. P. Martin,O. Klochan,A. P. Micolich,A. R. Hamilton","Abstract:We report the design and development of a piezoelectric sample rotation system, and its integration into anOxfordInstruments Kelvinox 100 dilution refrigerator, for orientation-dependent studies of quantum transport in semiconductor nanodevices at millikelvin temperatures in magnetic fields up to 10T. Our apparatus allows for continuous in situ rotation of…▽ MoreWe report the design and development of a piezoelectric sample rotation system, and its integration into anOxfordInstruments Kelvinox 100 dilution refrigerator, for orientation-dependent studies of quantum transport in semiconductor nanodevices at millikelvin temperatures in magnetic fields up to 10T. Our apparatus allows for continuous in situ rotation of a device through >100deg in two possible configurations. The first enables rotation of the field within the plane of the device, and the second allows the field to be rotated from in-plane to perpendicular to the device plane. An integrated angle sensor coupled with a closed-loop feedback system allows the device orientation to be known to within +/-0.03deg whilst maintaining the sample temperature below 100mK.△ Less"
Separating Bounded Arithmetics by Herbrand Consistency,Authors:Saeed Salehi,"Abstract:The problem ofΠ_1-separating the hierarchy of bounded arithmetic has been studied in the paper. It is shown that the notion of Herbrand Consistency, in its full generality, cannotΠ_1-separate the theory{\rm IΔ_0+\bigwedge_jΩ_j}from{\rm IΔ_0}; though it canΠ_1-separate{\rm IΔ_0+Exp}from{\rm IΔ_0}. This extends a result of L. A. Kołodziejczyk (2006), by showing the unprovabili…▽ MoreThe problem ofΠ_1-separating the hierarchy of bounded arithmetic has been studied in the paper. It is shown that the notion of Herbrand Consistency, in its full generality, cannotΠ_1-separate the theory{\rm IΔ_0+\bigwedge_jΩ_j}from{\rm IΔ_0}; though it canΠ_1-separate{\rm IΔ_0+Exp}from{\rm IΔ_0}. This extends a result of L. A. Kołodziejczyk (2006), by showing the unprovability of the Herbrand Consistency of{\rm IΔ_0}in the theory{\rm IΔ_0+\bigwedge_jΩ_j}.△ Less"
Interactions in Quantum Fluids,Authors:Thierry Giamarchi,"Abstract:In these notes I review the basic concepts of the effects of interactions on quantum particles. I focuss here mostly on the case of fermions, but several aspects of interacting bosons are mentioned as well. These notes have been voluntarily kept at an elementary level and should be suitable for students wanting to enter this field. I review the concept of Fermi liquid, and then move to a descripti…▽ MoreIn these notes I review the basic concepts of the effects of interactions on quantum particles. I focuss here mostly on the case of fermions, but several aspects of interacting bosons are mentioned as well. These notes have been voluntarily kept at an elementary level and should be suitable for students wanting to enter this field. I review the concept of Fermi liquid, and then move to a description of the interaction effects, as well as the main models that are used to tackle these questions. Finally I study the case of one dimensional interacting particles that constitutes a fascinating special case.△ Less"
The jamming scenario - an introduction and outlook,"Authors:Andrea J. Liu,Sidney R. Nagel,Wim van Saarloos,Matthieu Wyart","Abstract:The jamming scenario of disordered media, formulated about 10 years ago, has in recent years been advanced by analyzing model systems of granular media. This has led to various new concepts that are increasingly being explored in in a variety of systems. This chapter contains an introductory review of these recent developments and provides an outlook on their applicability to different physical sy…▽ MoreThe jamming scenario of disordered media, formulated about 10 years ago, has in recent years been advanced by analyzing model systems of granular media. This has led to various new concepts that are increasingly being explored in in a variety of systems. This chapter contains an introductory review of these recent developments and provides an outlook on their applicability to different physical systems and on future directions. The first part of the paper is devoted to an overview of the findings for model systems of frictionless spheres, focussing on the excess of low-frequency modes as the jamming point is approached. Particular attention is paid to a discussion of the cross-over frequency and length scales that govern this approach. We then discuss the effects of particle asphericity and static friction, the applicability to bubble models for wet foams in which the friction is dynamic, the dynamical arrest in colloids, and the implications for molecular glasses.△ Less"
"Relating toy models of quantum computation: comprehension, complementarity and dagger mix autonomous categories",Authors:Dusko Pavlovic,"Abstract:Toy models have been used to separate important features of quantum computation from the rich background of the standard Hilbert space model. Category theory, on the other hand, is a general tool to separate components of mathematical structures, and analyze one layer at a time. It seems natural to combine the two approaches, and several authors have already pursued this idea. We explore *categori…▽ MoreToy models have been used to separate important features of quantum computation from the rich background of the standard Hilbert space model. Category theory, on the other hand, is a general tool to separate components of mathematical structures, and analyze one layer at a time. It seems natural to combine the two approaches, and several authors have already pursued this idea. We explore *categorical comprehension construction* as a tool for adding features to toy models. We use it to comprehend quantum propositions and probabilities within the basic model of finite-dimensional Hilbert spaces. We also analyze complementary quantum observables over the category of sets and relations. This leads into the realm of *test spaces*, a well-studied model. We present one of many possible extensions of this model, enabled by the comprehension construction. Conspicuously, all models obtained in this way carry the same categorical structure, *extending* the familiar dagger compact framework with the complementation operations. We call the obtained structure *dagger mix autonomous*, because it extends mix autonomous categories, popular in computer science, in a similar way like dagger compact structure extends compact categories. Dagger mix autonomous categories seem to arise quite naturally in quantum computation, as soon as complementarity is viewed as a part of the global structure.△ Less"
Geometry of abstraction in quantum computation,Authors:Dusko Pavlovic,"Abstract:Quantum algorithms are sequences of abstract operations, performed on non-existent computers. They are in obvious need of categorical semantics. We present some steps in this direction, following earlier contributions of Abramsky, Coecke and Selinger. In particular, we analyze function abstraction in quantum computation, which turns out to characterize its classical interfaces. Some quantum algori…▽ MoreQuantum algorithms are sequences of abstract operations, performed on non-existent computers. They are in obvious need of categorical semantics. We present some steps in this direction, following earlier contributions of Abramsky, Coecke and Selinger. In particular, we analyze function abstraction in quantum computation, which turns out to characterize its classical interfaces. Some quantum algorithms provide feasible solutions of important hard problems, such as factoring and discrete log (which are the building blocks of modern cryptography). It is of a great practical interest to precisely characterize the computational resources needed to execute such quantum algorithms. There are many ideas how to build a quantum computer. Can we prove some necessary conditions? Categorical semantics help with such questions. We show how to implement an important family of quantum algorithms using just abelian groups and relations.△ Less"
On Particle Learning,"Authors:Nicolas Chopin,Alessandra Iacobucci,Jean-Michel Marin,Kerrie Mengersen,Christian P. Robert,Robin Ryder,Christian Schäfer","Abstract:This document is the aggregation of six discussions of Lopes et al. (2010) that we submitted to the proceedings of the Ninth Valencia Meeting, held in Benidorm, Spain, on June 3-8, 2010, in conjunction with Hedibert Lopes' talk at this meeting, and of a further discussion of the rejoinder by Lopes et al. (2010). The main point in those discussions is the potential for degeneracy in the particle le…▽ MoreThis document is the aggregation of six discussions of Lopes et al. (2010) that we submitted to the proceedings of the Ninth Valencia Meeting, held in Benidorm, Spain, on June 3-8, 2010, in conjunction with Hedibert Lopes' talk at this meeting, and of a further discussion of the rejoinder by Lopes et al. (2010). The main point in those discussions is the potential for degeneracy in the particle learning methodology, related with the exponential forgetting of the past simulations. We illustrate in particular the resulting difficulties in the case of mixtures.△ Less"
Extreme Eigenvalues of Wishart Matrices: Application to Entangled Bipartite System,Authors:Satya N. Majumdar,"Abstract:We discuss an application of the random matrix theory in the context of estimating the bipartite entanglement of a quantum system. We discuss how the Wishart ensemble (the earliest studied random matrix ensemble) appears in this quantum problem. The eigenvalues of the reduced density matrix of one of the subsystems have similar statistical properties as those of the Wishart matrices, except that t…▽ MoreWe discuss an application of the random matrix theory in the context of estimating the bipartite entanglement of a quantum system. We discuss how the Wishart ensemble (the earliest studied random matrix ensemble) appears in this quantum problem. The eigenvalues of the reduced density matrix of one of the subsystems have similar statistical properties as those of the Wishart matrices, except that their {\em trace is constrained to be unity}. We focus here on the smallest eigenvalue which serves as an important measure of entanglement between the two subsystems. In the hard edge case (when the two subsystems have equal sizes) one can fully characterize the probability distribution of the minimum eigenvalue for real, complex and quaternion matrices of all sizes. In particular, we discuss the important finite size effect due to the {\em fixed trace constraint}.△ Less"
Quantum probabilities: an information-theoretic interpretation,Authors:Jeffrey Bub,"Abstract:This Chapter develops a realist information-theoretic interpretation of the nonclassical features of quantum probabilities.  On this view, what is fundamental in the transition from classical to quantum physics is the recognition that \emph{information in the physical sense has new structural features}, just as the transition from classical to relativistic physics rests on the recognition that spa…▽ MoreThis Chapter develops a realist information-theoretic interpretation of the nonclassical features of quantum probabilities.  On this view, what is fundamental in the transition from classical to quantum physics is the recognition that \emph{information in the physical sense has new structural features}, just as the transition from classical to relativistic physics rests on the recognition that space-time is structurally different than we thought. Hilbert space, the event space of quantum systems, is interpreted  as a kinematic (i.e., pre-dynamic) framework for an indeterministic physics, in the sense that the geometric structure of Hilbert space imposes objective probabilistic or information-theoretic constraints on correlations between events, just as  the geometric structure of Minkowski space in special relativity imposes spatio-temporal kinematic constraints on events. The interpretation of quantum probabilities is more subjectivist in spirit than other discussions in this book (e.g., the chapter by Timpson), insofar as the quantum state is interpreted as a credence function---a bookkeeping device for keeping track of probabilities---but it is also objective (or intersubjective), insofar as the credences specified by the quantum state are understood as uniquely determined, via Gleason's theorem, by objective correlational constraints on events in the nonclassical quantum event space defined by the subspace structure of Hilbert space.△ Less"
Supersymmetry in Random Matrix Theory,Authors:Thomas Guhr,"Abstract:Supersymmetry is nowadays indispensable for many problems in Random Matrix Theory. It is presented here with an emphasis on conceptual and structural issues. An introduction to supermathematics is given. The Hubbard-Stratonovich transformation as well as its generalization and superbosonization are explained. The supersymmetric non-linear sigma model, Brownian motion in superspace and the color-fl…▽ MoreSupersymmetry is nowadays indispensable for many problems in Random Matrix Theory. It is presented here with an emphasis on conceptual and structural issues. An introduction to supermathematics is given. The Hubbard-Stratonovich transformation as well as its generalization and superbosonization are explained. The supersymmetric non-linear sigma model, Brownian motion in superspace and the color-flavor transformation are discussed.△ Less"
Disorder and interference: localization phenomena,"Authors:Cord A. Müller,Dominique Delande","Abstract:The specific problem we address in these lectures is the problem of transport and localization in disordered systems, when interference is present, as characteristic for waves, with a focus on realizations with ultracold atoms.The specific problem we address in these lectures is the problem of transport and localization in disordered systems, when interference is present, as characteristic for waves, with a focus on realizations with ultracold atoms.△ Less"
New some Hadamard's type inequalities for co-ordinated convex functions,"Authors:M. Z. Sarikaya,E. Set,M. E. Ozdemir,S. S. Dragomir","Abstract:In this paper, we establish new some Hermite-Hadamard's type inequalities of convex functions of 2-variables on the co-ordinates.In this paper, we establish new some Hermite-Hadamard's type inequalities of convex functions of 2-variables on the co-ordinates.△ Less"
Quantized vortices in superfluid helium and atomic Bose-Einstein condensates,"Authors:Makoto Tsubota,Kenichi Kasamatsu,Michikazu Kobayashi","Abstract:This article reviews recent developments in the physics of quantized vortices in superfluid helium and atomic Bose-Einstein condensates. Quantized vortices appear in low-temperature quantum condensed systems as the direct product of Bose-Einstein condensation. Quantized vortices were first discovered in superfluid 4He in the 1950s, and have since been studied with a primary focus on the quantum hy…▽ MoreThis article reviews recent developments in the physics of quantized vortices in superfluid helium and atomic Bose-Einstein condensates. Quantized vortices appear in low-temperature quantum condensed systems as the direct product of Bose-Einstein condensation. Quantized vortices were first discovered in superfluid 4He in the 1950s, and have since been studied with a primary focus on the quantum hydrodynamics of this system. Since the discovery of superfluid 3He in 1972, quantized vortices characteristic of the anisotropic superfluid have been studied theoretically and observed experimentally using rotating cryostats. The realization of atomic Bose-Einstein condensation in 1995 has opened new possibilities, because it became possible to control and directly visualize condensates and quantized vortices. Historically, many ideas developed in superfluid 4He and 3He have been imported to the field of cold atoms and utilized effectively. Here, we review and summarize our current understanding of quantized vortices, bridging superfluid helium and atomic Bose-Einstein condensates. This review article begins with a basic introduction, which is followed by discussion of modern topics such as quantum turbulence and vortices in unusual cold atom condensates.△ Less"
Approximation for a Toy Defective Ising Model,Authors:Adom Giffin,"Abstract:It has been previously shown that one can use the ME methodology (Caticha Giffin 2006) to reproduce a mean field solution for a simple fluid (Tseng 2004). One could easily use the case of a simple ferromagnetic material as well. The drawback to the mean field approach is that one must assume that all atoms must all act the same. The problem becomes more tractable when the agents are only allowed t…▽ MoreIt has been previously shown that one can use the ME methodology (Caticha Giffin 2006) to reproduce a mean field solution for a simple fluid (Tseng 2004). One could easily use the case of a simple ferromagnetic material as well. The drawback to the mean field approach is that one must assume that all atoms must all act the same. The problem becomes more tractable when the agents are only allowed to interact with their nearest neighbors and can be in only two possible states. The easiest case being an Ising model. The purpose of this paper is to illustrate the use of the ME method as an approximation tool. The paper show a simple case to compare with the traditional mean field approach. Then we show two examples that lie outside of traditional methodologies. These cases explore a ferromagnetic material with defects. The main result is that regardless of the case, the ME method provides good approximations for each case which would not otherwise be possible or at least well justified.△ Less"
Topos Quantum Logic and Mixed States,Authors:Andreas Doering,"Abstract:The topos approach to the formulation of physical theories includes a new form of quantum logic. We present this topos quantum logic, including some new results, and compare it to standard quantum logic, all with an eye to conceptual issues. In particular, we show that topos quantum logic is distributive, multi-valued, contextual and intuitionistic. It incorporates superposition without being base…▽ MoreThe topos approach to the formulation of physical theories includes a new form of quantum logic. We present this topos quantum logic, including some new results, and compare it to standard quantum logic, all with an eye to conceptual issues. In particular, we show that topos quantum logic is distributive, multi-valued, contextual and intuitionistic. It incorporates superposition without being based on linear structures, has a built-in form of coarse-graining which automatically avoids interpretational problems usually associated with the conjunction of propositions about incompatible physical quantities, and provides a material implication that is lacking from standard quantum logic. Importantly, topos quantum logic comes with a clear geometrical underpinning. The representation of pure states and truth-value assignments are discussed. It is briefly shown how mixed states fit into this approach.△ Less"
Systematic investigation of a family of gradient-dependent functionals for solids,"Authors:Philipp Haas,Fabien Tran,Peter Blaha,Luana S. Pedroza,Antonio J. R. da Silva,Mariana M. Odashima,Klaus Capelle","Abstract:…(PBE) generalized-gradient approximation (GGA), as well as variations of PBE GGA, such as PBEsol and similar functionals, PBE-type functionals employing a tighter Lieb-Oxfordbound, and combinations thereof. Several of these variations are proposed here for the first time. On a test set of 60 solids we perform a system-by-system analysis for selected functio…▽ MoreEleven density functionals are compared with regard to their performance for the lattice constants of solids. We consider standard functionals, such as the local-density approximation and the Perdew-Burke-Ernzerhof (PBE) generalized-gradient approximation (GGA), as well as variations of PBE GGA, such as PBEsol and similar functionals, PBE-type functionals employing a tighter Lieb-Oxfordbound, and combinations thereof. Several of these variations are proposed here for the first time. On a test set of 60 solids we perform a system-by-system analysis for selected functionals and a full statistical analysis for all of them. The impact of restoring the gradient expansion and of tightening the Lieb-Oxfordbound is discussed, and confronted with previous results obtained from other codes, functionals or test sets. No functional is uniformly good for all investigated systems, but surprisingly, and pleasingly, the simplest possible modifications to PBE turn out to have the most beneficial effect on its performance. The atomization energy of molecules was also considered and on a testing set of six molecules, we found that the PBE functional is clearly the best, the others leading to strong overbinding.△ Less"
Resonance Scattering of Waves in Chaotic Systems,"Authors:Y. V. Fyodorov,D. V. Savin","Abstract:This is a brief overview of RMT applications to quantum or wave chaotic resonance scattering, focusing mainly on theoretical results obtained via non-perturbative methods starting from mid-nineties.This is a brief overview of RMT applications to quantum or wave chaotic resonance scattering, focusing mainly on theoretical results obtained via non-perturbative methods starting from mid-nineties.△ Less"
Electromagnetic Non-contact Gears: Prelude,"Authors:Prachi Parashar,Kimball A. Milton,Inés Cavero-Peláez,K. V. Shajesh",Abstract:We calculate the lateral Lifshitz force between corrugated dielectric slabs of finite thickness. Taking the thickness of the plates to infinity leads us to the lateral Lifshitz force between corrugated dielectric surfaces of infinite extent. Taking the dielectric constant to infinity leads us to the conductor limit which has been evaluated earlier in the literature.We calculate the lateral Lifshitz force between corrugated dielectric slabs of finite thickness. Taking the thickness of the plates to infinity leads us to the lateral Lifshitz force between corrugated dielectric surfaces of infinite extent. Taking the dielectric constant to infinity leads us to the conductor limit which has been evaluated earlier in the literature.△ Less
Symmetry Classes,Authors:Martin R. Zirnbauer,"Abstract:Physical systems exhibiting stochastic or chaotic behavior are often amenable to treatment by random matrix models. In deciding on a good choice of model, random matrix physics is constrained and guided by symmetry considerations. The notion of 'symmetry class' (not to be confused with 'universality class') expresses the relevance of symmetries as an organizational principle. Dyson, in his 1962…▽ MorePhysical systems exhibiting stochastic or chaotic behavior are often amenable to treatment by random matrix models. In deciding on a good choice of model, random matrix physics is constrained and guided by symmetry considerations. The notion of 'symmetry class' (not to be confused with 'universality class') expresses the relevance of symmetries as an organizational principle. Dyson, in his 1962 paper referred to as the Threefold Way, gave the prime classification of random matrix ensembles based on a quantum mechanical setting with symmetries. In this article we review Dyson's Threefold Way from a modern perspective. We then describe a minimal extension of Dyson's setting to incorporate the physics of chiral Dirac fermions and disordered superconductors. In this minimally extended setting, where Hilbert space is replaced by Fock space equipped with the anti-unitary operation of particle-hole conjugation, symmetry classes are in one-to-one correspondence with the large families of Riemannian symmetric spaces.△ Less"
Non-Hermitian Random Matrix Ensembles,"Authors:B. A. Khoruzhenko,H. -J. Sommers","Abstract:This is a concise review of the complex, real and quaternion real Ginibre random matrix ensembles and their elliptic deformations. Eigenvalue correlations are exactly reduced to two-point kernels and discussed in the strongly and weakly non-Hermitian limits of large matrix size.This is a concise review of the complex, real and quaternion real Ginibre random matrix ensembles and their elliptic deformations. Eigenvalue correlations are exactly reduced to two-point kernels and discussed in the strongly and weakly non-Hermitian limits of large matrix size.△ Less"
"Chain of matrices, loop equations and topological recursion",Authors:Nicolas Orantin,"Abstract:Random matrices are used in fields as different as the study of multi-orthogonal polynomials or the enumeration of discrete surfaces. Both of them are based on the study of a matrix integral. However, this term can be confusing since the definition of a matrix integral in these two applications is not the same. These two definitions, perturbative and non-perturbative, are discussed in this chapt…▽ MoreRandom matrices are used in fields as different as the study of multi-orthogonal polynomials or the enumeration of discrete surfaces. Both of them are based on the study of a matrix integral. However, this term can be confusing since the definition of a matrix integral in these two applications is not the same. These two definitions, perturbative and non-perturbative, are discussed in this chapter as well as their relation. The so-called loop equations satisfied by integrals over random matrices coupled in chain is discussed as well as their recursive solution in the perturbative case when the matrices are Hermitean.△ Less"
Mapping the SKA Simulated Skies with the S3-Tools,"Authors:F. Levrier,R. J. Wilman,D. Obreschkow,H. -R. Kloeckner,I. Heywood,S. Rawlings","Abstract:…of Python-based routines and interfaces whose purpose is to provide user-friendly access to the SKA Simulated Skies (S3) set of simulations, an effort led by the University ofOxfordin the framework of the European Union's SKADS program (http://www.skads-eu.org). The databases built from the S3 simulations are hosted by the…▽ MoreThe S3-Tools are a set of Python-based routines and interfaces whose purpose is to provide user-friendly access to the SKA Simulated Skies (S3) set of simulations, an effort led by the University ofOxfordin the framework of the European Union's SKADS program (http://www.skads-eu.org). The databases built from the S3 simulations are hosted by theOxforde-Research Center (OeRC), and can be accessed through a web portal at http://s-cubed.physics.ox.ac.uk. This paper focuses on the practical steps involved to make radio images from the S3-SEX and S3-SAX simulations using the S3-Map tool and should be taken as a broad overview. For a more complete description, the interested reader should look up the user's guide. The output images can then be used as input to instrument simulators, e.g. to assess technical designs and observational strategies for the SKA and SKA pathfinders.△ Less"
Einstein energy associated with the Friedmann -Robertson -Walker metric,Authors:Abhas Mitra,"Abstract:…) of an arbitrary system (Tolman, R.C., Phys. Rev., 35(8), 875 (1930); Tolman, R.C., {\it Relativity, Thermodynamics & Cosmology}, Clarendon Press,Oxford, 1962)); Xulu, S.S., arXiv:hep-th/0308070 (2003)). For a static isolated system, in quasi-Cartesian coordinates, this formula leads to the well known result…▽ MoreFollowing Einstein's definition of Lagrangian density and gravitational field energy density (Einstein, A., Ann. Phys. Lpz., 49, 806 (1916); Einstein, A., Phys. Z., 19, 115 (1918); Pauli, W., {\it Theory of Relativity}, B.I. Publications, Mumbai, 1963, Trans. by G. Field), Tolman derived a general formula for the total matter plus gravitational field energy (P_0) of an arbitrary system (Tolman, R.C., Phys. Rev., 35(8), 875 (1930); Tolman, R.C., {\it Relativity, Thermodynamics & Cosmology}, Clarendon Press,Oxford, 1962)); Xulu, S.S., arXiv:hep-th/0308070 (2003)). For a static isolated system, in quasi-Cartesian coordinates, this formula leads to the well known resultP_0 = \int \sqrt{-g} (T_0^0 - T_1^1 -T_2^2 -T_3^3) ~d^3 x, wheregis the determinant of the metric tensor andT^a_bis the energy momentum tensor of the {\em matter}. Though in the literature, this is known as ""Tolman Mass"", it must be realized that this is essentially ""Einstein Mass"" because the underlying pseudo-tensor here is due to Einstein. In fact, Landau -Lifshitz obtained the same expression for the ""inertial mass"" of a static isolated system without using any pseudo-tensor at all and which points to physical significance and correctness of Einstein Mass (Landau, L.D., and Lifshitz, E.M., {\it The Classical Theory of Fields}, Pergamon Press,Oxford, 2th ed., 1962)! For the first time we apply this general formula to find an expression forP_0for the Friedmann- Robertson -Walker (FRW) metric by using the same quasi-Cartesian basis. As we analyze this new result, physically, a spatially flat model having no cosmological constant is suggested. Eventually, it is seen that conservation ofP_0is honoured only in the a static limit.△ Less"
Determinantal point processes,Authors:Alexei Borodin,"Abstract:We present a list of algebraic, combinatorial, and analytic mechanisms that give rise to determinantal point processes.We present a list of algebraic, combinatorial, and analytic mechanisms that give rise to determinantal point processes.△ Less"
Cooling lines as probes of the formation and buildup of galaxies and black holes,"Authors:P. P. van der Werf,M. Spaans","Abstract:We discuss the use of SPICA to study the cosmic history of star formation and accretion by supermassive black holes. The cooling lines, in particular the high-J rotational lines of CO, provide a clear-cut and unique diagnostic for separating the contributions of star formation and AGN accretion to the total infrared luminosity of active, gas-rich galaxies. We briefly review existing efforts for…▽ MoreWe discuss the use of SPICA to study the cosmic history of star formation and accretion by supermassive black holes. The cooling lines, in particular the high-J rotational lines of CO, provide a clear-cut and unique diagnostic for separating the contributions of star formation and AGN accretion to the total infrared luminosity of active, gas-rich galaxies. We briefly review existing efforts for studying high-J CO emission from galaxies at low and high redshift. We finally comment on the detectability of cooling radiation from primordial (very low metallicity) galaxies containing an accreting supermassive black hole with SPICA/SAFARI.△ Less"
Free Probability Theory,Authors:Roland Speicher,"Abstract:Free probability theory was created by Dan Voiculescu around 1985, motivated by his efforts to understand special classes of von Neumann algebras. His discovery in 1991 that also random matrices satisfy asymptotically the freeness relation transformed the theory dramatically. Not only did this yield spectacular results about the structure of operator algebras, but it also brought new concepts an…▽ MoreFree probability theory was created by Dan Voiculescu around 1985, motivated by his efforts to understand special classes of von Neumann algebras. His discovery in 1991 that also random matrices satisfy asymptotically the freeness relation transformed the theory dramatically. Not only did this yield spectacular results about the structure of operator algebras, but it also brought new concepts and tools into the realm of random matrix theory. In the following we will give, mostly from the random matrix point of view, a survey on some of the basic ideas and results of free probability theory.△ Less"
"Time, Topology and the Twin Paradox",Authors:J. -P. Luminet,"Abstract:The twin paradox is the best known thought experiment associated with Einstein's theory of relativity. An astronaut who makes a journey into space in a high-speed rocket will return home to find he has aged less than a twin who stayed on Earth. This result appears puzzling, since the situation seems symmetrical, as the homebody twin can be considered to have done the travelling with respect to t…▽ MoreThe twin paradox is the best known thought experiment associated with Einstein's theory of relativity. An astronaut who makes a journey into space in a high-speed rocket will return home to find he has aged less than a twin who stayed on Earth. This result appears puzzling, since the situation seems symmetrical, as the homebody twin can be considered to have done the travelling with respect to the traveller. Hence it is called a ""paradox"". In fact, there is no contradiction and the apparent paradox has a simple resolution in Special Relativity with infinite flat space. In General Relativity (dealing with gravitational fields and curved space-time), or in a compact space such as the hypersphere or a multiply connected finite space, the paradox is more complicated, but its resolution provides new insights about the structure of spacetime and the limitations of the equivalence between inertial reference frames.△ Less"
Quantum information: primitive notions and quantum correlations,Authors:Valerio Scarani,"Abstract:This series of introductory lectures consists of two parts. In the first part, I rapidly review the basic notions of quantum physics and many primitives of quantum information (i.e. notions that one must be somehow familiar with in the field, like cloning, teleportation, state estimation...). The second part is devoted to a detailed introduction to the topic of quantum correlations, covering the…▽ MoreThis series of introductory lectures consists of two parts. In the first part, I rapidly review the basic notions of quantum physics and many primitives of quantum information (i.e. notions that one must be somehow familiar with in the field, like cloning, teleportation, state estimation...). The second part is devoted to a detailed introduction to the topic of quantum correlations, covering the evidence for failure of alternative theories, some aspects of the formalism of no-signaling probability distributions and some hints towards some current research topics in the field.△ Less"
Handbook Article on Applications of Random Matrix Theory to QCD,Authors:J. J. M. Verbaarschot,"Abstract:In this chapter of theOxfordHandbook of Random Matrix Theory we introduce chiral Random Matrix Theories with the global symmetries of QCD. In the microscopic domain, these theories reproduce the mass and chemical potential dependence of QCD. The main focus of this chapter is on the spectral properties of the QCD Dirac operator and relations between chiral…▽ MoreIn this chapter of theOxfordHandbook of Random Matrix Theory we introduce chiral Random Matrix Theories with the global symmetries of QCD. In the microscopic domain, these theories reproduce the mass and chemical potential dependence of QCD. The main focus of this chapter is on the spectral properties of the QCD Dirac operator and relations between chiral Random Matrix Theories and chiral Lagrangians. Both spectra of the anti-hermitian Dirac operator and spectra of the nonhermitian Dirac operator at nonzero chemical potential are discussed.△ Less"
Financial Applications of Random Matrix Theory: a short review,"Authors:J. P. Bouchaud,M. Potters","Abstract:We discuss the applications of Random Matrix Theory in the context of financial markets and econometric models, a topic about which a considerable number of papers have been devoted to in the last decade. This mini-review is intended to guide the reader through various theoretical results (the Marcenko-Pastur spectrum and its various generalisations, random SVD, free matrices, largest eigenvalue…▽ MoreWe discuss the applications of Random Matrix Theory in the context of financial markets and econometric models, a topic about which a considerable number of papers have been devoted to in the last decade. This mini-review is intended to guide the reader through various theoretical results (the Marcenko-Pastur spectrum and its various generalisations, random SVD, free matrices, largest eigenvalue statistics, etc.) as well as some concrete applications to portfolio optimisation and out-of-sample risk estimation.△ Less"
Simulated CII observations for SPICA/SAFARI,"Authors:F. Levrier,M. Gerin,P. Hennebelle,E. Falgarone,F. Le Petit,J. R. Goicoechea","Abstract:We investigate the case of CII 158 micron observations for SPICA/SAFARI using a three-dimensional magnetohydrodynamical (MHD) simulation of the diffuse interstellar medium (ISM) and the Meudon PDR code. The MHD simulation consists of two converging flows of warm gas (10,000 K) within a cubic box 50 pc in length. The interplay of thermal instability, magnetic field and self-gravity leads to the f…▽ MoreWe investigate the case of CII 158 micron observations for SPICA/SAFARI using a three-dimensional magnetohydrodynamical (MHD) simulation of the diffuse interstellar medium (ISM) and the Meudon PDR code. The MHD simulation consists of two converging flows of warm gas (10,000 K) within a cubic box 50 pc in length. The interplay of thermal instability, magnetic field and self-gravity leads to the formation of cold, dense clumps within a warm, turbulent interclump medium. We sample several clumps along a line of sight through the simulated cube and use them as input density profiles in the Meudon PDR code. This allows us to derive intensity predictions for the CII 158 micron line and provide time estimates for the mapping of a given sky area.△ Less"
Heavy-tailed random matrices,"Authors:Z. Burda,J. Jurkiewicz","Abstract:We discuss non-Gaussian random matrices whose elements are random variables with heavy-tailed probability distributions. In probability theory heavy tails of the distributions describe rare but violent events which usually have dominant influence on the statistics. They also completely change universal properties of eigenvalues and eigenvectors of random matrices. We concentrate here on the univ…▽ MoreWe discuss non-Gaussian random matrices whose elements are random variables with heavy-tailed probability distributions. In probability theory heavy tails of the distributions describe rare but violent events which usually have dominant influence on the statistics. They also completely change universal properties of eigenvalues and eigenvectors of random matrices. We concentrate here on the universal macroscopic properties of (1) Wigner matrices belonging to the Levy basin of attraction, (2) matrices representing stable free random variables and (3) a class of heavy-tailed matrices obtained by parametric deformations of standard ensembles.△ Less"
Spectroscopic Cosmological Surveys in the Far-IR,"Authors:L. Spinoglio,M. Magliocchetti,S. Tommasin,A. M. Di Giorgio,C. Gruppioni,G. De Zotti,A. Franceschini,M. Vaccari,K. Isaak,F. Pozzi,M. A. Malkan","Abstract:We show the feasibility of spectroscopic cosmological surveys with the SAFARI instrument onboard of SPICA. The work is done through simulations that make use of both empirical methods, i.e. the use of observed luminosity functions and theoretical models for galaxy formation and evolution. The relations assumed between the line emission to trace AGN and star formation activity have been derived f…▽ MoreWe show the feasibility of spectroscopic cosmological surveys with the SAFARI instrument onboard of SPICA. The work is done through simulations that make use of both empirical methods, i.e. the use of observed luminosity functions and theoretical models for galaxy formation and evolution. The relations assumed between the line emission to trace AGN and star formation activity have been derived from the observations of local samples of galaxies. The results converge to indicate the use of blind spectroscopy with the SAFARI FTS at various resolutions to study galaxy evolution from the local to the distant (z~3) Universe. Specifically, two different and independent galaxy evolution models predict about 7-10 sources to be spectroscopically detected in more than one line in a 2'x 2'SAFARI field of view, down to the expected flux limits of SAFARI, with about 20% of sources to be detected at z>2. SPICA-SAFARI will be therefore excellent at detecting high-z sources and at assessing in a direct way their nature (e.g whether mainly AGN or Star Formation powered) thanks to blind spectroscopy.△ Less"
Replica Approach in Random Matrix Theory,Authors:Eugene Kanzieper,Abstract:This Chapter outlines the replica approach in Random Matrix Theory. Both fermionic and bosonic versions of the replica limit are introduced and its trickery is discussed. A brief overview of early heuristic treatments of zero-dimensional replica field theories is given to advocate an exact approach to replicas. The latter is presented in two elaborations: by viewing the $β=2$ replica partition f…▽ MoreThis Chapter outlines the replica approach in Random Matrix Theory. Both fermionic and bosonic versions of the replica limit are introduced and its trickery is discussed. A brief overview of early heuristic treatments of zero-dimensional replica field theories is given to advocate an exact approach to replicas. The latter is presented in two elaborations: by viewing the $β=2$ replica partition function as the Toda Lattice and by embedding the replica partition function into a more general theory of $τ$ functions.△ Less
Laws of Large Numbers of Subgraphs in Directed Random Geometric Networks,Authors:Yilun Shang,"Abstract:…such as wireless communication networks. We give some strong laws of large numbers of subgraph counts thus extending those results of Penrose [Random Geometric Graphs,OxfordUniversity Press, 2003].▽ MoreGiven independent random points $\mathcal{X}_n=\{X_1,...,X_n\}$ in $\mathbb{R}^2$, drawn according to some probability density function $f$ on $\mathbb{R}^2$, and a cutoff $r_n>0$ we construct a random geometric digraph $G(\mathcal{X}_n,\mathcal{Y}_n,r_n)$ with vertex set $\mathcal{X}_n$. Each vertex $X_i$ is assigned uniformly at random a sector $S_i$, of central angle $α$ with inclination $Y_i$, in a circle of radius $r_n$ (with vertex $X_i$ as the origin). An arc is present from $X_i$ to $X_j$, if $X_j$ falls in $S_i$. We also introduce another random geometric digraph $G(\mathcal{X}_n,\mathcal{R}_n)$ with vertex set $\mathcal{X}_n=\{X_1,...,X_n\}$ in $\mathbb{R}^d$, $d\ge1$ and an arc present from $X_i$ to $X_j$ if $||X_i-X_j||<R_{n,i}$. Here $\{R_{n,i}\}_{i\ge1}$ are i.i.d. random variables and we may take an arbitrary norm $||\cdot||$. In this paper we investigate two kinds of small subgraphs--induced and isolated--in the above two directed networks, which contribute to understanding the local topology of many spatial networks, such as wireless communication networks. We give some strong laws of large numbers of subgraph counts thus extending those results of Penrose [Random Geometric Graphs,OxfordUniversity Press, 2003].△ Less"
Probing the Space of Toric Quiver Theories,"Authors:Joseph Hewlett,Yang-Hui He","Abstract:We demonstrate a practical and efficient method for generating toric Calabi-Yau quiver theories, applicable to both D3 and M2 brane world-volume physics. A new analytic method is presented at low order parametres and an algorithm for the general case is developed which has polynomial complexity in the number of edges in the quiver. Using this algorithm, carefully implemented, we classify the qui…▽ MoreWe demonstrate a practical and efficient method for generating toric Calabi-Yau quiver theories, applicable to both D3 and M2 brane world-volume physics. A new analytic method is presented at low order parametres and an algorithm for the general case is developed which has polynomial complexity in the number of edges in the quiver. Using this algorithm, carefully implemented, we classify the quiver diagram and assign possible superpotentials for various small values of the number of edges and nodes. We examine some preliminary statistics on this space of toric quiver theories.△ Less"
Peakons,Authors:Darryl D Holm,"Abstract:The peakons discussed here are singular solutions of the dispersionless Camassa-Holm (CH) shallow water wave equation in one spatial dimension. These are reviewed in the context of asymptotic expansions and Euler-Poincaré variational principles. The dispersionless CH equation generalizes to the EPDiff equation, whose singular solutions are peakon wave fronts in higher dimensions. The reduction o…▽ MoreThe peakons discussed here are singular solutions of the dispersionless Camassa-Holm (CH) shallow water wave equation in one spatial dimension. These are reviewed in the context of asymptotic expansions and Euler-Poincaré variational principles. The dispersionless CH equation generalizes to the EPDiff equation, whose singular solutions are peakon wave fronts in higher dimensions. The reduction of these singular solutions of CH and EPDiff to canonical Hamiltonian dynamics on lower dimensional sets may be understood, by realizing that their solution ansatz is a momentum map, and momentum maps are Poisson.△ Less"
Quantifying Rational Belief,Authors:Ariel Caticha,Abstract:Some criticisms that have been raised against the Cox approach to probability theory are addressed. Should we use a single real number to measure a degree of rational belief? Can beliefs be compared? Are the Cox axioms obvious? Are there counterexamples to Cox? Rather than justifying Cox's choice of axioms we follow a different path and derive the sum and product rules of probability theory as t…▽ MoreSome criticisms that have been raised against the Cox approach to probability theory are addressed. Should we use a single real number to measure a degree of rational belief? Can beliefs be compared? Are the Cox axioms obvious? Are there counterexamples to Cox? Rather than justifying Cox's choice of axioms we follow a different path and derive the sum and product rules of probability theory as the unique (up to regraduations) consistent representations of the Boolean AND and OR operations.△ Less
"Feynman graphs, and nerve theorem for compact symmetric multicategories (extended abstract)","Authors:André Joyal,Joachim Kock",Abstract:We describe a category of Feynman graphs and show how it relates to compact symmetric multicategories (coloured modular operads) just as linear orders relate to categories and rooted trees relate to multicategories. More specifically we obtain the following nerve theorem: compact symmetric multicategories can be characterised as presheaves on the category of Feynman graphs subject to a Segal con…▽ MoreWe describe a category of Feynman graphs and show how it relates to compact symmetric multicategories (coloured modular operads) just as linear orders relate to categories and rooted trees relate to multicategories. More specifically we obtain the following nerve theorem: compact symmetric multicategories can be characterised as presheaves on the category of Feynman graphs subject to a Segal condition. This text is a write-up of the second-named author's QPL6 talk; a more detailed account of this material will appear elsewhere.△ Less
Information processing in convex operational theories,"Authors:Howard Barnum,Alexander Wilce","Abstract:In order to understand the source and extent of the greater-than-classical information processing power of quantum systems, one wants to characterize both classical and quantum mechanics as points in a broader space of possible theories. One approach to doing this, pioneered by Abramsky and Coecke, is to abstract the essential categorical features of classical and quantum mechanics that support…▽ MoreIn order to understand the source and extent of the greater-than-classical information processing power of quantum systems, one wants to characterize both classical and quantum mechanics as points in a broader space of possible theories. One approach to doing this, pioneered by Abramsky and Coecke, is to abstract the essential categorical features of classical and quantum mechanics that support various information-theoretic constraints and possibilities, e.g., the impossibility of cloning in the latter, and the possibility of teleportation in both. Another approach, pursued by the authors and various collaborators, is to begin with a very conservative, and in a sense very concrete, generalization of classical probability theory--which is still sufficient to encompass quantum theory--and to ask which ""quantum"" informational phenomena can be reproduced in this much looser setting. In this paper, we review the progress to date in this second programme, and offer some suggestions as to how to link it with the categorical semantics for quantum processes developed by Abramsky and Coecke.△ Less"
Environment or Outflows? New insight into the origin of narrow associated QSO absorbers,Authors:Vivienne Wild,"Abstract:Recent detailed studies of Narrow Absorption Line (NAL) systems in QSO-spectra have revealed that at least 50% of QSOs have NALs associated with the central engine, and in most cases they are found to be outflowing. Will studies of NALs provide the much sort after evidence for ubiquitous QSO feedback that can halt the formation of stars in galaxies? I present new results on the distribution of l…▽ MoreRecent detailed studies of Narrow Absorption Line (NAL) systems in QSO-spectra have revealed that at least 50% of QSOs have NALs associated with the central engine, and in most cases they are found to be outflowing. Will studies of NALs provide the much sort after evidence for ubiquitous QSO feedback that can halt the formation of stars in galaxies? I present new results on the distribution of line-of-sight velocity offsets between MgII absorbers and their background QSOs, based on a large catalogue of absorbers from SDSS DR6 and greatly improved QSO-redshift estimates. My analysis reveals a high-velocity population of MgII NALs extending out to at least 6000 km/s from the QSOs, which cannot be ascribed to the clustering of local galaxies, similar to that observed recently for CIV absorbers. The very existence of such low ionisation gas clouds in the intense ionising field of the QSO suggests that we may indeed be witnessing the mechanical expulsion of gas, alongside the heating previously observed. I also show that there is a significant excess of low-velocity MgII NALs in radio-loud QSOs compared to radio-quiet QSOs. In the near future, improved QSO clustering results will allow us to say whether this is due to environmental or feedback effects.△ Less"
Random matrices and Laplacian growth,Authors:A. Zabrodin,"Abstract:The theory of random matrices with eigenvalues distributed in the complex plane and more general ""beta-ensembles"" (logarithmic gases in 2D) is reviewed. The distribution and correlations of the eigenvalues are investigated in the large N limit. It is shown that in this limit the model is mathematically equivalent to a class of diffusion-controlled growth models for viscous flows in the Hele-Shaw…▽ MoreThe theory of random matrices with eigenvalues distributed in the complex plane and more general ""beta-ensembles"" (logarithmic gases in 2D) is reviewed. The distribution and correlations of the eigenvalues are investigated in the large N limit. It is shown that in this limit the model is mathematically equivalent to a class of diffusion-controlled growth models for viscous flows in the Hele-Shaw cell and other growth processes of Laplacian type. The analytical methods used involve the technique of boundary value problems in two dimensions and elements of the potential theory.△ Less"
From Entropic Dynamics to Quantum Theory,Authors:Ariel Caticha,"Abstract:Non-relativistic quantum theory is derived from information codified into an appropriate statistical model. The basic assumption is that there is an irreducible uncertainty in the location of particles: positions constitute a configuration space and the corresponding probability distributions constitute a statistical manifold. The dynamics follows from a principle of inference, the method of Max…▽ MoreNon-relativistic quantum theory is derived from information codified into an appropriate statistical model. The basic assumption is that there is an irreducible uncertainty in the location of particles: positions constitute a configuration space and the corresponding probability distributions constitute a statistical manifold. The dynamics follows from a principle of inference, the method of Maximum Entropy. The concept of time is introduced as a convenient way to keep track of change. A welcome feature is that the entropic dynamics notion of time incorporates a natural distinction between past and future. The statistical manifold is assumed to be a dynamical entity: its curved and evolving geometry determines the evolution of the particles which, in their turn, react back and determine the evolution of the geometry. Imposing that the dynamics conserve energy leads to the Schroedinger equation and to a natural explanation of its linearity, its unitarity, and of the role of complex numbers. The phase of the wave function is explained as a feature of purely statistical origin. There is a quantum analogue to the gravitational equivalence principle.△ Less"
Spin Currents in Semiconductor Nanostructures: A Nonequilibrium Green-Function Approach,"Authors:Branislav K. Nikolic,Liviu P. Zarbo,Satofumi Souma","Abstract:This chapter of ""TheOxfordHandbook of Nanoscience and Technology: Frontiers and Advances"" reviews nonequilibrium Green function (NEGF) approach to modeling spin current generation, transport, and detection in semiconductor nanostructures containing different types of spin-orbit (SO) couplings. Its tutorial style--with examples drawn from the field…▽ MoreThis chapter of ""TheOxfordHandbook of Nanoscience and Technology: Frontiers and Advances"" reviews nonequilibrium Green function (NEGF) approach to modeling spin current generation, transport, and detection in semiconductor nanostructures containing different types of spin-orbit (SO) couplings. Its tutorial style--with examples drawn from the field of the spin Hall effects (SHEs) and with treatment of the Rashba, Dresselhaus and extrinsic SO couplings--offers practical recipes to compute total spin and charge currents flowing out of the device, as well as the nonequilibrium local spin densities and spin fluxes within the multiterminal nanostructure. These quantities, which are obtained from the knowledge of spin-resolved NEGFs that can describe both ballistic and diffusive transport regimes while handling phase-coherent effects or dephasing mechanisms relevant at room temperature, can be employed to understand recent experiments on all-electrical detection of mesoscopic and quantum SHE in complicated low-dimensional nanostructures, as well as to model a multitude of ""second generation"" spintronic devices exploiting coherent spin dynamics. The chapter also provides extensive coverage of relevant technical and computational details, such as: (i) the construction of retarded and lesser Green functions for SO-coupled nanostructures attached to many electrodes; (ii) computation of self-energies introduced by different types of electrodes attached to the central region; and (iii) accelerated algorithms for NEGF evaluation that make possible spin transport modeling in devices of the size comparable to the spin precession length (typically few hundreds of nanometers) that sets the scale where mesoscopic SHE effect in ballistic SO-coupled nanostructures is expected to reach its optimal magnitude.△ Less"
Entropic Priors and Bayesian Model Selection,"Authors:Brendon J. Brewer,Matthew J. Francis","Abstract:We demonstrate that the principle of maximum relative entropy (ME), used judiciously, can ease the specification of priors in model selection problems. The resulting effect is that models that make sharp predictions are disfavoured, weakening the usual Bayesian ""Occam's Razor"". This is illustrated with a simple example involving what Jaynes called a ""sure thing"" hypothesis. Jaynes' resolution of…▽ MoreWe demonstrate that the principle of maximum relative entropy (ME), used judiciously, can ease the specification of priors in model selection problems. The resulting effect is that models that make sharp predictions are disfavoured, weakening the usual Bayesian ""Occam's Razor"". This is illustrated with a simple example involving what Jaynes called a ""sure thing"" hypothesis. Jaynes' resolution of the situation involved introducing a large number of alternative ""sure thing"" hypotheses that were possible before we observed the data. However, in more complex situations, it may not be possible to explicitly enumerate large numbers of alternatives. The entropic priors formalism produces the desired result without modifying the hypothesis space or requiring explicit enumeration of alternatives; all that is required is a good model for the prior predictive distribution for the data. This idea is illustrated with a simple rigged-lottery example, and we outline how this idea may help to resolve a recent debate amongst cosmologists: is dark energy a cosmological constant, or has it evolved with time in some way? And how shall we decide, when the data are in?△ Less"
A formal proof of the Born rule from decision-theoretic assumptions,Authors:David Wallace,"Abstract:I develop the decision-theoretic approach to quantum probability, originally proposed by David Deutsch, into a mathematically rigorous proof of the Born rule in (Everett-interpreted) quantum mechanics. I sketch the argument informally, then prove it formally, and lastly consider a number of proposed ``counter-examples'' to show exactly which premises of the argument they violate.I develop the decision-theoretic approach to quantum probability, originally proposed by David Deutsch, into a mathematically rigorous proof of the Born rule in (Everett-interpreted) quantum mechanics. I sketch the argument informally, then prove it formally, and lastly consider a number of proposed ``counter-examples'' to show exactly which premises of the argument they violate.△ Less"
"Approximate amenability of Schatten classes, Lipschitz algebras and second duals of Fourier algebras","Authors:Yemon Choi,Fereidoun Ghahramani","Abstract:Amenability of any of the algebras described in the title is known to force them to be finite-dimensional. The analogous problems for \emph{approximate} amenability have been open for some years now. In this article we give a complete solution for the first two classes, using a new criterion for showing that certain Banach algebras without bounded approximate identities cannot be approximately a…▽ MoreAmenability of any of the algebras described in the title is known to force them to be finite-dimensional. The analogous problems for \emph{approximate} amenability have been open for some years now. In this article we give a complete solution for the first two classes, using a new criterion for showing that certain Banach algebras without bounded approximate identities cannot be approximately amenable. The method also provides a unified approach to existing non-approximate amenability results, and is applied to the study of certain commutative Segal algebras.
  Using different techniques, we prove that \emph{bounded} approximate amenability of the second dual of a Fourier algebra implies that it is finite-dimensional. Some other results for related algebras are obtained.△ Less"
A newcomer's guide to zeta functions of groups and rings,Authors:Christopher Voll,"Abstract:These notes grew out of lectures given at the LMS-EPSRC Short Course on Asymptotic Methods in Infinite Group Theory, University ofOxford, 9-14 September 2007, organised by Dan Segal.These notes grew out of lectures given at the LMS-EPSRC Short Course on Asymptotic Methods in Infinite Group Theory, University ofOxford, 9-14 September 2007, organised by Dan Segal.△ Less"
Contraction-free proofs and finitary games for Linear Logic,"Authors:André Hirschowitz,Michel Hirschowitz,Tom Hirschowitz","Abstract:In the standard sequent presentations of Girard's Linear Logic (LL), there are two ""non-decreasing"" rules, where the premises are not smaller than the conclusion, namely the cut and the contraction rules. It is a universal concern to eliminate the cut rule. We show that, using an admissible modification of the tensor rule, contractions can be eliminated, and that cuts can be simultaneously limit…▽ MoreIn the standard sequent presentations of Girard's Linear Logic (LL), there are two ""non-decreasing"" rules, where the premises are not smaller than the conclusion, namely the cut and the contraction rules. It is a universal concern to eliminate the cut rule. We show that, using an admissible modification of the tensor rule, contractions can be eliminated, and that cuts can be simultaneously limited to a single initial occurrence. This view leads to a consistent, but incomplete game model for LL with exponentials, which is finitary, in the sense that each play is finite. The game is based on a set of inference rules which does not enjoy cut elimination. Nevertheless, the cut rule is valid in the model.△ Less"
Many Worlds in Context,Authors:Max Tegmark,"Abstract:Everett's Many-Worlds Interpretation of quantum mechanics is discussed in the context of other physics disputes and other proposed kinds of parallel universes. We find that only a small fraction of the usual objections to Everett's theory are specific to quantum mechanics, and that all of the most controversial issues crop up also in settings that have nothing to do with quantum mechanics.Everett's Many-Worlds Interpretation of quantum mechanics is discussed in the context of other physics disputes and other proposed kinds of parallel universes. We find that only a small fraction of the usual objections to Everett's theory are specific to quantum mechanics, and that all of the most controversial issues crop up also in settings that have nothing to do with quantum mechanics.△ Less"
Liquid crystals and harmonic maps in polyhedral domains,"Authors:A Majumdar,JM Robbins,M Zyskin","Abstract:Unit-vector fields $\nvec$ on a convex polyhedron $P$ subject to tangent boundary conditions provide a simple model of nematic liquid crystals in prototype bistable displays. The equilibrium and metastable configurations correspond to minimisers and local minimisers of the Dirichlet energy, and may be regarded as $S^2$-valued harmonic maps on $P$. We consider unit-vector fields which are continu…▽ MoreUnit-vector fields $\nvec$ on a convex polyhedron $P$ subject to tangent boundary conditions provide a simple model of nematic liquid crystals in prototype bistable displays. The equilibrium and metastable configurations correspond to minimisers and local minimisers of the Dirichlet energy, and may be regarded as $S^2$-valued harmonic maps on $P$. We consider unit-vector fields which are continuous away from the vertices of $P$.
  A lower bound for the infimum Dirichlet energy for a given homotopy class is obtained as a sum of minimal connections between fractional defects at the vertices of $P$. In certain cases, this lower bound can be improved by incorporating certain nonabelian homotopy invariants. For a rectangular prism, upper bounds for the infimum Dirichlet energy are obtained from locally conformal solutions of the Euler-Lagrange equations, with the ratio of the upper and lower bounds bounded independently of homotopy type. However, since the homotopy classes are not weakly closed, the infimum may not be realised; the existence and regularity properties of continuous local minimisers of given homotopy type are open questions. Numerical results suggest that some homotopy classes always contain smooth minimisers, while others may or may not depending on the geometry of $P$. Numerical results modelling a bistable device suggest that the observed nematic configurations may be distinguished topologically.△ Less"
Mass Determination of New Particle States,Authors:Mario Serna,"Abstract:We study theoretical and experimental facets of mass determination of new particle states. Assuming supersymmetry, we update the quark and lepton mass matrices at the grand unification scale accounting for threshold corrections enhanced by large tan beta. From the hypothesis that quark and lepton masses satisfy a classic set of relationships suggested in some Grand Unified Theories (GUTs), we pr…▽ MoreWe study theoretical and experimental facets of mass determination of new particle states. Assuming supersymmetry, we update the quark and lepton mass matrices at the grand unification scale accounting for threshold corrections enhanced by large tan beta. From the hypothesis that quark and lepton masses satisfy a classic set of relationships suggested in some Grand Unified Theories (GUTs), we predict tan beta needs to be large, and the gluino's soft mass needs to have the opposite sign to the wino's soft mass. Existing tools to measure the phase of the gluino's mass at upcoming hadron colliders require model-independent, kinematic techniques to determine the masses of the new supersymmetric particle states. We discuss the current techniques to determine the mass of invisible particles. We review the transverse mass kinematic variable M_{T2} and the use of invariant-mass edges to find relationships between masses. Next, we introduce a new technique to add additional constraints between the masses of new particle states using M_{T2} at different stages in a symmetric decay chain. These new relationships further constrain the mass differences between new particle states, but still leave the absolute mass weakly determined. Next, we introduce the constrained mass variables M_{2C,LB}, M_{2C,UB}, M_{3C,LB}, M_{3C,UB} to provide event-by-event lower-bounds and upper-bounds to the mass scale given mass differences. We demonstrate mass scale determination in realistic case studies of supersymmetry models by fitting ideal distributions to simulated data. We conclude that the techniques introduced in this thesis have precision and accuracy that rival or exceed the best known techniques for invisible-particle mass-determination at hadron colliders.△ Less"
"One world versus many: the inadequacy of Everettian accounts of evolution, probability, and scientific confirmation",Authors:Adrian Kent,"Abstract:There is a compelling intellectual case for exploring whether purely unitary quantum theory defines a sensible and scientifically adequate theory, as Everett originally proposed. Many different and incompatible attempts to define a coherent Everettian quantum theory have been made over the past fifty years. However, no known version of the theory (unadorned by extra ad hoc postulates) can account…▽ MoreThere is a compelling intellectual case for exploring whether purely unitary quantum theory defines a sensible and scientifically adequate theory, as Everett originally proposed. Many different and incompatible attempts to define a coherent Everettian quantum theory have been made over the past fifty years. However, no known version of the theory (unadorned by extra ad hoc postulates) can account for the appearance of probabilities and explain why the theory it was meant to replace, Copenhagen quantum theory, appears to be confirmed, or more generally why our evolutionary history appears to be Born-rule typical. This article reviews some ingenious and interesting recent attempts in this direction by Wallace, Greaves, Myrvold and others, and explains why they don't work. An account of one-world randomness, which appears scientifically satisfactory, and has no many-worlds analogue, is proposed. A fundamental obstacle to confirming many-worlds theories is illustrated by considering some toy many-worlds models. These models show that branch weights can exist without having any role in either rational decision-making or theory confirmation, and also that the latter two roles are logically separate. Wallace's proposed decision theoretic axioms for rational agents in a multiverse and claimed derivation of the Born rule are examined. It is argued that Wallace's strategy of axiomatizing a mathematically precise decision theory within a fuzzy Everettian quasiclassical ontology is incoherent. Moreover, Wallace's axioms are not constitutive of rationality either in Everettian quantum theory or in theories in which branchings and branch weights are precisely defined. In both cases, there exist coherent rational strategies that violate some of the axioms.△ Less"
Applications of random matrix theory to condensed matter and optical physics,Authors:C. W. J. Beenakker,"Abstract:This is a cursory overview of applications of concepts from random matrix theory (RMT) to quantum electronics and classical & quantum optics. The emphasis is on phenomena, predicted or explained by RMT, that have actually been observed in experiments on quantum wires, quantum dots, disordered wave guides, and chaotic resonators. Topics considered include universal conductance fluctuations, weak…▽ MoreThis is a cursory overview of applications of concepts from random matrix theory (RMT) to quantum electronics and classical & quantum optics. The emphasis is on phenomena, predicted or explained by RMT, that have actually been observed in experiments on quantum wires, quantum dots, disordered wave guides, and chaotic resonators. Topics considered include universal conductance fluctuations, weak localization and coherent backscattering, sub-Poissonian shot noise and open transmission channels, non-Gaussian conductance and thermopower distributions, mesoscopic superconductivity, grey-body radiation, and chaotic laser cavities.△ Less"
2D Arrays of Josephson Nanocontacts and Nanogranular Superconductors,Authors:Sergei Sergeenkov,"Abstract:By introducing a realistic model of nanogranular superconductors (NGS) based on 2D arrays of Josephson nanocontacts (created by a network of twin-boundary dislocations with strain fields acting as insulating barriers between hole-rich domains), in this Chapter we present some novel phenomena related to mechanical, magnetic, electric and transport properties of NGS in underdoped single crystals.…▽ MoreBy introducing a realistic model of nanogranular superconductors (NGS) based on 2D arrays of Josephson nanocontacts (created by a network of twin-boundary dislocations with strain fields acting as insulating barriers between hole-rich domains), in this Chapter we present some novel phenomena related to mechanical, magnetic, electric and transport properties of NGS in underdoped single crystals. In particular, we consider chemically induced magnetoelectric effects and flux driven temperature oscillations of thermal expansion coefficient. We also predict a giant enhancement of the nonlinear thermal conductivity of NGS reaching up to 500% when the intrinsically induced chemoelectric field (created by the gradient of the chemical potential due to segregation of hole producing oxygen vacancies) closely matches the externally produced thermoelectric field. The estimates of the model parameters suggest quite an optimistic possibility to experimentally realize these promising and important for applications effects in non-stoichiometric NGS and artificially prepared arrays of Josephson nanocontacts.△ Less"
Non-empirical hyper-generalized-gradient functionals constructed from the Lieb-Oxfordbound,"Authors:Mariana M. Odashima,K. Capelle","Abstract:A simple and completely general representation of the exact exchange-correlation functional of density-functional theory is derived from the universal Lieb-Oxfordbound, which holds for any Coulomb-interacting system. This representation leads to an alternative point of view on popular hybrid functionals, providing a rationale for why they work and how they…▽ MoreA simple and completely general representation of the exact exchange-correlation functional of density-functional theory is derived from the universal Lieb-Oxfordbound, which holds for any Coulomb-interacting system. This representation leads to an alternative point of view on popular hybrid functionals, providing a rationale for why they work and how they can be constructed. A similar representation of the exact correlation functional allows to construct fully non-empirical hyper-generalized-gradient approximations (HGGAs), radically departing from established paradigms of functional construction. Numerical tests of these HGGAs for atomic and molecular correlation energies and molecular atomization energies show that even simple HGGAs match or outperform state-of-the-art correlation functionals currently used in solid-state physics and quantum chemistry.△ Less"
"Positive Logic with Adjoint Modalities: Proof Theory, Semantics and Reasoning about Information","Authors:Mehrnoosh Sadrzadeh,Roy Dyckhoff","Abstract:We consider a simple modal logic whose non-modal part has conjunction and disjunction as connectives and whose modalities come in adjoint pairs, but are not in general closure operators. Despite absence of negation and implication, and of axioms corresponding to the characteristic axioms of (e.g.) T, S4 and S5, such logics are useful, as shown in previous work by Baltag, Coecke and the first aut…▽ MoreWe consider a simple modal logic whose non-modal part has conjunction and disjunction as connectives and whose modalities come in adjoint pairs, but are not in general closure operators. Despite absence of negation and implication, and of axioms corresponding to the characteristic axioms of (e.g.) T, S4 and S5, such logics are useful, as shown in previous work by Baltag, Coecke and the first author, for encoding and reasoning about information and misinformation in multi-agent systems. For such a logic we present an algebraic semantics, using lattices with agent-indexed families of adjoint pairs of operators, and a cut-free sequent calculus. The calculus exploits operators on sequents, in the style of ""nested"" or ""tree-sequent"" calculi; cut-admissibility is shown by constructive syntactic methods. The applicability of the logic is illustrated by reasoning about the muddy children puzzle, for which the calculus is augmented with extra rules to express the facts of the muddy children scenario.△ Less"
"Macroscopic Superpositions, Decoherent Histories and the Emergence of Hydrodynamic Behaviour",Authors:J. J. Halliwell,"Abstract:Macroscopic systems are described most completely by local densities (particle number, momentum and energy) yet the superposition states of such physical variables, indicated by the Everett interpretation, are not observed. In order to explain this, it is argued that histories of local number, momentum and energy density are approximately decoherent when coarse-grained over sufficiently large vo…▽ MoreMacroscopic systems are described most completely by local densities (particle number, momentum and energy) yet the superposition states of such physical variables, indicated by the Everett interpretation, are not observed. In order to explain this, it is argued that histories of local number, momentum and energy density are approximately decoherent when coarse-grained over sufficiently large volumes. Decoherence arises directly from the proximity of these variables to exactly conserved quantities (which are exactly decoherent), and not from environmentally-induced decoherence. We discuss the approach to local equilibrium and the subsequent emergence of hydrodynamic equations for the local densities. The results are general but we focus on a chain of oscillators as a specific example in which explicit calculations may be carried out. We discuss the relationships between environmentally-induced and conservation-induced decoherence and present a unified view of these two mechanisms.△ Less"
Spectrum of a fluid-loaded vibrating plate: the multiple resonance phenomenon,Authors:Pierre-Olivier Mattei,"Abstract:It was recently observed in a numerical study on a high order perturbation method under heavy fluid loading that a loaded vibrating plate results, not only in the classical frequency shift of the in vacuo single resonance (in both the real part because of the fluid added mass and the imaginary part because of energy lost by radiation), but also in an increase in the number of the resonance. As a…▽ MoreIt was recently observed in a numerical study on a high order perturbation method under heavy fluid loading that a loaded vibrating plate results, not only in the classical frequency shift of the in vacuo single resonance (in both the real part because of the fluid added mass and the imaginary part because of energy lost by radiation), but also in an increase in the number of the resonance. As a result of the loading, a single in vacuo resonance of the structure is transformed into a multiple resonance. Here we show that this phenomenon is a refinement of the Sanchez's classical result where it was established, using asymptotic analysis, that in the case of a light loading conditions "" the scattering frequencies of a fluid loaded elastic structure (ie the resonance frequencies) are nearly the real eigenfrequencies of the elastic body alone and the complex scattering frequencies of the fluid with a rigid solid "". A theoretical explanation of the multiple resonances is given using classical results on theory of entire functions. It is established that every single in vacuo resonance of a simply supported rectangular plate is transformed into an infinite number of resonances under fluid-loading condition.△ Less"
Simulation of the Polarized Sky at 1.4 GHz,"Authors:S. P. O'Sullivan,J. M. Stil,A. R. Taylor,R. Ricci,J. K. Grant,K. Shorten","Abstract:…4 GHz. As the basis for our polarization models, we use a semi-empirical simulation of the extragalactic total intensity (Stokes I) continuum sky developed at the University ofOxford(http://scubed.physics.ox.ac.uk) under the European SKA Design Study (SKADS) initiative, and polarization distributions derived from analysis of polarization observations. By c…▽ MoreWe present results from simulations of the extragalactic polarized sky at 1.4 GHz. As the basis for our polarization models, we use a semi-empirical simulation of the extragalactic total intensity (Stokes I) continuum sky developed at the University ofOxford(http://scubed.physics.ox.ac.uk) under the European SKA Design Study (SKADS) initiative, and polarization distributions derived from analysis of polarization observations. By considering a luminosity dependence for the polarization of AGN, we are able to fit the 1.4 GHz polarized source counts derived from the NVSS and the DRAO ELAIS N1 deep field survey down to approximately 1 mJy. This trend is confirmed by analysis of the polarization of a complete sample of bright AGN. We are unable to fit the additional flattening of the polarized source counts from the deepest observations of the ELAIS N1 survey, which go down to ~0.5 mJy. Below 1 mJy in Stokes I at 1.4 GHz, starforming galaxies become an increasingly important fraction of all radio sources. We use a spiral galaxy integrated polarization model to make realistic predictions of the number of polarized sources at microJy levels in polarized flux density and hence, realistic predictions of what the next generation radio telescopes such as ASKAP, other SKA pathfinders and the SKA itself will see.△ Less"
Fixed points of holomorphic transformations of operator balls,"Authors:M. I. Ostrovskii,V. S. Shulman,L. Turowska","Abstract:A new technique for proving fixed point theorems for families of holomorphic transformations of operator balls is developed. One of these theorems is used to show that a bounded representation in a real or complex Hilbert space is orthogonalizable or unitarizable (that is similar to an orthogonal or unitary representation), respectively, provided the representation has an invariant indefinite qu…▽ MoreA new technique for proving fixed point theorems for families of holomorphic transformations of operator balls is developed. One of these theorems is used to show that a bounded representation in a real or complex Hilbert space is orthogonalizable or unitarizable (that is similar to an orthogonal or unitary representation), respectively, provided the representation has an invariant indefinite quadratic form with finitely many negative squares.△ Less"
Statistical Mechanics of the Cosmological Many-body Problem and its Relation to Galaxy Clustering,"Authors:W. C. Saslaw,A. Yang","Abstract:The cosmological many-body problem is effectively an infinite system of gravitationally interacting masses in an expanding universe. Despite the interactions' long-range nature, an analytical theory of statistical mechanics describes the spatial and velocity distribution functions which arise in the quasi-equilibrium conditions that apply to many cosmologies. Consequences of this theory agree we…▽ MoreThe cosmological many-body problem is effectively an infinite system of gravitationally interacting masses in an expanding universe. Despite the interactions' long-range nature, an analytical theory of statistical mechanics describes the spatial and velocity distribution functions which arise in the quasi-equilibrium conditions that apply to many cosmologies. Consequences of this theory agree well with the observed distribution of galaxies. Further consequences such as thermodynamics provide insights into the physical properties of this system, including its robustness to mergers, and its transition from a grand canonical ensemble to a collection of microcanonical ensembles with negative specific heat.△ Less"
Molecular electronics based on self-assembled monolayers,Authors:D. Vuillaume,"Abstract:Since the first measurement of electron tunneling through an organic monolayer in 1971,(Mann and Kuhn, 1971) and the gedanken experiment of a molecular current rectifying diode in 1974,(Aviram and Ratner, 1974) molecular-scale electronics have attracted a growing interest, both for basic science at the nanoscale and for possible applications in nano-electronics. In the first case, molecules are…▽ MoreSince the first measurement of electron tunneling through an organic monolayer in 1971,(Mann and Kuhn, 1971) and the gedanken experiment of a molecular current rectifying diode in 1974,(Aviram and Ratner, 1974) molecular-scale electronics have attracted a growing interest, both for basic science at the nanoscale and for possible applications in nano-electronics. In the first case, molecules are quantum object by nature and their properties can be tailored by chemistry opening avenues for new experiments. In the second case, molecule-based devices are envisioned to complement silicon devices by providing new functions or already existing functions at a simpler process level and at a lower cost by virtue of their self-organization capabilities, moreover, they are not bound to von Neuman architecture and this may open the way to other architectural paradigms. After a brief overview of the nanofabrication of molecular devices, we review in this chapter, the electronic properties of several basic devices, from simple molecules such as molecular tunnel junctions and molecular wires, to more complex ones such as molecular rectifying diodes, molecular switches and memories.△ Less"
Dark Matter,Authors:Jaan Einasto,"Abstract:A review of the development of the concept of dark matter is given. I begin the review with the description of the discovery of the mass paradox in our Galaxy and in clusters of galaxies. In mid 1970s the amount of observational data was sufficient to suggest the presence of a massive and invisible population around galaxies and in clusters of galaxies. The nature of the dark population was not cl…▽ MoreA review of the development of the concept of dark matter is given. I begin the review with the description of the discovery of the mass paradox in our Galaxy and in clusters of galaxies. In mid 1970s the amount of observational data was sufficient to suggest the presence of a massive and invisible population around galaxies and in clusters of galaxies. The nature of the dark population was not clear at that time, but the hypotheses of stellar as well as of gaseous nature of the new population had serious difficulties. These difficulties disappeared when non-baryonic nature of dark matter was suggested in early 1980s. In addition to the presence of Dark Matter, recent observations suggest the presence of Dark Energy, which together with Dark Matter and ordinary baryonic matter makes the total matter/energy density of the Universe equal to the critical cosmological density. There are various hypothesis as for the nature of the dark matter particles, and generally some form of weakly interactive massive particles (WIMPs) are strongly favored. Both Dark Matter and Dark Energy are the greatest challenges for modern physics since their nature is unknown.△ Less"
Loss cone refilling by flyby encounters--A numerical study of massive black holes in galactic centres,Authors:Mimi Zhang,"Abstract:A gap in phase-space, the loss cone (LC), is opened up by a supermassive black hole (MBH) as it disrupts or accretes stars in a galactic centre. If a star enters the LC then, depending on its properties, its interaction with the MBH will either generate a luminous electromagnetic flare or give rise to gravitational radiation, both of which are expected to have directly observable consequences. A…▽ MoreA gap in phase-space, the loss cone (LC), is opened up by a supermassive black hole (MBH) as it disrupts or accretes stars in a galactic centre. If a star enters the LC then, depending on its properties, its interaction with the MBH will either generate a luminous electromagnetic flare or give rise to gravitational radiation, both of which are expected to have directly observable consequences. A thorough understanding of loss-cone refilling mechanisms is important for the prediction of astrophysical quantities, such as rates of tidal disrupting main-sequence stars, rates of capturing compact stellar remnants and timescales of merging binary MBHs. In this thesis, we use N-body simulations to investigate how noise from accreted satellites and other substructures in a galaxy's halo can affect the LC refilling rate.
  Any N-body model suffers from Poisson noise which is similar to, but much stronger than, the two-body diffusion occurring in real galaxies. To lessen this spurious Poisson noise, we apply the idea of importance sampling to develop a new scheme for constructing N-body realizations of a galaxy model, in which interesting regions of phase-space are sampled by many low-mass particles. We use multimass N-body models of galaxies with centrally-embedded MBHs to study the effects of satellite flybys on LC refilling rates. We find that although the flux of stars into the initially emptied LC is enhanced, but the fuelling rate averaged over the entire subhalos is increased by only a factor 3 over the rate one expects from the Poisson noise due the discreteness of the stellar distribution.△ Less"
Tightened Lieb-Oxfordbound for systems of fixed particle number,"Authors:Mariana M. Odashima,K. Capelle,S. B. Trickey","Abstract:The Lieb-Oxfordbound is a constraint upon approximate exchange-correlation functionals. We explore a non-empirical tightening of that bound in both universal and electron-number-dependent form. The test functional is PBE. Regarding both atomization energies (slightly worsened) and bond lengths (slightly bettered), we find the PBE functional to be remarkably…▽ MoreThe Lieb-Oxfordbound is a constraint upon approximate exchange-correlation functionals. We explore a non-empirical tightening of that bound in both universal and electron-number-dependent form. The test functional is PBE. Regarding both atomization energies (slightly worsened) and bond lengths (slightly bettered), we find the PBE functional to be remarkably insensitive to the value of the Lieb-Oxfordbound. This both rationalizes the use of the original Lieb-Oxfordconstant in PBE and suggests that enhancement factors more sensitive to sharpened constraints await discovery.△ Less"
De Broglie-Bohm Pilot-Wave Theory: Many Worlds in Denial?,Authors:Antony Valentini,"Abstract:We reply to claims (by Deutsch, Zeh, Brown and Wallace) that the pilot-wave theory of de Broglie and Bohm is really a many-worlds theory with a superfluous configuration appended to one of the worlds. Assuming that pilot-wave theory does contain an ontological pilot wave (a complex-valued field in configuration space), we show that such claims arise from not interpreting pilot-wave theory on its…▽ MoreWe reply to claims (by Deutsch, Zeh, Brown and Wallace) that the pilot-wave theory of de Broglie and Bohm is really a many-worlds theory with a superfluous configuration appended to one of the worlds. Assuming that pilot-wave theory does contain an ontological pilot wave (a complex-valued field in configuration space), we show that such claims arise from not interpreting pilot-wave theory on its own terms. Specifically, the theory has its own ('subquantum') theory of measurement, and in general describes a 'nonequilibrium' state that violates the Born rule. Furthermore, in realistic models of the classical limit, one does not obtain localised pieces of an ontological pilot wave following alternative macroscopic trajectories: from a de Broglie-Bohm viewpoint, alternative trajectories are merely mathematical and not ontological. Thus, from the perspective of pilot-wave theory itself, many worlds are an illusion. It is further argued that, even leaving pilot-wave theory aside, the theory of many worlds is rooted in the intrinsically unlikely assumption that quantum measurements should be modelled on classical measurements, and is therefore unlikely to be true.△ Less"
Machine learning techniques for astrophysical modelling and photometric redshift estimation of quasars in optical sky surveys,Authors:N. Daniel Kumar,"Abstract:Machine learning techniques are utilised in several areas of astrophysical research today. This dissertation addresses the application of ML techniques to two classes of problems in astrophysics, namely, the analysis of individual astronomical phenomena over time and the automated, simultaneous analysis of thousands of objects in large optical sky surveys. Specifically investigated are (1) techn…▽ MoreMachine learning techniques are utilised in several areas of astrophysical research today. This dissertation addresses the application of ML techniques to two classes of problems in astrophysics, namely, the analysis of individual astronomical phenomena over time and the automated, simultaneous analysis of thousands of objects in large optical sky surveys. Specifically investigated are (1) techniques to approximate the precise orbits of the satellites of Jupiter and Saturn given Earth-based observations as well as (2) techniques to quickly estimate the distances of quasars observed in the Sloan Digital Sky Survey. Learning methods considered include genetic algorithms, particle swarm optimisation, artificial neural networks, and radial basis function networks.
  The first part of this dissertation demonstrates that GAs and PSO can both be efficiently used to model functions that are highly non-linear in several dimensions. It is subsequently demonstrated in the second part that ANNs and RBFNs can be used as effective predictors of spectroscopic redshift given accurate photometry, especially in combination with other learning-based approaches described in the literature. Careful application of these and other ML techniques to problems in astronomy and astrophysics will contribute to a better understanding of stellar evolution, binary star systems, cosmology, and the large-scale structure of the universe.△ Less"
Nanoelectronic Devices: A Unified View,Authors:Supriyo Datta,"Abstract:Nanoscale electronic devices are of great interest for all kinds of applications like switching, energy conversion and sensing. The objective of this chapter, however, is not to discuss specific devices or applications. Rather it is to convey the conceptual framework that has emerged over the last twenty years, which is important not only because of the practical insights it provides into the de…▽ MoreNanoscale electronic devices are of great interest for all kinds of applications like switching, energy conversion and sensing. The objective of this chapter, however, is not to discuss specific devices or applications. Rather it is to convey the conceptual framework that has emerged over the last twenty years, which is important not only because of the practical insights it provides into the design of nanoscale devices, but also because of the conceptual insights it affords regarding the meaning of resistance and the essence of all non-equilibrium phenomena in general. We present a unified description applicable to a wide variety of devices from molecular conductors to carbon nanotubes to silicon transistors covering different transport regimes from the ballistic to the diffusive limit, based on what we call the NEGF-Landauer approach.△ Less"
Genetic Algorithms for multiple objective vehicle routing,Authors:Martin Josef Geiger,"Abstract:…support to the decision maker. The software has proved it's excellence at the finals of the European Academic Software Award EASA, held at the Keble college/ University ofOxford/ Great Britain.▽ MoreThe talk describes a general approach of a genetic algorithm for multiple objective optimization problems. A particular dominance relation between the individuals of the population is used to define a fitness operator, enabling the genetic algorithm to adress even problems with efficient, but convex-dominated alternatives. The algorithm is implemented in a multilingual computer program, solving vehicle routing problems with time windows under multiple objectives. The graphical user interface of the program shows the progress of the genetic algorithm and the main parameters of the approach can be easily modified. In addition to that, the program provides powerful decision support to the decision maker. The software has proved it's excellence at the finals of the European Academic Software Award EASA, held at the Keble college/ University ofOxford/ Great Britain.△ Less"
Yang-Mills Theory in Twistor Space,Authors:Wen Jiang,"Abstract:This thesis carries out a detailed investigation of the action for pure Yang- Mills theory which L. Mason formulated in twistor space. The rich structure of twistor space results in greater gauge freedom compared to the theory in ordinary space-time. One particular gauge choice, the CSW gauge, allows simplifications to be made at both the classical and quantum level.
  The equations of motion ha…▽ MoreThis thesis carries out a detailed investigation of the action for pure Yang- Mills theory which L. Mason formulated in twistor space. The rich structure of twistor space results in greater gauge freedom compared to the theory in ordinary space-time. One particular gauge choice, the CSW gauge, allows simplifications to be made at both the classical and quantum level.
  The equations of motion have an interesting form in the CSW gauge, which suggests a possible solution procedure. This is explored in three special cases. Explicit solutions are found in each case and connections with earlier work are examined. The equations are then reformulated in Minkowski space, in order to deal with an initial-value, rather than boundary-value, problem. An interesting form of the Yang-Mills equation is obtained, for which we propose an iteration procedure.
  The quantum theory is also simplified by adopting the CSW gauge. The Feynman rules are derived and are shown to reproduce the MHV diagram formalism straightforwardly, once LSZ reduction is taken into account. The three-point amplitude missing in the MHV formalism can be recovered in our theory. Finally, relations to Mansfield's canonical transformation approach are elucidated.△ Less"
Heterotic and M-theory Compactifications for String Phenomenology,Authors:Lara B. Anderson,"Abstract:In this thesis, we explore two approaches to string phenomenology. In the first half of the work, we investigate M-theory compactifications on spaces with co-dimension four, orbifold singularities. We construct M-theory on C^2/Z_N by coupling 11-dimensional supergravity to a seven-dimensional Yang-Mills theory located on the orbifold fixed-plane. The resulting action is supersymmetric to leading…▽ MoreIn this thesis, we explore two approaches to string phenomenology. In the first half of the work, we investigate M-theory compactifications on spaces with co-dimension four, orbifold singularities. We construct M-theory on C^2/Z_N by coupling 11-dimensional supergravity to a seven-dimensional Yang-Mills theory located on the orbifold fixed-plane. The resulting action is supersymmetric to leading non-trivial order in the 11-dim Newton constant. We thereby reduce M-theory on a G2 orbifold with C^2/Z_N singularities, explicitly incorporating the additional gauge fields at the singularities. We derive the Kahler potential, gauge-kinetic function and superpotential for the resulting N=1 four-dimensional theory. Blowing-up of the orbifold is described by a Higgs effect and the results are consistent with the corresponding ones obtained for smooth G2 spaces. Further, we consider flux and Wilson lines on singular loci of the G2 space, and discuss the relation to N=4 SYM theory.
  In the second half, we develop an algorithmic framework for E8 x E8 heterotic compactifications with monad bundles. We begin by considering cyclic Calabi-Yau manifolds where we classify positive monad bundles, prove stability, and compute the complete particle spectrum for all bundles. Next, we generalize the construction to bundles on complete intersection Calabi-Yau manifolds. We show that the class of positive monad bundles, subject to the heterotic anomaly condition, is finite (~7000 models). We compute the particle spectrum for these models and develop new techniques for computing the cohomology of line bundles. There are no anti-generations of particles and the spectrum is manifestly moduli-dependent. We further study the slope-stability of positive monad bundles and develop a new method for proving stability of SU(n) vector bundles.△ Less"
Universal Cellular Automata Based on the Collisions of Soft Spheres,Authors:Norman Margolus,"Abstract:Fredkin's Billiard Ball Model (BBM) is a continuous classical mechanical model of computation based on the elastic collisions of identical finite-diameter hard spheres. When the BBM is initialized appropriately, the sequence of states that appear at successive integer time-steps is equivalent to a discrete digital dynamics. Here we discuss some models of computation that are based on the elastic…▽ MoreFredkin's Billiard Ball Model (BBM) is a continuous classical mechanical model of computation based on the elastic collisions of identical finite-diameter hard spheres. When the BBM is initialized appropriately, the sequence of states that appear at successive integer time-steps is equivalent to a discrete digital dynamics. Here we discuss some models of computation that are based on the elastic collisions of identical finite-diameter soft spheres: spheres which are very compressible and hence take an appreciable amount of time to bounce off each other. Because of this extended impact period, these Soft Sphere Models (SSM's) correspond directly to simple lattice gas automata--unlike the fast-impact BBM. Successive time-steps of an SSM lattice gas dynamics can be viewed as integer-time snapshots of a continuous physical dynamics with a finite-range soft-potential interaction. We present both 2D and 3D models of universal CA's of this type, and then discuss spatially-efficient computation using momentum conserving versions of these models (i.e., without fixed mirrors). Finally, we discuss the interpretation of these models as relativistic and as semi-classical systems, and extensions of these models motivated by these interpretations.△ Less"
Higgs Bundles and Geometric Structures on Surfaces,Authors:William M. Goldman,"Abstract:This paper concerns the relationship between locally homogeneous geometric structures on topological surfaces and the moduli of polystable Higgs bundles on Riemann surfaces, due to Hitchin and Simpson. In particular we discuss the uniformization of Riemann surfaces by hyperbolic geometry from this viewpoint, and survey more recent developments in this theory.This paper concerns the relationship between locally homogeneous geometric structures on topological surfaces and the moduli of polystable Higgs bundles on Riemann surfaces, due to Hitchin and Simpson. In particular we discuss the uniformization of Riemann surfaces by hyperbolic geometry from this viewpoint, and survey more recent developments in this theory.△ Less"
More Efficient Purifying scheme via Controlled- Controlled NOT Gate,"Authors:Nasser Metwally,Abdel-Shafy F. Obada",Abstract:A new modified version of theOxfordpurification protocol is proposed. This version is based on the controlled-controlled NOT gate instead of controlled NOT in the original one. Comparisons between the results of the new version and the original and an earlier modification are given. It is found that the new version converges faster and consumes fewer initi…▽ MoreA new modified version of theOxfordpurification protocol is proposed. This version is based on the controlled-controlled NOT gate instead of controlled NOT in the original one. Comparisons between the results of the new version and the original and an earlier modification are given. It is found that the new version converges faster and consumes fewer initial qubit pairs of low fidelity per final qubit pair of high fidelity.△ Less
"Strong approximation methods in group theory, an LMS/EPSRC Short course lecture notes",Authors:Nikolay Nikolov,Abstract:These are the lecture notes for the LMS/EPSRC short course on strong approximation methods in linear groups organized by Dan Segal inOxfordin September 2007.These are the lecture notes for the LMS/EPSRC short course on strong approximation methods in linear groups organized by Dan Segal inOxfordin September 2007.△ Less
On quantum statistics in data analysis,Authors:Dusko Pavlovic,"Abstract:Originally, quantum probability theory was developed to analyze statistical phenomena in quantum systems, where classical probability theory does not apply, because the lattice of measurable sets is not necessarily distributive. On the other hand, it is well known that the lattices of concepts, that arise in data analysis, are in general also non-distributive, albeit for completely different rea…▽ MoreOriginally, quantum probability theory was developed to analyze statistical phenomena in quantum systems, where classical probability theory does not apply, because the lattice of measurable sets is not necessarily distributive. On the other hand, it is well known that the lattices of concepts, that arise in data analysis, are in general also non-distributive, albeit for completely different reasons. In his recent book, van Rijsbergen argues that many of the logical tools developed for quantum systems are also suitable for applications in information retrieval. I explore the mathematical support for this idea on an abstract vector space model, covering several forms of data analysis (information retrieval, data mining, collaborative filtering, formal concept analysis...), and roughly based on an idea from categorical quantum mechanics. It turns out that quantum (i.e., noncommutative) probability distributions arise already in this rudimentary mathematical framework. We show that a Bell-type inequality must be satisfied by the standard similarity measures, if they are used for preference predictions. The fact that already a very general, abstract version of the vector space model yields simple counterexamples for such inequalities seems to be an indicator of a genuine need for quantum statistics in data analysis.△ Less"
Family symmetries and the origin of fermion masses and mixings,Authors:Ivo de Medeiros Varzielas,"Abstract:Family symmetries are possibly the most conservative extension of the Standard Model that attempt explanations of the pattern of fermion masses and mixings. The observed large mixing angles in the lepton sector may be the first signal for the presence of a non-Abelian family symmetry. We investigate the possibilities of simultaneously explaining the observed pattern of masses of the quarks (hier…▽ MoreFamily symmetries are possibly the most conservative extension of the Standard Model that attempt explanations of the pattern of fermion masses and mixings. The observed large mixing angles in the lepton sector may be the first signal for the presence of a non-Abelian family symmetry. We investigate the possibilities of simultaneously explaining the observed pattern of masses of the quarks (hierarchical masses and small mixing angles) and of the leptons (near tri-bi-maximal mixing, thus large mixing angles). We show that such contrasting observations can be achieved naturally via the seesaw mechanism, whether in models with continuous or discrete family symmetries.
  We consider also in some detail the constraints on flavour changing neutral currents arising from introducing a continuous family symmetry. We show that, for a restricted choice of the flavon sector, continuous family symmetries are consistent with even the most conservative limits both for the case of gauge mediated supersymmetry breaking and the case of gravity mediated supersymmetry breaking.△ Less"
Equatorial Imaging with e-MERLIN Including the Chilbolton Antenna,"Authors:Ian Heywood,Hans-Rainer Kloeckner,Steve Rawlings","Abstract:We discuss the equatorial imaging benefits that arise from the addition of the 25-metre dish at Chilbolton to the e-MERLIN array. Its inclusion considerably enhances the capabilities of e-MERLIN on and below the equator. This will become particularly important in the era of ALMA and other upcoming southern hemisphere facilities. We present simulated observations of point sources in the equatoria…▽ MoreWe discuss the equatorial imaging benefits that arise from the addition of the 25-metre dish at Chilbolton to the e-MERLIN array. Its inclusion considerably enhances the capabilities of e-MERLIN on and below the equator. This will become particularly important in the era of ALMA and other upcoming southern hemisphere facilities. We present simulated observations of point sources in the equatorial region of the sky which is the target area for many existing sky surveys. We find that the additional baselines created by the inclusion of the Chilbolton dish favourably adjust the beam shape of e-MERLIN to a more compact and circular shape, with significantly reduced sidelobe structure. Putting aside the benefits of increased collecting area, the modified beam shape has implications for more rapidly reaching a given completeness limit for equatorial surveys.△ Less"
The 6dF Galaxy Survey: a low-redshift benchmark for bulge-dominated galaxies,"Authors:Matthew Colless,Heath Jones,Rob Proctor,Craig Harrison,Lachlan Campbel,Philip Lah","Abstract:The 6dF Galaxy Survey provides a very large sample of galaxies with reliable measurements of Lick line indices and velocity dispersions. This sample can be used to explore the correlations between mass and stellar population parameters such as age, metallicity and [alpha/Fe]. Preliminary results from such an analysis are presented here, and show that age and metallicity are significantly anti-co…▽ MoreThe 6dF Galaxy Survey provides a very large sample of galaxies with reliable measurements of Lick line indices and velocity dispersions. This sample can be used to explore the correlations between mass and stellar population parameters such as age, metallicity and [alpha/Fe]. Preliminary results from such an analysis are presented here, and show that age and metallicity are significantly anti-correlated for both passive and star-forming galaxies. Passive galaxies have strong correlations between mass and metallicity and between age and alpha-element over-abundance, which combine to produce a downsizing relation between age and mass. For old passive galaxies, the different trends of M/L with mass and luminosity in different passbands result from the differential effect of the mass-metallicity relation on the luminosities in each passband. Future work with this sample will examine the Fundamental Plane of bulge-dominated galaxies and the influence of environment on relations between stellar population parameters and mass.△ Less"
Scientific and Philosophical Challenges to Theism,Authors:Don N. Page,"Abstract:Modern science developed within a culture of Judeo-Christian theism, and science and theism have generally supported each other. However, there are certainly areas in both science and religion that puzzle me. Here I outline some puzzles that have arisen for me concerning everlasting life, human free will, divine free will, the simplicity and probability of God, the problem of evil, and the conve…▽ MoreModern science developed within a culture of Judeo-Christian theism, and science and theism have generally supported each other. However, there are certainly areas in both science and religion that puzzle me. Here I outline some puzzles that have arisen for me concerning everlasting life, human free will, divine free will, the simplicity and probability of God, the problem of evil, and the converse problem of elegance.△ Less"
Does God So Love the Multiverse?,Authors:Don N. Page,"Abstract:Monotheistic religions such as Judaism and Christianity affirm that God loves all humans and created them in His image. However, we have learned from Darwin that we were not created separately from other life on earth. Some Christians opposed Darwinian evolution because it undercut certain design arguments for the existence of God. Today there is the growing idea that the fine-tuned constants of…▽ MoreMonotheistic religions such as Judaism and Christianity affirm that God loves all humans and created them in His image. However, we have learned from Darwin that we were not created separately from other life on earth. Some Christians opposed Darwinian evolution because it undercut certain design arguments for the existence of God. Today there is the growing idea that the fine-tuned constants of physics might be explained by a multiverse with very many different sets of constants of physics. Some Christians oppose the multiverse for similarly undercutting other design arguments for the existence of God. However, undercutting one argument does not disprove its conclusion. Here I argue that multiverse ideas, though not automatically a solution to the problems of physics, deserve serious consideration and are not in conflict with Christian theology as I see it.
  Although this paper as a whole is {\it addressed} primarily to Christians in cosmology and others interested in the relation between the multiverse and theism, it should be of {\it interest} to a wider audience. Proper subsets of this paper are addressed to other Christians, to other theists, to other cosmologists, to other scientists, and to others interested in the multiverse and theism.△ Less"
Our Place in a Vast Universe,Authors:Don N. Page,"Abstract:Scientists have measured that what we can see of space is about a billion billion billion billion billion billion billion billion billion (10^81) times the volume of an average human. Inflationary theory suggests that the entirety of space is vastly larger. Quantum theory suggests that there are very many different copies of space of the same basic kind as ours (same laws of physics). String the…▽ MoreScientists have measured that what we can see of space is about a billion billion billion billion billion billion billion billion billion (10^81) times the volume of an average human. Inflationary theory suggests that the entirety of space is vastly larger. Quantum theory suggests that there are very many different copies of space of the same basic kind as ours (same laws of physics). String theory further suggests that there may be many different kinds of space. This whole collection of googolplexes of galaxies within each of googolplexes of different spaces within each of googols of kinds of space makes up an enormously vast universe or multiverse or holocosm. Human beings seem to be an incredibly small part of this universe in terms of physical size. Yet in other ways, we may still be a very significant part of our vast universe.△ Less"
$q$-Analogue of the Dunkl transform on the real line,"Authors:Néji Bettaibi,Rym H. bettaieb","Abstract:In this paper, we consider a $q$-analogue of the Dunkl operator on $\mathbb{R}$, we define and study its associated Fourier transform which is a $q$-analogue of the Dunkl transform. In addition to several properties, we establish an inversion formula and prove a Plancherel theorem for this $q$-Dunkl transform. Next, we study the $q$-Dunkl intertwining operator and its dual via the $q$-analogues…▽ MoreIn this paper, we consider a $q$-analogue of the Dunkl operator on $\mathbb{R}$, we define and study its associated Fourier transform which is a $q$-analogue of the Dunkl transform. In addition to several properties, we establish an inversion formula and prove a Plancherel theorem for this $q$-Dunkl transform. Next, we study the $q$-Dunkl intertwining operator and its dual via the $q$-analogues of the Riemann-Liouville and Weyl transforms. Using this dual intertwining operator, we provide a relation between the $q$-Dunkl transform and the $q^2$-analogue Fourier transform introduced and studied by R. Rubin.△ Less"
Two dogmas about quantum mechanics,"Authors:Jeffrey Bub,Itamar Pitowsky","Abstract:We argue that the intractable part of the measurement problem -- the 'big' measurement problem -- is a pseudo-problem that depends for its legitimacy on the acceptance of two dogmas. The first dogma is John Bell's assertion that measurement should never be introduced as a primitive process in a fundamental mechanical theory like classical or quantum mechanics, but should always be open to a comp…▽ MoreWe argue that the intractable part of the measurement problem -- the 'big' measurement problem -- is a pseudo-problem that depends for its legitimacy on the acceptance of two dogmas. The first dogma is John Bell's assertion that measurement should never be introduced as a primitive process in a fundamental mechanical theory like classical or quantum mechanics, but should always be open to a complete analysis, in principle, of how the individual outcomes come about dynamically. The second dogma is the view that the quantum state has an ontological significance analogous to the significance of the classical state as the 'truthmaker' for propositions about the occurrence and non-occurrence of events, i.e., that the quantum state is a representation of physical reality. We show how both dogmas can be rejected in a realist information-theoretic interpretation of quantum mechanics as an alternative to the Everett interpretation. The Everettian, too, regards the 'big' measurement problem as a pseudo-problem, because the Everettian rejects the assumption that measurements have definite outcomes, in the sense that one particular outcome, as opposed to other possible outcomes, actually occurs in a quantum measurement process. By contrast with the Everettians, we accept that measurements have definite outcomes. By contrast with the Bohmians and the GRW 'collapse' theorists who add structure to the theory and propose dynamical solutions to the 'big' measurement problem, we take the problem to arise from the failure to see the significance of Hilbert space as a new kinematic framework for the physics of an indeterministic universe, in the sense that Hilbert space imposes kinematic (i.e., pre-dynamic) objective probabilistic constraints on correlations between events.△ Less"
Empirical analysis of the Lieb-Oxfordbound in ions and molecules,"Authors:Mariana M. Odashima,K. Capelle","Abstract:…the exchange-correlation energy, inparticular, are important for the construction of improved density functionals. Here we investigate one such universal property -- the Lieb-Oxfordlower bound -- for ionic and molecular systems. In recent work [J. Chem. Phys. 127, 054106 (2007)], we observed that for atoms and electron liquids this bound may be substantiall…▽ MoreUniversal properties of the Coulomb interaction energy apply to all many-electron systems. Bounds on the exchange-correlation energy, inparticular, are important for the construction of improved density functionals. Here we investigate one such universal property -- the Lieb-Oxfordlower bound -- for ionic and molecular systems. In recent work [J. Chem. Phys. 127, 054106 (2007)], we observed that for atoms and electron liquids this bound may be substantially tightened. Calculations for a few ions and molecules suggested the same tendency, but were not conclusive due to the small number of systems considered. Here we extend that analysis to many different families of ions and molecules, and find that for these, too, the bound can be empirically tightened by a similar margin as for atoms and electron liquids. Tightening the Lieb-Oxfordbound will have consequences for the performance of various approximate exchange-correlation functionals.△ Less"
The Conception of Atom in Greek and Indian Physics,Authors:R. H. Narayan,Abstract:This paper contrasts the Greek and Indian conceptions of atom. It is shown that these two are quite different in spirit.This paper contrasts the Greek and Indian conceptions of atom. It is shown that these two are quite different in spirit.△ Less
Ages and metallicities of faint red galaxies in the Shapley Supercluster,"Authors:Russell J. Smith,John R. Lucey,Michael J. Hudson","Abstract:We present results on the stellar populations of 232 quiescent galaxies in the Shapley Supercluster, based on spectroscopy from the AAOmega spectrograph at the AAT. The key characteristic of this survey is its coverage of many low-luminosity objects (sigma ~ 50 km/s), with high signal-to-noise (~45 per Angstrom). Balmer-line age estimates are recovered with ~25% precision even for the faintest s…▽ MoreWe present results on the stellar populations of 232 quiescent galaxies in the Shapley Supercluster, based on spectroscopy from the AAOmega spectrograph at the AAT. The key characteristic of this survey is its coverage of many low-luminosity objects (sigma ~ 50 km/s), with high signal-to-noise (~45 per Angstrom). Balmer-line age estimates are recovered with ~25% precision even for the faintest sample members. We summarize the observations and absorption line data, and present correlations of derived ages and metallicities with mass and luminosity. We highlight the strong correlation between age and alpha-element abundance ratio, and the anti-correlation of age and metallicity at fixed mass, which is shown to extend into the low-luminosity regime.△ Less"
Conformal Field Theory In Four And Six Dimensions,Authors:Edward Witten,Abstract:The goal of these notes is to give a brief explanation of how electric-magnetic duality in four dimensions is related to the existence of an unusual conformal field theory in six dimensions.The goal of these notes is to give a brief explanation of how electric-magnetic duality in four dimensions is related to the existence of an unusual conformal field theory in six dimensions.△ Less
On universally stable elements,"Authors:I. J. Leary,B. Schuster,N. Yagita","Abstract:We show that certain subrings of the cohomology of a finite p-group P may be realised as the images of restriction from suitable virtually free groups. We deduce that the cohomology of P is a finite module for any such subring. Examples include the ring of `universally stable elements' defined by Evens and Priddy, and rings of invariants such as the mod-2 Dickson algebras.We show that certain subrings of the cohomology of a finite p-group P may be realised as the images of restriction from suitable virtually free groups. We deduce that the cohomology of P is a finite module for any such subring. Examples include the ring of `universally stable elements' defined by Evens and Priddy, and rings of invariants such as the mod-2 Dickson algebras.△ Less"
Particle Filters for Multiscale Diffusions,Authors:Anastasia Papavasiliou,Abstract:We consider multiscale stochastic systems that are partially observed at discrete points of the slow time scale. We introduce a particle filter that takes advantage of the multiscale structure of the system to efficiently approximate the optimal filter.We consider multiscale stochastic systems that are partially observed at discrete points of the slow time scale. We introduce a particle filter that takes advantage of the multiscale structure of the system to efficiently approximate the optimal filter.△ Less
The Galactic Bulge: A Review,"Authors:Dante Minniti,Manuela Zoccali","Abstract:The Milky Way is the only galaxy for which we can resolve individual stars at all evolutionary phases, from the Galactic center to the outskirt. The last decade, thanks to the advent of near IR detectors and 8 meter class telescopes, has seen a great progress in the understanding of the Milky Way central region: the bulge. Here we review the most recent results regarding the bulge structure, age…▽ MoreThe Milky Way is the only galaxy for which we can resolve individual stars at all evolutionary phases, from the Galactic center to the outskirt. The last decade, thanks to the advent of near IR detectors and 8 meter class telescopes, has seen a great progress in the understanding of the Milky Way central region: the bulge. Here we review the most recent results regarding the bulge structure, age, kinematics and chemical composition. These results have profound implications for the formation and evolution of the Milky Way and of galaxies in general. This paper provides a summary on our current understanding of the Milky Way bulge, intended mainly for workers on other fields.△ Less"
Ballistic Phase of Self-Interacting Random Walks,"Authors:Dmitry Ioffe,Yvan Velenik","Abstract:We explain a unified approach to a study of ballistic phase for a large family of self-interacting random walks with a drift and self-interacting polymers with an external stretching force. The approach is based on a recent version of the Ornstein-Zernike theory developed in earlier works. It leads to local limit results for various observables (e.g. displacement of the end-point or number of hi…▽ MoreWe explain a unified approach to a study of ballistic phase for a large family of self-interacting random walks with a drift and self-interacting polymers with an external stretching force. The approach is based on a recent version of the Ornstein-Zernike theory developed in earlier works. It leads to local limit results for various observables (e.g. displacement of the end-point or number of hits of a fixed finite pattern) on paths of n-step walks (polymers) on all possible deviation scales from CLT to LD. The class of models, which display ballistic phase in the ""universality class"" discussed in the paper, includes self-avoiding walks, Domb-Joyce model, random walks in an annealed random potential, reinforced polymers and weakly reinforced random walks.△ Less"
Glueballs in Flatland,Authors:Robert W. Johnson Jr,"Abstract:The pure gauge theory in 2+1 dimensions is explored, through both a phenomenological model and a lattice calculation. The Isgur-Paton model is extended to include a curvature term and various mixing mechanisms. The method of inferential statistics is used to extract the parameters of best fit and to compare the likelihoods of the various models when compared to existing lattice data. The convent…▽ MoreThe pure gauge theory in 2+1 dimensions is explored, through both a phenomenological model and a lattice calculation. The Isgur-Paton model is extended to include a curvature term and various mixing mechanisms. The method of inferential statistics is used to extract the parameters of best fit and to compare the likelihoods of the various models when compared to existing lattice data. The conventional assignment of spin 0 to the pseudoscalar state is called into question by the proximity of a spin 4 state in the model, which motivates calculating the mass of the spin 4 state on the lattice. Novel lattice operators are constructed from a matrix of effective Greens functions which attempt to overcome the lattice rotational ambiguities. Correlation functions are presented for the channels with even J, and effective masses extracted. The resulting masses compare well with the extended Isgur-Paton model.△ Less"
The z=0.8 precursors of today's bulges,"Authors:Marc Balcells,Lilian Dominguez-Palmero","Abstract:We study the color structure of disk galaxies in the Groth strip at redshifts 0.1<z<1.2. Our aim is to test formation models in which bulges form before/after the disk. We find smooth color distributions with gentle outward blueing across the galaxy image: bulges are not distinctly redder than their disks; and bulge colors strongly correlate with global colors. The results suggest a roughly coev…▽ MoreWe study the color structure of disk galaxies in the Groth strip at redshifts 0.1<z<1.2. Our aim is to test formation models in which bulges form before/after the disk. We find smooth color distributions with gentle outward blueing across the galaxy image: bulges are not distinctly redder than their disks; and bulge colors strongly correlate with global colors. The results suggest a roughly coeval evolution of bulges and disks. About 50% of the nuclei of galaxies with central light excesses above the outer exponential profile hold passively evolving red populations. The remainder 50% are galaxies with central blue colors similar to their disks. They may be bulges in formation, or the central parts of disks with non-exponential surface brightness profiles.△ Less"
AGN feedback from Jet-ISM/IGM interactions,"Authors:V. Antonuccio-Delogu,J. Silk","Abstract:We study the propagation of relativistic jets originating from AGNs within the Interstellar/Intergalactic Medium of their host galaxies, and use it to build a model for the suppression of stellar formation within the expanding cocoon.We study the propagation of relativistic jets originating from AGNs within the Interstellar/Intergalactic Medium of their host galaxies, and use it to build a model for the suppression of stellar formation within the expanding cocoon.△ Less"
How to form bulges/ellipticals in dark halos as fast as central black holes?,"Authors:HongSheng Zhao,Bing-Xiao Xu,Xue-Bing Wu","Abstract:Gravity is nearly a universal constant in the cusp of an NFW galaxy halo. Inside this external field an isothermal gas sphere will collapse and trigger a starburst if above a critical central pressure. Thus formed spheroidal stellar systems have Sersic-profile and satisfy the Faber-Jackson relation. The process is consistent with observed starbursts. We also recover the M_BH vs. velocity dispers…▽ MoreGravity is nearly a universal constant in the cusp of an NFW galaxy halo. Inside this external field an isothermal gas sphere will collapse and trigger a starburst if above a critical central pressure. Thus formed spheroidal stellar systems have Sersic-profile and satisfy the Faber-Jackson relation. The process is consistent with observed starbursts. We also recover the M_BH vs. velocity dispersion relation, if the gas collapse is regulated or resisted by the feedback from radiation from the central BH.△ Less"
The Vcirc-sigma0 Relation of Galaxies,"Authors:Stephane Courteau,Michael McDonald,Lawrence M. Widrow","Abstract:Courteau et al. (2007a) reported on the dependence of the ratio of a galaxy's maximum circular velocity, Vcirc, to its central velocity dispersion, sigma0, on morphology, or equivalently total light concentration. This Vcirc-sigma0 concentration relation, which involves details about the local and global galaxy physics, poses a fundamental challenge for galaxy structure models. Furthermore, not…▽ MoreCourteau et al. (2007a) reported on the dependence of the ratio of a galaxy's maximum circular velocity, Vcirc, to its central velocity dispersion, sigma0, on morphology, or equivalently total light concentration. This Vcirc-sigma0 concentration relation, which involves details about the local and global galaxy physics, poses a fundamental challenge for galaxy structure models. Furthermore, not only must these models reproduce the Vcirc-sigma0 relation and its various dependences, they must simultaneously match other fundamental scaling relations such as the velocity-size-luminosity and color-luminosity relations of galaxies. We focus here on the interpretation of parameters that enter the Vcirc-sigma0 relation to enable proper data-model comparisons and follow-up studies by galaxy modelers and observers.△ Less"
Adaptive Optics Imaging of Lyman Break Galaxies as Progenitors of Spheroids in the Local Universe,"Authors:M. Akiyama,Y. Minowa,N. Kobayashi,K. Ohta,I. Iwata","Abstract:In order to reveal the stellar mass distribution of z~3 galaxies, we are conducting deep imaging observations of U-dropout Lyman Break Galaxies (LBGs) with Adaptive Optics (AO) systems in K-band, which corresponds to rest-frame V-band of z~3 galaxies. The results of the Subaru intensive-program observations with AO36/NGS/IRCS indicate that 1) the K-band peaks of some of the LBGs brighter than K=…▽ MoreIn order to reveal the stellar mass distribution of z~3 galaxies, we are conducting deep imaging observations of U-dropout Lyman Break Galaxies (LBGs) with Adaptive Optics (AO) systems in K-band, which corresponds to rest-frame V-band of z~3 galaxies. The results of the Subaru intensive-program observations with AO36/NGS/IRCS indicate that 1) the K-band peaks of some of the LBGs brighter than K=22.0 mag show significant offset from those in the optical images, 2) the z~3 Mv* LBGs and serendipitously observed Distant Red Galaxies (DRGs) have flat profiles similar to disk galaxies in the local universe (i.e., Sersic with n<2), and 3) the surface stellar mass densities of the Mv* LBGs are 3-6 times larger than those of disk galaxies at z=0-1. Considering the lack of n>2 systems among the luminous z~3 LBGs and DRGs, and their strong spatial clustering, we infer that the dense n<2 disk-like structures evolve into the n>2 spheroids of nearby galaxies through relaxations due to major merger events.△ Less"
Hochschild homology and cohomology of {\ell}^1({\mathbb Z}_+^k),Authors:Yemon Choi,"Abstract:Building on the recent determination of the simplicial cohomology groups of the convolution algebra ${\ell}^1({\mathbb Z}_+^k)$ [Gourdeau, Lykova, White, 2005] we investigate what can be said for cohomology of this algebra with more general symmetric coefficients. Our approach leads us to a discussion of Harrison homology and cohomology in the context of Banach algebras, and a development of som…▽ MoreBuilding on the recent determination of the simplicial cohomology groups of the convolution algebra ${\ell}^1({\mathbb Z}_+^k)$ [Gourdeau, Lykova, White, 2005] we investigate what can be said for cohomology of this algebra with more general symmetric coefficients. Our approach leads us to a discussion of Harrison homology and cohomology in the context of Banach algebras, and a development of some of its basic features. As an application of our techniques we reprove some known results on second-degree cohomology.△ Less"
Formation and evolution of galactic spheroids by mergers,"Authors:Thorsten Naab,Andreas Burkert,Peter H. Johansson,Roland Jesseit","Abstract:Galactic spheroids can form as a result of galaxy interactions and mergers of disks. Detailed analyses of the photometric properties, the intrinsic orbital structure, the line-of-sight velocity distributions and the kinemetry of simulated merger remnants, which depend critically on the geometry and the gas content of the interacting progenitors, indicate that low and intermediate mass rotating e…▽ MoreGalactic spheroids can form as a result of galaxy interactions and mergers of disks. Detailed analyses of the photometric properties, the intrinsic orbital structure, the line-of-sight velocity distributions and the kinemetry of simulated merger remnants, which depend critically on the geometry and the gas content of the interacting progenitors, indicate that low and intermediate mass rotating ellipticals can form from mergers of disks. The masses and metallicities of all massive ellipticals and the kinematics of some massive non-rotating ellipticals cannot be explained by binary mergers. Thus these galaxies might have formed in a different way.△ Less"
Age and metallicity of the bulges in lenticular galaxies,Authors:Olga K. Sil'chenko,"Abstract:Panoramic spectroscopic data of the sample of 80 nearby lenticular galaxies obtained with the Multi-Pupil Fiber Spectrograph of the 6-m telescope are presented. The SSP-equivalent ages, [Z/H], and [Mg/Fe] are determined through the Lick indices H-beta, Mgb, and <Fe> separately for the nuclei and for the bulges. About a half of the sample contain chemically distinct nuclei, more metal-rich and yo…▽ MorePanoramic spectroscopic data of the sample of 80 nearby lenticular galaxies obtained with the Multi-Pupil Fiber Spectrograph of the 6-m telescope are presented. The SSP-equivalent ages, [Z/H], and [Mg/Fe] are determined through the Lick indices H-beta, Mgb, and <Fe> separately for the nuclei and for the bulges. About a half of the sample contain chemically distinct nuclei, more metal-rich and younger than the bulges. The correlations of the stellar population properties for the nearby S0s are discussed.△ Less"
Kalman Filtering with Equality and Inequality State Constraints,"Authors:Nachi Gupta,Raphael Hauser","Abstract:Both constrained and unconstrained optimization problems regularly appear in recursive tracking problems engineers currently address -- however, constraints are rarely exploited for these applications. We define the Kalman Filter and discuss two different approaches to incorporating constraints. Each of these approaches are first applied to equality constraints and then extended to inequality co…▽ MoreBoth constrained and unconstrained optimization problems regularly appear in recursive tracking problems engineers currently address -- however, constraints are rarely exploited for these applications. We define the Kalman Filter and discuss two different approaches to incorporating constraints. Each of these approaches are first applied to equality constraints and then extended to inequality constraints. We discuss methods for dealing with nonlinear constraints and for constraining the state prediction. Finally, some experiments are provided to indicate the usefulness of such methods.△ Less"
The formation of young B/PS bulges in edge-on barred galaxies,"Authors:H. Wozniak,L. Michel-Dansac",Abstract:We report about the fact that the stellar population that is born in the gas inflowing towards the central regions can be vertically unstable leading to a B/PS feature remarkably bluer that the surrounding bulge. Using new chemodynamical simulations we show that this young population does not remain as flat as the gaseous nuclear disc and buckles out of the plane to form a new boxy bulge. We sho…▽ MoreWe report about the fact that the stellar population that is born in the gas inflowing towards the central regions can be vertically unstable leading to a B/PS feature remarkably bluer that the surrounding bulge. Using new chemodynamical simulations we show that this young population does not remain as flat as the gaseous nuclear disc and buckles out of the plane to form a new boxy bulge. We show that such a young B/PS bulge can be detected in colour maps.△ Less
The Fundamental Plane of Bulges at Intermediate Redshift,"Authors:Lauren A. MacArthur,Richard S. Ellis,Tommaso Treu","Abstract:We report on a new study aimed at understanding the diversity and evolutionary properties of distant galactic bulges in the context of well-established trends for pure spheroidal galaxies. Bulges have been isolated for a sample of 137 spiral galaxies in the GOODS fields within the redshift range 0.1 < z < 1.2. Using proven photometric techniques we determine for each galaxy the characteristic pa…▽ MoreWe report on a new study aimed at understanding the diversity and evolutionary properties of distant galactic bulges in the context of well-established trends for pure spheroidal galaxies. Bulges have been isolated for a sample of 137 spiral galaxies in the GOODS fields within the redshift range 0.1 < z < 1.2. Using proven photometric techniques we determine for each galaxy the characteristic parameters (size, surface brightness, profile shape) in the 4 GOODS-ACS imaging bands of both the disk and bulge components. Using the DEIMOS spectrograph on Keck, precision stellar velocity dispersions were secured for a sizeable fraction of the bulges. This has enabled us to compare the Fundamental Plane of our distant bulges with that of field spheroidal galaxies in a similar redshift range. Bulges in spiral galaxies with a bulge-to-total luminosity ratio (B/T) > 0.2 show very similar patterns of evolution to those seen for low luminosity spheroidals. To first order, their recent mass assembly histories are equivalent.△ Less"
A SINFONI view of circum-nuclear star-forming rings in spiral galaxies,"Authors:J. Falcón-Barroso,T. Boeker,E. Schinnerer,J. H. Knapen,S. Ryder","Abstract:We present near-infrared (H- and K-band) SINFONI integral-field observations of the circumnuclear star formation rings in five nearby spiral galaxies. We made use of the relative intensities of different emission lines (i.e. [FeII], HeI, Brg) to age date the stellar clusters present along the rings. This qualitative, yet robust, method allows us to discriminate between two distinct scenarios tha…▽ MoreWe present near-infrared (H- and K-band) SINFONI integral-field observations of the circumnuclear star formation rings in five nearby spiral galaxies. We made use of the relative intensities of different emission lines (i.e. [FeII], HeI, Brg) to age date the stellar clusters present along the rings. This qualitative, yet robust, method allows us to discriminate between two distinct scenarios that describe how star formation progresses along the rings. Our findings favour a model where star formation is triggered predominantly at the intersection between the bar major axis and the inner Lindblad resonance and then passively evolves as the clusters rotate around the ring ('Pearls on a string' scenario), although models of stochastically distributed star formation ('Popcorn' model) cannot be completely ruled out.△ Less"
Star Formation in Bulges from GALEX,Authors:Sukyoung K. Yi,"Abstract:Early-type galaxies, considered as large bulges, have been found to have had a much-more-than-boring star formation history in recent years by the UV satellite GALEX. The most massive bulges, brightest cluster galaxies, appear to be relatively free of young stars. But smaller bulges, normal ellipticals and lenticulars, often show unambiguous sign of recent star formation in their UV flux. The fr…▽ MoreEarly-type galaxies, considered as large bulges, have been found to have had a much-more-than-boring star formation history in recent years by the UV satellite GALEX. The most massive bulges, brightest cluster galaxies, appear to be relatively free of young stars. But smaller bulges, normal ellipticals and lenticulars, often show unambiguous sign of recent star formation in their UV flux. The fraction of such UV-bright bulges in the volume-limited sample climbs up to the staggering 30%. The bulges of spirals follow similar trends but a larger fraction showing signs of current and recent star formation. The implication on the bulge formation and evolution is discussed.△ Less"
Gaseous Flows in Galaxies,Authors:F. Combes,"Abstract:The gas component plays a major role in the dynamics of spiral galaxies, because of its dissipative character, and its ability to exchange angular momentum with stars in the disk. Due to its small velocity dispersion, it triggers gravitational instabilities, and the corresponding non-axisymmetric patterns produce gravity torques, which mediate these angular momentum exchanges. When a srong bar p…▽ MoreThe gas component plays a major role in the dynamics of spiral galaxies, because of its dissipative character, and its ability to exchange angular momentum with stars in the disk. Due to its small velocity dispersion, it triggers gravitational instabilities, and the corresponding non-axisymmetric patterns produce gravity torques, which mediate these angular momentum exchanges. When a srong bar pattern develops with the same pattern speed all over the disk, only gas inside corotation can flow towards the center. But strong bars are not long lived in presence of gas, and multiple-speed spiral patterns can develop between bar phases, and help the galaxy to accrete external gas flowing from cosmic filaments. The gas is then intermittently driven to the galaxy center, to form nuclear starbursts and fuel an active nucleus. The various time-scales of these gaseous flows are described.△ Less"
Lopsidedness and Sloshing in Centres of Advanced Mergers of Galaxies,"Authors:Chanda J. Jog,Aparna Maybhate","Abstract:We measure the non-axisymmetry in the luminosity distribution in the central few kpc of a sample of advanced mergers of galaxies, by analyzing their 2MASS images. All mergers show a high central asymmetry: the centres of isophotes show a striking sloshing pattern with a spatial variation of upto 30 % within the central 1 kpc; and the Fourier amplitude for lopsidedness (m=1) shows high values upt…▽ MoreWe measure the non-axisymmetry in the luminosity distribution in the central few kpc of a sample of advanced mergers of galaxies, by analyzing their 2MASS images. All mergers show a high central asymmetry: the centres of isophotes show a striking sloshing pattern with a spatial variation of upto 30 % within the central 1 kpc; and the Fourier amplitude for lopsidedness (m=1) shows high values upto 0.2 within the central 5 kpc. The central asymmetry is estimated to be long-lived, lasting for ~ a few Gyr or ~ 100 local dynamical timescales. This will significantly affect the dynamical evolution of this region, by helping fuel the central active galactic nucleus, and also by causing the secular growth of the bulge driven by lopsidedness.△ Less"
"The Structural Parameters of Bulges, Bars and Discs in the Local Universe",Authors:Dimitri Alexei Gadotti,"Abstract:Image decomposition of galaxies is now routinely used to estimate the structural parameters of galactic components. In this work, I address questions on the reliability of this technique. In particular, do bars and AGN need to be taken into account to obtain the structural parameters of bulges and discs? And to what extent can we trust image decomposition when the physical spatial resolution is…▽ MoreImage decomposition of galaxies is now routinely used to estimate the structural parameters of galactic components. In this work, I address questions on the reliability of this technique. In particular, do bars and AGN need to be taken into account to obtain the structural parameters of bulges and discs? And to what extent can we trust image decomposition when the physical spatial resolution is relatively poor? With this aim, I performed multi-component (bar/bulge/disc/AGN) image decomposition of a sample of very nearby galaxies and their artificially redshifted images, and verified the effects of removing the bar and AGN components from the models. Neglecting bars can result in a overestimation of the bulge-to-total luminosity ratio of a factor of two, even if the resolution is low. Similar effects result when bright AGN are not considered in the models, but only when the resolution is high. I also show that the structural parameters of more distant galaxies can in general be reliably retrieved, at least up to the point where the physical spatial resolution is about 1.5 Kpc, but bulge parameters are prone to errors if its effective radius is small compared to the seeing radius, and might suffer from systematic effects. I briefly discuss the consequences of these results to our knowledge of the stellar mass budget in the local universe, and finish by showing preliminary results from a large SDSS sample on the dichotomy between classical and pseudo-bulges.△ Less"
Directed random growth models on the plane,Authors:Timo Seppalainen,"Abstract:This is a brief survey of laws of large numbers, fluctuation results and large deviation principles for asymmetric interacting particle systems that represent moving interfaces on the plane. We discuss the exclusion process, the Hammersley process and the related last-passage growth models.This is a brief survey of laws of large numbers, fluctuation results and large deviation principles for asymmetric interacting particle systems that represent moving interfaces on the plane. We discuss the exclusion process, the Hammersley process and the related last-passage growth models.△ Less"
Random Walks in Random Environments,Authors:L. V. Bogachev,"Abstract:Random walks provide a simple conventional model to describe various transport processes, for example propagation of heat or diffusion of matter through a medium. However, in many practical cases the medium is highly irregular due to defects, impurities, fluctuations etc., and it is natural to model this as random environment. In the random walks context, such models are referred to as Random Wa…▽ MoreRandom walks provide a simple conventional model to describe various transport processes, for example propagation of heat or diffusion of matter through a medium. However, in many practical cases the medium is highly irregular due to defects, impurities, fluctuations etc., and it is natural to model this as random environment. In the random walks context, such models are referred to as Random Walks in Random Environments (RWRE). This is a relatively new chapter in applied probability and physics of disordered systems, initiated in the 1970s. Early interest was motivated by some problems in biology, crystallography and metal physics, but later applications have spread through numerous areas. After 30 years of extensive work, RWRE remain a very active area of research, which has already led to many surprising discoveries. The goal of this article is to give a brief introduction to the beautiful area of RWRE. The principal model to be discussed is a random walk with nearest-neighbor jumps in independent identically distributed (i.i.d.) random environment in one dimension, although we shall also comment on some extensions and generalizations. The focus is on rigorous results; however, heuristics is used freely to motivate the ideas and explain the approaches and proofs. In a few cases, sketches of the proofs have been included, which should help the reader to appreciate the flavor of results and methods.△ Less"
Kinetics of Electrodeposition of Silver and Copper at Template Synthesis of Nanowires,"Authors:P. G. Globa,E. A. Zasavitsky,V. G. Kantser,S. P. Sidelinikova,A. I. Dikusar","Abstract:…deposits was obtained. A degree of pores filling, the morphology and chemical microanalysis were studied on cross-section of the membrane, using TESCAN SEM equipped with anOxfordInstruments INCA Enerqy EDX-system.▽ MoreThe results of investigation of kinetics of nanopores filling into membranes from aluminum oxide (pore diameter - 200 nm, porosity ~ 50%) at electrodeposition of copper and silver are described. It is shown, that at identical quantity of electricity passed through solutions, the degree of pores filling by metal (average thickness of a deposit) is various for copper and silver deposition. Calculated (according Faraday Law) and experimental dependences of deposition rates of these metals on quantity of electricity passed at direct and pulse currents are presented. Galvanodynamic i - v dependences have been obtained at various current scanning rates. The smaller rate of deposition allows to decrease concentration limitations of electrode process and to obtain higher average thickness of metal deposits and higher filling degree. The limiting values of quantity of electricity for direct and pulse currents were determined. The average thickness of silver and copper deposits was obtained. A degree of pores filling, the morphology and chemical microanalysis were studied on cross-section of the membrane, using TESCAN SEM equipped with anOxfordInstruments INCA Enerqy EDX-system.△ Less"
Y(5S): What has been learned and what can be learned,Authors:S. R. Blusk,Abstract:We present recent measurements of B and B^0_s production using data collected on the Y(5S) resonance at CLEO and Belle. We also briefly discuss what can be learned using sufficiently larger data samples in the future.We present recent measurements of B and B^0_s production using data collected on the Y(5S) resonance at CLEO and Belle. We also briefly discuss what can be learned using sufficiently larger data samples in the future.△ Less
How tight is the Lieb-Oxfordbound?,"Authors:Mariana M. Odashima,K. Capelle","Abstract:…constraints on the xc energy are important ingredients in the construction of improved functionals. Here we investigate one such universal property of xc functionals: the Lieb-Oxfordlower bound on the exchange-correlation energy, $E_{xc}[n] \ge -C \int d^3r n^{4/3}$, where $C\leq C_{LO}=1.68$. To this end, we perform a survey of available exact or near-exac…▽ MoreDensity-functional theory requires ever better exchange-correlation (xc) functionals for the ever more precise description of many-body effects on electronic structure. Universal constraints on the xc energy are important ingredients in the construction of improved functionals. Here we investigate one such universal property of xc functionals: the Lieb-Oxfordlower bound on the exchange-correlation energy, $E_{xc}[n] \ge -C \int d^3r n^{4/3}$, where $C\leq C_{LO}=1.68$. To this end, we perform a survey of available exact or near-exact data on xc energies of atoms, ions, molecules, solids, and some model Hamiltonians (the electron liquid, Hooke's atom and the Hubbard model). All physically realistic density distributions investigated are consistent with the tighter limit $C \leq 1$. For large classes of systems one can obtain class-specific (but not fully universal) similar bounds. The Lieb-Oxfordbound with $C_{LO}=1.68$ is a key ingredient in the construction of modern xc functionals, and a substantial change in the prefactor $C$ will have consequences for the performance of these functionals.△ Less"
Biased random walks on combs,"Authors:Tanya M Elliott,John F Wheater","Abstract:We develop rigorous, analytic techniques to study the behaviour of biased random walks on combs. This enables us to calculate exactly the spectral dimension of random comb ensembles for any bias scenario in the teeth or spine. Two specific examples of random comb ensembles are discussed; the random comb with nonzero probability of an infinitely long tooth at each vertex on the spine and the rand…▽ MoreWe develop rigorous, analytic techniques to study the behaviour of biased random walks on combs. This enables us to calculate exactly the spectral dimension of random comb ensembles for any bias scenario in the teeth or spine. Two specific examples of random comb ensembles are discussed; the random comb with nonzero probability of an infinitely long tooth at each vertex on the spine and the random comb with a power law distribution of tooth lengths. We also analyze transport properties along the spine for these probability measures.△ Less"
Topological-Like Features in Diagrammatical Quantum Circuits,"Authors:Yong Zhang,Louis H. Kauffman","Abstract:In this paper, we revisit topological-like features in the extended Temperley--Lieb diagrammatical representation for quantum circuits including the teleportation, dense coding and entanglement swapping. We perform these quantum circuits and derive characteristic equations for them with the help of topological-like operations. Furthermore, we comment on known diagrammatical approaches to quantum…▽ MoreIn this paper, we revisit topological-like features in the extended Temperley--Lieb diagrammatical representation for quantum circuits including the teleportation, dense coding and entanglement swapping. We perform these quantum circuits and derive characteristic equations for them with the help of topological-like operations. Furthermore, we comment on known diagrammatical approaches to quantum information phenomena from the perspectives of both tensor categories and topological quantum field theories. Moreover, we remark on the proposal for categorical quantum physics and information to be described by dagger ribbon categories.△ Less"
On the orientation of Roman towns in Italy,Authors:Giulio Magli,"Abstract:As is well known, several Roman sources report on the existence of a town foundation ritual, inherited from the Etruscans, which allegedly included astronomical references. However, the possible existence of astronomical orientations in the layout of Roman towns has never been tackled in a systematic way. As a first step in this direction, the orientation of virtually all Roman towns in Italy (3…▽ MoreAs is well known, several Roman sources report on the existence of a town foundation ritual, inherited from the Etruscans, which allegedly included astronomical references. However, the possible existence of astronomical orientations in the layout of Roman towns has never been tackled in a systematic way. As a first step in this direction, the orientation of virtually all Roman towns in Italy (38 cities) is studied here. Non-random orientation patterns emerge from these data, aiming at further research in this field.△ Less"
LHCb Upgrade Plans,Authors:F. Muheim,"Abstract:The LHCb experiment will operate for about five years at a luminosity of 2x10^32 cm^-2 s^-1 and plans are to accumulate a data sample of ~10 fb^-1. Here we present the physics programme and detector design for a future high luminosity phase of the LHCb experiment. An upgraded LHCb experiment would operate at ten times the design luminosity, i.e. at ~2x10^33 cm^-2 s^-1 and aims to collect a data…▽ MoreThe LHCb experiment will operate for about five years at a luminosity of 2x10^32 cm^-2 s^-1 and plans are to accumulate a data sample of ~10 fb^-1. Here we present the physics programme and detector design for a future high luminosity phase of the LHCb experiment. An upgraded LHCb experiment would operate at ten times the design luminosity, i.e. at ~2x10^33 cm^-2 s^-1 and aims to collect a data sample of ~100 fb^-1 over five years. This programme would allow the probe of new physics at an unprecedented level. Key measurements include the B^0_s mixing phase phi_s in B^0_s -> J/ψphi and B^0_s -> phi phi decays with a significant sensitivity to the small Standard Model prediction and a very precise measurement of the CKM angle gamma in tree diagram decays. Initial studies of the modified LHCb trigger and detectors are presented. The upgraded LHCb experiment can run with or without an LHC luminosity upgrade.△ Less"
Transition-metal dimers and physical limits on magnetic anisotropy,"Authors:Tor O. Strandberg,Carlo. M. Canali,Allan H. MacDonald","Abstract:…energy. In a giant spin-approximation, this Berry phase makes the effective reversal barrier thicker. [1] Gatteschi, D., Sessoli, R. & Villain, J. Molecular Nanomagnets. (Oxford, New York 2006).▽ MoreRecent advances in nanoscience have raised interest in the minimum bit size required for classical information storage, i.e. for bistability with suppressed quantum tunnelling and energy barriers that exceed ambient temperatures. In the case of magnetic information storage much attention has centred on molecular magnets[1] with bits consisting of ~ 100 atoms, magnetic uniaxial anisotropy energy barriers ~ 50 K, and very slow relaxation at low temperatures. In this article we draw attention to the remarkable magnetic properties of some transition metal dimers which have energy barriers approaching ~ 500 K with only two atoms. The spin dynamics of these ultra small nanomagnets is strongly affected by a Berry phase which arises from quasi-degeneracies at the electronic Highest Occupied Molecular Orbital (HOMO) energy. In a giant spin-approximation, this Berry phase makes the effective reversal barrier thicker. [1] Gatteschi, D., Sessoli, R. & Villain, J. Molecular Nanomagnets. (Oxford, New York 2006).△ Less"
A Framework for Conscious Information Processing,Authors:Balaram Das,"Abstract:This paper exploits the fact that the variability in the inter-spike intervals, in the spike train issuing from a neuron, carries substantial information regarding the input to the neuron. A framework for neuronal information processing is proposed which utilizes the above fact to distinguish phenomenal from non-phenomenal mental representation. In the process, an explanation is offered as to wh…▽ MoreThis paper exploits the fact that the variability in the inter-spike intervals, in the spike train issuing from a neuron, carries substantial information regarding the input to the neuron. A framework for neuronal information processing is proposed which utilizes the above fact to distinguish phenomenal from non-phenomenal mental representation. In the process, an explanation is offered as to what it is, in the nature of conscious mental states, that imparts them a subjective feeling: there is something it is like to be in those mental states. To give empirical support, it is shown how the proposed framework can neatly explain, the delay in eliciting conscious awareness as observed by Libet and the related backwards referral in time.△ Less"
Chevalley groups of type $G_2$ as automorphism groups of loops,Authors:Petr Vojtěchovský,"Abstract:Let $M^*(q)$ be the unique nonassociative finite simple Moufang loop constructed over $GF(q)$. We prove that $Aut(M^*(2))$ is the Chevalley group $G_2(2)$, by extending multiplicative automorphism of $M^*(2)$ into linear automorphisms of the unique split octonion algebra over GF(2). Many of our auxiliary results apply in the general case. In the course of the proof we show that every element of…▽ MoreLet $M^*(q)$ be the unique nonassociative finite simple Moufang loop constructed over $GF(q)$. We prove that $Aut(M^*(2))$ is the Chevalley group $G_2(2)$, by extending multiplicative automorphism of $M^*(2)$ into linear automorphisms of the unique split octonion algebra over GF(2). Many of our auxiliary results apply in the general case. In the course of the proof we show that every element of a split octonion algebra can be written as a sum of two elements of norm one.△ Less"
Penguin pollution estimates relevant for phi_2/alpha extraction,Authors:Jure Zupan,Abstract:A review of methods to extract the standard CKM unitarity triangle angle alpha is provided. The sizes of related theoretical errors are reviewed.A review of methods to extract the standard CKM unitarity triangle angle alpha is provided. The sizes of related theoretical errors are reviewed.△ Less
Beauty 2006 -- Conference Summary and Future Prospects,Authors:Tim Gershon,"Abstract:The status of $B$ physics, CP violation and related measurements at the time of the Beauty 2006 conference are summarized. Particular attention is given to the exciting prospects that lie ahead, at the commencement of the LHC era, and beyond.The status of $B$ physics, CP violation and related measurements at the time of the Beauty 2006 conference are summarized. Particular attention is given to the exciting prospects that lie ahead, at the commencement of the LHC era, and beyond.△ Less"
Status and Commissioning of the CMS Experiment,"Authors:O. Buchmueller,F. -P. Schilling","Abstract:After a brief overview of the Compact Muon Solenoid (CMS) experiment, the status of construction and installation is described in the first part of the note. The second part of the document is devoted to a discussion of the general commissioning strategy of the CMS experiment, with a particular emphasis on trigger, calibration and alignment. Aspects of b-physics, as well as examples for early ph…▽ MoreAfter a brief overview of the Compact Muon Solenoid (CMS) experiment, the status of construction and installation is described in the first part of the note. The second part of the document is devoted to a discussion of the general commissioning strategy of the CMS experiment, with a particular emphasis on trigger, calibration and alignment. Aspects of b-physics, as well as examples for early physics with CMS are also presented. CMS will be ready for data taking in time for the first collisions in the Large Hadron Collider (LHC) at CERN in late 2007.△ Less"
Charm Leptonic and Semileptonic Decays,Authors:Peter Zweber,Abstract:Experimental results for the pseudoscalar decay constants f_{D} and f_{D_s} are reviewed. Semileptonic form factor results from D -> (pseudoscalar) l nu and D -> (vector) l nu decays are also reviewed.Experimental results for the pseudoscalar decay constants f_{D} and f_{D_s} are reviewed. Semileptonic form factor results from D -> (pseudoscalar) l nu and D -> (vector) l nu decays are also reviewed.△ Less
Imaging algorithms in radio interferometry,"Authors:R. J. Sault,T. A. Oosterloo","Abstract:The paper reviews progress in imaging in radio interferometry for the period 1993-1996. Unlike an optical telescope, the basic measurements of a radio interferometer (correlations between antennas) are indirectly related to a sky brightness image. In a real sense, algorithms and computers are the lenses of a radio interferometer. In the last 20 years, whereas interferometer hardware advances hav…▽ MoreThe paper reviews progress in imaging in radio interferometry for the period 1993-1996. Unlike an optical telescope, the basic measurements of a radio interferometer (correlations between antennas) are indirectly related to a sky brightness image. In a real sense, algorithms and computers are the lenses of a radio interferometer. In the last 20 years, whereas interferometer hardware advances have resulted in improvements of a factor of a few, algorithm and computer advances have resulted in orders of magnitude improvement in image quality. Developing these algorithms has been a fruitful and comparatively inexpensive method of improving the performance of existing telescopes, and has made some newer telescopes possible. In this paper, we review recent developments in the algorithms used in the imaging part of the reduction process. What constitutes an `imaging algorithm'? Whereas once there was a steady `forward' progression in the reduction process of editing, calibrating, transforming and, finally, deconvolving, this is no longer true. The introduction of techniques such as self-calibration, and algorithms that go directly from visibilities to final images, have made the dividing lines less clear. Although we briefly consider self-calibration, for the purposes of this paper calibration issues are generally excluded. Most attention will be directed to the steps which form final images from the calibrated visibilities.△ Less"
M-Theory on Manifolds with G_2 Holonomy,Authors:Adam B. Barrett,"Abstract:We study M-theory on G_2 holonomy spaces that are constructed by dividing a seven-torus by some discrete symmetry group. We classify possible group elements that may be used in this construction and use them to find a set of possible orbifold groups that lead to co-dimension four singularities. We describe how to blow up such singularities, and then derive the moduli Kaehler potential for M-theo…▽ MoreWe study M-theory on G_2 holonomy spaces that are constructed by dividing a seven-torus by some discrete symmetry group. We classify possible group elements that may be used in this construction and use them to find a set of possible orbifold groups that lead to co-dimension four singularities. We describe how to blow up such singularities, and then derive the moduli Kaehler potential for M-theory on the resulting class of G_2 manifolds. To consider the singular limit it is necessary to derive the supergravity action for M-theory on the orbifold C^2/Z_N. We do this by coupling 11-dimensional supergravity to a seven-dimensional Yang-Mills theory located on the orbifold fixed plane. We show that the resulting action is supersymmetric to leading non-trivial order in the 11-dimensional Newton constant. Obtaining this action enables us to then reduce M-theory on a toroidal G_2 orbifold with co-dimension four singularities, taking explicitly into account the additional gauge fields at the singularities. The four-dimensional effective theory has N=1 supersymmetry with non-Abelian N=4 gauge theory sub-sectors. We present explicit formulae for the Kaehler potential, gauge-kinetic function and superpotential. In the four-dimensional theory, blowing-up of the orbifold is described by continuation along D-flat directions. Using this interpretation, we demonstrate consistency of our results for singular G_2 spaces with corresponding ones obtained for smooth G_2 spaces. In addition, we consider the effects of switching on flux and Wilson lines on singular loci of the G_2 space, and we discuss the relation to N=4 SYM theory.△ Less"
Hadronic B decays,Authors:M. Beneke,"Abstract:I briefly summarize the factorization approach to hadronic B decays emphasizing theoretical results that have become available recently. The discussion of its application to data is abridged, and only the determination of gamma=(71\pm 5) degrees from time-dependent CP asymmetries is included in some detail.I briefly summarize the factorization approach to hadronic B decays emphasizing theoretical results that have become available recently. The discussion of its application to data is abridged, and only the determination of gamma=(71\pm 5) degrees from time-dependent CP asymmetries is included in some detail.△ Less"
B_d and B_s mixing: mass and width differences and CP violation,Authors:Ulrich Nierste,"Abstract:B-B-bar mixing involves three physical parameters: the magnitudes of the off-diagonal elements of the mass and decay matrices and their relative phase. They are related to the mass and width differences between the mass eigenstates and to the CP asymmetry in flavour-specific decays, a_fs. Introducing a new operator basis I present new, more precise theory predictions for the width differences in…▽ MoreB-B-bar mixing involves three physical parameters: the magnitudes of the off-diagonal elements of the mass and decay matrices and their relative phase. They are related to the mass and width differences between the mass eigenstates and to the CP asymmetry in flavour-specific decays, a_fs. Introducing a new operator basis I present new, more precise theory predictions for the width differences in the B_s and B_d systems: in the Standard Model one finds Delta Gamma_s = 0.088 +/- 0.017 ps^{-1} and Delta Gamma_d = (26.7 +5.8/-6.5) * 10^{-4} ps^{-1}. Updates of the mass differences Delta M_d and Delta M_s and of a_{fs}^d and a_{fs}^s are also presented. Then I discuss how various present and future measurements can be combined to constrain new physics. The extraction of a new CP phase phi_s^Delta from data on a_{fs}^s also profits from our new operator basis. Confronting our new formulae with D0 data we find that sin phi_s^Delta deviates from zero by 2 sigma.△ Less"
Rare Decays as a Probe for New Physics,Authors:Tobias Hurth,"Abstract:We discuss the indirect search for new degrees of freedom beyond the standard model, within flavour physics. In particular, we analyse the minimal flavour violation hypothesis and its phenomenological implications, especially the large-tan beta scenario in supersymmetric models, and also compare it with the constrained minimal flavour violation scenario. Moreover, we briefly discuss some recent…▽ MoreWe discuss the indirect search for new degrees of freedom beyond the standard model, within flavour physics. In particular, we analyse the minimal flavour violation hypothesis and its phenomenological implications, especially the large-tan beta scenario in supersymmetric models, and also compare it with the constrained minimal flavour violation scenario. Moreover, we briefly discuss some recent progress in inclusive b to s transitions and present a status report of the so-called K-pi puzzle.△ Less"
Precision Determination of $|V_{ub}|$,Authors:Gil Paz,"Abstract:The last two years have seen an impressive improvement in the determination of $|V_{ub}|$, especially from inclusive decays. The error on $|V_{ub}|$ measured with inclusive decays was reduced from 18% (PDG 2004) to 8% (PDG 2006). This progress is a result of combined experimental and theoretical efforts. In this talk, the theoretical framework (BLNP) that enabled such progress is reviewed, as we…▽ MoreThe last two years have seen an impressive improvement in the determination of $|V_{ub}|$, especially from inclusive decays. The error on $|V_{ub}|$ measured with inclusive decays was reduced from 18% (PDG 2004) to 8% (PDG 2006). This progress is a result of combined experimental and theoretical efforts. In this talk, the theoretical framework (BLNP) that enabled such progress is reviewed, as well as other approaches to an inclusive determination of $|V_{ub}|$ (DGE, $M_X-q^2$ etc.). The prospects of improving $|V_{ub}|$ are discussed, addressing issues of weak annihilation, implications of leptonic B decays, and determination of $|V_{ub}|$ with exclusive decays.△ Less"
Branching fractions and direct CP asymmetries of charmless decay modes at the Tevatron,Authors:Michael Morello,Abstract:We present new CDF results on the branching fractions and time-integrated direct CP asymmetries for $B^{0}$ and $B^{0}_{s}$ decay modes into pairs of charmless charged hadrons (pion or kaon). The data set for this update amounts to 1 fb$^{-1}$ of $\bar{p}p$ collisions at $\sqrt{s}=1.96 \rm{TeV}$. We report the first observation of the $B^{0}_{s} \to K^{-}π^{+}$ mode and a measurement of its bran…▽ MoreWe present new CDF results on the branching fractions and time-integrated direct CP asymmetries for $B^{0}$ and $B^{0}_{s}$ decay modes into pairs of charmless charged hadrons (pion or kaon). The data set for this update amounts to 1 fb$^{-1}$ of $\bar{p}p$ collisions at $\sqrt{s}=1.96 \rm{TeV}$. We report the first observation of the $B^{0}_{s} \to K^{-}π^{+}$ mode and a measurement of its branching fraction and direct CP asymmetry. We also observe for the first time two charmless decays of $b$-baryon: $Λ^{0}_{b} \to pπ^{-}$ and $Λ^{0}_{b} \to pK^{-}$.△ Less
The Super Flavor Factory,Authors:A. J. Bevan,"Abstract:The main physics goals of a high luminosity e+e- flavor factory are discussed, including the possibilities to perform detailed studies of the CKM mechanism of quark mixing, and constrain virtual Higgs and non-standard model particle contributions to the dynamics of rare B_u,d,s decays. The large samples of $D$ mesons and tau leptons produced at a flavor factory will result in improved sensitivit…▽ MoreThe main physics goals of a high luminosity e+e- flavor factory are discussed, including the possibilities to perform detailed studies of the CKM mechanism of quark mixing, and constrain virtual Higgs and non-standard model particle contributions to the dynamics of rare B_u,d,s decays. The large samples of $D$ mesons and tau leptons produced at a flavor factory will result in improved sensitivities on D mixing and lepton flavor violation searches, respectively. One can also test fundamental concepts such as lepton universality to much greater precision than existing constraints and improve the precision on tests of CPT from B meson decays. Recent developments in accelerator physics have demonstrated the feasibility to build an accelerator that can achieve luminosities of O(10^36) cm^-2 s^-1.△ Less"
Measuring Dark Energy with the Wide-Field Multi-Object Spectrograph (WFMOS),Authors:Robert Nichol,"Abstract:Dark energy is one of the greatest scientific challenges of the 21st century. One of the key questions facing cosmologists is whether dark energy is either a breakdown of General Relativity on large scales or a new form of matter in the Universe with a negative effective pressure. This question can only be answered through a suite of different observations as a function of redshift. In this pape…▽ MoreDark energy is one of the greatest scientific challenges of the 21st century. One of the key questions facing cosmologists is whether dark energy is either a breakdown of General Relativity on large scales or a new form of matter in the Universe with a negative effective pressure. This question can only be answered through a suite of different observations as a function of redshift. In this paper, I briefly review various dark energy reports published in the last year, which all highlight the importance of the baryon acoustic oscillations (BAO) for probing the ""dark physics"" of the Universe. I also summarize the recent measurements of the BAO in large galaxy redshift surveys. I then look forward to a new instrument planned by the Subaru and Gemini communities called the ""Wide-Field Multi-Object Spectrograph"" (WFMOS) for the Subaru telescope. The baseline design of this facility includes ~4500 spectroscopic fibers over a field-of-view of 1.5 degree diameter, covering a wavelength range of 0.39 to 1 microns. The instrument is schedule for first-light early next decade and will perform massive spectroscopic surveys of both distant galaxies and faint stars in our own Galaxy. The WFMOS dark energy surveys will deliver ~1% errors on the angular-diameter distance and Hubble parameter to high redshift. WFMOS will also be a unique user-facility allowing astronomers to address a host of astrophysical problems like galaxy evolution, the intergalactic medium and calibrate photometric redshifts. The WFMOS archive will also provide a rich resource for further ancillary science much like the present-day SDSS archive.△ Less"
Metrics of Special Curvature with Symmetry,Authors:Brandon Dammerman,"Abstract:Various curvature conditions are studied on metrics admitting a symmetry group. We begin by examining a method of diagonalizing cohomogeneity-one Einstein manifolds and determine when this method can and cannot be used. Examples, including the well-known Stenzel metrics, are discussed. Next, we present a simplification of the Einstein condition on a compact four manifold with $T^{2}$-isometry to…▽ MoreVarious curvature conditions are studied on metrics admitting a symmetry group. We begin by examining a method of diagonalizing cohomogeneity-one Einstein manifolds and determine when this method can and cannot be used. Examples, including the well-known Stenzel metrics, are discussed. Next, we present a simplification of the Einstein condition on a compact four manifold with $T^{2}$-isometry to a system of second-order elliptic equations in two-variables with well-defined boundary conditions. We then study the Einstein and extremal Kahler conditions on Kahler toric manifolds. After constructing explicitly new extremal Kahler and constant scalar curvature metrics, we demonstrate how these metrics can be obtained by continuously deforming the Fubini-Study metric on complex projective space in dimension three. We also define a generalization of Kahler toric manifolds, which we call fiberwise Kahler toric manifolds, and construct new explicit extremal Kahler and constant scalar curvature metrics on both compact and non-compact manifolds in all even dimensions. We also calculate the Futaki invariant on manifolds of this type. After describing an Hermitian non-Kahler analogue to fiberwise Kahler toric geometry, we construct constant scalar curvature Hermitian metrics with $J$-invariant Riemannian tensor. In dimension four, we write down explicitly new constant scalar curvature Hermitian metrics with $J$-invariant Ricci tensor. Finally, we integrate the scalar curvature equation on a large class of cohomogeneity-one metrics.△ Less"
LOFAR - Opening up a new window on the Universe,"Authors:H. J. A. Rottgering,R. Braun,P. D. Barthel,M. P. van Haarlem,G. K. Miley,R. Morganti,I. Snellen,H. Falcke,A. G. de Bruyn,R. B. Stappers,W. H. W. M. Boland,H. R. Butcher,E. J. de Geus,L. Koopmans,R. Fender,J. Kuijpers,R. T. Schilizzi,C. Vogt,R. A. M. J. Wijers,M. Wise,W. N. Brouw,J. P. Hamaker,J. E. Noordam,T. Oosterloo,L. Bahren, et al. (9 additional authors not shown)","Abstract:LOFAR, the Low Frequency Array, is a next-generation radio telescope that is being built in Northern Europe and expected to be fully operational at the end of this decade. It will operate at frequencies from 15 to 240 MHz (corresponding to wavelengths of 20 to 1.2 m). Its superb sensitivity, high angular resolution, large field of view and flexible spectroscopic capabilities will represent a dra…▽ MoreLOFAR, the Low Frequency Array, is a next-generation radio telescope that is being built in Northern Europe and expected to be fully operational at the end of this decade. It will operate at frequencies from 15 to 240 MHz (corresponding to wavelengths of 20 to 1.2 m). Its superb sensitivity, high angular resolution, large field of view and flexible spectroscopic capabilities will represent a dramatic improvement over previous facilities at these wavelengths. As such, LOFAR will carry out a broad range of fundamental astrophysical studies.
  The design of LOFAR has been driven by four fundamental astrophysical applications: (i) The Epoch of Reionisation, (ii) Extragalactic Surveys and their exploitation to study the formation and evolution of clusters, galaxies and black holes, (iii) Transient Sources and their association with high energy objects such as gamma ray bursts, and (iv) Cosmic Ray showers and their exploitation to study the origin of ultra-high energy cosmic rays. In this conference the foreseen LOFAR work on the epoch of reionisation has been covered by de Bruyn and on cosmic ray showers by Falcke.
  During this contribution we will first present the LOFAR project with an emphasis on the challenges faced when carrying out sensitive imaging at low radio frequencies. Subsequently, we will discuss LOFAR's capabilities to survey the low-frequency radio sky. Main aims for the planned surveys are studies of z>6 radio galaxies, diffuse emission associated with distant clusters and starbursting galaxies at z>2.△ Less"
Toric Geometry and String Theory,Authors:Vincent Bouchard,"Abstract:In this thesis we probe various interactions between toric geometry and string theory. First, the notion of a top was introduced by Candelas and Font as a useful tool to investigate string dualities. These objects torically encode the local geometry of a degeneration of an elliptic fibration. We classify all tops and give a prescription for assigning an affine, possibly twisted Kac-Moody algebra…▽ MoreIn this thesis we probe various interactions between toric geometry and string theory. First, the notion of a top was introduced by Candelas and Font as a useful tool to investigate string dualities. These objects torically encode the local geometry of a degeneration of an elliptic fibration. We classify all tops and give a prescription for assigning an affine, possibly twisted Kac-Moody algebra to any such top. Tops related to twisted Kac-Moody algebras can be used to construct string compactifications with reduced rank of the gauge group. Secondly, we compute all loop closed and open topological string amplitudes on orientifolds of toric Calabi-Yau threefolds, by using geometric transitions involving SO/Sp Chern-Simons theory, localization on the moduli space of holomorphic maps with involution, and the topological vertex. In particular, we count Klein bottles and projective planes with any number of handles in some Calabi-Yau orientifolds. We determine the BPS structure of the amplitudes, and illustrate our general results in various examples with and without D-branes. We also present an application of our results to the BPS structure of the coloured Kauffman polynomial of knots.
  This thesis is based on hep-th/0303218 (with H. Skarke), hep-th/0405083 and hep-th/0411227 (with B. Florea and M. Marino).△ Less"
A new method of detecting high-redshift clusters,"Authors:Caroline van Breukelen,Lee Clewley,David Bonfield",Abstract:We present a new cluster-finding algorithm based on a combination of the Voronoi Tessellation and Friends-Of-Friends methods. The algorithm utilises probability distribution functions derived from a photometric redshift analysis. We test our algorithm on a set of simulated cluster-catalogues and have published elsewhere its employment on UKIDSS Ultra Deep Survey infrared J and K data combined wi…▽ MoreWe present a new cluster-finding algorithm based on a combination of the Voronoi Tessellation and Friends-Of-Friends methods. The algorithm utilises probability distribution functions derived from a photometric redshift analysis. We test our algorithm on a set of simulated cluster-catalogues and have published elsewhere its employment on UKIDSS Ultra Deep Survey infrared J and K data combined with 3.6 micro-m and 4.5 micro-m Spitzer bands and optical BVRi'z' imaging from the Subaru Telescope. This pilot study has detected clusters over 0.5 square degrees in the Subaru XMM-Newton Deep Field. The resulting cluster catalogue contains 13 clusters at redshifts 0.61 <= z <= 1.39 with luminosities 10 L* <~ L_tot <~ 50 L*.△ Less
Exact Solutions for Models of Cultural Transmission and Network Rewiring,"Authors:T. S. Evans,A. D. K. Plato","Abstract:We look at the evolution through rewiring of the degree distribution of a network so the number edges is constant. This is exactly equivalent to the evolution of probability distributions in models of cultural transmission with drift and innovation, or models of homogeneity in genes in the presence of mutation. We show that the mean field equations in the literature are incomplete and provide th…▽ MoreWe look at the evolution through rewiring of the degree distribution of a network so the number edges is constant. This is exactly equivalent to the evolution of probability distributions in models of cultural transmission with drift and innovation, or models of homogeneity in genes in the presence of mutation. We show that the mean field equations in the literature are incomplete and provide the full equations. We then give an exact solution for both their long time solution and for their approach to equilibrium. Numerical results show these are excellent approximations and confirm the characteristic simple inverse power law distributions with a large scale cutoff under certain conditions. The alternative is that we reach a completely homogeneous solution. We consider how such processes may arise in practice, using a recent Minority Game study as an example.△ Less"
Random Dynamical Systems,Authors:Vitor Araujo,"Abstract:The concept of random dynamical system is a comparatively recent development combining ideas and methods from the well developed areas of probability theory and dynamical systems.
  Due to our inaccurate knowledge of the particular physical system or due to computational or theoretical limitations (lack of sufficient computational power, inefficient algorithms or insufficiently developed mathema…▽ MoreThe concept of random dynamical system is a comparatively recent development combining ideas and methods from the well developed areas of probability theory and dynamical systems.
  Due to our inaccurate knowledge of the particular physical system or due to computational or theoretical limitations (lack of sufficient computational power, inefficient algorithms or insufficiently developed mathematical or physical theory, for example), the mathematical models never correspond exactly to the phenomenon they are meant to model. Moreover when considering practical systems we cannot avoid either external noise or measurement or inaccuracy errors, so every realistic mathematical model should allow for small errors along orbits not to disturb too much the long term behavior. To be able to cope with unavoidable uncertainty about the ``correct'' parameter values, observed initial states and even the specific mathematical formulation involved, we let randomness be embedded within the model to begin with.
  We present the most basic classes of models in what follows, then define the general concept and present some developments and examples of applications.△ Less"
The distribution of integers with at least two divisors in a short interval,"Authors:Kevin Ford,Gerald Tenenbaum","Abstract:Let H(x,y,z) be the number of integers $\le x$ with a divisor in (y,z] and let H_1(x,y,z) be the number of integers $\le x$ with exactly one such divisor. When y and z are close, it is expected that H_1(x,y,z) H(x,y,z), that is, an integer with a divisor in (y,z] usually has just one. We determine necessary and sufficient conditions on y and z so that H_1(x,y,z) H(x,y,z). In doing so, we answer…▽ MoreLet H(x,y,z) be the number of integers $\le x$ with a divisor in (y,z] and let H_1(x,y,z) be the number of integers $\le x$ with exactly one such divisor. When y and z are close, it is expected that H_1(x,y,z) H(x,y,z), that is, an integer with a divisor in (y,z] usually has just one. We determine necessary and sufficient conditions on y and z so that H_1(x,y,z) H(x,y,z). In doing so, we answer an open question from the paper ""The distribution of integers with a divisor in a given interval"", math.NT/0401223.△ Less"
Critical Integrated Raman Scattering Intensity near the cubic-tetragonal phase transition in Strontium Titanate,Authors:Jyoti Dhar Sharma,"Abstract:Emphasizing the contribution of Professor Roger A Cowley, FRS to the Theory of Raman Scattering from crystals, the development of the Theory of Raman cattering since 1928 has been briefly discussed. Some experimental studies of Strontium Titanate using Inelastic Neutron Scattering, Raman Scattering, Electro- paramagnetic resonance measurement and X-ray & Gamma Ray techniques has been briefly dis…▽ MoreEmphasizing the contribution of Professor Roger A Cowley, FRS to the Theory of Raman Scattering from crystals, the development of the Theory of Raman cattering since 1928 has been briefly discussed. Some experimental studies of Strontium Titanate using Inelastic Neutron Scattering, Raman Scattering, Electro- paramagnetic resonance measurement and X-ray & Gamma Ray techniques has been briefly discussed. Using Schwabl's semi-phenomenological theory for the soft mode and central peak, we have developed (a) a one-phonon Green's function exhibiting the three peaked structure and (b) a two phonon Green's function involving one hard mode under damped quasiharmonic phonon and one three peaked soft-mode phonon. We have developed the pre-cursor order induced Raman scattering near the displacive phase transition in terms of Green's functions. Using Group Theory, we have predicted the Raman-active modes in Strontium Titanate contributing to Critical Raman Scattering near hard-mode frequencies above and below critical temperature. We have calculated the Critical Integrated Raman Scattering Intensity and the Two-phonon Background Raman Scattering Intensity near hard-mode frequencies above and below the critical temperature. The results show the same trends as observed in some of the experimental observations.△ Less"
Probing dark energy with future surveys,Authors:Roberto Trotta,"Abstract:I review the observational prospects to constrain the equation of state parameter of dark energy and I discuss the potential of future imaging and redshift surveys.
  Bayesian model selection is used to address the question of the level of accuracy on the equation of state parameter that is required before explanations alternative to a cosmological constant become very implausible. I discuss res…▽ MoreI review the observational prospects to constrain the equation of state parameter of dark energy and I discuss the potential of future imaging and redshift surveys.
  Bayesian model selection is used to address the question of the level of accuracy on the equation of state parameter that is required before explanations alternative to a cosmological constant become very implausible. I discuss results in the prediction space of dark energy models. If no significant departure from w=-1 is detected, a precision on w of order 1% will translate into strong evidence against fluid-like dark energy, while decisive evidence will require a precision of order 10^-3.△ Less"
Cosmic Rays: Recent Progress and some Current Questions,Authors:A. M. Hillas,"Abstract:Recent progress suggests we are moving towards a quantitative understanding of the whole cosmic ray spectrum, and that many bumps due to different components and processes hide beneath a relatively smooth total flux between knee and ankle. The knee is much better understood: the KASCADE observations support a rather sharp rigidity cut-off; while theoretical developments (strong magnetic field ge…▽ MoreRecent progress suggests we are moving towards a quantitative understanding of the whole cosmic ray spectrum, and that many bumps due to different components and processes hide beneath a relatively smooth total flux between knee and ankle. The knee is much better understood: the KASCADE observations support a rather sharp rigidity cut-off; while theoretical developments (strong magnetic field generation) indicate that supernova remnants (SNR) of different types should indeed accelerate to a very similar rigidity. X-ray and TeV observations of shell-type SNR produce evidence in favour of acceleration at their outer boundaries. There is some still-disputed evidence that the transition to extragalactic cosmic rays has already occurred just above 10**17 eV, unmarked by an ""ankle"", in which case the whole spectrum can be well described by adding a single power-law source spectrum from many extragalactic sources (but modified by energy losses) to the SNR pre-knee component, if the sources photodisintegrate all nuclei. At the highest energy, the experiments using fluorscence light to calibrate energy do not yet show any conflict with an expected GZK ""termination"". GRBs seem not to make a significant contribution.△ Less"
Yang-Baxter Equations,"Authors:Jacques H. H. Perk,Helen Au-Yang","Abstract:Preprint of an Encyclopedia article (Encyclopedia of Mathematical Physics, eds. J.-P. Françoise, G.L. Naber and Tsou S.T.,Oxford: Elsevier, 2006 (ISBN 978-0-1251-2666-3), volume 5, pages 465-473) extended with an appendix on relations of electric networks and solvable models. We welcome comments, especially on the added appendix. Please, note that the refer…▽ MorePreprint of an Encyclopedia article (Encyclopedia of Mathematical Physics, eds. J.-P. Françoise, G.L. Naber and Tsou S.T.,Oxford: Elsevier, 2006 (ISBN 978-0-1251-2666-3), volume 5, pages 465-473) extended with an appendix on relations of electric networks and solvable models. We welcome comments, especially on the added appendix. Please, note that the reference list has been restricted to only giving a few historical items and sources explicitly used. We apologize in advance for the many omissions.△ Less"
Einstein-Cartan Theory,Authors:Andrzej Trautman,"Abstract:The Einstein--Cartan Theory (ECT) of gravity is a modification of General Relativity Theory (GRT), allowing space-time to have torsion, in addition to curvature, and relating torsion to the density of intrinsic angular momentum. This modification was put forward in 1922 by Elie Cartan, before the discovery of spin. Cartan was influenced by the work of the Cosserat brothers (1909), who considered…▽ MoreThe Einstein--Cartan Theory (ECT) of gravity is a modification of General Relativity Theory (GRT), allowing space-time to have torsion, in addition to curvature, and relating torsion to the density of intrinsic angular momentum. This modification was put forward in 1922 by Elie Cartan, before the discovery of spin. Cartan was influenced by the work of the Cosserat brothers (1909), who considered besides an (asymmetric) force stress tensor also a moments stress tensor in a suitably generalized continuous medium.△ Less"
Bias-Free Estimation in Multicomponent Maximum Likelihood Fits with Component-Dependent Templates,"Authors:P. Catastini,G. Punzi","Abstract:The possibility of strong biases in a multicomponent Maximum Likelihood fits with component-dependent templates has been demonstrated in some toy problems. We discuss here in detail a problem of practical interest, particle identification based on time-of-flight or dE/dx information. We show that large biases can occur in estimating particle fractions in a sample if differences between the momen…▽ MoreThe possibility of strong biases in a multicomponent Maximum Likelihood fits with component-dependent templates has been demonstrated in some toy problems. We discuss here in detail a problem of practical interest, particle identification based on time-of-flight or dE/dx information. We show that large biases can occur in estimating particle fractions in a sample if differences between the momentum spectra of particles are ignored, and we present a more robust fit technique, allowing bias-free estimation even when the particle spectra in the sample are unknown.△ Less"
Perspective alignment in spatial language,"Authors:L. Steels,M. Loetzsch","Abstract:It is well known that perspective alignment plays a major role in the planning and interpretation of spatial language. In order to understand the role of perspective alignment and the cognitive processes involved, we have made precise complete cognitive models of situated embodied agents that self-organise a communication system for dialoging about the position and movement of real world objects…▽ MoreIt is well known that perspective alignment plays a major role in the planning and interpretation of spatial language. In order to understand the role of perspective alignment and the cognitive processes involved, we have made precise complete cognitive models of situated embodied agents that self-organise a communication system for dialoging about the position and movement of real world objects in their immediate surroundings. We show in a series of robotic experiments which cognitive mechanisms are necessary and sufficient to achieve successful spatial language and why and how perspective alignment can take place, either implicitly or based on explicit marking.△ Less"
Chemically Induced Dynamic Nuclear Polarization of 19F Nuclei,Authors:Ilya Kuprov,"Abstract:This study explores, both theoretically and experimentally, the photochemically induced dynamic nuclear polarization (photo-CIDNP) of 19F nuclei, the associated spin relaxation, cross-relaxation and cross-correlation effects, as well as potential applications of 19F CIDNP to protein structure and folding problems. Keywords: CIDNP, relaxation, fluorine, NMR, NOE, TR-CIDNP, tc5b, Trp-cage, GFPThis study explores, both theoretically and experimentally, the photochemically induced dynamic nuclear polarization (photo-CIDNP) of 19F nuclei, the associated spin relaxation, cross-relaxation and cross-correlation effects, as well as potential applications of 19F CIDNP to protein structure and folding problems. Keywords: CIDNP, relaxation, fluorine, NMR, NOE, TR-CIDNP, tc5b, Trp-cage, GFP△ Less"
Pseudoriemannian Nilpotent Lie Groups,Authors:Phillip E. Parker,"Abstract:…with a limited list of references (as required by the publisher) which appears in the Encyclopedia of Mathematical Physics, eds. J.-P. Francoise, G.L. Naber and Tsou S.T.Oxford: Elsevier, 2006. vol.4, pp.94--104.▽ MoreThis is a survey article with a limited list of references (as required by the publisher) which appears in the Encyclopedia of Mathematical Physics, eds. J.-P. Francoise, G.L. Naber and Tsou S.T.Oxford: Elsevier, 2006. vol.4, pp.94--104.△ Less"
Einstein equations: exact solutions,Authors:Jiri Bicak,"Abstract:In Einstein's general relativity, with its nonlinear field equations, the discoveries and analyzes of various specific explicit solutions made a great impact on understanding many of the unforeseen features of the theory. Some solutions found fundamental applications in astrophysics, cosmology and, more recently, in the developments inspired by string theory. In this short article we survey the…▽ MoreIn Einstein's general relativity, with its nonlinear field equations, the discoveries and analyzes of various specific explicit solutions made a great impact on understanding many of the unforeseen features of the theory. Some solutions found fundamental applications in astrophysics, cosmology and, more recently, in the developments inspired by string theory. In this short article we survey the invariant characterization and classification of the solutions and describe the properties and role of the most relevant classes: Minkowski, (anti-)de Sitter spacetimes, spherical Schwarzschild and Reissner-Nordstroem metrics, stationary axisymmetric solutions, radiative metrics describing plane and cylindrical waves, radiative fields of uniformly accelerated sources and Robinson-Trautman solutions. Metrics representing regions of spacetimes filled with matter are also discussed and cosmological models are very briefly mentioned. Some parts of the text are based on a detailed survey which appeared in gr-qc/0004016 (see Ref. 2).△ Less"
Explaining Leibniz-equivalence as difference of non-inertial appearances: dis-solution of the Hole Argument and physical individuation of point-events,"Authors:Luca Lusanna,Massimo Pauri","Abstract:""The last remnant of physical objectivity of space-time"" is disclosed in the case of a continuous family of spatially non-compact models of general relativity (GR). The {\it physical individuation} of point-events is furnished by the intrinsic degrees of freedom of the gravitational field, (viz, the {\it Dirac observables}) that represent - as it were - the {\it ontic} part of the metric field.…▽ More""The last remnant of physical objectivity of space-time"" is disclosed in the case of a continuous family of spatially non-compact models of general relativity (GR). The {\it physical individuation} of point-events is furnished by the intrinsic degrees of freedom of the gravitational field, (viz, the {\it Dirac observables}) that represent - as it were - the {\it ontic} part of the metric field. The physical role of the {\it epistemic} part (viz. the {\it gauge} variables) is likewise clarified as emboding the unavoidable non-inertial aspects of GR. At the end the philosophical import of the {\it Hole Argument} is substantially weakened and in fact the Argument itself dis-solved, while a specific four-dimensional {\it holistic and structuralist} view of space-time, (called {\it point-structuralism}), emerges, including elements common to the tradition of both {\it substantivalism} and {\it relationism}. The observables of our models undergo real {\it temporal change}: this gives new evidence to the fact that statements like the {\it frozen-time} character of evolution, as other ontological claims about GR, are {\it model dependent}. \medskip Forthcoming in Studies in History and Philosophy of Modern Physics△ Less"
Complexity and Philosophy,"Authors:Francis Heylighen,Paul Cilliers,Carlos Gershenson","Abstract:The science of complexity is based on a new way of thinking that stands in sharp contrast to the philosophy underlying Newtonian science, which is based on reductionism, determinism, and objective knowledge. This paper reviews the historical development of this new world view, focusing on its philosophical foundations. Determinism was challenged by quantum mechanics and chaos theory. Systems the…▽ MoreThe science of complexity is based on a new way of thinking that stands in sharp contrast to the philosophy underlying Newtonian science, which is based on reductionism, determinism, and objective knowledge. This paper reviews the historical development of this new world view, focusing on its philosophical foundations. Determinism was challenged by quantum mechanics and chaos theory. Systems theory replaced reductionism by a scientifically based holism. Cybernetics and postmodern social science showed that knowledge is intrinsically subjective. These developments are being integrated under the header of ""complexity science"". Its central paradigm is the multi-agent system. Agents are intrinsically subjective and uncertain about their environment and future, but out of their local interactions, a global organization emerges. Although different philosophers, and in particular the postmodernists, have voiced similar ideas, the paradigm of complexity still needs to be fully assimilated by philosophy. This will throw a new light on old philosophical issues such as relativism, ethics and the role of the subject.△ Less"
Walker's theorem without coordinates,"Authors:Andrzej Derdzinski,Witold Roter","Abstract:We provide a coordinate-free version of the local classification, due to A. G. Walker [Quart. J. Math.Oxford(2) 1, 69 (1950)], of null parallel distributions on pseudo-Riemannian manifolds. The underlying manifold is realized, locally, as the total space of a fibre bundle, each fibre of which is an affine principal bundle over a pseudo-Riemannian manifold.…▽ MoreWe provide a coordinate-free version of the local classification, due to A. G. Walker [Quart. J. Math.Oxford(2) 1, 69 (1950)], of null parallel distributions on pseudo-Riemannian manifolds. The underlying manifold is realized, locally, as the total space of a fibre bundle, each fibre of which is an affine principal bundle over a pseudo-Riemannian manifold. All structures just named are naturally determined by the distribution and the metric, in contrast with the non-canonical choice of coordinates in the usual formulation of Walker's theorem.△ Less"
Classification of the Chiral Z_2 x Z_2 Heterotic String Models,Authors:S. E. M. Nooij,"Abstract:This thesis provides a classification of the chiral content of the heterotic $\mathbbm{Z}_2 \times \mathbbm{Z}_2$ orbifold models. We show that the chiral content of the heterotic $\mathbbm{Z}_2 \times \mathbbm{Z}_2$ orbifold models at any point in the moduli space can be described by a free fermionic model. We present a direct translation between the orbifold formulation and the free fermionic…▽ MoreThis thesis provides a classification of the chiral content of the heterotic $\mathbbm{Z}_2 \times \mathbbm{Z}_2$ orbifold models. We show that the chiral content of the heterotic $\mathbbm{Z}_2 \times \mathbbm{Z}_2$ orbifold models at any point in the moduli space can be described by a free fermionic model. We present a direct translation between the orbifold formulation and the free fermionic construction. We use the free fermionic description for the classification wherein we consider orbifolds with symmetric shifts.
  We show that perturbative three generation models are not obtained in the case of $\mathbbm{Z}_2 \times \mathbbm{Z}_2$ orbifolds with symmetric shifts on complex tori, and that the perturbative three generation models in this class necessarily employ an asymmetric shift. We show that the freedom in the modular invariant phases in the $N = 1$ vacua that control the chiral content, can be interpreted as vacuum expectation values of background fields of the underlying $N = 4$ theory, whose dynamical components are projected out by the $\mathbbm{Z}_2$ fermionic projections. In this class of vacua the chiral content of the models is determined by the underlying $N = 4$ mother theory.△ Less"
Quantum Cosmology,Authors:Martin Bojowald,"Abstract:Quantum cosmology in general denotes the application of quantum physics to the whole universe and thus gives rise to many realizations and examples, covering problems at different mathematical and conceptual levels. It is related to quantum gravity and more specifically describes the application to cosmological situations rather than the construction and analysis of quantum field equations. As t…▽ MoreQuantum cosmology in general denotes the application of quantum physics to the whole universe and thus gives rise to many realizations and examples, covering problems at different mathematical and conceptual levels. It is related to quantum gravity and more specifically describes the application to cosmological situations rather than the construction and analysis of quantum field equations. As there are several different approaches to quantum gravity, equations for quantum cosmology are not unique. Most investigations have been performed in the context of canonical quantization, where Wheeler--DeWitt like equations are the prime object. Applications are mostly conceptual, ranging from possible resolutions of classical singularities and explanations of the uniqueness of the universe to the origin of seeds for a classical world and its initial conditions.△ Less"
sPlot: A Quick Introduction,Authors:Muriel Pivk,"Abstract:The paper advocates the use of a statistical tool dedicated to the exploration of data samples populated by several sources of events. This new technique, called sPlot, is able to unfold the contributions of the different sources to the distribution of a data sample in a given variable. The sPlot tool applies in the context of a Likelihood fit which is performed on the data sample to determine t…▽ MoreThe paper advocates the use of a statistical tool dedicated to the exploration of data samples populated by several sources of events. This new technique, called sPlot, is able to unfold the contributions of the different sources to the distribution of a data sample in a given variable. The sPlot tool applies in the context of a Likelihood fit which is performed on the data sample to determine the yields of the various sources.△ Less"
Existence and uniqueness of solutions for a nonlocal parabolic thermistor-type problem,"Authors:Abderrahmane El Hachimi,Moulay Rchid Sidi Ammi,Delfim F. M. Torres",Abstract:In this paper we prove existence and uniqueness of solutions to a nonlocal parabolic problem which generalizes the electric heating problem of a conducting body.In this paper we prove existence and uniqueness of solutions to a nonlocal parabolic problem which generalizes the electric heating problem of a conducting body.△ Less
Nonconservative Noether's Theorem in Optimal Control,"Authors:Gastao S. F. Frederico,Delfim F. M. Torres","Abstract:We extend Noether's theorem to dynamical optimal control systems being under the action of nonconservative forces. A systematic way of calculating conservation laws for nonconservative optimal control problems is given. As a corollary, the conserved quantities previously obtained in the literature for nonconservative problems of mechanics and the calculus of variations are derived.We extend Noether's theorem to dynamical optimal control systems being under the action of nonconservative forces. A systematic way of calculating conservation laws for nonconservative optimal control problems is given. As a corollary, the conserved quantities previously obtained in the literature for nonconservative problems of mechanics and the calculus of variations are derived.△ Less"
The geometry of mirror symmetry,Authors:R. P. Thomas,"Abstract:We give a brief survey of some of the geometry of mirror symmetry, written in 2004 for the ""Encyclopaedia of Mathematical Physics"". Probably a little bit out of date now in a few places, but hey.We give a brief survey of some of the geometry of mirror symmetry, written in 2004 for the ""Encyclopaedia of Mathematical Physics"". Probably a little bit out of date now in a few places, but hey.△ Less"
"Projective Linking and Boundaries of Positive Holomorphic Chains in Projective Manifolds, Part I","Authors:F. Reese Harvey,H. Blaine Lawson Jr","Abstract:We introduce the notion of the projective linking number Link(M,Z) of a compact oriented real submanifold M of dimension 2p-1 in complex projective n-space P^n with an algebraic subvariety Z in P^n - M of codimension p. This notion is related to projective winding numbers and quasi-plurisubharmonic functions, and it generalizes directly from P^n to any projective manifold. Part 1 of this paper e…▽ MoreWe introduce the notion of the projective linking number Link(M,Z) of a compact oriented real submanifold M of dimension 2p-1 in complex projective n-space P^n with an algebraic subvariety Z in P^n - M of codimension p. This notion is related to projective winding numbers and quasi-plurisubharmonic functions, and it generalizes directly from P^n to any projective manifold. Part 1 of this paper establishes the following result for the case p=1. Let M be an oriented, stable, real analytic curve in P^n. Then M is the boundary of a positive holomorphic 1-chain T with Mass(T) < K in P^n if and only if Link(M,Z) > -K deg(Z) for all algebraic hypersurfaces Z in P^n - M.
  An analogous theorem is implied in any projective manifold. Part 2 of this paper studies similar results for p>1.△ Less"
SPM Bulletin 15,Authors:Boaz Tsaban,Abstract:CONTENTS: On Selective screenability and examples of R. Pol. Workshops and conferences: TheOxfordConference on Topology and Computer Science in Honour of Peter Collins and Mike Reed; Boise Extravaganza In Set Theory (BEST2006). Research announcements: The isometry group of the Urysohn space as a Levy group; Chasing Silver; Disjoint Non-Free Subgoups of Abe…▽ MoreCONTENTS: On Selective screenability and examples of R. Pol. Workshops and conferences: TheOxfordConference on Topology and Computer Science in Honour of Peter Collins and Mike Reed; Boise Extravaganza In Set Theory (BEST2006). Research announcements: The isometry group of the Urysohn space as a Levy group; Chasing Silver; Disjoint Non-Free Subgoups of Abelian Groups; Characterizing metric spaces whose hyperspaces are absolute neighborhood retracts; A Vitali set can be homeomorphic to its complement; o-Boundedness of free objects over a Tychonoff space; On the consistency strength of the Milner-Sauer conjecture; Reconstruction of manifolds and subsets of normed spaces from subgroups of their homeomorphism groups; Reconstruction theorem for homeomorphism groups without small sets and non-shrinking functions of a normed space; Locally Moving Groups and the Reconstruction Problem for Chains and Circles; Divisibility of countable metric spaces; Pre-compact families of finite sets of integers and weakly null sequences in Banach spaces; A semifilter approach to selection principles II: tau^*-covers; Parametrizing the abstract Ellentuck theorem; A notion of selective ultrafilter corresponding to topological Ramsey spaces; Compact spaces generated by retractions; Gromov-Hausdorff ultrametric; Computing the complexity of the relation of isometry between separable Banach spaces; On some classes of Lindelöf Sigma-spaces; On the depth of Boolean algebras. Problem of the Issue.△ Less
Consequences of the background in piezoresponse force microscopy on the imaging of ferroelectric domain structures,"Authors:T. Jungk,A. Hoffmann,E. Soergel","Abstract:The interpretation of ferroelectric domain images obtained with piezoresponse force microscopy (PFM) is discussed. The influences of an inherent experimental background on the domain contrast in PFM images (enhancement, nulling, inversion) as well as on the shape and the location of the domain boundaries are described. We present experimental results to evidence our analysis of the influence of…▽ MoreThe interpretation of ferroelectric domain images obtained with piezoresponse force microscopy (PFM) is discussed. The influences of an inherent experimental background on the domain contrast in PFM images (enhancement, nulling, inversion) as well as on the shape and the location of the domain boundaries are described. We present experimental results to evidence our analysis of the influence of the background on the domain contrast in PFM images.△ Less"
"Twin spin/charge roton mode and superfluid density: primary determining factors of $T_{c}$ in high-$T_{c}$ superconductors observed by neutron, ARPES, and $μ$SR",Authors:Y. J. Uemura,"Abstract:In the quest for primary factors which determine the transition temperature $T_{c}$ of high-$T_{c}$ cuprate superconductors (HTSC), we develop a phenomenological picture combining experimental results from muon spin relaxation ($μ$SR), neutron and Raman scattering, and angle-resolved photoemission (ARPES) measurements, guided by an analogy with superfluid $^{4}$He. The 41 meV neutron resonance m…▽ MoreIn the quest for primary factors which determine the transition temperature $T_{c}$ of high-$T_{c}$ cuprate superconductors (HTSC), we develop a phenomenological picture combining experimental results from muon spin relaxation ($μ$SR), neutron and Raman scattering, and angle-resolved photoemission (ARPES) measurements, guided by an analogy with superfluid $^{4}$He. The 41 meV neutron resonance mode and the ARPES superconducting coherence peak (SCP) can be viewed as direct observations of spin and charge soft modes, respectively, appearing near ($π,π$) and the center of the Brillouin zone, having identical energy transfers and dispersion relations. We present a conjecture that the mode energy of this twin spin/charge collective excitation, as a roton analogue in HTSC, plays a primary role in determining $T_{c}$, together with the superfluid density $n_{s}/m^{*}$ at $T \to 0$. We further propose a microscopic model for pairing based on a resonant spin-charge motion, which explains the extremely strong spin-charge coupling, relevant energy scales, disappearence of pairing in the overdoped region, and the contrasting spin-sensitivities of nodal and antinodal charges in HTSC systems. Comparing collective versus single-particle excitations, pair formation versus condensation, and local versus long-range phase coherence, we argue that many fundamental features of HTSC systems, including the region of the Nernst effect, can be understood in terms of condensation and fluctuation phenomena of bosonic correlations formed above $T_{c}$.△ Less"
Ordering Algorithms and Confidence Intervals in the Presence of Nuisance Parameters,Authors:Giovanni Punzi,"Abstract:We discuss some issues arising in the evaluation of confidence intervals in the presence of nuisance parameters (systematic uncertainties) by means of direct Neyman construction in multi-dimensional space. While this kind of procedure provides rigorous coverage, it may be affected by large overcoverage, and/or produce results with counterintuitive behavior with respect to the uncertainty on the…▽ MoreWe discuss some issues arising in the evaluation of confidence intervals in the presence of nuisance parameters (systematic uncertainties) by means of direct Neyman construction in multi-dimensional space. While this kind of procedure provides rigorous coverage, it may be affected by large overcoverage, and/or produce results with counterintuitive behavior with respect to the uncertainty on the nuisance parameters, or other undesirable properties. We describe a choice of ordering algorithm that provides results with good general properties, the correct behavior for small uncertainties, and limited overcoverage.△ Less"
"Likelihood ratio intervals with Bayesian treatment of uncertainties: coverage, power and combined experiments","Authors:Jan Conrad,Fredrik Tegenfeldt",Abstract:In this note we present studies of coverage and power for confidence intervals for a Poisson process with known background calculated using the Likelihood ratio (aka Feldman & Cousins) ordering with Bayesian treatment of uncertainties in nuisance parameters. We consider both the variant where the Bayesian integration is done in both the numerator and the denominator and the modification where th…▽ MoreIn this note we present studies of coverage and power for confidence intervals for a Poisson process with known background calculated using the Likelihood ratio (aka Feldman & Cousins) ordering with Bayesian treatment of uncertainties in nuisance parameters. We consider both the variant where the Bayesian integration is done in both the numerator and the denominator and the modification where the integration is done only in the numerator whereas in the denominator the likelihood is taken at the maximum likelihood estimate of the parameters. Furthermore we discuss how measurements can be combined in this framework and give an illustration with limits on the branching ratio of a rare B-meson decay recently presented by CDF/D0. A set of C++ classes has been developed which can be used to calculate confidence intervals for single or combining multiple experiments using the above algorithms and considering a variety of parameterizations to describe the uncertainties.△ Less
Statistical Challenges for Searches for New Physics at the LHC,Authors:Kyle Cranmer,"Abstract:Because the emphasis of the LHC is on 5 sigma discoveries and the LHC environment induces high systematic errors, many of the common statistical procedures used in High Energy Physics are not adequate. I review the basic ingredients of LHC searches, the sources of systematics, and the performance of several methods. Finally, I indicate the methods that seem most promising for the LHC and areas t…▽ MoreBecause the emphasis of the LHC is on 5 sigma discoveries and the LHC environment induces high systematic errors, many of the common statistical procedures used in High Energy Physics are not adequate. I review the basic ingredients of LHC searches, the sources of systematics, and the performance of several methods. Finally, I indicate the methods that seem most promising for the LHC and areas that are in need of further study.△ Less"
Quasiconvexity versus group invariance,Authors:Marius Buliga,"Abstract:The lower invariance under a given arbitrary group of diffeomorphisms extends the notion of quasiconvexity. The non-commutativity of the group operation (the function composition) modifies the classical equivalence between lower semicontinuity and quasiconvexity.
  In this context null lagrangians are particular cases of integral invariants of the group.
  Developments of parts of this paper can…▽ MoreThe lower invariance under a given arbitrary group of diffeomorphisms extends the notion of quasiconvexity. The non-commutativity of the group operation (the function composition) modifies the classical equivalence between lower semicontinuity and quasiconvexity.
  In this context null lagrangians are particular cases of integral invariants of the group.
  Developments of parts of this paper can be found in math.FA/0105097 . Further informations at http://irmi.epfl.ch/cag/buliga_necv.html .△ Less"
"Compressions, convex geometry and the Freiman-Bilu theorem","Authors:Ben Green,Terence Tao","Abstract:We note a link between combinatorial results of Bollobás and Leader concerning sumsets in the grid, the Brunn-Minkowski theorem and a result of Freiman and Bilu concerning the structure of sets of integers with small doubling.
  Our main result is the following. If eps > 0 and if A is a finite nonempty subset of a torsion-free abelian group with |A + A| <= K|A|, then A may be covered by exp(K^C)…▽ MoreWe note a link between combinatorial results of Bollobás and Leader concerning sumsets in the grid, the Brunn-Minkowski theorem and a result of Freiman and Bilu concerning the structure of sets of integers with small doubling.
  Our main result is the following. If eps > 0 and if A is a finite nonempty subset of a torsion-free abelian group with |A + A| <= K|A|, then A may be covered by exp(K^C) progressions of dimension [log_2 K + eps] and size at most |A|.△ Less"
Towards Reconciliation between Bayesian and Frequentist Reasoning,"Authors:Tomaz Podobnik,Tomi Zivko","Abstract:A theory of quantitative inference about the parameters of sampling distributions is constructed deductively by following very general rules, referred to as the Cox-Polya-Jaynes Desiderata. The inferences are made in terms of probability distributions that are assigned to the parameters. The Desiderata, focusing primarily on consistency of the plausible reasoning, lead to unique assignments of t…▽ MoreA theory of quantitative inference about the parameters of sampling distributions is constructed deductively by following very general rules, referred to as the Cox-Polya-Jaynes Desiderata. The inferences are made in terms of probability distributions that are assigned to the parameters. The Desiderata, focusing primarily on consistency of the plausible reasoning, lead to unique assignments of these probabilities in the case of sampling distributions that are invariant under Lie groups. In the scalar cases, e.g. in the case of inferring a single location or scale parameter, the requirement for logical consistency is equivalent to the requirement for calibration: the consistent probability distributions are automatically also the ones with the exact calibration and vice versa. This equivalence speaks in favour of reconciliation between the Bayesian and Frequentist schools of reasoning.△ Less"
NMR Quantum Information Processing with Para-Hydrogen,Authors:M. S. Anwar,"Abstract:This thesis addresses the problems of initialization and separability in liquid state NMR based quantum information processors. We prepare pure quantum states lying above the entanglement threshold. Our pure state quantum computer derives its purity from the highly polarized nuclear spin states in the para-hydrogen molecule. The thesis begins with a critique of conventional NMR based quantum inf…▽ MoreThis thesis addresses the problems of initialization and separability in liquid state NMR based quantum information processors. We prepare pure quantum states lying above the entanglement threshold. Our pure state quantum computer derives its purity from the highly polarized nuclear spin states in the para-hydrogen molecule. The thesis begins with a critique of conventional NMR based quantum information processing outlining the major strengths and weaknesses of the technology. We describe the enhanced magnetic ordering of the nuclear spin states in para-hydrogen and an initialization experiment exploiting this effect to achieve pure, entangled states. These states can indeed be used as initial states in implementing quantum algorithms: we describe mplementations of the Deutsch and the Grover quantum algorithms. The ""twirl"" operation converts a completely arbitrary input state to a Werner singlet. The NMR implementation of this operation is taken up. We also analyze the possibility of sharing the purity of some highly polarized qubits in a quantum computer onto quantum subspaces of arbitrary dimensions, and whether these sharing operations increase or decrease the likelihood of entanglement.△ Less"
Glueball Regge Trajectories,Authors:Harvey B. Meyer,"Abstract:We investigate the spectrum of glueballs in 3D and 4D SU(N) gauge theories. Our motivation is to determine whether the states lie on straight Regge trajectories. To this end we develop new lattice techniques to reliably determine the states carrying higher spin. It has been conjectured for a long time that glueballs are the physical states lying on the pomeron, the trajectory responsible for the…▽ MoreWe investigate the spectrum of glueballs in 3D and 4D SU(N) gauge theories. Our motivation is to determine whether the states lie on straight Regge trajectories. To this end we develop new lattice techniques to reliably determine the states carrying higher spin. It has been conjectured for a long time that glueballs are the physical states lying on the pomeron, the trajectory responsible for the slowly rising hadronic cross-sections at large centre-of-mass energy.△ Less"
Susceptibility inhomogeneity and non-Fermi liquid behavior in UCu_{5-x}Pt_x,"Authors:D. E. MacLaughlin,M. S. Rose,O. O. Bernal,R. H. Heffner,G. J. Nieuwenhuys,R. Chau,M. B. Maple","Abstract:Transverse-field muSR shifts and relaxation rates have been measured in the non-Fermi liquid (NFL) alloy system UCu_{5-x}Pt_x, x = 1.0, 1.5, and 2.5. At low temperatures the fractional spread in Knight shifts delta K/K approx deltachi/chi is gtrsim 2 for x = 1, but is only half this value for x = 1.5 and 2.5. In a disorder-driven scenario where the NFL behavior is due to a broadly distributed (K…▽ MoreTransverse-field muSR shifts and relaxation rates have been measured in the non-Fermi liquid (NFL) alloy system UCu_{5-x}Pt_x, x = 1.0, 1.5, and 2.5. At low temperatures the fractional spread in Knight shifts delta K/K approx deltachi/chi is gtrsim 2 for x = 1, but is only half this value for x = 1.5 and 2.5. In a disorder-driven scenario where the NFL behavior is due to a broadly distributed (Kondo or Griffiths-phase cluster) characteristic energy E, our results indicate that delta E/E_{rm av} approx (delta K/K)_{T=0} is similar for UCu_{5-x}Pd_x (x = 1 and 1.5) and UCu_4Pt, but is reduced for UCu_{5-x}Pt_x, x = 1.5 and 2.5. This reduction is due to a marked increase of E with increasing x; the spread delta E is found to be roughly independent of x. Our results correlate with the observed suppression of other NFL anomalies for x > 1 in UCu_{5-x}Pt_x but not in UCu_{5-x}Pd_x, and are further evidence for the importance of disorder in the NFL behavior of both these alloy systems.△ Less"
Effect of annealing on glassy dynamics and non-Fermi liquid behavior in UCu_4Pd,"Authors:D. E. MacLaughlin,M. S. Rose,J. E. Anderson,O. O. Bernal,R. H. Heffner,G. J. Nieuwenhuys,R. E. Baumbach,N. P. Butch,M. B. Maple","Abstract:Longitudinal-field muon spin relaxation (LF-muSR) experiments have been performed in unannealed and annealed samples of the heavy-fermion compound UCu_4Pd to study the effect of disorder on non-Fermi liquid behavior in this material. The muon spin relaxation functions G(t,H) obey the time-field scaling relation G(t,H) = G(t/H^gamma) previously observed in this compound. The observed scaling expo…▽ MoreLongitudinal-field muon spin relaxation (LF-muSR) experiments have been performed in unannealed and annealed samples of the heavy-fermion compound UCu_4Pd to study the effect of disorder on non-Fermi liquid behavior in this material. The muon spin relaxation functions G(t,H) obey the time-field scaling relation G(t,H) = G(t/H^gamma) previously observed in this compound. The observed scaling exponent gamma = 0.3 pm 0.1, independent of annealing. Fits of the stretched-exponential relaxation function G(t) = exp[-(Lambda t)^K] to the data yielded stretching exponentials K < 1 for all samples. Annealed samples exhibited a reduction of the relaxation rate at low temperatures, indicating that annealing shifts fluctuation noise power to higher frequencies. There was no tendency of the inhomogeneous spread in rates to decrease with annealing, which modifies but does not eliminate the glassy spin dynamics reported previously in this compound. The correlation with residual resistivity previously observed for a number of NFL heavy-electron materials is also found in the present work.△ Less"
Critical slowing down in the geometrically frustrated pyrochlore antiferromagnet Gd_2Ti_2O_7,"Authors:D. E. MacLaughlin,M. S. Rose,J. E. Anderson,Lei Shu,R. H. Heffner,T. Kimura,G. D. Morris,O. O. Bernal","Abstract:Longitudinal-field muon spin relaxation experiments have been carried out in the paramagnetic state of single-crystal Gd_2Ti_2O_7 just above the phase transition at T_m = 1.0 K. At high applied fields the exponential relaxation time T_1 is proportional to field, whereas T_1 saturates below a crossover field B_c that is ~2.5 T at 1.5 K and decreases as T_m is approached. At low fields the relaxat…▽ MoreLongitudinal-field muon spin relaxation experiments have been carried out in the paramagnetic state of single-crystal Gd_2Ti_2O_7 just above the phase transition at T_m = 1.0 K. At high applied fields the exponential relaxation time T_1 is proportional to field, whereas T_1 saturates below a crossover field B_c that is ~2.5 T at 1.5 K and decreases as T_m is approached. At low fields the relaxation rate increases markedly as the freezing temperature is approached, as expected for critical slowing down of the spin fluctuations, but the increase is suppressed by applied field. This behavior is consistent with the very long autocorrelation function cutoff time implied by the low value of B_c.△ Less"
Evolution of the heavy fermion state in Ce2IrIn8,"Authors:R. H. Heffner,G. D. Morris,E. D. Bauer,J. L. Sarrao,J. D. Thompson,D. E. MacLaughlin,L. Shu","Abstract:We report muon spin rotation Knight shift and susceptibility studies for 1 T applied field along the crystalline c- and a-axes of the heavy fermion compound Ce2IrIn8. Below a characteristic temperature T* one observes a `Knight-shift anomaly' in which the Knight shift constant K no longer scales linearly with susceptibility chi. This anomaly is consistent with a scaling law in which chi is compo…▽ MoreWe report muon spin rotation Knight shift and susceptibility studies for 1 T applied field along the crystalline c- and a-axes of the heavy fermion compound Ce2IrIn8. Below a characteristic temperature T* one observes a `Knight-shift anomaly' in which the Knight shift constant K no longer scales linearly with susceptibility chi. This anomaly is consistent with a scaling law in which chi is composed of a high-temperature component corresponding to non-interacting local moments and a low-temperature component chi_cf proportional to (1-T/T*)\ln(T*/T) which characterizes the heavy-electron state below T*. We find that T* is anisotropic, with T_a* = 59(3)K and T_c* = 24(1)K, and derive the magnitudes of chi_cf for H along the a- and c-axes.△ Less"
New limits on the ordered moments in alpha-Pu and Ga-stabilized delta-Pu,"Authors:R. H. Heffner,G. D. Morris,M. J. Fluss,B. Chung,D. E. MacLaughlin,L. Shu,J. E. Anderson","Abstract:We present the first muon spin relaxation measurements ever performed on elemental Pu, and set the most stringent upper limits to date on the magnitude of the ordered moment in alpha-Pu and delta-stabilized Pu (alloyed with 4.3 at. % Ga). Assuming a nominal hyperfine coupling field of 1 kOe per Bohr magneton we set an upper limit of 0.001 Bohr magnetons for both materials at T = 4 K.We present the first muon spin relaxation measurements ever performed on elemental Pu, and set the most stringent upper limits to date on the magnitude of the ordered moment in alpha-Pu and delta-stabilized Pu (alloyed with 4.3 at. % Ga). Assuming a nominal hyperfine coupling field of 1 kOe per Bohr magneton we set an upper limit of 0.001 Bohr magnetons for both materials at T = 4 K.△ Less"
Quantum Error Correction and Fault-Tolerance,Authors:Daniel Gottesman,"Abstract:I give an overview of the basic concepts behind quantum error correction and quantum fault tolerance. This includes the quantum error correction conditions, stabilizer codes, CSS codes, transversal gates, fault-tolerant error correction, and the threshold theorem.I give an overview of the basic concepts behind quantum error correction and quantum fault tolerance. This includes the quantum error correction conditions, stabilizer codes, CSS codes, transversal gates, fault-tolerant error correction, and the threshold theorem.△ Less"
"Structure, Individuality and Quantum Gravity",Authors:John Stachel,"Abstract:After reviewing various interpretations of structural realism, I adopt here a definition that allows both relations between things that are already individuated (which I call ``relations between things'') and relations that individuate previously un-individuated entities (""things between relations""). Since both space-time points in general relativity and elementary particles in quantum theory fa…▽ MoreAfter reviewing various interpretations of structural realism, I adopt here a definition that allows both relations between things that are already individuated (which I call ``relations between things'') and relations that individuate previously un-individuated entities (""things between relations""). Since both space-time points in general relativity and elementary particles in quantum theory fall into the latter category, I propose a principle of maximal permutability as a criterion for the fundamental entities of any future theory of ``quantum gravity''; i.e., a theory yielding both general relativity and quantum field theory in appropriate limits. Then I review of a number of current candidates for such a theory. First I look at the effective field theory and asymptotic quantization approaches to general relativity, and then at string theory. Then a discussion of some issues common to all approaches to quantum gravity based on the full general theory of relativity argues that processes, rather than states should be taken as fundamental in any such theory. A brief discussion of the canonical approach is followed by a survey of causal set theory, and a new approach to the question of which space-time structures should be quantized ends the paper.△ Less"
Current-driven switching of magnetisation- theory and experiment,"Authors:D. M. Edwards,F. Federici",Abstract:We review experimental and theoretical work on current-driven switching of magnetisation and present quantitative results for the Co/Cu/Co (001) system.We review experimental and theoretical work on current-driven switching of magnetisation and present quantitative results for the Co/Cu/Co (001) system.△ Less
Finite-dimensional algebras and quivers,Authors:Alistair Savage,"Abstract:This is an overview article on finite-dimensional algebras and quivers, written for the Encyclopedia of Mathematical Physics. We cover path algebras, Ringel-Hall algebras and the quiver varieties of Lusztig and Nakajima.This is an overview article on finite-dimensional algebras and quivers, written for the Encyclopedia of Mathematical Physics. We cover path algebras, Ringel-Hall algebras and the quiver varieties of Lusztig and Nakajima.△ Less"
The Fourier-Mukai Transform in String Theory,Authors:Bjorn Andreas,"Abstract:The article surveys aspects of the Fourier-Mukai transform, its relative version and some of its applications in string theory. To appear in Encyclopedia of Mathematical Physics, published by Elsevier in early 2006. Comments/corrections welcome.The article surveys aspects of the Fourier-Mukai transform, its relative version and some of its applications in string theory. To appear in Encyclopedia of Mathematical Physics, published by Elsevier in early 2006. Comments/corrections welcome.△ Less"
Supersymmetry and the MSSM: An Elementary Introduction,Authors:Ian J R Aitchison,"Abstract:These notes are an expanded version of a short course of lectures given for graduate students in particle physics atOxford. The level was intended to be appropriate for students in both experimental and theoretical particle physics.The purpose is to present an elementary and self-contained introduction to SUSY that follows on, relatively straightforwardly,…▽ MoreThese notes are an expanded version of a short course of lectures given for graduate students in particle physics atOxford. The level was intended to be appropriate for students in both experimental and theoretical particle physics.The purpose is to present an elementary and self-contained introduction to SUSY that follows on, relatively straightforwardly, from graduate-level courses in relativistic quantum mechanics and introductory quantum field theory. The notation adopted, at least initially, is one widely used in RQM courses, rather than the `spinor calculus' (dotted and undotted indices) notation found in most SUSY sources, though the latter is introduced in optional Asides. There is also a strong preference for a `do-it-yourself' constructive approach, rather than for a top-down formal deductive treatment. The main goal is to provide a practical understanding of how the softly broken MSSM is constructed. Relatively less space is devoted to phenomenology, though simple `classic' results are covered, including gauge unification, the bound on the mass of the lightest Higgs boson, and sparticle mixing. By the end of the course students (readers) should be provided with access to the contemporary phenomenological literature.△ Less"
TheOxford-Dartmouth Thirty Degree Survey II: Clustering of Bright Lyman Break Galaxies - Strong Luminosity Dependent Bias at z=4,"Authors:Paul D. Allen,Leonidas A. Moustakas,Gavin Dalton,Emily MacDonald,Chris Blake,Lee Clewley,Catherine Heymans,Gary Wegner","Abstract:…4 Lyman Break Galaxies (LBGs) selected from theOxford-Dartmouth Thirty Degree Survey (ODT). We describe techniques used to select and evaluate our candidates and calculate the angular correlation function which we find best fitted by a power law,ω(θ)=A_{w}θ^{-β}withA_{w}=15.4(withθin arcseconds), using a constrained slope ofβ=0.8. Using a red…▽ MoreWe present measurements of the clustering properties of bright (L>L_{*}) z\sim4 Lyman Break Galaxies (LBGs) selected from theOxford-Dartmouth Thirty Degree Survey (ODT). We describe techniques used to select and evaluate our candidates and calculate the angular correlation function which we find best fitted by a power law,ω(θ)=A_{w}θ^{-β}withA_{w}=15.4(withθin arcseconds), using a constrained slope ofβ=0.8. Using a redshift distribution consistent with photometric models, we deproject this correlation function and find a comovingr_{0}=11.4_{-1.9}^{+1.7}h_{100}^{-1}Mpc in aΩ_m=0.3flatΛcosmology fori_{AB}\leq24.5. This corresponds to a linear bias value ofb=8.1_{-2.6}^{+2.0}(assumingσ_{8}=0.9). These data show a significantly largerr_{0}andbthan previous studies atz\sim4. We interpret this as evidence that the brightest LBGs have a larger bias than fainter ones, indicating a strong luminosity dependence for the measured bias of an LBG sample. Comparing this against recent results in the literature at fainter (sub-L_{*}) limiting magnitudes, and with simple models describing the relationship between LBGs and dark matter haloes, we discuss the implications on the implied environments and nature of LBGs. It seems that the brightest LBGs (in contrast with the majority sub-L_{*}population), have clustering properties, and host dark matter halo masses, that are consistent with them being progenitors of the most massive galaxies today.△ Less"
Scientific Culture and Its Role in International Negotiations,Authors:Klaus L. Gottstein,"Abstract:Scientists as diplomats (historical examples). Scientists as advisors to governments and the public. Nongovernmental organisations and international conferences on security questions, initiated by scientists and based on their professional culture.Scientists as diplomats (historical examples). Scientists as advisors to governments and the public. Nongovernmental organisations and international conferences on security questions, initiated by scientists and based on their professional culture.△ Less"
General covariance and the objectivity of space-time point-events,"Authors:L. Lusanna,M. Pauri","Abstract:…undergo real {\it temporal change} and thereby provide a counter-example to the thesis of the {\it frozen-time} picture of evolution.
  Invited Contribution to the ESF 2004OxfordConference on Space-Time▽ More""The last remnant of physical objectivity of space-time"" is disclosed, beyond the Leibniz equivalence, in the case of a continuous family of spatially non-compact models of general relativity. The {\it physical individuation} of point-events is furnished by the intrinsic degrees of freedom of the gravitational field, (viz, the {\it Dirac observables}) that represent - as it were - the {\it ontic} part of the metric field. The physical role of the {\it epistemic} part (viz. the {\it gauge} variables) is likewise clarified. At the end, a peculiar four-dimensional {\it holistic and structuralist} view of space-time emerges which includes elements common to the tradition of both {\it substantivalism} and {\it relationism}. The observables of our models undergo real {\it temporal change} and thereby provide a counter-example to the thesis of the {\it frozen-time} picture of evolution.
  Invited Contribution to the ESF 2004OxfordConference on Space-Time△ Less"
A new method for direct rf power absorption studies in CMR materials and high T_c superconductors,"Authors:S. Sarangi,S. V. Bhat","Abstract:…determined from the measured change in the current supplied to the oscillator circuit. A customized low temperature insert is used to integrate the experiment with a commercialOxfordInstruments cryostat and temperature controller. The oscillator working in the rf range between 1 MHz to 25 MHz is built around an IC 74LS04. The temperature can be varied from…▽ MoreThe design, fabrication and performance of an apparatus for the measurement of direct rf power absorption in colossal magnetoresistive (CMR) and superconducting samples are described. The system consists of a self-resonant LC tank circuit of an oscillator driven by a NOT logic gate. The samples under investigation are placed in the core of the coil forming the inductance L and the absorbed power is determined from the measured change in the current supplied to the oscillator circuit. A customized low temperature insert is used to integrate the experiment with a commercialOxfordInstruments cryostat and temperature controller. The oscillator working in the rf range between 1 MHz to 25 MHz is built around an IC 74LS04. The temperature can be varied from 4.2 to 400 K and the magnetic field from 0 to 1.4 T. The apparatus is capable of measuring direct power absorption in CMR and superconducting samples of volume as small as 1/1000 cm^3 with a signal to noise ratio of 10:1. Further increase in the sensitivity can be obtained by summing the results of repeated measurements obtained at a given temperature. The system performance is evaluated by measuring the absorbed power in La_0.7 Sr_0.3 MnO_3 (LSMO) CMR manganite samples and superconducting Y Ba_2 Cu_3 O_7 (YBCO) samples at different rf frequencies. All operations during the measurements are automated using a computer with a menu-driven software system, user input being required only for the initiation of the measurement sequence.△ Less"
An Independent Derivation of theOxfordJet Kinetic Luminosity Formula,Authors:Brian Punsly,"Abstract:…luminosity. The expression yields jet powers that are quantitatively similar to a more sophisticated empirical relation published by the Willott, Blundell and Rawlings atOxford. The formula allows one to estimate the jet kinetic luminosity from the measurement of the optically thin radio lobe emission in quasars and radio galaxies. Motivated by recent X-ray…▽ MoreThis letter presents a theoretical derivation of an estimate for a radio source jet kinetic luminosity. The expression yields jet powers that are quantitatively similar to a more sophisticated empirical relation published by the Willott, Blundell and Rawlings atOxford. The formula allows one to estimate the jet kinetic luminosity from the measurement of the optically thin radio lobe emission in quasars and radio galaxies. Motivated by recent X-ray observation, the derivation assumes that most of the energy in the lobes is in plasma thermal energy with a negligible contribution from magnetic energy (not equipartition). The close agreement of the two independent expressions makes the veracity of these estimates seem very plausible.△ Less"
Special metric structures and closed forms,Authors:Frederik Witt,"Abstract:The primary aim of this thesis is to investigate metrics which are induced by a differential form and arise as a critical point of Hitchin's variational principle. Firstly, we investigate metrics associated with the structure group PSU(3) acting in its adjoint representation. We derive various obstructions to the existence of a topological reduction to PSU(3). For compact manifolds, we also find…▽ MoreThe primary aim of this thesis is to investigate metrics which are induced by a differential form and arise as a critical point of Hitchin's variational principle. Firstly, we investigate metrics associated with the structure group PSU(3) acting in its adjoint representation. We derive various obstructions to the existence of a topological reduction to PSU(3). For compact manifolds, we also find sufficient conditions if the PSU(3)-structure lifts to an SU(3)-structure. We give a Riemannian characterisation of topological PSU(3)-structures through an invariant spinor valued 1-form and show that the PSU(3)-structure is integrable if and only if the spinor valued 1-form defines a co-closed Rarita-Schwinger field. Moreover, we construct non-symmetric (compact) examples. Secondly, we consider even or odd forms which can be naturally interpreted as spinors for a spin structure onT\oplus T^*. As such, the forms we consider induce a reduction fromSpin(7,7)toG_2\times G_2. We give a topological classification ofG_2\times G_2-structures. We prove that the condition for being a critical point is equivalent to the supersymmetry equations on spinors in supergravity theory of type IIA/B with NS-NS background fields. Examples are systematically constructed by the device of T-duality.△ Less"
New aspects of the ddc-lemma,Authors:Gil R. Cavalcanti,"Abstract:We produce examples of generalized complex structures on manifolds by generalizing results from symplectic and complex geometry. We produce generalized complex structures on symplectic fibrations over a generalized complex base. We study in some detail different invariant generalized complex structures on compact Lie groups and provide a thorough description of invariant structures on nilmanifol…▽ MoreWe produce examples of generalized complex structures on manifolds by generalizing results from symplectic and complex geometry. We produce generalized complex structures on symplectic fibrations over a generalized complex base. We study in some detail different invariant generalized complex structures on compact Lie groups and provide a thorough description of invariant structures on nilmanifolds, achieving a classification on 6-nilmanifolds.
  We study implications of the `dd^c-lemma' in the generalized complex setting. Similarly to the standard dd^c-lemma, its generalized version induces a decomposition of the cohomology of a manifold and causes the degeneracy of the spectral sequence associated to the splitting d = \del + \delbar at E_1. But, in contrast with the dd^c-lemma, its generalized version is not preserved by symplectic blow-up or blow-down (in the case of a generalized complex structure induced by a symplectic structure) and does not imply formality.△ Less"
Quantum Information Theory and the Foundations of Quantum Mechanics,Authors:Christopher Gordon Timpson,"Abstract:This thesis is a contribution to the debate on the implications of quantum information theory for the foundations of quantum mechanics.
  In Part 1, the logical and conceptual status of various notions of information is assessed. It is emphasized that the everyday notion of information is to be firmly distinguished from the technical notions arising in information theory; however it is maintaine…▽ MoreThis thesis is a contribution to the debate on the implications of quantum information theory for the foundations of quantum mechanics.
  In Part 1, the logical and conceptual status of various notions of information is assessed. It is emphasized that the everyday notion of information is to be firmly distinguished from the technical notions arising in information theory; however it is maintained that in both settings `information' functions as an abstract noun, hence does not refer to a particular or substance (the worth of this point is illustrated in application to quantum teleportation). The claim that `Information is Physical' is assessed and argued to face a destructive dilemma. Accordingly, the slogan may not be understood as an ontological claim, but at best, as a methodological one. The reflections of Bruckner and Zeilinger (2001) and Deutsch and Hayden (2000) on the nature of information in quantum mechanics are critically assessed and some results presented on the characterization of entanglement in the Deutsch-Hayden formalism. Some philosophical aspects of quantum computation are discussed and general morals drawn concerning the nature of quantum information theory.
  In Part II, following some preliminary remarks, two particular information-theoretic approaches to the foundations of quantum mechanics are assessed in detail. It is argued that Zeilinger's (1999) Foundational Principle is unsuccessful as a foundational principle for quantum mechanics. The information-theoretic characterization theorem of Clifton, Bub and Halvorson (2003) is assessed more favourably, but the generality of the approach is questioned and it is argued that the implications of the theorem for the traditional foundational problems in quantum mechanics remains obscure.△ Less"
The 3-D clustering of radio galaxies in the TONS survey,"Authors:Kate Brand,Steve Rawlings,Gary J. Hill,Joseph R. Tufts","Abstract:We present a clustering analysis of the Texas-OxfordNVSS Structure (TONS) radio galaxy redshift survey. This complete flux-limited survey consists of 268 radio galaxies with spectroscopic redshifts in three separate regions of the sky covering a total of 165 deg^2. By going to faint radio flux densities (s_1.4>3 mJy) but imposing relatively bright optica…▽ MoreWe present a clustering analysis of the Texas-OxfordNVSS Structure (TONS) radio galaxy redshift survey. This complete flux-limited survey consists of 268 radio galaxies with spectroscopic redshifts in three separate regions of the sky covering a total of 165 deg^2. By going to faint radio flux densities (s_1.4>3 mJy) but imposing relatively bright optical limits (E R 19.5), the TONS sample is optimised for looking at the clustering properties of low luminosity radio galaxies in a region of moderate (0 < z < 0.5) redshifts. We use the two point correlation function to determine the clustering strength of the combined TONS08 and TONS12 sub-samples and find a clustering strength of r_0(z)=8.7+/-1.6 Mpc (h=0.7). If we assume growth of structure by linear theory and that the median redshift is 0.3, this corresponds to r_0(0)=11.0+/-2.0 Mpc which is consistent with the clustering strength of the underlying host galaxies (~ 2.5 Lstar ellipticals) of the TONS radio galaxy population.△ Less"
Sleeping Beauty Reconsidered: Conditioning and Reflection in Asynchronous Systems,Authors:Joseph Y. Halpern,"Abstract:A careful analysis of conditioning in the Sleeping Beauty problem is done, using the formal model for reasoning about knowledge and probability developed by Halpern and Tuttle. While the Sleeping Beauty problem has been viewed as revealing problems with conditioning in the presence of imperfect recall, the analysis done here reveals that the problems are not so much due to imperfect recall as to…▽ MoreA careful analysis of conditioning in the Sleeping Beauty problem is done, using the formal model for reasoning about knowledge and probability developed by Halpern and Tuttle. While the Sleeping Beauty problem has been viewed as revealing problems with conditioning in the presence of imperfect recall, the analysis done here reveals that the problems are not so much due to imperfect recall as to asynchrony. The implications of this analysis for van Fraassen's Reflection Principle and Savage's Sure-Thing Principle are considered.△ Less"
Experimental observation of the spin-Hall effect in a two dimensional spin-orbit coupled semiconductor system,"Authors:Joerg Wunderlich,Bernd Kaestner,Jairo Sinova,Tomas Jungwirth","Abstract:We report the experimental observation of the spin-Hall effect in a two-dimensional (2D) hole system with Rashba spin-orbit coupling.
 The 2D hole layer is a part of a p-n junction light-emitting diode with a specially designed co-planar geometry which allows an angle-resolved polarization detection at opposite edges of the 2D hole system. In equilibrium the angular momenta of the Rashba split h…▽ MoreWe report the experimental observation of the spin-Hall effect in a two-dimensional (2D) hole system with Rashba spin-orbit coupling.
 The 2D hole layer is a part of a p-n junction light-emitting diode with a specially designed co-planar geometry which allows an angle-resolved polarization detection at opposite edges of the 2D hole system. In equilibrium the angular momenta of the Rashba split heavy hole states lie in the plane of the 2D layer. When an electric field is applied across the hole channel a non zero out-of-plane component of the angular momentum is detected whose sign depends on the sign of the electric field and is opposite for the two edges. Microscopic quantum transport calculations show only a weak effect of disorder suggesting that the clean limit spin-Hall conductance description (intrinsic spin-Hall effect) might apply to our system.△ Less"
Detecting the growth of free group automorphisms by their action on the homology of subgroups of finite index,Authors:Adam Piggott,"Abstract:We prove that if F is a finitely generated free group and f:F -> F is an automorphism with polynomial growth of degree d, then there exists a characteristic subgroup S < F of finite index such that the induced automorphism of the abelianisation of S also grows polynomially of degree d. The proof is geometric in nature and makes use of improved relative train track representatives.We prove that if F is a finitely generated free group and f:F -> F is an automorphism with polynomial growth of degree d, then there exists a characteristic subgroup S < F of finite index such that the induced automorphism of the abelianisation of S also grows polynomially of degree d. The proof is geometric in nature and makes use of improved relative train track representatives.△ Less"
The Arithmetic of Calabi--Yau Manifolds and Mirror Symmetry,Authors:Shabnam N. Kadir,"Abstract:We study mirror symmetric pairs of Calabi--Yau manifolds over finite fields. In particular we compute the number of rational points of the manifolds as a function of the complex structure parameters. The data of the number of rational points of a Calabi--YauX/\mathbb{F}_qcan be encoded in a generating function known as the congruent zeta function. The Weil Conjectures (proved in the 1970s) s…▽ MoreWe study mirror symmetric pairs of Calabi--Yau manifolds over finite fields. In particular we compute the number of rational points of the manifolds as a function of the complex structure parameters. The data of the number of rational points of a Calabi--YauX/\mathbb{F}_qcan be encoded in a generating function known as the congruent zeta function. The Weil Conjectures (proved in the 1970s) show that for smooth varieties, these functions take a very interesting form in terms of the Betti numbers of the variety. This has interesting implications for mirror symmetry, as mirror symmetry exchanges the odd and even Betti numbers. Here the zeta functions for a one-parameter family of K3 surfaces,\mathbb{P}_3[4], and a two-parameter family of octics in weighted projective space,\mathbb{P}_4{}^{(1, 1, 2, 2, 2)} [8], are computed. The form of the zeta function at points in the moduli space of complex structures where the manifold is singular (where the Weil conjectures apart from rationality are not applicable), is investigated. The zeta function appears to be sensitive to monomial and non-monomial deformations of complex structure (or equivalently on the mirror side, toric and non-toric divisors). Various conjectures about the form of the zeta function for mirror symmetric pairs are made in light of the results of this calculation. Connections withL-functions associated to both elliptic and Siegel modular forms are suggested.△ Less"
Large-scale Structure with the SKA,Authors:Peter Coles,"Abstract:A standard theoretical paradigm for the formation of large-scale structure in the distribution of galaxies has now been established, based on the gravitational instability of cold dark matter in a background cosmology dominated by vacuum energy. Significant uncertainties remain in the modelling of complex astrophysical processes involved in galaxy formation, perhaps most fundamentally in the rel…▽ MoreA standard theoretical paradigm for the formation of large-scale structure in the distribution of galaxies has now been established, based on the gravitational instability of cold dark matter in a background cosmology dominated by vacuum energy. Significant uncertainties remain in the modelling of complex astrophysical processes involved in galaxy formation, perhaps most fundamentally in the relationship between the distributions of luminous galaxies and the underlying dark matter. I argue that the Square Kilometre Array is likely to provide information crucial to understanding this relationship and how it evolves with time.△ Less"
Transient phenomena,Authors:G. Woan,"Abstract:The SKA's design is driven by the needs of cutting-edge radio astronomy for sensitivity and spacial resolution. However its design is also driven by the desire to explore the transient radio Universe. In addition to pulsars, the SKA will be able to carry out high time resolution observations of several classes of known and predicted transient sources. Here we consider a selection of them, and de…▽ MoreThe SKA's design is driven by the needs of cutting-edge radio astronomy for sensitivity and spacial resolution. However its design is also driven by the desire to explore the transient radio Universe. In addition to pulsars, the SKA will be able to carry out high time resolution observations of several classes of known and predicted transient sources. Here we consider a selection of them, and describe how observational demands affect the instrument's design.△ Less"
RandomOxfordGraphs,"Authors:Jonah Blasiak,Rick Durrett","Abstract:Inspired by a concept in comparative genomics, we investigate properties of randomly chosen members of G_1(m,n,t), the set of bipartite graphs withmleft vertices, n right vertices, t edges, and each vertex of degree at least one. We give asymptotic results for the number of such graphs and the number of(i,j)trees they contain. We compute the thresholds for the emergence of a giant compon…▽ MoreInspired by a concept in comparative genomics, we investigate properties of randomly chosen members of G_1(m,n,t), the set of bipartite graphs withmleft vertices, n right vertices, t edges, and each vertex of degree at least one. We give asymptotic results for the number of such graphs and the number of(i,j)trees they contain. We compute the thresholds for the emergence of a giant component and for the graph to be connected.△ Less"
Matrix Model Thermodynamics,Authors:Gordon W. Semenoff,"Abstract:Some recent work on the thermodynamic behavior of the matrix model of M-theory on a pp-wave background is reviewed. We examine a weak coupling limit where computations can be done explicitly. In the large N limit, we find a phase transition between two distinct phases which resembles a ``confinement-deconfinement'' transition in gauge theory and which we speculate must be related to a geometric…▽ MoreSome recent work on the thermodynamic behavior of the matrix model of M-theory on a pp-wave background is reviewed. We examine a weak coupling limit where computations can be done explicitly. In the large N limit, we find a phase transition between two distinct phases which resembles a ``confinement-deconfinement'' transition in gauge theory and which we speculate must be related to a geometric transition in M-theory. We review arguments that the phase transition is also related to the Hagedorn transition of little string theory in a certain limit of the 5-brane geometry.△ Less"
TheOxford-Dartmouth Thirty Degree Survey I: Observations and Calibration of a Wide-Field Multi-Band Survey,"Authors:Emily C. MacDonald,Paul Allen,Gavin Dalton,Leonidas A. Moustakas,Catherine Heymans,Edward Edmondson,Chris Blake,Lee Clewley,Molly C. Hammell,Ed Olding,Lance Miller,Steve Rawlings,Jasper Wall,Gary Wegner,Christian Wolf,.","Abstract:TheOxfordDartmouth Thirty Degree Survey (ODTS) is a deep, wide, multi-band imaging survey designed to cover a total of 30 square degrees in BVRi'Z, with a subset of U and K band data, in four separate fields of 5-10 deg^2 centred at 00:18:24 +34:52, 09:09:45 +40:50, 13:40:00 +02:30 and 16:39:30 +45:24. Observations have been made using the Wide Field C…▽ MoreTheOxfordDartmouth Thirty Degree Survey (ODTS) is a deep, wide, multi-band imaging survey designed to cover a total of 30 square degrees in BVRi'Z, with a subset of U and K band data, in four separate fields of 5-10 deg^2 centred at 00:18:24 +34:52, 09:09:45 +40:50, 13:40:00 +02:30 and 16:39:30 +45:24. Observations have been made using the Wide Field Camera on the 2.5-m Isaac Newton Telescope in La Palma to average limiting depths (5 sigma Vega, aperture magnitudes) of U=24.8, B=25.6, V=25.0, R=24.6, and i'=23.5, with observations taken in ideal conditions reaching the target depths of U=25.3, B=26.2, V=25.7, R=25.4, and i'=24.6. The INT Z band data was found to be severely effected by fringing and, consequently, is now being obtained at the MDM observatory in Arizona. A complementary K-band survey has also been carried out at MDM, reaching an average depth of K_{5σ}~18.5. At present, approximately 23 deg^2 of the ODTS have been observed, with 3.5 deg^2 of the K band survey completed. This paper details the survey goals, field selection, observation strategy and data reduction procedure, focusing on the photometric calibration and catalogue construction. Preliminary photometric redshifts have been obtained for a subsample of the objects with R <= 23. These results are presented alongside a brief description of the photometric redshift determination technique used. The median redshift of the survey is estimated to be z~0.7 from a combination of the ODTS photometric redshifts and comparison with the redshift distributions of other surveys. Finally, galaxy number counts for the ODTS are presented which are found to be in excellent agreement with previous studies.△ Less"
Quantum Quandaries: a Category-Theoretic Perspective,Authors:John C. Baez,"Abstract:General relativity may seem very different from quantum theory, but work on quantum gravity has revealed a deep analogy between the two. General relativity makes heavy use of the category nCob, whose objects are (n-1)-dimensional manifolds representing ""space"" and whose morphisms are n-dimensional cobordisms representing ""spacetime"". Quantum theory makes heavy use of the category Hilb, whose obj…▽ MoreGeneral relativity may seem very different from quantum theory, but work on quantum gravity has revealed a deep analogy between the two. General relativity makes heavy use of the category nCob, whose objects are (n-1)-dimensional manifolds representing ""space"" and whose morphisms are n-dimensional cobordisms representing ""spacetime"". Quantum theory makes heavy use of the category Hilb, whose objects are Hilbert spaces used to describe ""states"", and whose morphisms are bounded linear operators used to describe ""processes"". Moreover, the categories nCob and Hilb resemble each other far more than either resembles Set, the category whose objects are sets and whose morphisms are functions. In particular, both Hilb and nCob but not Set are *-categories with a noncartesian monoidal structure. We show how this accounts for many of the famously puzzling features of quantum theory: the failure of local realism, the impossibility of duplicating quantum information, and so on. We argue that these features only seem puzzling when we try to treat Hilb as analogous to Set rather than nCob, so that quantum theory will make more sense when regarded as part of a theory of spacetime.△ Less"
A survey of the topological properties of symplectomorphism groups,Authors:Dusa McDuff,"Abstract:The special structures that arise in symplectic topology (particularly Gromov--Witten invariants and quantum homology) place as yet rather poorly understood restrictions on the topological properties of symplectomorphism groups. This article surveys some recent work by Abreu, Lalonde, McDuff, Polterovich and Seidel, concentrating particularly on the homotopy properties of the action of the group…▽ MoreThe special structures that arise in symplectic topology (particularly Gromov--Witten invariants and quantum homology) place as yet rather poorly understood restrictions on the topological properties of symplectomorphism groups. This article surveys some recent work by Abreu, Lalonde, McDuff, Polterovich and Seidel, concentrating particularly on the homotopy properties of the action of the group of Hamiltonian symplectomorphisms on the underlying manifold M. It sketches the proof that the evaluation map π_1(Ham(M))\to π_1(M) given by {φ_t}\mapsto {φ_t(x_0)} is trivial, as well as explaining similar vanishing results for the action of the homology of Ham(M) on the homology of M. Applications to Hamiltonian stability are discussed.△ Less"
The Triggering and Bias of Radio Galaxies,"Authors:Kate Brand,Steve Rawlings,Joe Tufts,Gary Hill","Abstract:We present new results on the clustering and three-dimensional distribution of radio galaxies from the Texas-OxfordNVSS Structure (TONS) survey. The TONS survey was constructed to look at the distribution of radio galaxies in a region of moderate (0 < z < 0.5) redshifts by matching NVSS sources with objects in APM catalogues to obtain a sample of opti…▽ MoreWe present new results on the clustering and three-dimensional distribution of radio galaxies from the Texas-OxfordNVSS Structure (TONS) survey. The TONS survey was constructed to look at the distribution of radio galaxies in a region of moderate (0 < z < 0.5) redshifts by matching NVSS sources with objects in APM catalogues to obtain a sample of optically bright (R < 19.5), radio faint (1.4-GHz flux density S_1.4 > 3 mJy) radio galaxies over large areas on the sky. We find that redshift spikes, which represent large concentrations of radio galaxies which trace (~ 100 Mpc^3) super-structures are a common phenomena in these surveys. Under the assumption of quasi-linear structure formation theory and a canonical radio galaxy bias, the structures represent ~ 4-5 sigma peaks in the primordial density field and their expected number is low. The most plausible explanation for these low probabilities is an increase in the radio galaxy bias with redshift. To investigate potential mechanisms which have triggered the radio activity in these galaxies - and hence may account for an increase in the bias of this population, we performed imaging studies of the cluster environment of the radio galaxies in super-structure regions. Preliminary results show that these radio galaxies may reside preferentially at the edges of rich clusters. If radio galaxies are preferentially triggered as they fall towards rich clusters then they would effectively adopt the cluster bias.△ Less"
H^1-Projective Banach Spaces,Authors:Omran Kouba,"Abstract:We study theH^1-projective Banach spaces. We prove that they have the Analytic Radon-Nikodym Property, and that they are cotype 2 spaces which satisfy Grothendieck's Theorem. We show also that the ultraproduct ofH^1-projective spaces isH^1-projective. Other results are also discussed.We study theH^1-projective Banach spaces. We prove that they have the Analytic Radon-Nikodym Property, and that they are cotype 2 spaces which satisfy Grothendieck's Theorem. We show also that the ultraproduct ofH^1-projective spaces isH^1-projective. Other results are also discussed.△ Less"
Generalized complex geometry,Authors:Marco Gualtieri,"Abstract:Generalized complex geometry, as developed by Hitchin, contains complex and symplectic geometry as its extremal special cases. In this thesis, we explore novel phenomena exhibited by this geometry, such as the natural action of a B-field. We provide new examples, including some on manifolds admitting no known complex or symplectic structure. We prove a generalized Darboux theorem which yields a…▽ MoreGeneralized complex geometry, as developed by Hitchin, contains complex and symplectic geometry as its extremal special cases. In this thesis, we explore novel phenomena exhibited by this geometry, such as the natural action of a B-field. We provide new examples, including some on manifolds admitting no known complex or symplectic structure. We prove a generalized Darboux theorem which yields a local normal form for the geometry. We show that there is an elliptic deformation theory and establish the existence of a Kuranishi moduli space.
  We then define the concept of a generalized Kahler manifold. We prove that generalized Kahler geometry is equivalent to a bi-Hermitian geometry with torsion first discovered by physicists. We then use this result to solve an outstanding problem in 4-dimensional bi-Hermitian geometry: we prove that there exists a Riemannian metric on the complex projective plane which admits exactly two distinct Hermitian complex structures with equal orientation.
  Finally, we introduce the concept of generalized complex submanifold, and show that such sub-objects correspond to D-branes in the topological A- and B-models of string theory.△ Less"
Two-Point Functions and Boundary States in Boundary Logarithmic Conformal Field Theories,Authors:Yukitaka Ishimoto,"Abstract:Our main aim in this thesis is to address the results and prospects of boundary logarithmic conformal field theories: theories with boundaries that contain the above Jordan cell structure. We have investigated c_{p,q} boundary theory in search of logarithmic theories and have found logarithmic solutions of two-point functions in the context of the Coulomb gas picture. Other two-point functions h…▽ MoreOur main aim in this thesis is to address the results and prospects of boundary logarithmic conformal field theories: theories with boundaries that contain the above Jordan cell structure. We have investigated c_{p,q} boundary theory in search of logarithmic theories and have found logarithmic solutions of two-point functions in the context of the Coulomb gas picture. Other two-point functions have also been studied in the free boson construction of BCFT with SU(2)_k symmetry. In addition, we have analyzed and obtained the boundary Ishibashi state for a rank-2 Jordan cell structure [hep-th/0103064]. We have also examined the (generalised) Ishibashi state construction and the symplectic fermion construction at c=-2 for boundary states in the context of the c=-2 triplet model. The differences between two constructions are interpreted, resolved and extended beyond each case.△ Less"
Multi-component power spectra estimation method for multi-detector observations of the Cosmic Microwave Background,Authors:Guillaume Patanchon,"Abstract:We present a new method for multi-component power spectra estimation in multi-frequency observations of the CMB. Our method is based on matching a model to the cross and auto power spectra of observed maps. All the component power spectra are estimated, as well as their mixing matrix. Noise power spectra are also estimated. The method has been applied to full-sky Planck simulations containing fi…▽ MoreWe present a new method for multi-component power spectra estimation in multi-frequency observations of the CMB. Our method is based on matching a model to the cross and auto power spectra of observed maps. All the component power spectra are estimated, as well as their mixing matrix. Noise power spectra are also estimated. The method has been applied to full-sky Planck simulations containing five astrophysical components and white noise. The beam smoothing effect is taken into account.△ Less"
Symmetries in QFT,"Authors:K. M. Hamilton,J. F. Wheater","Abstract:This document contains notes from the graduate lecture course, ""Symmetries in QFT"" given by J.F.Wheater atOxfordUniversity in Hilary term. The course gives an informal introduction to QFT.This document contains notes from the graduate lecture course, ""Symmetries in QFT"" given by J.F.Wheater atOxfordUniversity in Hilary term. The course gives an informal introduction to QFT.△ Less"
The Easiest Hard Problem: Number Partitioning,Authors:Stephan Mertens,"Abstract:Number partitioning is one of the classical NP-hard problems of combinatorial optimization. It has applications in areas like public key encryption and task scheduling. The random version of number partitioning has an ""easy-hard"" phase transition similar to the phase transitions observed in other combinatorial problems likek-SAT. In contrast to most other problems, number partitioning is simp…▽ MoreNumber partitioning is one of the classical NP-hard problems of combinatorial optimization. It has applications in areas like public key encryption and task scheduling. The random version of number partitioning has an ""easy-hard"" phase transition similar to the phase transitions observed in other combinatorial problems likek-SAT. In contrast to most other problems, number partitioning is simple enough to obtain detailled and rigorous results on the ""hard"" and ""easy"" phase and the transition that separates them. We review the known results on random integer partitioning, give a very simple derivation of the phase transition and discuss the algorithmic implications of both phases.△ Less"
New constraints on varyingα,"Authors:G. Rocha,R. Trotta,C. J. A. P. Martins,A. Melchiorri,P. P. Avelino,P. T. P. Viana","Abstract:We present a summary of recent constraints on the value of the fine-structure constant at the epoch of decoupling from the recent observations made by the Wilkinson Microwave Anisotropy Probe (WMAP) satellite. Within the set of models considered, a variation of the value ofαat decoupling with respect to the present-day value is now bounded to be smaller than 2% (6%) at 95% confidence level.…▽ MoreWe present a summary of recent constraints on the value of the fine-structure constant at the epoch of decoupling from the recent observations made by the Wilkinson Microwave Anisotropy Probe (WMAP) satellite. Within the set of models considered, a variation of the value ofαat decoupling with respect to the present-day value is now bounded to be smaller than 2% (6%) at 95% confidence level. We point out that the existence of an early reionization epoch as suggested by the above measurements will, when more accurate cosmic microwave background polarization data is available, lead to considerably tighter constraints. We find that the tightest possible constraint onαis about0.1 %using CMB data alone--tighter constraints will require further (non-CMB) priors.△ Less"
A cross-correlation of WMAP and ROSAT,"Authors:J. M. Diego,J. Silk,W. Sliwa","Abstract:We cross-correlate the recent CMB WMAP 1 year data with the diffuse soft X-ray background map of ROSAT. We look for common signatures due to galaxy clusters (SZ effect in CMB, bremsstrahlung in X-rays) by cross-correlating the two maps in real and in Fourier space. We do not find any significant correlation and we explore the different reasons for this lack of correlation. The most likely candid…▽ MoreWe cross-correlate the recent CMB WMAP 1 year data with the diffuse soft X-ray background map of ROSAT. We look for common signatures due to galaxy clusters (SZ effect in CMB, bremsstrahlung in X-rays) by cross-correlating the two maps in real and in Fourier space. We do not find any significant correlation and we explore the different reasons for this lack of correlation. The most likely candidates are the possibility that we live in a lowσ_8universe (σ_8 < 0.9) and/or systematic effects in the data especially in the diffuse X-ray maps which may suffer from significant cluster signal subtraction during the point source removal process.△ Less"
Photometric Redshifts for Galaxies in the GOODS Southern Field,"Authors:B. Mobasher,R. Idzi,N. Benitez,A. Cimatti,S. Cristiani,E. Daddi,T. Dahlen,M. Dickinson,T. Erben,H. C. Ferguson,M. Giavalisco,N. A. Grogin,A. M. Koekemoer,M. Mignoli,L. A. Moustakas,M. Nonino,P. Rosati,M. Schirmer,D. Stern,E. Vanzella,C. Wolf,G. Zamorani,STScI,JHU,Osservatorio Astrofisico di Arcetri, et al. (6 additional authors not shown)","Abstract:We use extensive multi-wavelength photometric data from the Great Observatories Origins Deep Survey (GOODS) to estimate photometric redshifts for a sample of 434 galaxies with spectroscopic redshifts in the Chandra Deep Field South. Using the Bayesian method, which incorporates redshift/magnitude priors, we estimate photometric redshifts for galaxies in the range 18 < R (AB) < 25.5, giving an rm…▽ MoreWe use extensive multi-wavelength photometric data from the Great Observatories Origins Deep Survey (GOODS) to estimate photometric redshifts for a sample of 434 galaxies with spectroscopic redshifts in the Chandra Deep Field South. Using the Bayesian method, which incorporates redshift/magnitude priors, we estimate photometric redshifts for galaxies in the range 18 < R (AB) < 25.5, giving an rms scatter of 0.11. The outlier fraction is < 10%, with the outlier-clipped rms being 0.047. We examine the accuracy of photometric redshifts for several, special sub--classes of objects. The results for extremely red objects are more accurate than those for the sample as a whole, with rms of 0.051 and very few outliers (3%). Photometric redshifts for active galaxies, identified from their X-ray emission, have a dispersion of 0.104, with 10% outlier fraction, similar to that for normal galaxies. Employing a redshift/magnitude prior in this process seems to be crucial in improving the agreement between photometric and spectroscopic redshifts.△ Less"
"Quantum Hall Systems: Braid groups, composite fermions, and fractional charge","Authors:Lucjan Jacak,Piotr Sitko,Konrad Wieczorek,Arkadiusz Wójs","Abstract:The book presents the wide range of topics in two-dimensional physics of quantum Hall systems, especially fractional quantum Hall states. It starts with the fundamental problems of quantum statistics in two dimensions and the corresponding braid group formalism. The braid group formalism of anyons (previously known) is developed for composite fermions. The main formalism used in many-body quantu…▽ MoreThe book presents the wide range of topics in two-dimensional physics of quantum Hall systems, especially fractional quantum Hall states. It starts with the fundamental problems of quantum statistics in two dimensions and the corresponding braid group formalism. The braid group formalism of anyons (previously known) is developed for composite fermions. The main formalism used in many-body quantum Hall theories -- the Chern-Simons theory is also presented. The Chern-Simons theory of anyons (particles obeying fractional statistics) and composite fermions (related to Hall systems) is given, in detail. Numerical studies, which play the important role in quantum Hall theories, are presented for spherical systems (Haldane sphere). The composite fermion theory is tested in numerical studies. The concept of the hierarchy of condensed states of composite fermion excitations is introduced (in analogy to the Haldane hierarchy)1). The hierarchies of odd-denominator states and even-denominator states are presented. The BCS paired Hall state is also discussed. The introduction into multi-component quantum Hall systems and spin quantum Hall systems is sketched.
  1)First condensed states of composite fermion excitations have been very recently confirmed in the experiment (Pan et al. Phys. Rev. Lett. 90 (2003) 016801). a sample of this book is available at http://www.oup.co.uk/isbn/0-19-852870-1△ Less"
Initialization of a nuclear spin system over the quantum Hall regime for quantum information processing,"Authors:R G Mani,W B Johnson,V. Narayanamurti","Abstract:The application of the quantum mechanical properties of physical systems to realize novel computational schemes and innovative device functions have been topics of recent interest. Proposals for associated devices are to be found in diverse branches of physics. Here, we are concerned with the experimental realization of some elements needed for quantum information processing using nuclear spin i…▽ MoreThe application of the quantum mechanical properties of physical systems to realize novel computational schemes and innovative device functions have been topics of recent interest. Proposals for associated devices are to be found in diverse branches of physics. Here, we are concerned with the experimental realization of some elements needed for quantum information processing using nuclear spin immersed in a confined electronic system in the quantum Hall regime. Thus, we follow a spin-handling approach that (a) uses the Overhauser effect in the quantum Hall regime to realize a large nuclear polarization at relatively high temperatures, (b) detects the nuclear spin state by measuring the influence of the associated magnetic field on Electron Spin Resonance, and (c) seeks to apply the electronic spin exciton as the spin transfer mechanism. Some measurements examining the viability of this approach are shown, and the utility of the approach for initializing a nuclear spin system at a relatively high temperature is pointed out△ Less"
Excitonic complexes in quantum Hall systems,"Authors:Arkadiusz Wojs,John J Quinn","Abstract:The formation and various possible decay processes of neutral and charged excitonic complexes in electronic integral and fractional quantum Hall systems are discussed. The excitonic complexes are bound states of a small number of the relevant negatively and positively charged quasiparticles (e.g., conduction electrons and valence holes, reversed-spin electrons and spin holes, Laughlin quasielect…▽ MoreThe formation and various possible decay processes of neutral and charged excitonic complexes in electronic integral and fractional quantum Hall systems are discussed. The excitonic complexes are bound states of a small number of the relevant negatively and positively charged quasiparticles (e.g., conduction electrons and valence holes, reversed-spin electrons and spin holes, Laughlin quasielectrons and quasiholes, composite fermions) that occur in an electron system under specific conditions (e.g., electron density, well width, electric and magnetic fields, or hydrostatic pressure). The examples of such bound states are interband neutral and charged excitons, fractionally charged ""anyon excitons"", spin waves, skyrmions, or ""skyrmion excitons"". Their possible decay processes include radiative recombination, experimentally observed in photoluminescence or far infrared emission, or spin transitions, important in the context of nuclear spin relaxation.△ Less"
Complex structures on affine motion groups,"Authors:M. L. Barberis,I. Dotti","Abstract:We study existence of complex structures on semidirect products\g \oplus_ρ \vwhere\gis a real Lie algebra andρis a representation of\gon\v. Our first examples, the Euclidean algebra\e(3)and the Poincaré algebra\e(2,1), carry complex structures obtained by deformation of a regular complex structure on\sl (2, \c). We also exhibit a complex structure on the Galilean al…▽ MoreWe study existence of complex structures on semidirect products\g \oplus_ρ \vwhere\gis a real Lie algebra andρis a representation of\gon\v. Our first examples, the Euclidean algebra\e(3)and the Poincaré algebra\e(2,1), carry complex structures obtained by deformation of a regular complex structure on\sl (2, \c). We also exhibit a complex structure on the Galilean algebra\G(3,1). We construct next a complex structure on\g \oplus_ρ \vstarting with one on\gunder certain compatibility assumptions onρ.
  As an application of our results we obtain that there existsk\in \{0,1\}such that(S^1)^k \times E(n)admits a left invariant complex structure, whereS^1is the circle and E(n) denotes the Euclidean group. We also prove that the Poincaré groupP^{4k+3}has a natural left invariant complex structure.
  In case\dim \g= \dim \v, then there is an adapted complex structure on\g\oplus_ρ \vprecisely whenρdetermines a flat, torsion-free connection on\g. Ifρis self-dual,\g \oplus_ρ\vcarries a natural symplectic structure as well. If, moreover,ρcomes from a metric connection then\g\oplus_ρ \vpossesses a pseudo-Kähler structure.
  We prove that the tangent bundleTGof a Lie groupGcarrying a flat torsion free connection\nablaand a parallel complex structure possesses a hypercomplex structure. More generally, by an iterative procedure, we can obtain Lie groups carrying a family of left invariant complex structures which generate any prescribed real Clifford algebra.△ Less"
Cut vertices in commutative graphs,"Authors:James Conant,Ferenc Gerlits,Karen Vogtmann","Abstract:The homology of Kontsevich's commutative graph complex parameterizes finite type invariants of odd dimensional manifolds. This {\it graph homology} is also the twisted homology of Outer Space modulo its boundary, so gives a nice point of contact between geometric group theory and quantum topology. In this paper we give two different proofs (one algebraic, one geometric) that the commutative grap…▽ MoreThe homology of Kontsevich's commutative graph complex parameterizes finite type invariants of odd dimensional manifolds. This {\it graph homology} is also the twisted homology of Outer Space modulo its boundary, so gives a nice point of contact between geometric group theory and quantum topology. In this paper we give two different proofs (one algebraic, one geometric) that the commutative graph complex is quasi-isomorphic to the quotient complex obtained by modding out by graphs with cut vertices. This quotient complex has the advantage of being smaller and hence more practical for computations. In addition, it supports a Lie bialgebra structure coming from a bracket and cobracket we defined in a previous paper. As an application, we compute the rational homology groups of the commutative graph complex up to rank 7.△ Less"
The high temperature phase transition in SU(N) gauge theories,"Authors:Biagio Lucini,Michael Teper,Urs Wenger","Abstract:We calculate the continuum value of the deconfining temperature in units of the string tension for SU(4), SU(6) and SU(8) gauge theories, and we recalculate its value for SU(2) and SU(3). We find that theN-dependence for2 \leq N \leq 8is well fitted byT_c/\sqrt{sigma} = 0.596(4) + 0.453(30)/N^2, showing a rapid convergence to the large-N limit. We confirm our earlier result that the ph…▽ MoreWe calculate the continuum value of the deconfining temperature in units of the string tension for SU(4), SU(6) and SU(8) gauge theories, and we recalculate its value for SU(2) and SU(3). We find that theN-dependence for2 \leq N \leq 8is well fitted byT_c/\sqrt{sigma} = 0.596(4) + 0.453(30)/N^2, showing a rapid convergence to the large-N limit. We confirm our earlier result that the phase transition is first order forN \geq 3and that it becomes stronger with increasingN. We also confirm that asNincreases the finite volume corrections become rapidly smaller and the phase transition becomes visible on ever smaller volumes. We interpret the latter as being due to the fact that the tension of the domain wall that separates the confining and deconfining phases increases rapidly withN. We speculate on the connection to Eguchi-Kawai reduction and to the idea of a Master Field.△ Less"
Anyon exciton revisited,"Authors:D. G. W. Parfitt,M. E. Portnoi",Abstract:We review the main results of the anyon exciton model in light of recent criticism by Wojs and Quinn. We show that the appearance of fractionally charged anyon ions at the bottom of their numerically calculated excitation spectra is an artefact caused by finite-size effects in a spherical geometry.We review the main results of the anyon exciton model in light of recent criticism by Wojs and Quinn. We show that the appearance of fractionally charged anyon ions at the bottom of their numerically calculated excitation spectra is an artefact caused by finite-size effects in a spherical geometry.△ Less
Inflationary parameters and primordial perturbation spectra,Authors:David Wands,"Abstract:I discuss how parameters describing inflation in the very early universe may be related to primordial perturbation spectra. Precision observations of anisotropies in the cosmic microwave background (CMB) such as those provided by the WMAP satellite offer an unprecedented window onto the physics of the very early universe. To theorists exploring speculative models of physics at high energies, the…▽ MoreI discuss how parameters describing inflation in the very early universe may be related to primordial perturbation spectra. Precision observations of anisotropies in the cosmic microwave background (CMB) such as those provided by the WMAP satellite offer an unprecedented window onto the physics of the very early universe. To theorists exploring speculative models of physics at high energies, the CMB offers us the chance to put our ideas to the test.△ Less"
Recent Results from the MAXIMA Experiment,"Authors:Andrew H. Jaffe,Matthew Abroe,Julian Borrill,Jeff Collins,Pedro Ferreira,Shaul Hanany,Brad Johnson,Adrian T. Lee,Tomotake Matsumura,Bahman Rabii,Tom Renbarger,Paul Richards,George F. Smoot,Radek Stompor,Huan Tran,Celeste Winant,Jiun-Huei Proty Wu","Abstract:MAXIMA is a balloon-borne platform for measuring the anisotropy of the Cosmic Microwave Background (CMB). It has measured the CMB power spectrum with a ten-arcminute FWHM beam, corresponding to a detection of the power spectrum out to spherical harmonic multipole l~1000. The spectrum is consistent with a flat Universe with a nearly scale-invariant initial spectrum of adiabatic density fluctuatio…▽ MoreMAXIMA is a balloon-borne platform for measuring the anisotropy of the Cosmic Microwave Background (CMB). It has measured the CMB power spectrum with a ten-arcminute FWHM beam, corresponding to a detection of the power spectrum out to spherical harmonic multipole l~1000. The spectrum is consistent with a flat Universe with a nearly scale-invariant initial spectrum of adiabatic density fluctuations. Moreover, the MAXIMA data are free from any notable non-Gaussian contamination and from foreground dust emission. In the same region, the WMAP experiment observes the same structure as that observed by MAXIMA, as evinced by analysis of both maps and power spectra. The next step in the evolution of the MAXIMA program is MAXIPOL, which will observe the polarization of the CMB with comparable resolution and high sensitivity over a small patch of the sky.△ Less"
Time machine (1988--2001),Authors:S. Krasnikov,Abstract:A very brief and popular account of the time machine problem.A very brief and popular account of the time machine problem.△ Less
Weighting CMB and Galactic synchrotron polarisation,Authors:Carlo Baccigalupi,"Abstract:We review the present knowledge of the diffuse Galactic synchrotron emission in polarisation. At microwave frequencies, we assess the expected contamination to the CMB polarisation angular power spectrum, forEandBmodes, as expected after the WMAP first year measurements.We review the present knowledge of the diffuse Galactic synchrotron emission in polarisation. At microwave frequencies, we assess the expected contamination to the CMB polarisation angular power spectrum, forEandBmodes, as expected after the WMAP first year measurements.△ Less"
Cosmological constraints in Lambda-CDM and Quintessence paradigms with Archeops,"Authors:M. Douspis,A. Riazuelo,Y. Zolnierowski,A. Blanchard,the Archeops Collaboration","Abstract:We review the cosmological constraints put by the current CMB experiment including the recent ARCHEOPS data, in the framework of Lambda-CDM and quintessence paradigm. We show that well chosen combinations of constraints from different cosmological observations lead to precise measurements of cosmological parameters. The Universe seems flat with a 70 percents contribution of dark energy with an e…▽ MoreWe review the cosmological constraints put by the current CMB experiment including the recent ARCHEOPS data, in the framework of Lambda-CDM and quintessence paradigm. We show that well chosen combinations of constraints from different cosmological observations lead to precise measurements of cosmological parameters. The Universe seems flat with a 70 percents contribution of dark energy with an equation of state very close to those of the vacuum.△ Less"
The value of the equation of state of dark energy,Authors:Raul Jimenez,"Abstract:From recent CMB and Large Scale Structure observations the value of the equation of state of dark energy, assuming it to be constant in time, is constrained to be -1.3<w<-0.7 at the 95% confidence level: consistent with dark energy being a classical vacuum term. Here we describe two novel and independent methods, sensitive to different systematics, that give the same value for w and similar conf…▽ MoreFrom recent CMB and Large Scale Structure observations the value of the equation of state of dark energy, assuming it to be constant in time, is constrained to be -1.3<w<-0.7 at the 95% confidence level: consistent with dark energy being a classical vacuum term. Here we describe two novel and independent methods, sensitive to different systematics, that give the same value for w and similar confidence regions. This suggests that systematics are not an issue in current determinations of w. The first method yields a measurement ofwthat relies on the minimum number of model-dependent parameters; the second method is a non-parametric measurement of the time dependence of w(z). We also present a method to statistically determine the edge of a distribution.△ Less"
The shape of the CMB power spectrum,Authors:C. J. Odman,"Abstract:The recent WMAP data represents a milestone in cosmology and helps constrain cosmological parameters with unprecedented accuracy. In this work we combine the WMAP data with previous CMB anisotropy measurements at smaller angular scales to characterize the shape of the CMB anisotropy power spectrum. We carry out a phenomenological analysis of the data. By allowing non-physical shapes of the power…▽ MoreThe recent WMAP data represents a milestone in cosmology and helps constrain cosmological parameters with unprecedented accuracy. In this work we combine the WMAP data with previous CMB anisotropy measurements at smaller angular scales to characterize the shape of the CMB anisotropy power spectrum. We carry out a phenomenological analysis of the data. By allowing non-physical shapes of the power spectrum we analyse high and low frequency experiments separately and together. We find that WMAP dramatically constrains the power spectrum up to l \~ 700. On smaller scales, the data show discrepancies that can be associated with experimental systematics. If we combine all types of experiments, the observable features in the power spectrum are in excellent agreement with the WMAP cosmological parameter estimation. This work illustrates the advantages of a model-independent approach to understanding experimental systematics that might affect CMB observations.△ Less"
A simplification of entanglement purification,"Authors:Jin-Yuan Hsieh,Che-Ming Li,Der-San Chuu","Abstract:…purification local operations and classical communications. The protocols proposed in this work, in which two state transformations are used, perform better than the IBM andOxfordprotocols in the sense that they require fewer operation times in yielding a same amount of the desired pure state. One of the proposed protocol in this work can even lead to a hi…▽ MoreAn idea of hybrid maps is proposed to establish standard entanglement purification protocols which guarantee to purify any distillable state to a desired maximally entangled pure state all by the standard purification local operations and classical communications. The protocols proposed in this work, in which two state transformations are used, perform better than the IBM andOxfordprotocols in the sense that they require fewer operation times in yielding a same amount of the desired pure state. One of the proposed protocol in this work can even lead to a higher improved output yield when it is combined with the hashing protocol, as compared with the combined algorithm consisting of theOxfordand the hashing protocol.△ Less"
Virtual Time Horizon Control via Communication Network Design,"Authors:Z. Toroczkai,G. Korniss,M. A. Novotny,H. Guclu","Abstract:We consider massively parallel discrete event simulations where the communication topology among the processing elements is a complex graph. In the case of regular topologies we review recent results on virtual time horizon management. First we analyze the computational scalability of the conservative massively parallel update scheme for discrete event simulations by using the analogy with a wel…▽ MoreWe consider massively parallel discrete event simulations where the communication topology among the processing elements is a complex graph. In the case of regular topologies we review recent results on virtual time horizon management. First we analyze the computational scalability of the conservative massively parallel update scheme for discrete event simulations by using the analogy with a well-known surface growth model, then we show that a simple modification of the regular PE communication topology to a small-world topology will also ensure measurement scalability. This leads to a fully scalable parallel simulation for systems with asynchronous dynamics and short-range interactions. Finally, we present numerical results for the evolution of the virtual time horizon on scale-free Barabasi-Albert networks serving as communication topology among the processing elements.△ Less"
The cosmological constant and the paradigm of adiabaticity,Authors:Roberto Trotta,"Abstract:We discuss the value of the cosmological constant as recovered from CMB and LSS data and the robustness of the results when general isocurvature initial conditions are allowed for, as opposed to purely adiabatic perturbations. The Bayesian and frequentist statistical approaches are compared. It is shown that pre-WMAP CMB and LSS data tend to be incompatible with a non-zero cosmological constant,…▽ MoreWe discuss the value of the cosmological constant as recovered from CMB and LSS data and the robustness of the results when general isocurvature initial conditions are allowed for, as opposed to purely adiabatic perturbations. The Bayesian and frequentist statistical approaches are compared. It is shown that pre-WMAP CMB and LSS data tend to be incompatible with a non-zero cosmological constant, regardless of the type of initial conditions and of the statistical approach. The non-adiabatic contribution is constrained to be < 40% (2sigma c.l.).△ Less"
CMB-induced Cluster Polarization as a Cosmological Probe,"Authors:Daniel Baumann,Asantha Cooray","Abstract:Scattering of the temperature anisotropy quadrupole by free electrons in galaxy clusters leads to a secondary polarization signal in the cosmic microwave background (CMB) fluctuations. At low redshifts, the temperature quadrupole contains a significant contribution from the integrated Sachs-Wolfe (ISW) effect associated with the growth of density fluctuations. Using polarization data from a samp…▽ MoreScattering of the temperature anisotropy quadrupole by free electrons in galaxy clusters leads to a secondary polarization signal in the cosmic microwave background (CMB) fluctuations. At low redshifts, the temperature quadrupole contains a significant contribution from the integrated Sachs-Wolfe (ISW) effect associated with the growth of density fluctuations. Using polarization data from a sample of clusters over a wide range in redshift, one can statistically establish the presence of the ISW effect and determine its redshift evolution. Given the strong dependence of the ISW effect on the background cosmology, cluster polarization can eventually be used as a powerful probe of dark energy. As a further application, we also discuss how it might be used to understand the potential lack of power on large scales.△ Less"
A polarized view of string topology,"Authors:Ralph L. Cohen,Veronique Godin","Abstract:Let M be a closed, connected manifold, and LM its loop space. In this paper we describe closed string topology operations in h_*(LM), where h_* is a generalized homology theory that supports an orientation of M. We will show that these operations give h_*(LM) the structure of a unital, commutative Frobenius algebra without a counit. Equivalently they describe a positive boundary, two dimensional…▽ MoreLet M be a closed, connected manifold, and LM its loop space. In this paper we describe closed string topology operations in h_*(LM), where h_* is a generalized homology theory that supports an orientation of M. We will show that these operations give h_*(LM) the structure of a unital, commutative Frobenius algebra without a counit. Equivalently they describe a positive boundary, two dimensional topological quantum field theory associated to h_*(LM). This implies that there are operations corresponding to any surface with p incoming and q outgoing boundary components, so long as q >0. The absence of a counit follows from the nonexistence of an operation associated to the disk, D^2, viewed as a cobordism from the circle to the empty set. We will study homological obstructions to constructing such an operation, and show that in order for such an operation to exist, one must take h_*(LM) to be an appropriate homological pro-object associated to the loop space. Motivated by this, we introduce a prospectrum associated to LM when M has an almost complex structure. Given such a manifold its loop space has a canonical polarization of its tangent bundle, which is the fundamental feature needed to define this prospectrum. We refer to this as the ""polarized Atiyah - dual"" of LM . An appropriate homology theory applied to this prospectrum would be a candidate for a theory that supports string topology operations associated to any surface, including closed surfaces.△ Less"
Nonlinear Thomas-Fermi-Poisson theory of screening for a Hall bar under strong magnetic fields,"Authors:A. Siddiki,Rolf R. Gerhardts","Abstract:Low-temperature screening properties of the inhomogeneous two-dimensional electron gas in a Hall bar subjected to a strong perpendicular magnetic field are explored using a self-consitent approach. An external oscillating modulation potential with an amplitude of the order of the cyclotron energy is added to the electron-confining background potential, and the resulting change of the self-consis…▽ MoreLow-temperature screening properties of the inhomogeneous two-dimensional electron gas in a Hall bar subjected to a strong perpendicular magnetic field are explored using a self-consitent approach. An external oscillating modulation potential with an amplitude of the order of the cyclotron energy is added to the electron-confining background potential, and the resulting change of the self-consistent potential is investigated as a function of modulation strength, magnetic field, and temperature. The consequences of Landau-level pinning and the interplay of compressible and incompressible regions for the resulting strongly non-linear screening phenomena are explained.△ Less"
Foundations of quantum theory and quantum information applications,Authors:Ernesto F. Galvao,"Abstract:This thesis establishes a number of connections between foundational issues in quantum theory, and some quantum information applications. It starts with a review of quantum contextuality and non-locality, multipartite entanglement characterisation, and of a few quantum information protocols.
  Quantum non-locality and contextuality are shown to be essential for different implementations of quant…▽ MoreThis thesis establishes a number of connections between foundational issues in quantum theory, and some quantum information applications. It starts with a review of quantum contextuality and non-locality, multipartite entanglement characterisation, and of a few quantum information protocols.
  Quantum non-locality and contextuality are shown to be essential for different implementations of quantum information protocols known as quantum random access codes and quantum communication complexity protocols. I derive sufficient experimental conditions for tests of these quantum properties.
  I also discuss how the distribution of quantum information through quantum cloning processes can be useful in quantum computing. Regarding entanglement characterisation, some results are obtained relating two problems, that of additivity of the relative entropy of entanglement, and that of identifying different types of tripartite entanglement in the asymptotic regime of manipulations of many copies of a given state.
  The thesis ends with a description of an information processing task in which a single qubit substitutes for an arbitrarily large amount of classical communication. This result is interpreted in different ways: as a gap between quantum and classical computation space complexity; as a bound on the amount of classical communication necessary to simulate entanglement; and as a basic result on hidden-variable theories for quantum mechanics. I also show that the advantage of quantum over classical communication can be established in a feasible experiment.△ Less"
Entropic Geometry from Logic,Authors:Bob Coecke,"Abstract:We produce a probabilistic space from logic, both classical and quantum, which is in addition partially ordered in such a way that entropy is monotone. In particular do we establish the following equation:
  Quantitative Probability = Logic + Partiality of Knowledge + Entropy.
  That is: 1. A finitary probability space Δ^n (=all probability measures on {1,...,n}) can be fully and faithfully repr…▽ MoreWe produce a probabilistic space from logic, both classical and quantum, which is in addition partially ordered in such a way that entropy is monotone. In particular do we establish the following equation:
  Quantitative Probability = Logic + Partiality of Knowledge + Entropy.
  That is: 1. A finitary probability space Δ^n (=all probability measures on {1,...,n}) can be fully and faithfully represented by the pair consisting of the abstraction D^n (=the object up to isomorphism) of a partially ordered set (Δ^n,\sqsubseteq), and, Shannon entropy; 2. D^n itself can be obtained via a systematic purely order-theoretic procedure (which embodies introduction of partiality of knowledge) on an (algebraic) logic. This procedure applies to any poset A; D_A\cong(Δ^n,\sqsubseteq) when A is the n-element powerset and D_A\cong(Ω^n,\sqsubseteq), the domain of mixed quantum states, when A is the lattice of subspaces of a Hilbert space.
  (We refer to http://web.comlab.ox.ac.uk/oucl/publications/tr/rr-02-07.html for a domain-theoretic context providing the notions of approximation and content.)△ Less"
Equivalence Postulate and the Quantum Potential of Two Free Particles,Authors:Marco Matone,"Abstract:Commutativity of the diagram of the maps connecting three one--particle state, implied by the Equivalence Postulate (EP), gives a cocycle condition which unequivocally leads to the quantum Hamilton--Jacobi equation. Energy quantization is a direct consequences of the local homeomorphicity of the trivializing map. We review the EP and show that the quantum potential for two free particles, which…▽ MoreCommutativity of the diagram of the maps connecting three one--particle state, implied by the Equivalence Postulate (EP), gives a cocycle condition which unequivocally leads to the quantum Hamilton--Jacobi equation. Energy quantization is a direct consequences of the local homeomorphicity of the trivializing map. We review the EP and show that the quantum potential for two free particles, which depends on constants which may have a geometrical interpretation, plays the role of interaction term that admits solutions which do not vanish in the classical limit.△ Less"
TeV-scale Z' bosons in intersecting D-brane SM-like models,Authors:D. M. Ghilencea,"Abstract:Recent string constructions with intersecting D6 and D5 brane models succeeded in predicting in the low energy limit a symmetry group and a fermionic spectrum similar to that of the SM. In such constructions additional U(1) fields are a generic presence and they (or linear combinations thereof) become massive through a string mechanism which couples them to RR two-forms. $U(1)_Y$ of hypercharge…▽ MoreRecent string constructions with intersecting D6 and D5 brane models succeeded in predicting in the low energy limit a symmetry group and a fermionic spectrum similar to that of the SM. In such constructions additional U(1) fields are a generic presence and they (or linear combinations thereof) become massive through a string mechanism which couples them to RR two-forms. $U(1)_Y$ of hypercharge emerges as a linear combination of the initial U(1) symmetries while the massive U(1) fields induce (through mixing with Z boson) corrections on Z mass. $ρ$ parameter constraints on the latter allow one to set lower bounds on the string scale in the TeV region. These can then be used to {\it predict} lower bounds on the masses of the additional $Z'$ bosons, without specific assumptions about the compactification volume.△ Less"
Quantum Hall effect in a p-type heterojunction with a lateral surface quantum dot superlattice,"Authors:V. Ya. Demikhovskii,D. V. Khomitskiy","Abstract:The quantization of Hall conductance in a p-type heterojunction with lateral surface quantum dot superlattice is investigated. The topological properties of the four-component hole wavefunction are studied both in r- and k-spaces. New method of calculation of the Hall conductance in a 2D hole gas described by the Luttinger Hamiltonian and affected by lateral periodic potential is proposed, based…▽ MoreThe quantization of Hall conductance in a p-type heterojunction with lateral surface quantum dot superlattice is investigated. The topological properties of the four-component hole wavefunction are studied both in r- and k-spaces. New method of calculation of the Hall conductance in a 2D hole gas described by the Luttinger Hamiltonian and affected by lateral periodic potential is proposed, based on the investigation of four-component wavefunction singularities in k-space. The deviations from the quantization rules for Hofstadter ""butterfly"" for electrons are found, and the explanation of this effect is proposed. For the case of strong periodic potential the mixing of magnetic subbands is taken into account, and the exchange of the Chern numbers between magnetic subands is discussed.△ Less"
Constraint Satisfaction by Survey Propagation,"Authors:A. Braunstein,M. Mezard,M. Weigt,R. Zecchina","Abstract:Survey Propagation is an algorithm designed for solving typical instances of random constraint satisfiability problems. It has been successfully tested on random 3-SAT and random $G(n,\frac{c}{n})$ graph 3-coloring, in the hard region of the parameter space. Here we provide a generic formalism which applies to a wide class of discrete Constraint Satisfaction Problems.Survey Propagation is an algorithm designed for solving typical instances of random constraint satisfiability problems. It has been successfully tested on random 3-SAT and random $G(n,\frac{c}{n})$ graph 3-coloring, in the hard region of the parameter space. Here we provide a generic formalism which applies to a wide class of discrete Constraint Satisfaction Problems.△ Less"
Temperature fluctuations and mixtures of equilibrium states in the canonical ensemble,Authors:Hugo Touchette,"Abstract:It has been suggested recently that `$q$-exponential' distributions which form the basis of Tsallis' non-extensive thermostatistical formalism may be viewed as mixtures of exponential (Gibbs) distributions characterized by a fluctuating inverse temperature. In this paper, we revisit this idea in connection with a detailed microscopic calculation of the energy and temperature fluctuations present…▽ MoreIt has been suggested recently that `$q$-exponential' distributions which form the basis of Tsallis' non-extensive thermostatistical formalism may be viewed as mixtures of exponential (Gibbs) distributions characterized by a fluctuating inverse temperature. In this paper, we revisit this idea in connection with a detailed microscopic calculation of the energy and temperature fluctuations present in a finite vessel of perfect gas thermally coupled to a heat bath. We find that the probability density related to the inverse temperature of the gas has a form similar to a $χ^2$ density, and that the `mixed' Gibbs distribution inferred from this density is non-Gibbsian. These findings are compared with those obtained by a number of researchers who worked on mixtures of Gibbsian distributions in the context of velocity difference measurements in turbulent fluids as well as secondaries distributions in nuclear scattering experiments.△ Less"
Phase Synchronization in Temperature and Precipitation Records,"Authors:Diego Rybski,Shlomo Havlin,Armin Bunde",Abstract:…phase synchronization between atmospheric variables such as daily mean temperature and daily precipitation records. We find significant phase synchronization between records ofOxfordand Vienna as well as between the records of precipitation and temperature in each city. To find the time delay in the synchronization between the records we study the time lag…▽ MoreWe study phase synchronization between atmospheric variables such as daily mean temperature and daily precipitation records. We find significant phase synchronization between records ofOxfordand Vienna as well as between the records of precipitation and temperature in each city. To find the time delay in the synchronization between the records we study the time lag phase synchronization when the records are shifted by a variable time interval of days. We also compare the results of the method with the classical cross-correlation method and find that in certain cases the phase synchronization yields more significant results.△ Less
"Warped brane world supergravity, flipping, and the Scherk-Schwarz mechanism","Authors:Zygmunt Lalak,Radoslaw Matyszkiewicz","Abstract:We demonstrate the relation between the Scherk-Schwarz mechanism and flipped gauged brane-bulk supergravities in five dimensions. We discuss the form of supersymmetry violating Scherk-Schwarz terms in pure supergravity and in supergravity coupled to matter. Although the Lagrangian mass terms that arise as the result of the Scherk-Schwarz redefinition of fields are naturally of the order of the i…▽ MoreWe demonstrate the relation between the Scherk-Schwarz mechanism and flipped gauged brane-bulk supergravities in five dimensions. We discuss the form of supersymmetry violating Scherk-Schwarz terms in pure supergravity and in supergravity coupled to matter. Although the Lagrangian mass terms that arise as the result of the Scherk-Schwarz redefinition of fields are naturally of the order of the inverse radius of the orbifold, the effective 4d physical mass terms are rather set by the scale \sqrt{|\barΛ|}, where \barΛ is the 4d cosmlogical constant.△ Less"
Cosmology in Horava-Witten M-Theory,"Authors:R. Arnowitt,James Dent,B. Dutta","Abstract:The cosmology of the Horava-Witten M-Theory reduced to five dimensions retaining the volume modulus is considered. Brane matter is considered as a perturbation on the vacuum solution, and the question of under what circumstances does the theory give rise to the standard RWF cosmology is examined. It is found that for static solutions, one obtains a consistent solution of the bulk field equations…▽ MoreThe cosmology of the Horava-Witten M-Theory reduced to five dimensions retaining the volume modulus is considered. Brane matter is considered as a perturbation on the vacuum solution, and the question of under what circumstances does the theory give rise to the standard RWF cosmology is examined. It is found that for static solutions, one obtains a consistent solution of the bulk field equations and the brane boundary conditions only for pure radiation on the branes. (A similar result holds if additional 5-branes are added in the bulk.) If one stabilizes the fifth dimension in an ad hoc manner, a similar inconsistency still occurs (at least for a Hubble constant that has no dependence on y, the fifth dimension.) Within this framework, the possibility of recovering the RWF cosmology still remains if the volume modulus and /or the distance between branes becomes time dependent, under which circumstances the Hubble constant must then depend on y (unless the fifth dimension and volume modulus expand at precisely the same rate).△ Less"
"S-Branes, Negative Tension Branes and Cosmology","Authors:F. Quevedo,G. Tasinato,I. Zavala","Abstract:A general class of solutions of string background equations is studied and its physical interpretations are presented. These solutions correspond to generalizations of the standard black p-brane solutions to surfaces with curvature k=-1,0. The relation with the recently introduced S-branes is provided. The mass, charge, entropy and Hawking temperature are computed, illustrating the interpretatio…▽ MoreA general class of solutions of string background equations is studied and its physical interpretations are presented. These solutions correspond to generalizations of the standard black p-brane solutions to surfaces with curvature k=-1,0. The relation with the recently introduced S-branes is provided. The mass, charge, entropy and Hawking temperature are computed, illustrating the interpretation in terms of negative tension branes. Their cosmological interpretation is discussed as well as their potential instability under small perturbations.△ Less"
Twisted Moduli and Supersymmetry Breaking,"Authors:S. F. King,D. A. J. Rayner",Abstract:We discuss how localized twisted moduli in type I string theory can provide a string realization of brane world supersymmetry breaking models.We discuss how localized twisted moduli in type I string theory can provide a string realization of brane world supersymmetry breaking models.△ Less
Supersymmetric Higgs Bosons in a 5D Orbifold Model,"Authors:V. Di Clemente,S. F. King,D. A. J. Rayner","Abstract:We analyze the phenomenology of the Higgs sector in a 5D model compactified on an $S_1/Z_2$ orbifold with a compactification scale $M_C \sim {\mathcal O}(TeV)$ where supersymmetry breaking is localized on a brane at one of the fixed points. We show that the conventional MSSM Higgs boson mass bounds in 4D can be violated when we allow the gauge sector, Higgs and third family multiplets to live in…▽ MoreWe analyze the phenomenology of the Higgs sector in a 5D model compactified on an $S_1/Z_2$ orbifold with a compactification scale $M_C \sim {\mathcal O}(TeV)$ where supersymmetry breaking is localized on a brane at one of the fixed points. We show that the conventional MSSM Higgs boson mass bounds in 4D can be violated when we allow the gauge sector, Higgs and third family multiplets to live in the fifth extra dimension.△ Less"
F-enomenology,Authors:Dimitri V. Nanopoulos,"Abstract:The advantages of Flipped SU(5) over conventional Supersymmetric GUTs, like SU(5), are discussed. Recent values of the strong coupling at M_Z, sin-squared theta-Weinberg, g-2 of the muon, and the lower limit on the proton lifetime for the (K+, anti-neutrino) mode point directly to Flipped SU(5) as the simplest way to avoid potential pitfalls. It is shown that ""F(lipped)-enomenology"" accomodates…▽ MoreThe advantages of Flipped SU(5) over conventional Supersymmetric GUTs, like SU(5), are discussed. Recent values of the strong coupling at M_Z, sin-squared theta-Weinberg, g-2 of the muon, and the lower limit on the proton lifetime for the (K+, anti-neutrino) mode point directly to Flipped SU(5) as the simplest way to avoid potential pitfalls. It is shown that ""F(lipped)-enomenology"" accomodates easily all presently available low-energy data, favoring a rather ""light"" supersymmetric spectrum while yielding the right amount of Cold Dark Matter and a proton lifetime in the ((e+/muon+), pi-zero) mode which is beyond the present experimental limit yet still possibly accessible to a further round of experiments.△ Less"
Anomaly Mediated Supersymmetry Breaking From a String Theory Perspective,Authors:Brent D. Nelson,"Abstract:The general form of soft supersymmetry breaking terms at one loop for effective supergravity models based on the weakly-coupled heterotic string are presented, with special emphasis on those terms arising from the superconformal anomaly. The standard ""anomaly-mediated"" soft terms are observed to be a special case of a more general class of models. Some phenomenological consequences are considere…▽ MoreThe general form of soft supersymmetry breaking terms at one loop for effective supergravity models based on the weakly-coupled heterotic string are presented, with special emphasis on those terms arising from the superconformal anomaly. The standard ""anomaly-mediated"" soft terms are observed to be a special case of a more general class of models. Some phenomenological consequences are considered and a comparison is made with scenarios involving universal soft terms.△ Less"
"Quark, Lepton and Neutrino Masses In Horava-Witten Inspired Models","Authors:R. Arnowitt,B. Dutta,B. Hu","Abstract:Horava-Witten M-Theory offers new ways in achieving the quark and lepton mass hierarchies not naturally available in supergravity unified models. In previous work, based on a torus fibered Calabi-Yau manifold with a del Pezzo base dP_7,a three generation SU(5)model with Wilson line breaking to the Standard Model was constructed. It was seen that if the additional 5- branes clustered near the dis…▽ MoreHorava-Witten M-Theory offers new ways in achieving the quark and lepton mass hierarchies not naturally available in supergravity unified models. In previous work, based on a torus fibered Calabi-Yau manifold with a del Pezzo base dP_7,a three generation SU(5)model with Wilson line breaking to the Standard Model was constructed. It was seen that if the additional 5- branes clustered near the distant orbifold plane, it was possible that such models could generate the observed hierarchies of quark masses without undue fine tuning. We update these results here and extend them to include the charged leptons. A new mechanism for generating small neutrino masses (different from the ususal seesaw mechanism)arises naturally from a possible cubic holomorphic term in the Kahler potential when supersymmetry is broken. We show that the LMA solution to neutrino masses can occur, with a good fit to all neutrino oscillation data. The model implies the existance of the μ-> e γdecay even for universal slepton soft breaking masses, at rates accessible to the next round of experiments.△ Less"
Grand unification of flavor by orbifold twisting $Z_2$ and $Z_2\times Z_2^\prime$,Authors:Jihn E. Kim,"Abstract:The grand unification of flavor(GUF) in extra dimensions is discussed. After reviewing the old GUF, I present a GUF model SU(7) in 5D with the recently popular field theoretic orbifold compactification.The grand unification of flavor(GUF) in extra dimensions is discussed. After reviewing the old GUF, I present a GUF model SU(7) in 5D with the recently popular field theoretic orbifold compactification.△ Less"
Extra matter at low energy,Authors:C. Munoz,"Abstract:Assuming that the Standard Model arises from the $E_8\times E_8$ Heterotic Superstring, we try to solve the discrepancy between the unification scale predicted by this theory ($\approx g_{GUT}\times 5.27\cdot 10^{17}$ GeV) and the value deduced from LEP experiments ($\approx 2\cdot 10^{16}$ GeV). This will allow us to predict the presence at low energies of three generations of supersymmetric Hi…▽ MoreAssuming that the Standard Model arises from the $E_8\times E_8$ Heterotic Superstring, we try to solve the discrepancy between the unification scale predicted by this theory ($\approx g_{GUT}\times 5.27\cdot 10^{17}$ GeV) and the value deduced from LEP experiments ($\approx 2\cdot 10^{16}$ GeV). This will allow us to predict the presence at low energies of three generations of supersymmetric Higgses and vector-like colour triplets.△ Less"
Higgs and SUSY Particle Predictions from SO(10) Yukawa Unification,Authors:Stuart Raby,"Abstract:In this talk we assume SO(10) boundary conditions at the GUT scale, including unification for the third generation Yukawa couplings $λ_t = λ_b = λ_τ$. We find that this assumption is only consistent with the low energy data in a narrow region of soft SUSY breaking parameter space. We discuss the consequences of this result for Higgs and SUSY searches.In this talk we assume SO(10) boundary conditions at the GUT scale, including unification for the third generation Yukawa couplings $λ_t = λ_b = λ_τ$. We find that this assumption is only consistent with the low energy data in a narrow region of soft SUSY breaking parameter space. We discuss the consequences of this result for Higgs and SUSY searches.△ Less"
An investigation into a half page from Newton's Principia in the wake of Chandra,"Authors:R. Golub,W. M. Snow","Abstract:There is a section in Chandrashekar's ''Newton's Principia for the Common Reader '', (Clarendon Press,Oxford, 1995) in which he claims to find a small error in the Principia. . However we believe that there is a mistake of interpretation underlying Chandra's claim and that the Principia is correct as it stands. This short paper d…▽ MoreThere is a section in Chandrashekar's ''Newton's Principia for the Common Reader '', (Clarendon Press,Oxford, 1995) in which he claims to find a small error in the Principia. . However we believe that there is a mistake of interpretation underlying Chandra's claim and that the Principia is correct as it stands. This short paper describes Chandra's misinterpretation of a geometric construction of Newton and gives an outline of Newton's demonstration by following the standard English version of the Principia line by line and converting it into modern mathematical notation in the spirit of Chandra's book.△ Less"
Tachyon Dynamics and Brane Cosmology,Authors:Gary Shiu,Abstract:We discuss the cosmological implications of some recent advances in understanding the dynamics of tachyon condensation in string theory.We discuss the cosmological implications of some recent advances in understanding the dynamics of tachyon condensation in string theory.△ Less
Standard-like models from D-branes,Authors:D. Bailin,"Abstract:I describe the main features of new intersecting D4- and D5-brane orbifold models that yield the non-supersymmetric standard model, up to vector-like matter and, in some cases, extra U(1) factors in the gauge group. There are six-stack D4-brane models that have charged-singlet scalar tachyons and which either contain all of the Yukawa couplings to the tachyonic Higgs doublets that are needed to…▽ MoreI describe the main features of new intersecting D4- and D5-brane orbifold models that yield the non-supersymmetric standard model, up to vector-like matter and, in some cases, extra U(1) factors in the gauge group. There are six-stack D4-brane models that have charged-singlet scalar tachyons and which either contain all of the Yukawa couplings to the tachyonic Higgs doublets that are needed to generate mass terms for the fermions at renormalisable level or possess an unwanted extra U(1) gauge symmetry after spontaneous symmetry breaking. In the D5-brane models a minimum of eight stacks is needed.△ Less"
Magnetized (Shift-)Orientifolds,Authors:Gianfranco Pradisi,"Abstract:We study four dimensional $Z_2 \times Z_2$ (shift)-orientifolds in presence of internal magnetic fields and NS-NS $B$-field backgrounds, describing in some detail one explicit example with N=1 supersymmetry. These models are related by $T$-duality to orientifolds with $D$-branes intersecting at angles and exhibit, due to the background fields, a rank reduction of the gauge group and multiple mat…▽ MoreWe study four dimensional $Z_2 \times Z_2$ (shift)-orientifolds in presence of internal magnetic fields and NS-NS $B$-field backgrounds, describing in some detail one explicit example with N=1 supersymmetry. These models are related by $T$-duality to orientifolds with $D$-branes intersecting at angles and exhibit, due to the background fields, a rank reduction of the gauge group and multiple matter families. Moreover, the low-energy spectra are chiral and anomaly free if $D5$-branes are present along the magnetized directions.△ Less"
The Standard Model on the Quintic,"Authors:Ralph Blumenhagen,Volker Braun,Boris Kors,Dieter Lust","Abstract:We describe the general geometrical framework of brane world constructions in orientifolds of type IIA string theory with D6-branes wrapping 3-cycles in a Calabi-Yau 3-fold, and point out their immediate phenomenological relevance. These branes generically intersect in points, and the patterns of intersections govern the chiral fermion spectra and issues of gauge and supersymmetry breaking in th…▽ MoreWe describe the general geometrical framework of brane world constructions in orientifolds of type IIA string theory with D6-branes wrapping 3-cycles in a Calabi-Yau 3-fold, and point out their immediate phenomenological relevance. These branes generically intersect in points, and the patterns of intersections govern the chiral fermion spectra and issues of gauge and supersymmetry breaking in the low energy effective gauge theory on their world volume. In particular, we provide an example of an intersecting brane world scenario on the quintic Calabi-Yau with the gauge group and the chiral spectrum of the Standard Model and discuss its properties in some detail. Additionally we explain related technical advancements in the construction of supersymmetric orientifold vacua with intersecting D-branes. Six-dimensional orientifolds of this type generalize the rather limited set of formerly known orbifolds of type I, and the presented techniques provide a short-cut to obtain their spectra. Finally, we comment on lifting configurations of intersecting D6-branes to M-theory on non-compact G_2 manifolds.△ Less"
SU(2)_k Logarithmic Conformal Field Theories,Authors:A. Nichols,"Abstract:We analyse the SU(2)_k WZNW models beyond the integrable representations and in particular the case of SU(2)_0. We find that these are good examples of logarithmic conformal field theories as indecomposable representations are naturally produced in the fusion of discrete irreducible representations. We also find extra, chiral and non-chiral, multiplet structure in the theory. The chiral fields,…▽ MoreWe analyse the SU(2)_k WZNW models beyond the integrable representations and in particular the case of SU(2)_0. We find that these are good examples of logarithmic conformal field theories as indecomposable representations are naturally produced in the fusion of discrete irreducible representations. We also find extra, chiral and non-chiral, multiplet structure in the theory. The chiral fields, which we construct explicitly in SU(2)_0, generate extended algebras within the model. We also study the process of quantum hamiltonian reduction of SU(2)_0, giving the c=-2 triplet model, in both the free field approach and at the level of correlation functions. For rational level SU(2)_k this gives us a useful technique to study the h_{1,s} correlators of the c_{p,q} models and we find very similar structures to SU(2)_0. We also discuss LCFT as a limit of a sequence of ordinary CFTs and some of the subtleties that can occur.△ Less"
Moving Five-Branes and Cosmology,Authors:Andre Lukas,Abstract:We discuss low-energy heterotic M-theory with five-branes in four and five dimensions and its application to moving brane cosmology.We discuss low-energy heterotic M-theory with five-branes in four and five dimensions and its application to moving brane cosmology.△ Less
"Vacuum Energy, Cosmological Supersymmetry Breaking and Inflation from Colliding Brane Worlds",Authors:N. E. Mavromatos,"Abstract:In the context of colliding brane worlds I discuss a toy cosmological model, developed in collaboration with E. Gravanis, which arguably produces inflation and a relaxing to zero cosmological ``constant'' hierarchically small as compared to the supersymmetry breaking (TeV) scale. Supersymmetry breaking is induced by compactification of the brane worlds on magnetized tori. The crucial ingredient…▽ MoreIn the context of colliding brane worlds I discuss a toy cosmological model, developed in collaboration with E. Gravanis, which arguably produces inflation and a relaxing to zero cosmological ``constant'' hierarchically small as compared to the supersymmetry breaking (TeV) scale. Supersymmetry breaking is induced by compactification of the brane worlds on magnetized tori. The crucial ingredient is the non-criticality (non conformality) of string theory on the observable brane world induced at the collision, which is thus viewed as a cause for departure from equilibrium in this system. The hierarchical smallness of the present-era vacuum energy, as compared to the SUSY breaking scale, is thus attributed to relaxation.△ Less"
Exact Standard Model Structures from Intersecting Branes,Authors:Christos Kokorelis,"Abstract:I discuss two types of non-supersymmetric string model constructions that give at low energy exactly the Standard model (SM) with no additional matter/and or gauge group factors. The construction is based on D6 branes intersecting at angles in a compactification of type IIA theory on a decomposable orientifolded $T^6$ torus. The first type is based on five and six stack SM-like constructions at…▽ MoreI discuss two types of non-supersymmetric string model constructions that give at low energy exactly the Standard model (SM) with no additional matter/and or gauge group factors. The construction is based on D6 branes intersecting at angles in a compactification of type IIA theory on a decomposable orientifolded $T^6$ torus. The first type is based on five and six stack SM-like constructions at the string scale while, the other construction is based on a four stack GUT left-right symmetric structure centered around the Pati-Salam $SU(4)_C \times SU(2)_L \times SU(2)_R$ gauge group. All classes of models exbibit important phenomenological properties including a stable proton and sizes of neutrino masses in consistency with neutrino oscillation experiments. The models are non-SUSY, but amazingly, they allow the existence of supersymmetric particles!△ Less"
Localization and anomalies on orbifolds,"Authors:S. Groot Nibbelink,H. P. Nilles","Abstract:In this talk we discus some properties of supersymmetric theories on orbifolds in five dimensions. The structure of FI--tadpoles may lead to (strong) localization of charged bulk scalars. Orbifold theories may suffer from various kinds of anomalies. The parity anomaly may render the construction of the orbifold theory ill--defined. The gauge anomaly on the orbifold are localized at the fixed poi…▽ MoreIn this talk we discus some properties of supersymmetric theories on orbifolds in five dimensions. The structure of FI--tadpoles may lead to (strong) localization of charged bulk scalars. Orbifold theories may suffer from various kinds of anomalies. The parity anomaly may render the construction of the orbifold theory ill--defined. The gauge anomaly on the orbifold are localized at the fixed points, which can sometimes be canceled by a Chern--Simons term.△ Less"
"String Theory, Unification and Supersymmetry",Authors:Michael Dine,"Abstract:One cannot yet point to any firm string prediction. While many approximate string ground states are known with interesting properties, we do not have any argument that one or another describes what we observe around us, and for reasons which appear fundamental we do not know how to systematically determine even any rough quantitative properties. I argue here that we should examine large classes…▽ MoreOne cannot yet point to any firm string prediction. While many approximate string ground states are known with interesting properties, we do not have any argument that one or another describes what we observe around us, and for reasons which appear fundamental we do not know how to systematically determine even any rough quantitative properties. I argue here that we should examine large classes of string ground states, trying to determine whether features such as low energy supersymmetry, the pattern of supersymmetry breaking, the presence of axions, large dimensions, or others might be generic.△ Less"
Surface reheating as a new paradigm,"Authors:Anupam Mazumdar,Kari Enqvist,Shinta Kasuya",Abstract:In this talk we briefly review the standard idea of reheating and then present a new paradigm of reheating the Universe through surface evaporation.In this talk we briefly review the standard idea of reheating and then present a new paradigm of reheating the Universe through surface evaporation.△ Less
Nonperturbative flipped SU(5) vacua in Horava-Witten theory,"Authors:A. E. Faraggi,R. Garavuso,J. M. Isidro","Abstract:There is good support for the embedding of the Standard Model fermions in the chiral 16 representation of SO(10). Such an embedding is provided by the realistic free fermionic heterotic-string models. In this talk we demonstrate the existence of solutions with 3 generations and SO(10) observable gauge group, in the case of compactification on a torus-fibred Calabi-Yau space over a Hirzebruch bas…▽ MoreThere is good support for the embedding of the Standard Model fermions in the chiral 16 representation of SO(10). Such an embedding is provided by the realistic free fermionic heterotic-string models. In this talk we demonstrate the existence of solutions with 3 generations and SO(10) observable gauge group, in the case of compactification on a torus-fibred Calabi-Yau space over a Hirzebruch base surface. The SO(10)symmetry is broken to SU(5)xU(1) by a Wilson line. The overlap with the realistic free fermionic heterotic-string models is discussed.△ Less"
Conformality and Unification of Gauge Couplings,Authors:Paul H. Frampton,"Abstract:By use of the AdS/CFT correspondence on orbifolds, models are derived which can contain the standard model of particle phenomenology. It will be assumed that the theory becomes conformally invariant at a renormalization-group fixed-point in the TeV region. A recent application to TeV unification is briefly mentioned.By use of the AdS/CFT correspondence on orbifolds, models are derived which can contain the standard model of particle phenomenology. It will be assumed that the theory becomes conformally invariant at a renormalization-group fixed-point in the TeV region. A recent application to TeV unification is briefly mentioned.△ Less"
Energy Spectrum for Neutral Collective Excitations in Striped Hall States,"Authors:N. Maeda,T. Aoyama,Y. Ishizuka,K. Ishikawa",Abstract:Neutral collective excitations in the striped Hall state are studied by using the single mode approximation and Hartree-Fock approximation at the half-filled third and fourth Landau level. We find that the spectrum includes anisotropic NG modes and periodic line nodes.Neutral collective excitations in the striped Hall state are studied by using the single mode approximation and Hartree-Fock approximation at the half-filled third and fourth Landau level. We find that the spectrum includes anisotropic NG modes and periodic line nodes.△ Less
Power-Law Persistence in the Atmosphere: Analysis and Applications,"Authors:Armin Bunde,Jan Eichner,Rathinaswamy Govindan,Shlomo Havlin,Eva Koscielny-Bunde,Diego Rybski,Dmitry Vjushin","Abstract:We review recent results on the appearance of long-term persistence in climatic records and their relevance for the evaluation of global climate models and rare events.The persistence can be characterized, for example, by the correlation C(s) of temperature variations separated by s days.We show that, contrary to previous expectations, C(s) decays for large s as a power law, C(s) ~ s^(-gamma). F…▽ MoreWe review recent results on the appearance of long-term persistence in climatic records and their relevance for the evaluation of global climate models and rare events.The persistence can be characterized, for example, by the correlation C(s) of temperature variations separated by s days.We show that, contrary to previous expectations, C(s) decays for large s as a power law, C(s) ~ s^(-gamma). For continental stations, the exponent gamma is always close to 0.7, while for stations on islands gamma is around 0.4. In contrast to the temperature fluctuations, the fluctuations of the rainfall usually cannot be characterized by long-term power-law correlations but rather by pronounced short-term correlations. The universal persistence law for the temperature fluctuations on continental stations represents an ideal (and uncomfortable) test-bed for the state of-the-art global climate models and allows us to evaluate their performance. In addition, the presence of long-term correlations leads to a novel approach for evaluating the statistics of rare events.△ Less"
String Inspired Neutrino Mass Models,Authors:S. F. King,Abstract:I discuss a class of hierarchical neutrino mass models based on the see-saw mechanism with single right-handed neutrino dominance. I apply this mechanism to a string inspired SUSY Pati-Salam model and indicate how it may emerge from intersecting D-branes.I discuss a class of hierarchical neutrino mass models based on the see-saw mechanism with single right-handed neutrino dominance. I apply this mechanism to a string inspired SUSY Pati-Salam model and indicate how it may emerge from intersecting D-branes.△ Less
"Modular symmetry, twisted sectors and flavour",Authors:Thomas Dent,"Abstract:We investigate the implications for fermion mass models in heterotic orbifolds of the modular symmetry mixing twisted states localized at different fixed points. We show that, unlike in the case of continuous gauge symmetries, the mass eigenstates do not mix under the symmetry; thus coupling constants in the low-energy theory are invariant functions of the moduli.We investigate the implications for fermion mass models in heterotic orbifolds of the modular symmetry mixing twisted states localized at different fixed points. We show that, unlike in the case of continuous gauge symmetries, the mass eigenstates do not mix under the symmetry; thus coupling constants in the low-energy theory are invariant functions of the moduli.△ Less"
"Superstring Relics, Supersymmetric Fragmentation and UHECR","Authors:A. Cafarella,C. Coriano',M. Guzzi,D. Martello","Abstract:Superstring theory predicts the existence of relic metastable particles whose average lifetime is longer than the age of the universe and which could, in principle, be good dark matter candidates. At the same time, these states would be responsible for the Ultra High Energy Cosmic Rays (UHECR) events which will be searched for by various experimental collaborations in the near future. We describ…▽ MoreSuperstring theory predicts the existence of relic metastable particles whose average lifetime is longer than the age of the universe and which could, in principle, be good dark matter candidates. At the same time, these states would be responsible for the Ultra High Energy Cosmic Rays (UHECR) events which will be searched for by various experimental collaborations in the near future. We describe a possible phenomenological path which could be followed in order to search for new physics in their detection.△ Less"
D-Moduli Stabilization,Authors:Joel Giedt,"Abstract:The matter sector of four-dimensional effective supergravity models obtained from the weakly coupled heterotic string contains many moduli. In particular, flat directions of the D-term part of the scalar potential in the presence of an anomalous U(1) give rise to massless chiral multiplets which have been referred to elsewhere as D-moduli. The stabilization of these moduli is necessary for the d…▽ MoreThe matter sector of four-dimensional effective supergravity models obtained from the weakly coupled heterotic string contains many moduli. In particular, flat directions of the D-term part of the scalar potential in the presence of an anomalous U(1) give rise to massless chiral multiplets which have been referred to elsewhere as D-moduli. The stabilization of these moduli is necessary for the determination of the large vacuum expectation values of complex scalar fields induced by the corresponding Fayet-Illiopoulos term. This stabilization is of phenomenological importance since these background values determine the effective theory below the scale of the anomalous U(1) symmetry breaking. In some simple models we illustrate the stabilization of these moduli due to the nonperturbative dynamics associated with gaugino condensation in a hidden sector. We find that background field configurations which are stable above the condensation scale no longer represent global minima once dynamical supersymmetry breaking occurs. The implications for low energy models based on promising ``flat'' directions are discussed.△ Less"
Metallic and Insulating behaviour in p-SiGe at nu = 3/2,"Authors:P. T. Coleridge,R. L. Williams,J. Lapointe,P. Zawadzki","Abstract:Shubnikov-de Haas data is presented for a p-SiGe sample exhibiting strongly insulating behaviour at nu = 3/2. In addition to the fixed points defining a high field metal- insulator transition into this phase separate fixed points can also be identified for the nu = 3 --> 2 and 2 --> 1 Integer quantum Hall transitions. Another feature of the data, that the Hall resistivity approaches zero in the…▽ MoreShubnikov-de Haas data is presented for a p-SiGe sample exhibiting strongly insulating behaviour at nu = 3/2. In addition to the fixed points defining a high field metal- insulator transition into this phase separate fixed points can also be identified for the nu = 3 --> 2 and 2 --> 1 Integer quantum Hall transitions. Another feature of the data, that the Hall resistivity approaches zero in the insulating phase, indicates it is not a re-entrant Hall insulator. The behaviour is explained in terms of the strong exchange interactions. At integer filling factors these cause the 0 uparrow and 1 downarrow Landau levels to cross and be well separated but at non-integer values of nu screening reduces exchange effects and causes the levels to stick together. It is suggested the insulating behaviour, and high field metal/insulator transition, is a consequence of the strong exchange interactions.△ Less"
The deconfinement transition in SU(N) gauge theories,"Authors:B. Lucini,M. Teper,U. Wenger","Abstract:We investigate the properties of the deconfinement transition in SU(4) and SU(6) gauge theories. We find that it is a `normal' first order transition in both cases, from which we conclude that the transition is first order in the N->infinity limit. Comparing our preliminary estimates of the continuum values of Tc/sqrt(K) with existing values for SU(2) and SU(3) demonstrates a weak dependence on…▽ MoreWe investigate the properties of the deconfinement transition in SU(4) and SU(6) gauge theories. We find that it is a `normal' first order transition in both cases, from which we conclude that the transition is first order in the N->infinity limit. Comparing our preliminary estimates of the continuum values of Tc/sqrt(K) with existing values for SU(2) and SU(3) demonstrates a weak dependence on N for all values of N.△ Less"
SU(N) gauge theories in 2+1 dimensions -- further results,"Authors:B. Lucini,M. Teper","Abstract:We calculate the string tension and part of the mass spectrum of SU(4) and SU(6) gauge theories in 2+1 dimensions using lattice techniques. We combine these new results with older results for N=2,...,5 so as to obtain more accurate extrapolations to N=infinity. The qualitative conclusions of the earlier work are unchanged: SU(N) theories in 2+1 dimensions are linearly confining as N->infinity; t…▽ MoreWe calculate the string tension and part of the mass spectrum of SU(4) and SU(6) gauge theories in 2+1 dimensions using lattice techniques. We combine these new results with older results for N=2,...,5 so as to obtain more accurate extrapolations to N=infinity. The qualitative conclusions of the earlier work are unchanged: SU(N) theories in 2+1 dimensions are linearly confining as N->infinity; the limit is achieved by keeping g.g.N fixed; SU(3), and even SU(2), are `close' to SU(infinity). We obtain more convincing evidence than before that the leading large-N correction is O(1/N.N). We look for the multiplication of states that one expects in simple flux loop models of glueballs, but find no evidence for this.△ Less"
The Architecture of Complex Systems,"Authors:Vito Latora,Massimo Marchiori",Abstract:A short review of the recent results and models of complex networks.A short review of the recent results and models of complex networks.△ Less
Phonology,Authors:Steven Bird,"Abstract:Phonology is the systematic study of the sounds used in language, their internal structure, and their composition into syllables, words and phrases. Computational phonology is the application of formal and computational techniques to the representation and processing of phonological information. This chapter will present the fundamentals of descriptive phonology along with a brief overview of co…▽ MorePhonology is the systematic study of the sounds used in language, their internal structure, and their composition into syllables, words and phrases. Computational phonology is the application of formal and computational techniques to the representation and processing of phonological information. This chapter will present the fundamentals of descriptive phonology along with a brief overview of computational phonology.△ Less"
Computational Phonology,Authors:Steven Bird,"Abstract:Phonology, as it is practiced, is deeply computational. Phonological analysis is data-intensive and the resulting models are nothing other than specialized data structures and algorithms. In the past, phonological computation - managing data and developing analyses - was done manually with pencil and paper. Increasingly, with the proliferation of affordable computers, IPA fonts and drawing softw…▽ MorePhonology, as it is practiced, is deeply computational. Phonological analysis is data-intensive and the resulting models are nothing other than specialized data structures and algorithms. In the past, phonological computation - managing data and developing analyses - was done manually with pencil and paper. Increasingly, with the proliferation of affordable computers, IPA fonts and drawing software, phonologists are seeking to move their computation work online. Computational Phonology provides the theoretical and technological framework for this migration, building on methodologies and tools from computational linguistics. This piece consists of an apology for computational phonology, a history, and an overview of current research.△ Less"
Constructing compact manifolds with exceptional holonomy,Authors:Dominic Joyce,"Abstract:…a Calabi-Yau manifold or orbifold.
  All the material in this paper is covered in much more detail in the author's book, ""Compact manifolds with special holonomy"",OxfordUniversity Press, 2000.▽ MoreThe exceptional holonomy groups are G2 in 7 dimensions, and Spin(7) in 8 dimensions. Riemannian manifolds with these holonomy groups are Ricci-flat. This is a survey paper on constructions for compact 7- and 8-manifolds with holonomy G2 and Spin(7).
  The simplest such constructions work by using techniques from complex geometry and Calabi-Yau analysis to resolve the singularities of a torus orbifold T^7/G or T^8/G, for G a finite group preserving a flat G2 or Spin(7)-structure on T^7 or T^8. There are also more complicated constructions which begin with a Calabi-Yau manifold or orbifold.
  All the material in this paper is covered in much more detail in the author's book, ""Compact manifolds with special holonomy"",OxfordUniversity Press, 2000.△ Less"
Gelfand theory for non-commutative Banach algebras,"Authors:Rachid Choukri,El Hossein Illoussamen,Volker Runde","Abstract:LetAbe a Banach algebra. We call a pair(G, B)a Gelfand theory forAif the following axioms are satisfied: (G 1)Bis aC^\ast-algebra, andG : A \to Bis a homomorphism; (G 2) the assignmentL \mapsto G^{-1}(L)is a bijection between the sets of maximal modularleft ideals ofBandA, respectively; (G 3) for each maximal modular left idealLofB, the linear map…▽ MoreLetAbe a Banach algebra. We call a pair(G, B)a Gelfand theory forAif the following axioms are satisfied: (G 1)Bis aC^\ast-algebra, andG : A \to Bis a homomorphism; (G 2) the assignmentL \mapsto G^{-1}(L)is a bijection between the sets of maximal modularleft ideals ofBandA, respectively; (G 3) for each maximal modular left idealLofB, the linear mapG_L : A / G^{-1}(L) \to B /Linduced byBhas dense range. The Gelfand theory of a commutative Banach algebra is easily seen to be characterized by these axioms. Gelfand theories of arbitrary Banach algebras enjoy many of the properties of commutative Gelfand theory. We show that unital, homogeneous Banach algebras always have a Gelfand theory. For liminalC^\ast-algebras with discrete spectrum, we show that the identity is the only Gelfand theory (up to an appropriate notion of equivalence).△ Less"
3-D General Relativistic MHD Simulations of Generating Jets,"Authors:K. -I. Nishikawa,S. Koide,K. Shibata,T. Kudoh,H. Sol","Abstract:We have performed a first fully 3-D GRMHD simulation with Schwarzschild black hole with a free falling corona. The initial simulation results show that a jet is created as in previous axisymmetric simulations. However, the time to generate the jet is longer than in the 2-D simulations. We expect that due to the additional azimuthal dimension the dynamics of jet formation can be modified.We have performed a first fully 3-D GRMHD simulation with Schwarzschild black hole with a free falling corona. The initial simulation results show that a jet is created as in previous axisymmetric simulations. However, the time to generate the jet is longer than in the 2-D simulations. We expect that due to the additional azimuthal dimension the dynamics of jet formation can be modified.△ Less"
On the index of Farey sequences,"Authors:F. P. Boca,R. N. Gologan,A. Zaharescu",Abstract:We prove some asymptotic formulae concerning the distribution of the index of Farey fractions of order Q asQ\to \infty.We prove some asymptotic formulae concerning the distribution of the index of Farey fractions of order Q asQ\to \infty.△ Less
Kinetic equilibrium of iron in the atmospheres of cool dwarf stars II. Weak Fe I lines in the solar spectrum,"Authors:T. Gehren,A. J. Korn,J. Shi","Abstract:…particular the bulk of the data from the work of May et al. and O'Brian et al. is not adequate for accurate abundance work. Based on f-values measured by the Hannover andOxfordgroups alone, the FeI LTE abundances are eps(FeI,Sun)=7.57 for the empirical and eps(FeI,Sun) = 7.48 ... 7.51 for the line-blanketed solar model. The solar Fe ionization equilibr…▽ MoreNLTE line formation calculations of FeI in the solar atmosphere are extended to include weak optical lines. Previously established atomic models are used to discriminate between different ways of treating collisional interaction processes. To derive a common solar FeI abundance from both strong and weak lines, fine-tuning of the microturbulence velocity parameter and the van-der- Waals damping constants is required. The solar FeI abundances based on all available f-values are dominated by the large scatter already found for the stronger lines. In particular the bulk of the data from the work of May et al. and O'Brian et al. is not adequate for accurate abundance work. Based on f-values measured by the Hannover andOxfordgroups alone, the FeI LTE abundances are eps(FeI,Sun)=7.57 for the empirical and eps(FeI,Sun) = 7.48 ... 7.51 for the line-blanketed solar model. The solar Fe ionization equilibrium obtained for different atomic and atmospheric models rules out NLTE atomic models with a low efficiency of hydrogen collisions. At variance with Paper I, it is now in better agreement with laboratory FeII f-values for all types of line-blanketed models. Our final model assumptions consistent with a single unique solar Fe abundance eps(Fe,Sun) = 7.48 ... 7.51 calculated from NLTE line formation are (a) a line-blanketed solar model atmosphere, (b) an iron model atom with hydrogen collision rates 0.5 < S_H < 5 times the standard value to compensate for the large photoionization cross-sections, (c) a microturbulence velocity xi = 1.0 kms, (d) van-der-Waals damping parameters decreased by Delta(log C6) = -0.10...-0.15 as compared to Anstee & O'Mara's calculations, depending on S_H, (e) FeII f-values as published by Schnabel et al., and (f) FeI f-values published by the Hannover andOxfordgroups.△ Less"
Particles and Fields in Radio Galaxies: A Summary,Authors:R. D. Blandford,"Abstract:A summary is presented of a meeting on Particles and Fields in Radio Galaxies held inOxfordin August 2000. Recent detailed studies of radio maps and the first X-ray images from Chandra X-ray Observatory, together with new capabilities in simulating hydromagnetic flows, are transforming our understanding of how jets are formed by accretion disks orbiting ma…▽ MoreA summary is presented of a meeting on Particles and Fields in Radio Galaxies held inOxfordin August 2000. Recent detailed studies of radio maps and the first X-ray images from Chandra X-ray Observatory, together with new capabilities in simulating hydromagnetic flows, are transforming our understanding of how jets are formed by accretion disks orbiting massive black holes, how these jets dissipate and how they inflate the giant radio-emitting lobes by which they were first identified. As they can be imaged in such detail, extragalactic radio sources remain central to the study of jet outflows in general although there is a strong convergence with corresponding studies of Galactic superluminal sources, gamma ray bursts and young stellar objects.△ Less"
The TexOx Survey of Radio-Selected Galaxy Clusters,"Authors:Steve Croft,Steve Rawlings,Gary J. Hill,Pamela L. Gay,Joseph R. Tufts","Abstract:We present some initial results from the TexOx (Texas-Oxford) Cluster (TOC) survey - a new method of selecting distant galaxy clusters. The cosmic evolution of the radio source population suggests that some massive clusters at high redshift will contain several radio-loud AGN. We searched for extreme over-densities at ~mJy levels in 7' x 7' boxes wit…▽ MoreWe present some initial results from the TexOx (Texas-Oxford) Cluster (TOC) survey - a new method of selecting distant galaxy clusters. The cosmic evolution of the radio source population suggests that some massive clusters at high redshift will contain several radio-loud AGN. We searched for extreme over-densities at ~mJy levels in 7' x 7' boxes within the NVSS radio catalogue, covering a large (~1100 square degree) sky area. We have acquired optical images for ~130 cluster candidates, and followed up a subset of these with the VLA, and with Calar Alto near-IR imaging. Ryle Telescope observations have yielded at least one Sunyaev-Zel'dovich (SZ) detection of a massive z~1 system. Spectroscopic follow-up with 8-m class telescopes is in progress.△ Less"
A more efficient variant of theOxfordprotocol,Authors:Nasser Metwally,"Abstract:An alternative presentation of theOxfordpurification protocol is obtained by using dynamical variables. I suggest to introduce the degree of separability as a purification parameter, where the purified state has a smaller degree of separability than the initial one. An improved version of theOxfordprotocol is descr…▽ MoreAn alternative presentation of theOxfordpurification protocol is obtained by using dynamical variables. I suggest to introduce the degree of separability as a purification parameter, where the purified state has a smaller degree of separability than the initial one. An improved version of theOxfordprotocol is described, in which local unitary transformations optimize each step.△ Less"
BUSSTEPP Lectures on Supersymmetry,Authors:Jose Figueroa-O'Farrill,Abstract:…written version of the supersymmetry lectures delivered at the 30th and 31st British Universities Summer Schools in Theoretical Elementary Particle Physics (BUSSTEPP) held inOxfordin September 2000 and in Manchester in August-September 2001.▽ MoreThis is the written version of the supersymmetry lectures delivered at the 30th and 31st British Universities Summer Schools in Theoretical Elementary Particle Physics (BUSSTEPP) held inOxfordin September 2000 and in Manchester in August-September 2001.△ Less
Mindless Sensationalism: A Quantum Framework for Consciousness,Authors:Don N. Page,"Abstract:The ideas of Sensible Quantum Mechanics are expressed in lay terms for philosophers of consciousness and others. A framework is proposed and explained for the `psycho-physical-parallelism' between conscious experiences and the mathematical structures of quantum physics (e.g., a set of quantum operators obeying some algebra, and a quantum state giving the expectation value of each operator). In p…▽ MoreThe ideas of Sensible Quantum Mechanics are expressed in lay terms for philosophers of consciousness and others. A framework is proposed and explained for the `psycho-physical-parallelism' between conscious experiences and the mathematical structures of quantum physics (e.g., a set of quantum operators obeying some algebra, and a quantum state giving the expectation value of each operator). In particular, it is proposed that each set of possible conscious experiences has a measure given by the expectation value of a corresponding operator (a positive-operator-valued measure). Then one has a generalization of the Weak Anthropic Principle named the Conditional Aesthemic Principle: given that we are conscious beings, our conscious experiences are likely to be typical experiences in the set of all conscious experiences with its measure.△ Less"
Effects of non-perturbatively improved dynamical fermions in QCD at fixed lattice spacing,"Authors:UKQCD Collaboration,C. R. Allton,S. P. Booth,K. C. Bowler,J. Garden,A. Hart,D. Hepburn,A. C. Irving,B. Joo,R. D. Kenway,C. M. Maynard,C. McNeile,C. Michael,S. M. Pickles,J. C. Sexton,K. J. Sharkey,Z. Sroczynski,M. Talevi,M. Teper,H. Wittig","Abstract:We present results for the static inter-quark potential, lightest glueballs, light hadron spectrum and topological susceptibility using a non-perturbatively improved action on a16^3\times 32lattice at a set of values of the bare gauge coupling and bare dynamical quark mass chosen to keep the lattice size fixed in physical units (\sim 1.7fm). By comparing these measurements with a matched…▽ MoreWe present results for the static inter-quark potential, lightest glueballs, light hadron spectrum and topological susceptibility using a non-perturbatively improved action on a16^3\times 32lattice at a set of values of the bare gauge coupling and bare dynamical quark mass chosen to keep the lattice size fixed in physical units (\sim 1.7fm). By comparing these measurements with a matched quenched ensemble, we study the effects due to two degenerate flavours of dynamical quarks. With the greater control over residual lattice spacing effects which these methods afford, we find some evidence of charge screening and some minor effects on the light hadron spectrum over the range of quark masses studied (M_{PS}/M_{V}\ge0.58). More substantial differences between quenched and unquenched simulations are observed in measurements of topological quantities.△ Less"
A new airborne detector for atmospheric muons,"Authors:C. P. Achenbach,J. H. Cobb",Abstract:The University ofOxfordhas started the design and development of the new experiment ADLER (Airborne Detector for Low Energy Rays). This apparatus will measure the cosmic-ray muon flux at an altitude of 10 - 13km. The detector should be flown by aircrafts on transatlantic routes crossing the magnetic equator to investigate the flux at different geomagnetic…▽ MoreThe University ofOxfordhas started the design and development of the new experiment ADLER (Airborne Detector for Low Energy Rays). This apparatus will measure the cosmic-ray muon flux at an altitude of 10 - 13km. The detector should be flown by aircrafts on transatlantic routes crossing the magnetic equator to investigate the flux at different geomagnetic latitudes. The goal of the experiment is to obtain better constraints on the low energy atmospheric neutrino flux and the results will be of importance to the atmospheric neutrino anomaly.△ Less
Extreme Ultraviolet (EUV) Sources for Lithography based on Synchrotron Radiation,"Authors:G. Dattoli,A. Doria,G. P. Gallerano,L. Giannessi,K. Hesch,H. O. Moser,P. L. Ottaviani,E. Pellegrin,R. Rossmanith,R. Steininger,V. Saile,J. Wuest","Abstract:…power outside this bandwidth. Three options were investigated. The first two deal with radiation from bending magnets and undulators. The results confirm the earlier work byOxfordsInstrument and others that these light-sources lack in-band power while emitting excessive out-of-band radiation. The third approach is a FEL (Free Electron Laser) driven by a 50…▽ MoreThe study presented here was initiated by a discussion to investigate the possibility of using synchrotron radiation as a source for the Next Generation Lithography (NGL) based on the EUV-concept (Extreme Ultra-Violet; here 13.5 nm or 11.3 nm radiation, respectively). The requirements are: 50 W, 2% bandwidth and minimal power outside this bandwidth. Three options were investigated. The first two deal with radiation from bending magnets and undulators. The results confirm the earlier work byOxfordsInstrument and others that these light-sources lack in-band power while emitting excessive out-of-band radiation. The third approach is a FEL (Free Electron Laser) driven by a 500 MeV linear accelerator with a superconducting mini-undulator as radiation emitting device. Such a device would produce in-band EUV-power in excess of 50 W with negligible out-of-band power.△ Less"
SU(N) gauge theories in four dimensions: exploring the approach to N = infinity,"Authors:B. Lucini,M. Teper","Abstract:We calculate the string tension, K, and some of the lightest glueball masses, M, in 3+1 dimensional SU(N) lattice gauge theories for N=2,3,4,5 . From the continuum extrapolation of the lattice values, we find that the mass ratios, M/sqrt(K), appear to show a rapid approach to the large-N limit, and, indeed, can be described all the way down to SU(2) using just a leading O(1/NxN) correction. We c…▽ MoreWe calculate the string tension, K, and some of the lightest glueball masses, M, in 3+1 dimensional SU(N) lattice gauge theories for N=2,3,4,5 . From the continuum extrapolation of the lattice values, we find that the mass ratios, M/sqrt(K), appear to show a rapid approach to the large-N limit, and, indeed, can be described all the way down to SU(2) using just a leading O(1/NxN) correction. We confirm that the smooth large-N limit we find, is obtained by keeping a constant 't Hooft coupling. We also calculate the topological charge of the gauge fields. We observe that, as expected, the density of small-size instantons vanishes rapidly as N increases, while the topological susceptibility appears to have a non-zero N=infinity limit.△ Less"
A High-Frequency and Multi-Epoch VLBI Study of 3C 273,"Authors:T. P. Krichbaum,D. A. Graham,A. Witzel,J. A. Zensus,A. Greve,M. Grewing,A. Marscher,A. J. Beasley","Abstract:We show results from a 7 year VLBI monitoring programme of 3C273 at millimeter wavelengths. We find evidence for component acceleration, motion or rotation of fluid dynamical patterns in the VLBI jet, and evidence for an outburst-ejection relation between Gamma-ray flares and the appearance of new jet components.We show results from a 7 year VLBI monitoring programme of 3C273 at millimeter wavelengths. We find evidence for component acceleration, motion or rotation of fluid dynamical patterns in the VLBI jet, and evidence for an outburst-ejection relation between Gamma-ray flares and the appearance of new jet components.△ Less"
The interaction of radio sources and cooling flows,Authors:A. C. Fabian,"Abstract:The X-ray emission in many clusters of galaxies shows a central peak in surface brightness coincident with a drop in temperature. These characterize a cooling flow. There is often a radio source also at the centre of such regions. Data from Chandra now enables us to map the interaction between the radio source and the intracluster medium. Preliminary work shows no sign of heating of the gas beyo…▽ MoreThe X-ray emission in many clusters of galaxies shows a central peak in surface brightness coincident with a drop in temperature. These characterize a cooling flow. There is often a radio source also at the centre of such regions. Data from Chandra now enables us to map the interaction between the radio source and the intracluster medium. Preliminary work shows no sign of heating of the gas beyond the radio lobes, which are often devoid of cooler gas and so appear as holes. In the case of the Perseus cluster around 3C84, the coolest X-ray emitting gas occurs immediately around the inner radio lobes.△ Less"
Optical Structure and Physics of the M87 Jet,"Authors:Eric Perlman,William Sparks,John Biretta,Duccio Macchetto,J. Patrick Leahy","Abstract:We summarize HST observations of the M87 jet, concentrating on polarimetry and spectral index maps, and compare its optical and radio structures. The evidence now supports a stratified model for the structure of the jet, whereby high-energy, optical synchrotron emitting particles occupy physically different regions of the jet, closer to the jet axis, with different magnetic field configurations.…▽ MoreWe summarize HST observations of the M87 jet, concentrating on polarimetry and spectral index maps, and compare its optical and radio structures. The evidence now supports a stratified model for the structure of the jet, whereby high-energy, optical synchrotron emitting particles occupy physically different regions of the jet, closer to the jet axis, with different magnetic field configurations. It is in these regions where the shocks that produce the knots in the inner jet appear to originate. Knot regions have optical spectra which are much flatter than average for the jet, with the flattest-spectrum regions coinciding with flux maxima of knots. These same regions are preceded by regions where perpendicular magnetic fields are seen. Thus not only do we see all the necessary ingredients for {\it in situ} particle acceleration in the knots, but there is now fairly direct evidence for it as well. By tracking the changes in radio-optical and optical spectral index in the knot regions, we can comment on acceleration and cooling timescales in each knot.△ Less"
Numerical analysis of the Minimal and Two-Liquid models of the Market Microstructure,"Authors:David L. C. Chan,David Eliezer,Ian I. Kogan","Abstract:We present results of numerical analysis of several simple models for the microstructure of a double auction market without intermediaries which were introduced in cond-mat/9808240. We perform computer simulations of the minimal model in order to verify liquidity scaling laws. A logarithmic correction to the scaling law for midmarket variance is observed, but not for bid-offer spread or its fluc…▽ MoreWe present results of numerical analysis of several simple models for the microstructure of a double auction market without intermediaries which were introduced in cond-mat/9808240. We perform computer simulations of the minimal model in order to verify liquidity scaling laws. A logarithmic correction to the scaling law for midmarket variance is observed, but not for bid-offer spread or its fluctuation, because they are fundamentally different quantities.
  Time to midmarket sale (τ_S) is found to scale as 1/J while its fluctuation goes as0.73/J. A ``reduced'' time (τ_{reduced}) is also studied, and found to scale in a non-trivial way. Asymmetric fluxes are introduced to the minimal model and analytical result derived earlier for the speed of the moving midmarket agrees with numerical results. Simulation of the two-liquid model which describes a market with both market order and limit order traders, reveals widening of the bid-offer spread when the flux of market order traders exceeds that of limit order traders. The variation of the spread with the fraction of market-order traders is investigated. The formula for asymmetric fluxes is applied to the two-liquid model and its predictions are found to agree with experiment. The critical point is approximately determined, and the ratio of the midmarkets forf = 0.0andf = 0.5(wherefis the fraction of market-order traders) is calculated.△ Less"
Chandra X-ray Observations of Cygnus A and Pictor A,"Authors:A. S. Wilson,A. J. Young,P. L. Shopbell","Abstract:We describe observations of the two nearest, powerful radio galaxies with the Chandra X-ray Observatory: Cygnus A (z = 0.0562) and Pictor A (z = 0.035). Since the early results from these observations are published elsewhere (Wilson, Young & Shopbell 2000, 2001), we here confine ourselves to a summary of the main conclusions and a few remarks about other aspects of the results on Cygnus A, which…▽ MoreWe describe observations of the two nearest, powerful radio galaxies with the Chandra X-ray Observatory: Cygnus A (z = 0.0562) and Pictor A (z = 0.035). Since the early results from these observations are published elsewhere (Wilson, Young & Shopbell 2000, 2001), we here confine ourselves to a summary of the main conclusions and a few remarks about other aspects of the results on Cygnus A, which will be amplified in a future paper.△ Less"
Numerical Simulations Of Particle Acceleration In Relativistic Shocks With Application To AGN Central Engines,"Authors:John Quenby,Athina Meli","Abstract:Numerical modelling is performed for extreme relativistic parallel shocks with upstream Lorentz factorΓ=50. Assuming the scattering is either large angle or over pitch angles\sim Γ^{-1}, spectral flattening and shock aaceleration speed-up is found. The energy gain for the first shock cycle is\sim Γ^{2}. The likely output from relativistic shocks due to the infall from the accretion disk…▽ MoreNumerical modelling is performed for extreme relativistic parallel shocks with upstream Lorentz factorΓ=50. Assuming the scattering is either large angle or over pitch angles\sim Γ^{-1}, spectral flattening and shock aaceleration speed-up is found. The energy gain for the first shock cycle is\sim Γ^{2}. The likely output from relativistic shocks due to the infall from the accretion disk to the AGN black hole is computed. Neutrinos from proton-gamma interactions may be detectable with planned neutrino telescopes.△ Less"
String models of glueballs and the spectrum of SU(N) gauge theories in 2+1 dimensions,"Authors:Robert W. Johnson,Michael J. Teper","Abstract:The spectrum of glueballs in 2+1 dimensions is calculated within an extended class of Isgur-Paton flux tube models and is compared to lattice calculations of the low-lying SU(N) glueball mass spectra. Our modifications of the model include a string curvature term and different ways of dealing with the flux tube width. We find that the generic model is remarkably successful at reproducing the pos…▽ MoreThe spectrum of glueballs in 2+1 dimensions is calculated within an extended class of Isgur-Paton flux tube models and is compared to lattice calculations of the low-lying SU(N) glueball mass spectra. Our modifications of the model include a string curvature term and different ways of dealing with the flux tube width. We find that the generic model is remarkably successful at reproducing the positive charge conjugation, C=+, sector of the spectrum. The only large (and robust) discrepancy involves the 0-+ state. This raises the interesting possibility that the lattice spin identification is mistaken and that this state is in fact 4-+. In addition, the Isgur-Paton model does not incorporate any mechanism for splitting C=+ from C=- (in contrast to the case in 3+1 dimensions), while the `observed' spectrum shows a substantial splitting. We explore several modifications of the model in an attempt to incorporate this physics in a natural way. At the qualitative level we find that this constrains our choice to a picture in which the C=+/- splitting is driven by mixing with new states built on closed loops of adjoint flux. However a detailed numerical comparison suggests that a model incorporating an additional direct mixing between loops of opposite orientation is likely to work better; and that, in any case, a non-zero curvature term will be required. We also point out that a characteristic of any string model of glueballs is that the SU(N=infinity) mass spectrum will consist of multiple towers of states that are scaled up copies of each other. To test this will require a lattice mass spectrum that extends to somewhat larger masses than currently available.△ Less"
The k=2 string tension in four dimensional SU(N) gauge theories,"Authors:B. Lucini,M. Teper","Abstract:We calculate the k=2 string tensions in SU(4) and SU(5) gauge theories in 3+1 dimensions, and compare them to the k=1 fundamental string tensions. We find, from the continuum extrapolation of our lattice calculations, that K(k=2)/K(k=1) = 1.40(8) in the SU(4) gauge theory, and that K(k=2)/K(k=1) = 1.56(10) in SU(5). We remark upon the way this might constrain the dynamics of confinement and the…▽ MoreWe calculate the k=2 string tensions in SU(4) and SU(5) gauge theories in 3+1 dimensions, and compare them to the k=1 fundamental string tensions. We find, from the continuum extrapolation of our lattice calculations, that K(k=2)/K(k=1) = 1.40(8) in the SU(4) gauge theory, and that K(k=2)/K(k=1) = 1.56(10) in SU(5). We remark upon the way this might constrain the dynamics of confinement and the intriguing implications it might have for the mass spectrum of SU(N) gauge theories. We also note that these results agree closely with the MQCD-inspired conjecture that the SU(N) string tensions satisfy K(k)/K(1) = sin(k.pi/N)/sin(pi/N).△ Less"
The parsec-scale central components of FR I radio galaxies,"Authors:Preeti Kharb,Prajval Shastri","Abstract:A majority of a complete sample of 3CR FR I radio galaxies show unresolved optical nuclear sources on the scales of 0.1 arcsec. About half of the 3CR FR II radio galaxies observed with the HST also show Compact Central Cores (CCC). These CCCs have been interpreted as the optical counterparts of the non-thermal radio cores in these radio galaxies (Chiaberge, Capetti, & Celotti 1999). We show that…▽ MoreA majority of a complete sample of 3CR FR I radio galaxies show unresolved optical nuclear sources on the scales of 0.1 arcsec. About half of the 3CR FR II radio galaxies observed with the HST also show Compact Central Cores (CCC). These CCCs have been interpreted as the optical counterparts of the non-thermal radio cores in these radio galaxies (Chiaberge, Capetti, & Celotti 1999). We show that the optical flux density of the CCCs in FR Is is correlated with the radio core prominence. This correlation supports the argument of Chiaberge et al. that the CCC radiation is of a non-thermal synchrotron origin, which is relativistically beamed along with the radio emission.△ Less"
Current Problems for X-ray Emission from Radio Jets,Authors:D. E. Harris,"Abstract:A list is presented of known extragalactic radio jets which also have associated X-ray emission. The canonical emission processes for the production of X-rays are reviewed and the sources are categorized on the basis of our current understanding. Although it seems clear that the X-ray emission is non-thermal, the two competing processes, synchrotron and inverse Compton emissions, arise from extr…▽ MoreA list is presented of known extragalactic radio jets which also have associated X-ray emission. The canonical emission processes for the production of X-rays are reviewed and the sources are categorized on the basis of our current understanding. Although it seems clear that the X-ray emission is non-thermal, the two competing processes, synchrotron and inverse Compton emissions, arise from extremely high energy (synchrotron) or extremely low energy (beaming models with IC emission), relativistic electrons. Only synchrotron self-Compton emission from a few hotspots provides information on the `normal' energy range of the electrons responsible for the observed radio emission.△ Less"
A multi-frequency study of the radio galaxy NGC326,"Authors:M. Murgia,P. Parma,R. Fanti,H. R. de Ruiter,R. D. Ekers,E. B. Fomalont","Abstract:We present preliminary results of a multi-frequency study of the inversion symmetric radio galaxy NGC326 based on VLA observations at 1.4, 1.6, 4.8, 8.5, and 14.9 GHz. These data allow us to investigate in detail the morphological, spectral and polarization properties of this peculiar object at different levels of spatial resolution.We present preliminary results of a multi-frequency study of the inversion symmetric radio galaxy NGC326 based on VLA observations at 1.4, 1.6, 4.8, 8.5, and 14.9 GHz. These data allow us to investigate in detail the morphological, spectral and polarization properties of this peculiar object at different levels of spatial resolution.△ Less"
Geometric monodromy and the hyperbolic disc,Authors:Ivan Smith,"Abstract:Symplectic four-manifolds give rise to Lefschetz fibrations, which are determined by monodromy representations of free groups in mapping class groups. We study the topology of Lefschetz fibrations by analysing the action of the monodromy on the universal cover of a smooth fibre. We give new and simple proofs that Lefschetz fibrations arising from pencils (i.e. with exceptional sections) never sp…▽ MoreSymplectic four-manifolds give rise to Lefschetz fibrations, which are determined by monodromy representations of free groups in mapping class groups. We study the topology of Lefschetz fibrations by analysing the action of the monodromy on the universal cover of a smooth fibre. We give new and simple proofs that Lefschetz fibrations arising from pencils (i.e. with exceptional sections) never split as non-trivial fibre sums, and that no simple closed curve can be invariant to isotopy under the monodromy representation.△ Less"
Orthogonal polynomials associated with root systems,Authors:Ian G. Macdonald,"Abstract:…, they conincide with the symmetric polynomials described in I. G. Macdonald, Symmetric Functions and Hall Polynomials, 2nd edition,OxfordUniversity Press (1995), Chapter VI.▽ MoreLet R and S be two irreducible root systems spanning the same vector space and having the same Weyl group W, such that S (but not necessarily R) is reduced. For each such pair (R,S) we construct a family of W-invariant orthogonal polynomials in several variables, whose coefficients are rational functions of parametersq,t_1,t_2,...,t_r, where r (=1,2 or 3) is the number of W-orbits in R. For particular values of these parameters, these polynomials give the values of zonal spherical functions on real and p-adic symmetric spaces. Also when R=S is of typeA_n, they conincide with the symmetric polynomials described in I. G. Macdonald, Symmetric Functions and Hall Polynomials, 2nd edition,OxfordUniversity Press (1995), Chapter VI.△ Less"
Models of Polarized and Variable Radio Emission for IDV Source 0917+624,"Authors:T. Beckert,A. Kraus,T. P. Krichbaum,A. Witzel,J. A. Zensus","Abstract:We examine the power spectra of IDV and show the information, which is to be gained by wavelet analysis of light curves of the quasar 0917+624. Results for total and polarized flux at 11cm are shown. Both interstellar scattering and intrinsic models have difficulties in explaining the 1 day period variations. A theoretical model for the time averaged emission is presented, which provides the bas…▽ MoreWe examine the power spectra of IDV and show the information, which is to be gained by wavelet analysis of light curves of the quasar 0917+624. Results for total and polarized flux at 11cm are shown. Both interstellar scattering and intrinsic models have difficulties in explaining the 1 day period variations. A theoretical model for the time averaged emission is presented, which provides the basis for the analysis of possible variations.△ Less"
A high-resolution multi-wavelength study of the jet in 3C 273,"Authors:S. Jester,H. -J. Roeser,K. Meisenheimer,R. Perley,S. Garrington","Abstract:We present HST images at 622 nm and 300 nm of the optical jet in 3C273 and determine the run of the optical spectral index at 0.2"" along the jet. We find no evidence for localized acceleration or loss sites, and support for a little-changing spectral shape throughout the jet. We consider this further evidence in favour of a distributed acceleration process.We present HST images at 622 nm and 300 nm of the optical jet in 3C273 and determine the run of the optical spectral index at 0.2"" along the jet. We find no evidence for localized acceleration or loss sites, and support for a little-changing spectral shape throughout the jet. We consider this further evidence in favour of a distributed acceleration process.△ Less"
Confronting Hydrodynamic Simulations Of Relativistic Jets With Data: What Do We Learn About Particles & Fields?,Authors:Philip A. Hughes,"Abstract:We review recent relativistic hydrodynamic simulations of jets, and their interpretation in terms of the results from linear stability analysis. These studies show that, interpreted naively, the distribution of synchrotron intensity will in general be a poor guide to the physical state (density and pressure) of the underlying flow, and that even if the physical state can be inferred, it, in turn…▽ MoreWe review recent relativistic hydrodynamic simulations of jets, and their interpretation in terms of the results from linear stability analysis. These studies show that, interpreted naively, the distribution of synchrotron intensity will in general be a poor guide to the physical state (density and pressure) of the underlying flow, and that even if the physical state can be inferred, it, in turn, may prove to be a poor guide to the source dynamics, in terms of the transport of energy and momentum from the central engine. However, we demonstrate that an interplay of simulation and linear stability analysis provides a powerful tool for elucidating the nature and character of structures that jets may sustain. From such studies we can explain the complex behavior of observed jets, which manifest both stationary and propagating structures, without recourse to ad hoc macroscopic disturbances. This provides a framework for the interpretation of multi-epoch total intensity data wherein an understanding of the character of individual flow features will allow the effects of physical state and dynamics to be deconvolved.△ Less"
NGC 6251 at multiple scales and wavelengths,"Authors:P. N. Werner,D. M. Worrall,M. Birkinshaw","Abstract:We have studied the FR I radio galaxy NGC 6251 and its environment at several wavelengths and scale lengths. On the large scale, we have probed the gravity field by measuring the velocity dispersion of the cluster members associated with NGC 6251 and relating this to the cluster's X-ray emission. On the small scale, the gravitational information is provided by cold HI near the nucleus and the di…▽ MoreWe have studied the FR I radio galaxy NGC 6251 and its environment at several wavelengths and scale lengths. On the large scale, we have probed the gravity field by measuring the velocity dispersion of the cluster members associated with NGC 6251 and relating this to the cluster's X-ray emission. On the small scale, the gravitational information is provided by cold HI near the nucleus and the distribution of stars and gas near the centre of the galaxy. The cold HI gas which we have measured explains the absorption of the central X-ray emission and is consistent with the extinction through the recently discovered HST gas disc of NGC 6251.△ Less"
Estimating jet power in proton blazar models,"Authors:R. J. Protheroe,A. Mücke",Abstract:We discuss the various contributions to the jet luminosity in proton blazar models of active galactic nuclei and describe a method of estimating the jet luminosity from the observed spectral energy distribution (SED) and the fitted model parameters. We apply this to a synchrotron proton blazar (SPB) model for Markarian 501.We discuss the various contributions to the jet luminosity in proton blazar models of active galactic nuclei and describe a method of estimating the jet luminosity from the observed spectral energy distribution (SED) and the fitted model parameters. We apply this to a synchrotron proton blazar (SPB) model for Markarian 501.△ Less
Are Cluster Radio Relics Revived Fossil Radio Cocoons?,"Authors:Torsten A. Ensslin,Gopal-Krishna","Abstract:A new model for the, so called, `cluster radio relics' is presented (Ensslin & Gopal-Krishna 2000). Fossil radio cocoons, resulting from the former activity of radio galaxies, should contain a low energy relativistic electron population and magnetic fields. Electrons with an age of even up to 2 Gyr can be re-accelerated adiabatically to radio emitting energies, if the fossil radio plasma gets co…▽ MoreA new model for the, so called, `cluster radio relics' is presented (Ensslin & Gopal-Krishna 2000). Fossil radio cocoons, resulting from the former activity of radio galaxies, should contain a low energy relativistic electron population and magnetic fields. Electrons with an age of even up to 2 Gyr can be re-accelerated adiabatically to radio emitting energies, if the fossil radio plasma gets compressed in an environmental shock wave. Such a wave can be caused by a merging event of galaxy clusters, or by the accretion onto galaxy clusters. An implication of this model is the existence of a population of diffuse, ultra-steep spectrum, very low-frequency radio sources located inside and possibly outside of clusters of galaxies, tracing the revival of aged fossil radio plasma by the shock waves associated with large-scale structure formation.△ Less"
Orientation effects on bent extragalactic jets,"Authors:Stephen Higgins,Tim O'Brien,James Dunlop","Abstract:We have investigated how varying several parameters affects the results of a collision between an extragalactic jet and a dense, intergalactic cloud, through a series of hydrodynamic simulations. We have produced synthetic radio images for comparison with observations. These show that a variety of structures may be produced from simple jet-cloud collisions. Moderate Mach numbers and density cont…▽ MoreWe have investigated how varying several parameters affects the results of a collision between an extragalactic jet and a dense, intergalactic cloud, through a series of hydrodynamic simulations. We have produced synthetic radio images for comparison with observations. These show that a variety of structures may be produced from simple jet-cloud collisions. Moderate Mach numbers and density contrasts are needed to produce observable bends. We investigate the effect of viewing from various angles on the appearance of such sources.△ Less"
Challenges in Affecting U.S. Attitudes Towards Space Science,Authors:Howard A. Smith,"Abstract:Space Science enjoys broad public support in the US. People say they follow developments with interest, and that the programs are worth their cost. This attitude seems to arise in part because of the intrinsic accessibility and appeal of the subject matter. Yet at the same time the public is often disturbingly ignorant about basic facts. In this article I summarize recent survey results, includi…▽ MoreSpace Science enjoys broad public support in the US. People say they follow developments with interest, and that the programs are worth their cost. This attitude seems to arise in part because of the intrinsic accessibility and appeal of the subject matter. Yet at the same time the public is often disturbingly ignorant about basic facts. In this article I summarize recent survey results, including attitudes towards basic research, and also consider some of the criticisms of the polling methods. I argue that the results imply vigorous and innovative education and outreach programs can and should be made even more effective -- but that the consequences of success for Space Science research itself are not obvious. I outline two popular education programs which address some of the new challenges facing Space Science outreach.△ Less"
Concepts in Gauge Theory Leading to Electric--Magnetic Duality,Authors:S. T. Tsou,"Abstract:…course given in the Summer School on {\em Geometric Methods in Quantum Field Theory}, Villa de Leyva, Colombia, July 1999, as well as a series of graduate lectures given inOxfordin Trinity Term of 1999 and 2000.▽ MoreGauge theory, which is the basis of all particle physics, is itself based on a few fundamental concepts, the consequences of which are often as beautiful as they are deep. In this short lecture course I shall try to give an introduction to these concepts, both from the physical and mathematical points of view. Then I shall show how these considerations lead to a nonabelian generalization of the well-known electric--magnetic duality in electromagnetism. I shall end by sketching some of the many consequences in quantum field theory that this duality engenders in particle physics.
  These are notes from a lecture course given in the Summer School on {\em Geometric Methods in Quantum Field Theory}, Villa de Leyva, Colombia, July 1999, as well as a series of graduate lectures given inOxfordin Trinity Term of 1999 and 2000.△ Less"
Line formation in solar granulation: II. The photospheric Fe abundance,"Authors:M. Asplund,AA. Nordlund,R. Trampedach,R. F. Stein","Abstract:…dispute regarding FeI lines no longer enter the analysis, leaving the transition probabilities as the main uncertainty. Both FeI (using the samples of lines of both theOxfordand Kiel studies) and FeII lines have been investigated, which give consistent results: log FeI = 7.44 +- 0.05 and log FeII = 7.45 +- 0.10. Also the wings of strong FeI lines return co…▽ MoreThe solar photospheric Fe abundance has been determined using realistic ab initio 3D, time-dependent, hydrodynamical model atmospheres. The study is based on the excellent agreement between the predicted and observed line profiles directly rather than equivalent width, since the intrinsic Doppler broadening from the convective motions and oscillations provide the necessary non-thermal broadening. Thus, three of the four hotly debated parameters (equivalent widths, microturbulence and damping enhancement factors) in the center of the recent solar Fe abundance dispute regarding FeI lines no longer enter the analysis, leaving the transition probabilities as the main uncertainty. Both FeI (using the samples of lines of both theOxfordand Kiel studies) and FeII lines have been investigated, which give consistent results: log FeI = 7.44 +- 0.05 and log FeII = 7.45 +- 0.10. Also the wings of strong FeI lines return consistent abundances, log FeII = 7.42 +- 0.03, but due to the uncertainties inherent in analyses of strong lines we give this determination lower weight than the results from weak and intermediate strong lines. In view of the recent slight downward revision of the meteoritic Fe abundance log Fe = 7.46 +- 0.01, the agreement between the meteoritic and photospheric values is very good, thus appearingly settling the debate over the photospheric Fe abundance from FeI lines.△ Less"
"Spectra, Pseudospectra, and Localization for Random Bidiagonal Matrices","Authors:Lloyd N. Trefethen,Marco Contedini,Mark Embree","Abstract:There has been much recent interest, initiated by work of the physicists Hatano and Nelson, in the eigenvalues of certain random non-Hermitian periodic tridiagonal matrices and their bidiagonal limits. These eigenvalues cluster along a ""bubble with wings"" in the complex plane, and the corresponding eigenvectors are localized in the wings, delocalized in the bubble. Here, in addition to eigenvalu…▽ MoreThere has been much recent interest, initiated by work of the physicists Hatano and Nelson, in the eigenvalues of certain random non-Hermitian periodic tridiagonal matrices and their bidiagonal limits. These eigenvalues cluster along a ""bubble with wings"" in the complex plane, and the corresponding eigenvectors are localized in the wings, delocalized in the bubble. Here, in addition to eigenvalues, pseudospectra are analyzed, making it possible to treat the non-periodic analogues of these random matrix problems. Inside the bubble, the resolvent norm grows exponentially with the dimension. Outside, it grows subexponentially in a bounded region that is the spectrum of the infinite-dimensional operator. Localization and delocalization correspond to resolvent matrices whose entries exponentially decrease or increase, respectively, with distance from the diagonal. This article presents theorems that characterize the spectra, pseudospectra, and numerical range for the four cases of finite bidiagonal matrices, infinite bidiagonal matrices (""stochastic Toeplitz operators""), finite periodic matrices, and doubly infinite bidiagonal matrices (""stochastic Laurent operators"").△ Less"
Radiative corrections to e+e- --> f+f-,"Authors:P. Christova,M. Jack,S. Riemann,T. Riemann","Abstract:The past ten years of physics with e+e- colliding experiments at LEP and SLAC have shown the success of these experiments on not only impressively proving the theoretical predictions of the Standard Model (SM), but also to help provide stringent bounds on physics beyond the SM. With this experience in mind, there appear two equally fascinating opportunities for studying fermion-pair production p…▽ MoreThe past ten years of physics with e+e- colliding experiments at LEP and SLAC have shown the success of these experiments on not only impressively proving the theoretical predictions of the Standard Model (SM), but also to help provide stringent bounds on physics beyond the SM. With this experience in mind, there appear two equally fascinating opportunities for studying fermion-pair production processes at a future Linear Collider (LC). On the one hand, performing high precision measurements to the SM, for example, when running with high luminosity at the Z boson resonance, could be a quick and feasible enterprise in order to pin down the symmetry breaking mechanism of the electroweak sector through indirectly determining the masses of a light SM or MSSM Higgs boson or supersymmetric particles via virtual corrections. On the other hand, looking for such particles in direct production or other `New Physics' effects at energies between, for example, roughly 500 and 800 GeV will naturally be the main motivation to pursue the challenging endeavor of building and utilizing such a unique facility. These two scenarios for the LC shall be sketched here, with particular emphasis on the semi-analytical program ZFITTER for fermion-pair production in comparison with numerical programs like TOPAZ0, KK2f, and others.△ Less"
Biologically Motivated Distributed Designs for Adaptive Knowledge Management,"Authors:Luis M. Rocha,Johan Bollen","Abstract:We discuss how distributed designs that draw from biological network metaphors can largely improve the current state of information retrieval and knowledge management of distributed information systems. In particular, two adaptive recommendation systems named TalkMine and @ApWeb are discussed in more detail. TalkMine operates at the semantic level of keywords. It leads different databases to lea…▽ MoreWe discuss how distributed designs that draw from biological network metaphors can largely improve the current state of information retrieval and knowledge management of distributed information systems. In particular, two adaptive recommendation systems named TalkMine and @ApWeb are discussed in more detail. TalkMine operates at the semantic level of keywords. It leads different databases to learn new and adapt existing keywords to the categories recognized by its communities of users using distributed algorithms. @ApWeb operates at the structural level of information resources, namely citation or hyperlink structure. It relies on collective behavior to adapt such structure to the expectations of users. TalkMine and @ApWeb are currently being implemented for the research library of the Los Alamos National Laboratory under the Active Recommendation Project. Together they define a biologically motivated information retrieval system, recommending simultaneously at the level of user knowledge categories expressed in keywords, and at the level of individual documents and their associations to other documents. Rather than passive information retrieval, with this system, users obtain an active, evolving interaction with information resources.△ Less"
Some uses of moduli spaces in particle and field theory,Authors:ST Tsou,"Abstract:In this talk I shall try to give an elementary introduction to certain areas of mathematical physics where the idea of moduli space is used to help solve problems or to further our understanding. In the wide area of gauge theory, I shall mention instantons, monopoles and duality. Then, under the general heading of string theory, I shall indicate briefly the use of moduli space in conformal field…▽ MoreIn this talk I shall try to give an elementary introduction to certain areas of mathematical physics where the idea of moduli space is used to help solve problems or to further our understanding. In the wide area of gauge theory, I shall mention instantons, monopoles and duality. Then, under the general heading of string theory, I shall indicate briefly the use of moduli space in conformal field theory andM-theory.△ Less"
The triple Higgs self-coupling at future e+e- colliders: a signal-to-background study for the standard model,"Authors:D. J. Miller,S. Moretti","Abstract:The experimental reconstruction of the Higgs self-energy potential is essential to a verification of the Higgs boson's role in spontaneous electroweak symmetry breaking. The first step towards this goal, the measurement of the triple Higgs self-coupling, can possibly be accomplished at the next generation of linear colliders. Here we present a background study of the most promising channel, doub…▽ MoreThe experimental reconstruction of the Higgs self-energy potential is essential to a verification of the Higgs boson's role in spontaneous electroweak symmetry breaking. The first step towards this goal, the measurement of the triple Higgs self-coupling, can possibly be accomplished at the next generation of linear colliders. Here we present a background study of the most promising channel, double Higgs-strahlung off a Z boson, e+e- -> HHZ, with the subsequent decay H -> b bbar, and evaluate the feasibility of its measurement.△ Less"
Nahm transform of doubly-periodic instantons,Authors:Marcos Jardim,"Abstract:This work concerns the study of certain finite-energy solutions of the anti-self-dual Yang-Mills equations on Euclidean 4-dimensional space which are periodic in two directions, so-called doubly-periodic instantons. We establish a circle of ideas involving equivalent analytical and algebraic-geometric descriptions of these objects.
  In the first introductory chapter we provide an overview of th…▽ MoreThis work concerns the study of certain finite-energy solutions of the anti-self-dual Yang-Mills equations on Euclidean 4-dimensional space which are periodic in two directions, so-called doubly-periodic instantons. We establish a circle of ideas involving equivalent analytical and algebraic-geometric descriptions of these objects.
  In the first introductory chapter we provide an overview of the problem and state the main results to be proven in the thesis.
  In chapter 2, we study the asymptotic behaviour of the connections we are concerned with, and show that the coupled Dirac operator is Fredholm.
  After laying these foundations, we are ready to address the main topic of the thesis, the construction of a Nahm transform of doubly-periodic instantons. By combining differential-geometric and holomorphic methods, we show in chapters 3 through 5 that doubly-periodic instantons correspond bijectively to certain singular Higgs pairs, i.e. meromorphic solutions of Hitchin's equations defined over an elliptic curve.
  The circle of ideas is finally closed in chapter 8. We start by presenting a construction due to Friedman, Morgan and Witten that associates to each doubly-periodic instanton a spectral pair consisting of a Riemann surface plus a line bundle over it. On the other hand, it was shown by Hitchin that Higgs pairs are equivalent to a similar set of data. We show that the Friedman, Morgan and Witten spectral pair associated with a doubly-periodic instanton coincides with the Hitchin spectral pair associated with its Nahm transform.
  (This thesis contains the papers math.DG/9909069, math.DG/9910120 and math.AG/9909146.)△ Less"
The electroweak chiral Lagrangian revisited,Authors:Andreas Nyffeler,"Abstract:Using a manifestly gauge-invariant approach we show that the set of low-energy constants in the electroweak chiral Lagrangian currently used in the literature is redundant. In particular, by employing the equations of motion for the gauge fields, one can choose to remove two low-energy constants which contribute to the self-energies of the gauge bosons. The relation of this result to the experim…▽ MoreUsing a manifestly gauge-invariant approach we show that the set of low-energy constants in the electroweak chiral Lagrangian currently used in the literature is redundant. In particular, by employing the equations of motion for the gauge fields, one can choose to remove two low-energy constants which contribute to the self-energies of the gauge bosons. The relation of this result to the experimentally determined values for the oblique parameters S,T and U is discussed. We then evaluate the matching relation between gauge-invariant Green's functions in the full and the effective theory for the case of the Standard Model with a heavy Higgs boson and compare the results for the independent low-energy constants with those for a simple Technicolor model. Since the pattern of the low-energy constants is very different in these two models it may be misleading to mimic any strongly interacting symmetry breaking sector by a heavy Higgs boson. From our investigation we conclude that current electroweak precision data do not really rule out such strongly interacting models.△ Less"
Almost Kahler 4-manifolds with J-invariant Ricci tensor and special Weyl tensor,"Authors:Vestislav Apostolov,Tedi Draghici","Abstract:We study the question of integrability of a compatible almost complex structure on a compact symplectic 4-manifold, under various natural assumptions on the curvature of the associated almost Kahler metric.We study the question of integrability of a compatible almost complex structure on a compact symplectic 4-manifold, under various natural assumptions on the curvature of the associated almost Kahler metric.△ Less"
Higgs radiation off top-antitop pairs at future Linear Colliders: a background study,Authors:S. Moretti,"Abstract:The process e+e- -> Htt can be exploited at future Linear Colliders to measure the Higgs-top Yukawa coupling. In this note, we estimate the size of the irreducible backgrounds in the channel Htt -> bbbb WW -> bbbb ln qq, for the case of a Standard Model Higgs boson with mass between 100 and 140 GeV.The process e+e- -> Htt can be exploited at future Linear Colliders to measure the Higgs-top Yukawa coupling. In this note, we estimate the size of the irreducible backgrounds in the channel Htt -> bbbb WW -> bbbb ln qq, for the case of a Standard Model Higgs boson with mass between 100 and 140 GeV.△ Less"
Riemann Solvers in General Relativistic Hydrodynamics,"Authors:J. M. Ibanez,M. A. Aloy,J. A. Font,J. M. Marti,J. A. Miralles,J. A. Pons","Abstract:Our contribution concerns with the numerical solution of the 3D general relativistic hydrodynamical system of equations within the framework of the 3+1 formalism. We summarize the theoretical ingredients which are necessary in order to build up a numerical scheme based on the solution of local Riemann problems. Hence, the full spectral decomposition of the Jacobian matrices of the system, i.e.,…▽ MoreOur contribution concerns with the numerical solution of the 3D general relativistic hydrodynamical system of equations within the framework of the 3+1 formalism. We summarize the theoretical ingredients which are necessary in order to build up a numerical scheme based on the solution of local Riemann problems. Hence, the full spectral decomposition of the Jacobian matrices of the system, i.e., the eigenvalues and the right and left eigenvectors, is explicitly shown. An alternative approach consists in using any of the special relativistic Riemann solvers recently developed for describing the evolution of special relativistic flows. Our proposal relies on a local change of coordinates in terms of which the spacetime metric is locally Minkowskian and permits an accurate description of numerical general relativistic hydrodynamics.△ Less"
The Nature of Radio Emission from Distant Galaxies,Authors:E. A. Richards,"Abstract:I describe an observational program aimed at understanding the radio emission from distant, rapidly evolving galaxy populations. These observations were carried out at 1.4 and 8.5 GHz with the VLA centered on the Hubble Deep Field. Further MERLIN observations of the HDF region at 1.4 GHz provided an angular resolution of 0.2"" and when combined with the VLA data produced an image with an unpreced…▽ MoreI describe an observational program aimed at understanding the radio emission from distant, rapidly evolving galaxy populations. These observations were carried out at 1.4 and 8.5 GHz with the VLA centered on the Hubble Deep Field. Further MERLIN observations of the HDF region at 1.4 GHz provided an angular resolution of 0.2"" and when combined with the VLA data produced an image with an unprecedented rms noise of 4μJy. All radio sources detected in the VLA complete sample are resolved with a median angular size of 1-2"". The differential count of the radio sources is marginally sub-Euclidean (γ= -2.4\pm0.1) and fluctuation analysis suggests nearly 60 sources per arcmin^2are present at the 1μJy level. A correlation analysis indicates spatial clustering among the 371 radio sources on angular scales of 1- 40 arcmin.
  Optical identifications are made primarily with bright (I = 22) disk systems composed of irregulars, peculiars, interacting/merging galaxies, and a few isolated field spirals. Available redshifts span the range 0.2 - 3. These clues coupled with the steep spectral index of the 1.4 GHz selected sample are indicative of diffuse synchrotron radiation in distant galactic disks. Thus the evolution in the microjansky radio population is driven principally by star-formation.
  I have isolated a number of optically faint radio sources (about 25% of the overall sample) which remain unidentified to I = 26-28 in the HDF and flanking optical fields. Several of these objects have extremely red counterparts and constitute a new class of radio sources which are candidate high-z dusty protogalaxies.△ Less"
Relativistic Jets from Collapsars,"Authors:Miguel A. Aloy,Ewald Mueller,Jose M. Ibanez,Jose M. Marti,Andrew MacFadyen",Abstract:We have studied the relativistic beamed outflow proposed to occur in the collapsar model of gamma-ray bursts. A jet forms as a consequence of an assumed energy deposition of\sim 10^{50}- 10^{51}erg/s within a30^{\circ}cone around the rotation axis of the progenitor star. The generated jet flow is strongly beamed (\lafew degrees) and reaches the surface of the stellar progenitor (r…▽ MoreWe have studied the relativistic beamed outflow proposed to occur in the collapsar model of gamma-ray bursts. A jet forms as a consequence of an assumed energy deposition of\sim 10^{50}- 10^{51}erg/s within a30^{\circ}cone around the rotation axis of the progenitor star. The generated jet flow is strongly beamed (\lafew degrees) and reaches the surface of the stellar progenitor (r\approx 3 10^{10}cm) intact. At break-out the maximum Lorentz factor of the jet flow is about 33. Simulations have been performed with the GENESIS multi-dimensional relativistic hydrodynamic code.△ Less
Simulations of Relativistic Jets with GENESIS,"Authors:Miguel A. Aloy,Jose M. Ibanez,Jose M. Marti,Jose L. Gomez,Ewald Mueller",Abstract:The multidimensional relativistic hydrodynamical code GENESIS has been used to obtain first results of {\it 3D} simulations of relativistic jets. We have studied the influence of a slight perturbation of the injection velocity field on the morphodynamics of otherwise axisymmetric relativistic jets.The multidimensional relativistic hydrodynamical code GENESIS has been used to obtain first results of {\it 3D} simulations of relativistic jets. We have studied the influence of a slight perturbation of the injection velocity field on the morphodynamics of otherwise axisymmetric relativistic jets.△ Less
An exact Riemann Solver for multidimensional special relativistic hydrodynamics,"Authors:J. Pons,J. Ma. Marti,E. Muller","Abstract:We have generalised the exact solution of the Riemann problem in special relativistic hydrodynamics for arbitrary tangential flow velocities. The solution is obtained by solving the jump conditions across shocks plus an ordinary differential equation arising from the self-similarity condition along rarefaction waves, in a similar way as in purely normal flow. The dependence of the solution on th…▽ MoreWe have generalised the exact solution of the Riemann problem in special relativistic hydrodynamics for arbitrary tangential flow velocities. The solution is obtained by solving the jump conditions across shocks plus an ordinary differential equation arising from the self-similarity condition along rarefaction waves, in a similar way as in purely normal flow. The dependence of the solution on the tangential velocities is analysed. This solution has been used to build up an exact Riemann solver implemented in a multidimensional relativistic (Godunov-type) hydro-code.△ Less"
Numerical simulations of relativistic wind accretion on to black holes using Godunov-type methods,"Authors:Jose A. Font,Jose M. Ibanez,Philippos Papadopoulos","Abstract:We have studied numerically the so-called Bondi-Hoyle (wind) accretion on to a rotating (Kerr) black hole in general relativity. We have used the Kerr-Schild form of the Kerr metric, free of coordinate singularities at the black hole horizon. The `test-fluid' approximation has been adopted, assuming no dynamical evolution of the gravitational field. We have used a recent formulation of the gener…▽ MoreWe have studied numerically the so-called Bondi-Hoyle (wind) accretion on to a rotating (Kerr) black hole in general relativity. We have used the Kerr-Schild form of the Kerr metric, free of coordinate singularities at the black hole horizon. The `test-fluid' approximation has been adopted, assuming no dynamical evolution of the gravitational field. We have used a recent formulation of the general relativistic hydrodynamic equations which casts them into a first-order hyperbolic system of conservation laws. Our studies have been performed using a Godunov-type scheme based on Marquina's flux-formula.
  We find that regardless of the value of the black hole spin the final accretion pattern is always stable, leading to constant accretion rates of mass and momentum. The flow is characterized by a strong tail shock which is increasingly wrapped around the central black hole as the hole angular momentum increases. The rotation induced asymmetry in the pressure field implies that besides the well known drag, the black hole will experience also a lift normal to the flow direction.△ Less"
Infinitely Coloured Black Holes,"Authors:Nick E. Mavromatos,Elizabeth Winstanley","Abstract:We formulate the field equations for $SU(\infty)$ Einstein-Yang-Mills theory, and find spherically symmetric black-hole solutions. This model may be motivated by string theory considerations, given the enormous gauge symmetries which characterize string theory. The solutions simplify considerably in the presence of a negative cosmological constant, particularly for the limiting cases of a very l…▽ MoreWe formulate the field equations for $SU(\infty)$ Einstein-Yang-Mills theory, and find spherically symmetric black-hole solutions. This model may be motivated by string theory considerations, given the enormous gauge symmetries which characterize string theory. The solutions simplify considerably in the presence of a negative cosmological constant, particularly for the limiting cases of a very large cosmological constant or very small gauge field. The situation of an arbitrarily small gauge field is relevant for holography and we comment on the AdS/CFT conjecture in this light. The black holes possess infinite amounts of gauge field hair, and we speculate on possible consequences of this for quantum decoherence, which, however, we do not tackle here.△ Less"
Associated Sturm-Liouville systems,Authors:M. M. Crum,Abstract:Crum's seminal result of 1955 is archived hereCrum's seminal result of 1955 is archived here△ Less
Variable Field Bending Magnets for Recirculating Linacs,Authors:Bruce J. King,Abstract:A lattice of single aperture superconducting variable field bending magnets is proposed as a cheap and practical way to recirculate the beams in recirculating linear accelerators. It is shown that the VFBM's can be configured to provide strong focusing in both transverse planes for the full range of beam momenta transported by the lattice.A lattice of single aperture superconducting variable field bending magnets is proposed as a cheap and practical way to recirculate the beams in recirculating linear accelerators. It is shown that the VFBM's can be configured to provide strong focusing in both transverse planes for the full range of beam momenta transported by the lattice.△ Less
Late evolution of cataclysmic variables: the loss of AM Her systems,"Authors:F. Meyer,E. Meyer-Hofmeister","Abstract:The white dwarf in AM Her systems is strongly magnetic and keeps in synchronous rotation with the orbit by magnetic coupling to the secondary star. As the latter evolves through mass loss to a cool, degenerate brown dwarf it can no longer sustain its own magnetic field and coupling is lost. Angular momentum accreted then spins up the white dwarf and the system no longer appears as an AM Her syst…▽ MoreThe white dwarf in AM Her systems is strongly magnetic and keeps in synchronous rotation with the orbit by magnetic coupling to the secondary star. As the latter evolves through mass loss to a cool, degenerate brown dwarf it can no longer sustain its own magnetic field and coupling is lost. Angular momentum accreted then spins up the white dwarf and the system no longer appears as an AM Her system. Possible consequences are run-away mass transfer and mass ejection from the system. Some of the unusual cataclysmic variable systems at low orbital periods may be the outcome of this evolution.△ Less"
V751 Cyg and V Sge as transient supersoft X-ray sources,Authors:J. Greiner,"Abstract:I review the observational evidence for luminous, soft X-ray emission during optical low-states in the two cataclysmic variables V751 Cyg and V Sge, and discuss the possible link to the canonical supersoft X-ray sources.I review the observational evidence for luminous, soft X-ray emission during optical low-states in the two cataclysmic variables V751 Cyg and V Sge, and discuss the possible link to the canonical supersoft X-ray sources.△ Less"
"The Decay b -> s gamma, the Higgs Boson Mass, and Yukawa Unification without R-Parity",Authors:Marco Aurelio Diaz,"Abstract:We review some properties of Bilinear R-Parity Violating models: simple extensions of the Minimal Supersymmetric Standard Model motivated by spontaneous breaking of R-Parity. We concentrate on the relaxation of the bounds on the charged Higgs mass imposed by the measurement of B(b -> s gamma), the effect on the mass of the lightest neutral Higgs boson, the radiative breaking of the electroweak s…▽ MoreWe review some properties of Bilinear R-Parity Violating models: simple extensions of the Minimal Supersymmetric Standard Model motivated by spontaneous breaking of R-Parity. We concentrate on the relaxation of the bounds on the charged Higgs mass imposed by the measurement of B(b -> s gamma), the effect on the mass of the lightest neutral Higgs boson, the radiative breaking of the electroweak symmetry, the unification of bottom and tau Yukawa couplings, and the relation of these phenomena to the radiatively generated tau-neutrino mass.△ Less"
Non-linear Dynamics in QED_3 and Non-trivial Infrared Structure,"Authors:N. E. Mavromatos,J. Papavassiliou","Abstract:In this work we consider a coupled system of Schwinger-Dyson equations for self-energy and vertex functions in QED_3. Using the concept of a semi-amputated vertex function, we manage to decouple the vertex equation and transform it in the infrared into a non-linear differential equation of Emden-Fowler type. Its solution suggests the following picture: in the absence of infrared cut-offs there i…▽ MoreIn this work we consider a coupled system of Schwinger-Dyson equations for self-energy and vertex functions in QED_3. Using the concept of a semi-amputated vertex function, we manage to decouple the vertex equation and transform it in the infrared into a non-linear differential equation of Emden-Fowler type. Its solution suggests the following picture: in the absence of infrared cut-offs there is only a trivial infrared fixed-point structure in the theory. However, the presence of masses, for either fermions or photons, changes the situation drastically, leading to a mass-dependent non-trivial infrared fixed point. In this picture a dynamical mass for the fermions is found to be generated consistently. The non-linearity of the equations gives rise to highly non-trivial constraints among the mass and effective (`running') gauge coupling, which impose lower and upper bounds on the latter for dynamical mass generation to occur. Possible implications of this to the theory of high-temperature superconductivity are briefly discussed.△ Less"
"Spin-gap physics, ground state degeneracy, and bound states on the depleted kagome lattice","Authors:C. Hooley,A. M. Tsvelik","Abstract:We analyse the antiferromagnetic spin-1/2 Heisenberg model on a depleted kagome lattice, where some bonds have been reduced to exchange integral J_2 << J_1. The fully depleted system consists of 1D chains, each with a doubly degenerate singlet-pair ground state and a spectral gap (like the Majumdar-Ghosh model). There are localised and itinerant low-energy excitations. The modes from the lowest…▽ MoreWe analyse the antiferromagnetic spin-1/2 Heisenberg model on a depleted kagome lattice, where some bonds have been reduced to exchange integral J_2 << J_1. The fully depleted system consists of 1D chains, each with a doubly degenerate singlet-pair ground state and a spectral gap (like the Majumdar-Ghosh model). There are localised and itinerant low-energy excitations. The modes from the lowest branch of excitations are incapable of lifting the 2D system's ground state degeneracy at finite J_2/J_1 << 1. Low-energy excitations of the 2D system are dominated by coherently propagating bound states of the 1D excitations.△ Less"
"Moduli Space of Self-Dual Gauge Fields, Holomorphic Bundles and Cohomology Sets",Authors:Tatiana A. Ivanova,Abstract:We discuss the twistor correspondence between complex vector bundles over a self-dual four-dimensional manifold and holomorphic bundles over its twistor space and describe the moduli space of self-dual Yang-Mills fields in terms of Cech and Dolbeault cohomology sets. The cohomological description provides the geometric interpretation of symmetries of the self-dual Yang-Mills equations.We discuss the twistor correspondence between complex vector bundles over a self-dual four-dimensional manifold and holomorphic bundles over its twistor space and describe the moduli space of self-dual Yang-Mills fields in terms of Cech and Dolbeault cohomology sets. The cohomological description provides the geometric interpretation of symmetries of the self-dual Yang-Mills equations.△ Less
Symmetries and reversing symmetries of trace maps,"Authors:Michael Baake,John A. G. Roberts","Abstract:A (discrete) dynamical system may have various symmetries and reversing symmetries, which together form its so-called reversing symmetry group. We study the set of 3D trace maps (obtained from two-letter substitution rules) which preserve the Fricke-Vogt invariant I(x,y,z). This set of dynamical systems forms a group G isomorphic with the projective linear (or modular) group PGL(2,Z). For such t…▽ MoreA (discrete) dynamical system may have various symmetries and reversing symmetries, which together form its so-called reversing symmetry group. We study the set of 3D trace maps (obtained from two-letter substitution rules) which preserve the Fricke-Vogt invariant I(x,y,z). This set of dynamical systems forms a group G isomorphic with the projective linear (or modular) group PGL(2,Z). For such trace maps, we give a complete characterization of the reversing symmetry group as a subgroup of the group A of all polynomial mappings that preserve I(x,y,z).△ Less"
Standard Model Large-E_T Processes and Searches for New Physics at HERA,"Authors:H. Spiesberger,M. W. Krasny",Abstract:Existing and missing calculations of standard model processes producing large transverse energy in electron-proton interactions at HERA are reviewed. The adequacy of the existing standard model Monte Carlo programs for generic searches of exotic processes is analyzed.Existing and missing calculations of standard model processes producing large transverse energy in electron-proton interactions at HERA are reviewed. The adequacy of the existing standard model Monte Carlo programs for generic searches of exotic processes is analyzed.△ Less
High P_T Leptons and W Production at HERA,"Authors:C. Diaconu,J. Kalinowski,T. Matsushita,H. Spiesberger,D. S. Waters","Abstract:Details are given of the observation by H1 of events containing high P_T leptons in addition to large missing P_T. A closely related ZEUS analysis, including a preliminary measurement of the W production cross section, is discussed and the two experiments are compared. Some possible non-Standard Model sources for the events are considered.Details are given of the observation by H1 of events containing high P_T leptons in addition to large missing P_T. A closely related ZEUS analysis, including a preliminary measurement of the W production cross section, is discussed and the two experiments are compared. Some possible non-Standard Model sources for the events are considered.△ Less"
On the Emergence of Time in Quantum Gravity,"Authors:C. J. Isham,J. Butterfield","Abstract:We discuss from a philosophical perspective the way in which the normal concept of time might be said to `emerge' in a quantum theory of gravity. After an introduction, we briefly discuss the notion of emergence, without regard to time (Section 2). We then introduce the search for a quantum theory of gravity (Section 3); and review some general interpretative issues about space, time and matter…▽ MoreWe discuss from a philosophical perspective the way in which the normal concept of time might be said to `emerge' in a quantum theory of gravity. After an introduction, we briefly discuss the notion of emergence, without regard to time (Section 2). We then introduce the search for a quantum theory of gravity (Section 3); and review some general interpretative issues about space, time and matter Section 4). We then discuss the emergence of time in simple quantum geometrodynamics, and in the Euclidean approach (Section 5). Section 6 concludes.△ Less"
D-branes and the Non-commutative Structure of Quantum Spacetime,"Authors:Nick E. Mavromatos,Richard J. Szabo",Abstract:A worldsheet approach to the study of non-abelian D-particle dynamics is presented based on viewing matrix-valued D-brane coordinate fields as coupling constants of a deformed sigma-model which defines a logarithmic conformal field theory. The short-distance structure of spacetime is shown to be naturally captured by the Zamolodchikov metric on the corresponding moduli space which encodes the ge…▽ MoreA worldsheet approach to the study of non-abelian D-particle dynamics is presented based on viewing matrix-valued D-brane coordinate fields as coupling constants of a deformed sigma-model which defines a logarithmic conformal field theory. The short-distance structure of spacetime is shown to be naturally captured by the Zamolodchikov metric on the corresponding moduli space which encodes the geometry of the string interactions between D-particles. Spacetime quantization is induced directly by the string genus expansion and leads to new forms of uncertainty relations which imply that general relativity at very short-distance scales is intrinsically described by a non-commutative geometry. The indeterminancies exhibit decoherence effects suggesting the natural incorporation of quantum gravity by short-distance D-particle probes. Some potential experimental tests are briefly described.△ Less
Searching for Shakespeare in the Stars,Authors:Eric Lewin Altschuler,"Abstract:The question of the authorship of Shakespeare's plays has long been debated. The two leading contenders are W. Shakspere (1564-1616) and Edward de Vere the 13th Earl ofOxford(1550-1604). Here I note that Shakespeare's references to important events and discoveries in astronomy and geophysics in 1572 and 1600, but not to similarly important events o…▽ MoreThe question of the authorship of Shakespeare's plays has long been debated. The two leading contenders are W. Shakspere (1564-1616) and Edward de Vere the 13th Earl ofOxford(1550-1604). Here I note that Shakespeare's references to important events and discoveries in astronomy and geophysics in 1572 and 1600, but not to similarly important events of 1604, 1609 and 1610, especially given Shakespeare's frequent references to and knowledge of the physical sciences, might be able to shed some light on the authorship question.△ Less"
Vector models in the large $N$ limit: a few applications,Authors:Jean Zinn-Justin,"Abstract:…and double scaling limit. Some sections in these notes are directly adapted from the work Zinn-Justin J., 1989, Quantum Field Theory and Critical Phenomena, Clarendon Press (Oxfordthird ed. 1996).▽ MoreIn these lecture notes prepared for the 11th Taiwan Spring School, Taipei 1997}, and updated for the Saalburg summer school 1998, we review the solutions of O(N) or U(N) models in the large N limit and as 1/N expansions, in the case of vector representations. The general idea is that invariant composite fields have small fluctuations for N large. Therefore the method relies on constructing effective field theories for these composite fields after integration over the initial degrees of freedom. We illustrate these ideas by showing that the large N expansion allows to relate the phib^2^2 theory and the non-linear sigma-model, models which are renormalizable in different dimensions. In the same way large N techniques allow to relate the Gross--Neveu, an example of a theory with four-fermi self-interaction, with a Yukawa-type theory renormalizable in four dimensions, a topic relevant for four dimensional field theory. Among other issues for which large N methods are also useful we will briefly discuss finite size effects and finite temperature field theory, because they involve a crossover between different dimensions.\par Finally we consider the case of a general scalar V(phib^2) field theory, explain how the large N techniques can be generalized, and discuss some connected issues like tricritical behaviour and double scaling limit. Some sections in these notes are directly adapted from the work Zinn-Justin J., 1989, Quantum Field Theory and Critical Phenomena, Clarendon Press (Oxfordthird ed. 1996).△ Less"
New supersymmetry algebras from partial supersymmetry breaking,"Authors:Jonathan A. Bagger,Richard Altendorfer","Abstract:In this talk we will study the partial breaking of supersymmetry in flat and anti de Sitter space. We will see that partial breaking in flat space can be accomplished using either of two representations for the massive N=1 spin-3/2 multiplet. We will ""unHiggs"" each representation and find a new N=2 supergravity and a new N=2 supersymmetry algebra. We will also see that partial supersymmetry brea…▽ MoreIn this talk we will study the partial breaking of supersymmetry in flat and anti de Sitter space. We will see that partial breaking in flat space can be accomplished using either of two representations for the massive N=1 spin-3/2 multiplet. We will ""unHiggs"" each representation and find a new N=2 supergravity and a new N=2 supersymmetry algebra. We will also see that partial supersymmetry breaking in AdS space can give rise to a new N=2 supersymmetry algebra, one that is necessarily nonlinearly realized.△ Less"
Supersymmetric particle production at hadron colliders,Authors:Michael Krämer,"Abstract:The theoretical status of MSSM particle production at the hadron colliders Tevatron and LHC is reviewed, including next-to-leading order supersymmetric QCD corrections. The higher-order corrections significantly reduce the theoretical uncertainty and lead to a rise of the lower bounds on supersymmetric particle masses, as demonstrated for the case of top-squark and gaugino pair production at the…▽ MoreThe theoretical status of MSSM particle production at the hadron colliders Tevatron and LHC is reviewed, including next-to-leading order supersymmetric QCD corrections. The higher-order corrections significantly reduce the theoretical uncertainty and lead to a rise of the lower bounds on supersymmetric particle masses, as demonstrated for the case of top-squark and gaugino pair production at the Tevatron.△ Less"
The Polarisation Signatures of Microlensing,"Authors:A. M. Newsam,J. F. L. Simmons,M. A. Hendry,I. J. Coleman",Abstract:It has already been shown that microlensing can give rise to a non-zero variable polarisation signal. Here we use realistic simulations to demonstrate the additional information that can be gained from polarimetric observations of lensing events.It has already been shown that microlensing can give rise to a non-zero variable polarisation signal. Here we use realistic simulations to demonstrate the additional information that can be gained from polarimetric observations of lensing events.△ Less
Microlensing of Extended Stellar Sources,"Authors:M. A. Hendry,I. J. Coleman,N. Gray,A. M. Newsam,J. F. L. Simmons","Abstract:We investigate the feasibility of reconstructing the radial intensity profile of extended stellar sources by inverting their microlensed light curves. Using a simple, linear, limb darkening law as an illustration, we show that the intensity profile can be accurately determined, at least over the outer part of the stellar disc, with realistic light curve sampling and photometric errors. The princ…▽ MoreWe investigate the feasibility of reconstructing the radial intensity profile of extended stellar sources by inverting their microlensed light curves. Using a simple, linear, limb darkening law as an illustration, we show that the intensity profile can be accurately determined, at least over the outer part of the stellar disc, with realistic light curve sampling and photometric errors. The principal requirement is that the impact parameter of the lens be less than or equal to the stellar radius. Thus, the analysis of microlensing events provides a powerful method for testing stellar atmosphere models.△ Less"
Nonthermal Supermassive Dark Matter,"Authors:Daniel J. H. Chung,Edward W. Kolb,Antonio Riotto","Abstract:We discuss several cosmological production mechanisms for nonthermal supermassive dark matter and argue that dark matter may be elementary particles of mass much greater than the weak scale. Searches for dark matter should not be limited to weakly interacting particles with mass of the order of the weak scale, but should extend into the supermassive range as well.We discuss several cosmological production mechanisms for nonthermal supermassive dark matter and argue that dark matter may be elementary particles of mass much greater than the weak scale. Searches for dark matter should not be limited to weakly interacting particles with mass of the order of the weak scale, but should extend into the supermassive range as well.△ Less"
Induced Magnetic moments in three-dimensional gauge theories with external magnetic fields,"Authors:N. E. Mavromatos,A. Momen","Abstract:We study the appearance of induced parity-violating magnetic moment, in the presence of external magnetic fields, for even-number of fermion species coupled to dynamical fields in three dimensions. Specifically, we use a SU(2)xU(1) gauge model for dynamical gauge symmetry breaking, which is also proposed recently as a field theoretical model for high-temperature superconductors. By decomposing t…▽ MoreWe study the appearance of induced parity-violating magnetic moment, in the presence of external magnetic fields, for even-number of fermion species coupled to dynamical fields in three dimensions. Specifically, we use a SU(2)xU(1) gauge model for dynamical gauge symmetry breaking, which is also proposed recently as a field theoretical model for high-temperature superconductors. By decomposing the fermionic degrees of freedom in terms of Landau levels, we show that, in the effective theory with the lowest Landau levels, a parity-violating magnetic moment interaction is induced by the higher Landau levels when the fermions are massive. The possible relevance of this result for a recently observed phenomenon in high-temperature superconductors is also discussed.△ Less"
Superheavy dark matter,"Authors:Daniel J. H. Chung,Edward W. Kolb,Antonio Riotto","Abstract:We show that in large-field inflationary scenarios, superheavy (many orders of magnitude larger than the weak scale) dark matter will be produced in cosmologically interesting quantities if superheavy stable particles exist in the mass spectrum. We show that these particles may be produced naturally during the transition from the inflationary phase to either a matter-dominated or radiation-domin…▽ MoreWe show that in large-field inflationary scenarios, superheavy (many orders of magnitude larger than the weak scale) dark matter will be produced in cosmologically interesting quantities if superheavy stable particles exist in the mass spectrum. We show that these particles may be produced naturally during the transition from the inflationary phase to either a matter-dominated or radiation-dominated phase as a result of the expansion of the background spacetime acting on vacuum quantum fluctuations of the dark matter field. We find that as long as there are stable particles whose mass is of the order of the inflaton mass (presumably around $10^{13}$GeV), they will be produced in sufficient abundance to give $Ω_0 =1$ quite independently of any details of the non-gravitational interactions of the dark-matter field.△ Less"
Topological Structure of the SU(3) Vacuum,"Authors:Douglas A. Smith,Michael J. Teper","Abstract:We investigate the topological structure of the vacuum in SU(3) lattice gauge theory. We use under-relaxed cooling to remove the high-frequency fluctuations and a variety of ""filters"" to identify the topological charges in the resulting smoothened field configurations. We find a densely packed vacuum with an average instanton size, in the continuum limit, of about 0.5 fm. The density at large si…▽ MoreWe investigate the topological structure of the vacuum in SU(3) lattice gauge theory. We use under-relaxed cooling to remove the high-frequency fluctuations and a variety of ""filters"" to identify the topological charges in the resulting smoothened field configurations. We find a densely packed vacuum with an average instanton size, in the continuum limit, of about 0.5 fm. The density at large sizes decreases as a large inverse power of the size. At small sizes we see some sign of a trend towards the asymptotic perturbative behaviour. We find that an interesting polarisation phenomenon occurs: the large topological charges tend to have, on the average, the same sign and are over-screened by the smaller charges which tend to have, again on the average, the opposite sign to the larger instantons. We also calculate the topological susceptibility for which we obtain a continuum value of about 187 MeV. We perform the calculations for various volumes, lattice spacings and numbers of cooling sweeps, so as to obtain some control over the associated systematic errors. The coupling range is from beta=6.0 to beta=6.4 and the lattice volumes range from 16x16x16x48 to 32x32x32x64.△ Less"
Energy and angular momentum of the weak gravitational waves on the Schwarzschild background -- quasilocal gauge-invariant formulation,Authors:Jacek Jezierski,"Abstract:…1063-1069 (1957)) and for the polar one we get Zerilli result (Phys. Rev. D2, 2141-2160 (1970)), see also Chandrasekhar (The Mathematical Theory of Black Holes,(Clarendon PressOxford, 1983)), Moncrief (Annals of Physics 88, 323-342 (1974)) for both. An important ingredient of the analysis is the concept of quasilocality which does duty for the separation of…▽ MoreIt is shown that the axial and polar perturbations of the spherically symmetric black hole can be described in a gauge-invariant way. The reduced phase space describing gravitational waves outside of the horizon is described by the gauge-invariant quantities. Both degrees of freedom fulfill generalized scalar wave equation. For the axial degree of freedom the radial part of the equation corresponds to the Regge-Wheeler result (Phys. Rev. 108, 1063-1069 (1957)) and for the polar one we get Zerilli result (Phys. Rev. D2, 2141-2160 (1970)), see also Chandrasekhar (The Mathematical Theory of Black Holes,(Clarendon PressOxford, 1983)), Moncrief (Annals of Physics 88, 323-342 (1974)) for both. An important ingredient of the analysis is the concept of quasilocality which does duty for the separation of the angular variables in the usual approach. Moreover, there is no need to represent perturbations by normal modes (with time dependence $\exp(-ikt)$), we have fields in spacetime and the Cauchy problem for them is well defined outside of the horizon. The reduced symplectic structure explains the origin of the axial and polar invariants. It allows to introduce an energy and angular momentum for the gravitational waves which is invariant with respect to the gauge transformations. Both generators represent quadratic approximation of the ADM nonlinear formulae in terms of the perturbations of the Schwarzschild metric. We also discuss the boundary-initial value problem for the linearized Einstein equations on a Schwarzschild background outside of the horizon.△ Less"
Classical lifting processes and multiplicative vector fields,"Authors:Kirill Mackenzie,Ping Xu",Abstract:We extend the calculus of multiplicative vector fields and differential forms and their intrinsic derivatives from Lie groups to Lie groupoids; this generalization turns out to include also the classical process of complete lifting from arbitrary manifolds to tangent and cotangent bundles. Using this calculus we give a new description of the Lie bialgebroid structure associated with a Poisson gr…▽ MoreWe extend the calculus of multiplicative vector fields and differential forms and their intrinsic derivatives from Lie groups to Lie groupoids; this generalization turns out to include also the classical process of complete lifting from arbitrary manifolds to tangent and cotangent bundles. Using this calculus we give a new description of the Lie bialgebroid structure associated with a Poisson groupoid.△ Less
A k-space transport analysis of the BEEM spectroscopy of Au/Si Schottky barriers,"Authors:U. Hohenester,P. Kocevar,P. L. de Andres,F. Flores","Abstract:We address the question of the spatial resolution of ballistic electron emission microscopy (BEEM) of Shottky barriers in Au(111)/Si(100) and Au(111)/Si(111) interfaces. A novel combination of Green-function and k-space Ensemble-Monte-Carlo techniques is used to obtain new insights into the spatial and energetic evolution of the STM-tip-induced electrons during their passage through the metallic…▽ MoreWe address the question of the spatial resolution of ballistic electron emission microscopy (BEEM) of Shottky barriers in Au(111)/Si(100) and Au(111)/Si(111) interfaces. A novel combination of Green-function and k-space Ensemble-Monte-Carlo techniques is used to obtain new insights into the spatial and energetic evolution of the STM-tip-induced electrons during their passage through the metallic layer before reaching the metal-semiconductor interface. In particular, it is shown how the effect of band-structure-induced directional focusing of the electrons enforces a reinterpretation of existing experimental data.△ Less"
Report of the working group on the measurement of triple gauge boson couplings,"Authors:F. Berends,T. Bowcock,D. Charlton,P. Clarke,J. Conboy,C. Hartmann,P. Kyberd,C. G. Papadopoulos,H. T. Phillips,R. Sekulin,A. Skillman","Abstract:The working group discussed several aspects of triple gauge coupling analysis viewed in the light of experiences with the first high energy data recorded at energies above the W pair threshold. Some analysis methods were reviewed briefly, and consideration given to better ways of characterising the data. The measurement of CP violating parameters was discussed. Results were prepared to further q…▽ MoreThe working group discussed several aspects of triple gauge coupling analysis viewed in the light of experiences with the first high energy data recorded at energies above the W pair threshold. Some analysis methods were reviewed briefly, and consideration given to better ways of characterising the data. The measurement of CP violating parameters was discussed. Results were prepared to further quantify the precision attainable on anaomalous couplings in the four-quark channel using jet-charge methods, and finally the trade off between maximum LEP energy-vs-luminosity was quantified.△ Less"
"Report of the Working Group on `W Mass and QCD' (Phenomenology Workshop on LEP2 Physics,Oxford, April 1997)","Authors:A. Ballestrero,D. G. Charlton,G. Cowan,P. Dornan,R. Edgecock,J. Ellis,E. W. N. Glover,C. Hawkes,H. Hwang,R. Jones,V. Kartvelishvili,Z. Kunszt,E. Maina,D. J. Miller,S. Moretti,A. Moutoussi,C. Parkes,P. B. Renton,D. A. Ross,W. J. Stirling,J. C. Thompson,M. Thomson,D. R. Ward,C. P. Ward,J. Ward, et al. (3 additional authors not shown)","Abstract:The W Mass and QCD Working Group discussed a wide variety of topics relating to present and future measurements of M(W) at LEP2, including QCD backgrounds to W+W- production. Particular attention was focused on experimental issues concerning the direct reconstruction and threshold mass measurements, and on theoretical and experimental issues concerning the four jet final state. This report summa…▽ MoreThe W Mass and QCD Working Group discussed a wide variety of topics relating to present and future measurements of M(W) at LEP2, including QCD backgrounds to W+W- production. Particular attention was focused on experimental issues concerning the direct reconstruction and threshold mass measurements, and on theoretical and experimental issues concerning the four jet final state. This report summarises the main conclusions.△ Less"
Experimental aspects of colour reconnection,"Authors:M. F. Watson,N. K. Watson","Abstract:…of colour reconnection in W+W- production, concentrating on charged multiplicity and event shapes, which were carried out as part of the Phenomenology Workshop on LEP2 Physics,Oxford, Physics Department and Keble College, 14-18 April, 1997. The work includes new estimates of the systematic uncertainty which may be attributed to colour reconnection effects i…▽ MoreThis report summarises experimental aspects of the phenomena of colour reconnection in W+W- production, concentrating on charged multiplicity and event shapes, which were carried out as part of the Phenomenology Workshop on LEP2 Physics,Oxford, Physics Department and Keble College, 14-18 April, 1997. The work includes new estimates of the systematic uncertainty which may be attributed to colour reconnection effects in experimental measurements of Mw.△ Less"
Corrigenda to: Introduction to Symplectic Topology,"Authors:Dusa McDuff,Dietmar Salamon","Abstract:This contains a list of (mostly very minor) corrections to the book Introduction to Symplectic Topology, Clarendon Press,Oxford, (1995), together with rewritten versions of two lemmas and some additional comments.This contains a list of (mostly very minor) corrections to the book Introduction to Symplectic Topology, Clarendon Press,Oxford, (1995), together with rewritten versions of two lemmas and some additional comments.△ Less"
Two Photon Physics at LEP2,"Authors:Susan Cartwright,Michael H Seymour,Klaus Affholderbach,Frank Close,Glen Cowan,Alex Finch,Jan Lauber,Mark Lehto,Alison Wright","Abstract:The working group on two photon physics concentrated on three main subtopics: modelling the hadronic final state of deep inelastic scattering on a photon; unfolding the deep inelastic scattering data to obtain the photon structure function; and resonant production of exclusive final states, particularly of glueball candidates. In all three areas, new results were presented.The working group on two photon physics concentrated on three main subtopics: modelling the hadronic final state of deep inelastic scattering on a photon; unfolding the deep inelastic scattering data to obtain the photon structure function; and resonant production of exclusive final states, particularly of glueball candidates. In all three areas, new results were presented.△ Less"
Colour reconnection and Bose-Einstein effects,Authors:B. R. Webber,"Abstract:Final-state interactions and interference phenomena that could affect the value of the W mass reconstructed from hadronic WW decays at LEP2 are reviewed, and possible areas for future investigation are identified.Final-state interactions and interference phenomena that could affect the value of the W mass reconstructed from hadronic WW decays at LEP2 are reviewed, and possible areas for future investigation are identified.△ Less"
Report of the 1997 LEP2 Phenomenology Working Group on `Searches' (Oxford),"Authors:B. C. Allanach,G. A. Blair,M. A. Diaz,H. Dreiner,J. Ellis,P. Igo-Kemenes,S. F. King,P. Morawitz,W. Murray,A. Normand,D. A. Ross,P. Teixeira-Dias,M. D. Williams,G. W. Wilson,T. Wyatt",Abstract:The Searches Working Group discussed a variety of topics relating to present and future measurements of searches at LEP 2. The individual contributions are included separately.The Searches Working Group discussed a variety of topics relating to present and future measurements of searches at LEP 2. The individual contributions are included separately.△ Less
Two Photon Physics at LEP2; including data Monte-Carlo comparison,Authors:D. J. Miller,"Abstract:A partisan review of some of the most important $γγ$ channels accessible at LEP 2, with special stress on the measurement of the photon structure function $F_{2}^γ$ and on associated problems with Monte Carlo modelling.A partisan review of some of the most important $γγ$ channels accessible at LEP 2, with special stress on the measurement of the photon structure function $F_{2}^γ$ and on associated problems with Monte Carlo modelling.△ Less"
"Centering, Anaphora Resolution, and Discourse Structure",Authors:Marilyn A. Walker,"Abstract:Centering was formulated as a model of the relationship between attentional state, the form of referring expressions, and the coherence of an utterance within a discourse segment (Grosz, Joshi and Weinstein, 1986; Grosz, Joshi and Weinstein, 1995). In this chapter, I argue that the restriction of centering to operating within a discourse segment should be abandoned in order to integrate centerin…▽ MoreCentering was formulated as a model of the relationship between attentional state, the form of referring expressions, and the coherence of an utterance within a discourse segment (Grosz, Joshi and Weinstein, 1986; Grosz, Joshi and Weinstein, 1995). In this chapter, I argue that the restriction of centering to operating within a discourse segment should be abandoned in order to integrate centering with a model of global discourse structure. The within-segment restriction causes three problems. The first problem is that centers are often continued over discourse segment boundaries with pronominal referring expressions whose form is identical to those that occur within a discourse segment. The second problem is that recent work has shown that listeners perceive segment boundaries at various levels of granularity. If centering models a universal processing phenomenon, it is implausible that each listener is using a different centering algorithm.The third issue is that even for utterances within a discourse segment, there are strong contrasts between utterances whose adjacent utterance within a segment is hierarchically recent and those whose adjacent utterance within a segment is linearly recent. This chapter argues that these problems can be eliminated by replacing Grosz and Sidner's stack model of attentional state with an alternate model, the cache model. I show how the cache model is easily integrated with the centering algorithm, and provide several types of data from naturally occurring discourses that support the proposed integrated model. Future work should provide additional support for these claims with an examination of a larger corpus of naturally occurring discourses.△ Less"
Intrasentential Centering: A Case Study,Authors:Megumi Kameyama,"Abstract:One of the necessary extensions to the centering model is a mechanism to handle pronouns with intrasentential antecedents. Existing centering models deal only with discourses consisting of simple sentences. It leaves unclear how to delimit center-updating utterance units and how to process complex utterances consisting of multiple clauses. In this paper, I will explore the extent to which a stra…▽ MoreOne of the necessary extensions to the centering model is a mechanism to handle pronouns with intrasentential antecedents. Existing centering models deal only with discourses consisting of simple sentences. It leaves unclear how to delimit center-updating utterance units and how to process complex utterances consisting of multiple clauses. In this paper, I will explore the extent to which a straightforward extension of an existing intersentential centering model contributes to this effect. I will motivate an approach that breaks a complex sentence into a hierarchy of center-updating units and proposes the preferred interpretation of a pronoun in its local context arbitrarily deep in the given sentence structure. This approach will be substantiated with examples from naturally occurring written discourses.△ Less"
Amalgamated Codazzi Raychaudhuri identity for foliation,Authors:Brandon Carter,Abstract:It is shown how a pure background tensor formalism provides a concise but explicit and highly flexible machinery for the generalised curvature analysis of individual embedded surfaces and foliations such as arise in the theory of topological defects in cosmological and other physical contexts. The unified treatment provided here shows how the relevant extension of the Raychaudhuri identity is re…▽ MoreIt is shown how a pure background tensor formalism provides a concise but explicit and highly flexible machinery for the generalised curvature analysis of individual embedded surfaces and foliations such as arise in the theory of topological defects in cosmological and other physical contexts. The unified treatment provided here shows how the relevant extension of the Raychaudhuri identity is related to the correspondingly extended Codazzi identity.△ Less
From quantum-codemaking to quantum code-breaking,Authors:Artur Ekert,"Abstract:…its current impact on both cryptography and cryptanalysis. The paper is based on the lecture given at the conference ""Geometric Issues in the Foundations of Science"" (Oxford, June 1996) in honor of Roger Penrose.▽ MoreThis is a semi-popular overview of quantum entanglement as an important physical resource in the field of data security and quantum computing. After a brief outline of entanglement's key role in philosophical debates about the meaning of quantum mechanics I describe its current impact on both cryptography and cryptanalysis. The paper is based on the lecture given at the conference ""Geometric Issues in the Foundations of Science"" (Oxford, June 1996) in honor of Roger Penrose.△ Less"
Dilatonic Black Holes in Higher-Curvature String Gravity II: Linear Stability,"Authors:P. Kanti,N. E. Mavromatos,J. Rizos,K. Tamvakis,E. Winstanley","Abstract:We demonstrate linear stability of the dilatonic Black Holes appearing in a string-inspired higher-derivative gravity theory with a Gauss- Bonnet curvature-squared term. The proof is accomplished by mapping the system to a one-dimensional Schrodinger problem which admits no bound states. This result is important in that it constitutes a linearly stable example of a black hole that bypasses the `…▽ MoreWe demonstrate linear stability of the dilatonic Black Holes appearing in a string-inspired higher-derivative gravity theory with a Gauss- Bonnet curvature-squared term. The proof is accomplished by mapping the system to a one-dimensional Schrodinger problem which admits no bound states. This result is important in that it constitutes a linearly stable example of a black hole that bypasses the `no-hair conjecture'. However, the dilaton hair is `secondary'in the sense that it is not accompanied by any new quantum number for the black hole solution.△ Less"
Z(3) Interfaces in Lattice Gauge Theory,Authors:S. T. West,"Abstract:A study is made of properties of the Z(3) interface which forms between the different ordered phases of pure SU(3) gauge theory above a critical temperature. The theory is simulated on a (2+1)-D lattice at various temperatures above this critical point. At high temperatures, the interface tension is shown to agree well with the prediction of perturbation theory. Near the critical temperature, th…▽ MoreA study is made of properties of the Z(3) interface which forms between the different ordered phases of pure SU(3) gauge theory above a critical temperature. The theory is simulated on a (2+1)-D lattice at various temperatures above this critical point. At high temperatures, the interface tension is shown to agree well with the prediction of perturbation theory. Near the critical temperature, the interface behaviour is characterised by various displacement moments, and modelled by an interacting scalar field theory. This thesis is provided for reference, as it gives full details of the computational and statistical methods outlined only briefly in preprints hep-lat/9605040 and hep-lat/9607005.△ Less"
Data Analysis Techniques for Resolving Nonlinear Processes in Plasmas : a Review,Authors:T. Dudok de Wit,Abstract:The growing need for a better understanding of nonlinear processes in plasma physics has in the last decades stimulated the development of new and more advanced data analysis techniques. This review lists some of the basic properties one may wish to infer from a data set and then presents appropriate analysis techniques with some recent applications. The emphasis is put on the investigation of n…▽ MoreThe growing need for a better understanding of nonlinear processes in plasma physics has in the last decades stimulated the development of new and more advanced data analysis techniques. This review lists some of the basic properties one may wish to infer from a data set and then presents appropriate analysis techniques with some recent applications. The emphasis is put on the investigation of nonlinear wave phenomena and turbulence in space plasmas.△ Less
Interaction-Free Measurements,Authors:Lev Vaidman,Abstract:…measurements (IFM) is presented. The IFM is a solution of a quantum puzzle: How to test a bomb which explodes on every test without exploding it? This paper was given in theOxfordconference in honor of Roger Penrose.▽ MoreA brief review of interaction-free measurements (IFM) is presented. The IFM is a solution of a quantum puzzle: How to test a bomb which explodes on every test without exploding it? This paper was given in theOxfordconference in honor of Roger Penrose.△ Less
Subalgebras of Cohen algebras need not be Cohen,"Authors:Sabine Koppelberg,Saharon Shelah",Abstract:We give an example of a regular and complete subalgebra of a Cohen algebra which is not Cohen.We give an example of a regular and complete subalgebra of a Cohen algebra which is not Cohen.△ Less
A Gravitational Explanation for Quantum Mechanics,Authors:Mark J. Hadley,"Abstract:It is shown that certain structures in classical General Relativity can give rise to non-classical logic, normally associated with Quantum Mechanics. A 4-geon model of an elementary particle is proposed which is asymptotically flat, particle-like and has a non-trivial causal structure. The usual Cauchy data are no longer sufficient to determine a unique evolution. The measurement apparatus itsel…▽ MoreIt is shown that certain structures in classical General Relativity can give rise to non-classical logic, normally associated with Quantum Mechanics. A 4-geon model of an elementary particle is proposed which is asymptotically flat, particle-like and has a non-trivial causal structure. The usual Cauchy data are no longer sufficient to determine a unique evolution. The measurement apparatus itself can impose non-redundant boundary conditions. Measurements of such an object would fail to satisfy the distributive law of classical physics. This model reconciles General Relativity and Quantum Mechanics without the need for Quantum Gravity. The equations of Quantum Mechanics are unmodified but it is not universal; classical particles and waves could exist and there is no graviton.△ Less"
Discourse Coherence and Shifting Centers in Japanese Texts,Authors:Masayo Iida,"Abstract:In languages such as Japanese, the use of {\it zeros}, unexpressed arguments of the verb, in utterances that shift the topic involves a risk that the meaning intended by the speaker may not be transparent to the hearer. However, this potentially undesirable conversational strategy often occurs in the course of naturally-occurring discourse. In this chapter, I report on an empirical study of 250…▽ MoreIn languages such as Japanese, the use of {\it zeros}, unexpressed arguments of the verb, in utterances that shift the topic involves a risk that the meaning intended by the speaker may not be transparent to the hearer. However, this potentially undesirable conversational strategy often occurs in the course of naturally-occurring discourse. In this chapter, I report on an empirical study of 250 utterances with {\it zeros} in 20 Japanese newspaper articles. Each utterance is analyzed in terms of centering transitions and the form in which centers are realized by referring expressions. I also examine lexical subcategorization information, and tense and aspect in order to test the hypothesis that the speaker expects the hearer to use this information in determining global discourse structure. I explain the occurrence of {\it zeros} in {\sc retain} and {\sc rough-shift} centering transitions, by claiming that a {\it zero} can only be used in these cases when the shift of centers is supported by contextual information such as lexical semantics, tense and aspect, and agreement features. I then propose an algorithm by which centering can incorporate these observations to integrate centering with global discourse structure, and thus enhance its ability for non-local pronoun resolution.△ Less"
D-Brane Recoil and Logarithmic Operators,"Authors:Ian I. Kogan,Nick E. Mavromatos,John F. Wheater",Abstract:We construct the pair of logarithmic operators associated with the recoil of a $D$-brane. This construction establishes a connection between a translation in time and a world-sheet rescaling. The problem of measuring the centre of mass coordinate of the $D$-brane is considered and the relation between the string uncertainty principle and the logarithmic operators is discussed.We construct the pair of logarithmic operators associated with the recoil of a $D$-brane. This construction establishes a connection between a translation in time and a world-sheet rescaling. The problem of measuring the centre of mass coordinate of the $D$-brane is considered and the relation between the string uncertainty principle and the logarithmic operators is discussed.△ Less
Eluding the No-Hair Conjecture for Black Holes,Authors:N. E. Mavromatos,"Abstract:I discuss a recent analytic proof of bypassing the no-hair conjecture for two interesting (and quite generic) cases of four-dimensional black holes: (i) black holes in Einstein-Yang-Mills-Higgs (EYMH) systems and (ii) black holes in higher-curvature (Gauss-Bonnet (GB) type) string-inspired gravity. Both systems are known to possess black-hole solutions with non-trivial scalar hair outside the ho…▽ MoreI discuss a recent analytic proof of bypassing the no-hair conjecture for two interesting (and quite generic) cases of four-dimensional black holes: (i) black holes in Einstein-Yang-Mills-Higgs (EYMH) systems and (ii) black holes in higher-curvature (Gauss-Bonnet (GB) type) string-inspired gravity. Both systems are known to possess black-hole solutions with non-trivial scalar hair outside the horizon. The `spirit' of the no-hair conjecture, however, seems to be maintained either because the black holes are unstable (EYMH), or because the hair is of secondary type (GB), i.e. it does not lead to new conserved quantum numbers.△ Less"
Coulomb Blockade and Digital Single-Electron Devices,Authors:Alexander N. Korotkov,"Abstract:Tunneling of single electrons has been thoroughly studied both theoretically and experimentally during last ten years. By the present time the basic physics is well understood, and creation of useful single-electron devices becomes the important issue. Single-electron tunneling seems to be the most promising candidate to be used in the future integrated digital circuits with the typical size sca…▽ MoreTunneling of single electrons has been thoroughly studied both theoretically and experimentally during last ten years. By the present time the basic physics is well understood, and creation of useful single-electron devices becomes the important issue. Single-electron tunneling seems to be the most promising candidate to be used in the future integrated digital circuits with the typical size scale of few nanometers and below, i.e. in the molecular electronics. In the review we first briefly discuss the physics of single-electron tunneling and the operation of the single-electron transistor. After that, we concentrate on the hypothetical ultradense digital single-electron circuits and discuss the different proposed families of them. The last part of the review considers the issues of the discrete energy spectrum and the finite tunnel barrier height which are important for the molecular-size single-electron devices.△ Less"
Aspects of hairy black holes in spontaneously-broken Einstein-Yang-Mills systems: Stability analysis and Entropy considerations,"Authors:N. E. Mavromatos,Elizabeth Winstanley","Abstract:We analyze (3+1)-dimensional black-hole space-times in spontaneously broken Yang-Mills gauge theories that have been recently presented as candidates for an evasion of the scalar-no-hair theorem. Although we show that in principle the conditions for the no-hair theorem do not apply to this case, however we prove that the `spirit' of the theorem is not violated, in the sense that there exist inst…▽ MoreWe analyze (3+1)-dimensional black-hole space-times in spontaneously broken Yang-Mills gauge theories that have been recently presented as candidates for an evasion of the scalar-no-hair theorem. Although we show that in principle the conditions for the no-hair theorem do not apply to this case, however we prove that the `spirit' of the theorem is not violated, in the sense that there exist instabilities, in both the sphaleron and gravitational sectors. The instability analysis of the sphaleron sector, which was expected to be unstable for topological reasons, is performed by means of a variational method. As shown, there exist modes in this sector that are unstable against linear perturbations. Instabilities exist also in the gravitational sector. A method for counting the gravitational unstable modes, which utilizes a catastrophe-theoretic approach is presented. The rôle of the catastrophe functional is played by the mass functional of the black hole. The Higgs vacuum expectation value (v.e.v.) is used as a control parameter, having a critical value beyond which instabilities are turned on. The (stable) Schwarzschild solution is then understood from this point of view. The catastrophe-theory appproach facilitates enormously a universal stability study of non-Abelian black holes, which goes beyond linearized perturbations. Some elementary entropy considerations are also presented...△ Less"
B Meson Form Factors and Exclusive Decays,"Authors:A. Khodjamirian,R. Rückl","Abstract:We present results for the $B \rightarrow π$ and $D\rightarrow π$ form factors derived from QCD sum rules on the light-cone. Our predictions are compared with experiment and used to extract the quark mixing parameter $|V_{ub}|$ from a recent CLEO measurement of $B \ra πl^+ ν$. Furthermore, we discuss the factorization approximation for exclusive nonleptonic matrix elements, and describe a first…▽ MoreWe present results for the $B \rightarrow π$ and $D\rightarrow π$ form factors derived from QCD sum rules on the light-cone. Our predictions are compared with experiment and used to extract the quark mixing parameter $|V_{ub}|$ from a recent CLEO measurement of $B \ra πl^+ ν$. Furthermore, we discuss the factorization approximation for exclusive nonleptonic matrix elements, and describe a first QCD estimate of the nonfactorizable contribution to the amplitude of $B \rightarrow J/ψK $.△ Less"
Theoretical Review of B-Physics,Authors:Andrzej J. Buras,"Abstract:We review several aspects of B-Physics. In particular we discuss:
  i) The theoretical framework for B-decays,
  ii) Weak decays beyond leading logarithms,
  iii) Standard analysis of the unitarity triangle,
  iv) Rare B-decays,
  v) CP violation including the issue of electroweak penguins
  vi) Future visions of this field.We review several aspects of B-Physics. In particular we discuss:
  i) The theoretical framework for B-decays,
  ii) Weak decays beyond leading logarithms,
  iii) Standard analysis of the unitarity triangle,
  iv) Rare B-decays,
  v) CP violation including the issue of electroweak penguins
  vi) Future visions of this field.△ Less"
Errata to `Automorphisms of First-order Structures',Authors:Richard Kaye,"Abstract:This is a list of the typographic and other errors that occur in `Automorphisms of first-order structures', by R. W. Kaye and H. D. Macpherson (eds),OxfordUniversity Press, 1994.This is a list of the typographic and other errors that occur in `Automorphisms of first-order structures', by R. W. Kaye and H. D. Macpherson (eds),OxfordUniversity Press, 1994.△ Less"
Splitting the multiphase point,Authors:J. M. Yeomans,"Abstract:Models with competing interactions, for example the ANNNI model, can have special points at which the ground state is infinitely degenerate, so-called multiphase points. Small perturbations can lift this degeneracy and give rise to infinite sequences of long-period phases. This paper compares the effect of three possible perturbations, quantum fluctuations, thermal fluctuations and the softening…▽ MoreModels with competing interactions, for example the ANNNI model, can have special points at which the ground state is infinitely degenerate, so-called multiphase points. Small perturbations can lift this degeneracy and give rise to infinite sequences of long-period phases. This paper compares the effect of three possible perturbations, quantum fluctuations, thermal fluctuations and the softening of the spins from their quantised positions.△ Less"
REMARKS CONCERNING THE GEOMETRIES OF GRAVITY AND GAUGE FIELDS,Authors:Jeeva Anandan,"Abstract:An important limitation is shown in the analogy between the Aharonov-Bohm effect and the parallel transport on a cone. It illustrates a basic difference between gravity and gauge fields due to the existence of the solder form for the space-time geometry. This difference is further shown by the observability of the gravitational phase for open paths. This reinforces a previous suggestion that the…▽ MoreAn important limitation is shown in the analogy between the Aharonov-Bohm effect and the parallel transport on a cone. It illustrates a basic difference between gravity and gauge fields due to the existence of the solder form for the space-time geometry. This difference is further shown by the observability of the gravitational phase for open paths. This reinforces a previous suggestion that the fundamental variables for quantizing the gravitational field are the solder form and the connection, and not the metric.△ Less"
On Near-Infrared H-Alpha Searches for High-Redshift Galaxies,"Authors:A. J. Bunker,S. J. Warren,P. C. Hewett,D. L. Clements","Abstract:The lack of success of Lyman-alpha searches for high-redshift (z>2) field galaxies may be due to extinction by dust, suggesting that surveys based on lines of longer wavelength, particularly H-alpha, may be more effective. To test the dust hypothesis we have undertaken deep broad- (K') and narrow-band (5000 km/s, 2.177 micron) imaging of the field towards the quasar PHL957, in an attempt to detect…▽ MoreThe lack of success of Lyman-alpha searches for high-redshift (z>2) field galaxies may be due to extinction by dust, suggesting that surveys based on lines of longer wavelength, particularly H-alpha, may be more effective. To test the dust hypothesis we have undertaken deep broad- (K') and narrow-band (5000 km/s, 2.177 micron) imaging of the field towards the quasar PHL957, in an attempt to detect H-alpha emission from a known galaxy of redshift z=2.313. We cover an area of 4.9 square arcmin (0.28 h^{-2}Mpc^2) to a 4-sigma limiting narrow-band flux f=2.7x10^{-16}erg/s/cm^2, a factor of several deeper than previously published surveys. We detect the H-alpha+[N II] emission line in this galaxy at the 3.3-sigma level, inferring a star formation rate of 18 h^{-2} solar masses per year. This is a factor only a few times larger than the rate seen in some Sc galaxies today. The faint flux level reached in this work demonstrates the promise of narrow-band imaging in the near-infrared as a technique for finding normal galaxies at high redshifts.△ Less"
Reaction-Diffusion Processes of Hard-Core Particles,Authors:Gunter M. Schütz,"Abstract:We study a 12-parameter stochastic process involving particles with two-site interaction and hard-core repulsion on a $d$-dimensional lattice. In this model, which includes the asymmetric exclusion process, contact processes and other processes, the stochastic variables are particle occupation numbers taking values $n_{\vec{x}}=0,1$. We show that on a 10-parameter submanifold the $k$-point equal…▽ MoreWe study a 12-parameter stochastic process involving particles with two-site interaction and hard-core repulsion on a $d$-dimensional lattice. In this model, which includes the asymmetric exclusion process, contact processes and other processes, the stochastic variables are particle occupation numbers taking values $n_{\vec{x}}=0,1$. We show that on a 10-parameter submanifold the $k$-point equal-time correlation functions $\exval{n_{\vec{x}_1} \cdots n_{\vec{x}_k}}$ satisfy linear differential- difference equations involving no higher correlators. In particular, the average density $\exval{n_{\vec{x}}} $ satisfies an integrable diffusion-type equation. These properties are explained in terms of dual processes and various duality relations are derived. By defining the time evolution of the stochastic process in terms of a quantum Hamiltonian $H$, the model becomes equivalent to a lattice model in thermal equilibrium in $d+1$ dimensions. We show that the spectrum of $H$ is identical to the spectrum of the quantum Hamiltonian of a $d$-dimensional, anisotropic spin-1/2 Heisenberg model. In one dimension our results hint at some new algebraic structure behind the integrability of the system.△ Less"
A Freely Available Syntactic Lexicon for English,"Authors:Dania Egedi,Patrick Martin","Abstract:This paper presents a syntactic lexicon for English that was originally derived from theOxfordAdvanced Learner's Dictionary and theOxfordDictionary of Current Idiomatic English, and then modified and augmented by hand. There are more than 37,000 syntactic entries from all 8 parts of speech. An X-windows based t…▽ MoreThis paper presents a syntactic lexicon for English that was originally derived from theOxfordAdvanced Learner's Dictionary and theOxfordDictionary of Current Idiomatic English, and then modified and augmented by hand. There are more than 37,000 syntactic entries from all 8 parts of speech. An X-windows based tool is available for maintaining the lexicon and performing searches. C and Lisp hooks are also available so that the lexicon can be easily utilized by parsers and other programs.△ Less"
Coleman-Weinberg Phase Transition in Two-Scalar Models,"Authors:S. Bornholdt,N. Tetradis,C. Wetterich","Abstract:We explore the Coleman-Weinberg phase transition in regions outside the validity of perturbation theory. For this purpose we study a Euclidean field theory with two scalars and discrete symmetry in four dimensions. The phase diagram is established by a numerical solution of a suitable truncation of exact non-perturbative flow equations. We find regions in parameter space where the phase transiti…▽ MoreWe explore the Coleman-Weinberg phase transition in regions outside the validity of perturbation theory. For this purpose we study a Euclidean field theory with two scalars and discrete symmetry in four dimensions. The phase diagram is established by a numerical solution of a suitable truncation of exact non-perturbative flow equations. We find regions in parameter space where the phase transition (in dependence on the mass term) is of the second or the first order, separated by a triple point. Our quantitative results for the first order phase transition compare well to the standard perturbative Coleman-Weinberg calculation of the effective potential.△ Less"
Monte Carlo Methods for the Self-Avoiding Walk,Authors:Alan D. Sokal,"Abstract:This article is a pedagogical review of Monte Carlo methods for the self-avoiding walk, with emphasis on the extraordinarily efficient algorithms developed over the past decade.This article is a pedagogical review of Monte Carlo methods for the self-avoiding walk, with emphasis on the extraordinarily efficient algorithms developed over the past decade.△ Less"
The Principle of Least Action and Clustering in Cosmology,"Authors:Mikel Susperregi,James Binney","Abstract:A scheme is developed which enables one to trace backwards in time the cosmic density and velocity fields, and to determine accurately the current-epoch velocity field from the current-epoch density field, or vice versa. The scheme implements the idea of Giavalisco \etal\ (1993) that the principle of least action should be used to formulate gravitational instability as a two-point boundary-value…▽ MoreA scheme is developed which enables one to trace backwards in time the cosmic density and velocity fields, and to determine accurately the current-epoch velocity field from the current-epoch density field, or vice versa. The scheme implements the idea of Giavalisco \etal\ (1993) that the principle of least action should be used to formulate gravitational instability as a two-point boundary-value problem. We argue that the Eulerian formulation of the problem is to be preferred to the Lagrangian one, on grounds of computational simplicity, of ease of interfacing with observational data, and of internal consistency at early times. The scheme is successfully tested on an exact solution in one dimension, and on currently Gaussian fields in one and two dimensions. The application of the scheme to real observational data appears to be eminently feasible, though computationally costly.△ Less"
Non-equilibrium Dynamics of Finite Interfaces,"Authors:D. B. Abraham,T. J. Newman,G. M. Schütz","Abstract:We present an exact solution to an interface model representing the dynamics of a domain wall in a two-phase Ising system. The model is microscopically motivated, yet we find that in the scaling regime our results are consistent with those obtained previously from a phenomenological, coarse-grained Langevin approach.We present an exact solution to an interface model representing the dynamics of a domain wall in a two-phase Ising system. The model is microscopically motivated, yet we find that in the scaling regime our results are consistent with those obtained previously from a phenomenological, coarse-grained Langevin approach.△ Less"
Universal parametric correlations in the transmission eigenvalue spectra of disordered conductors,Authors:A. M. S. Macedo,"Abstract:We study the response of the transmission eigenvalue spectrum of disordered metallic conductors to an arbitrary external perturbation. For systems without time-reversal symmetry we find an exact non-perturbative solution for the two-point correlation function, which exhibits a new kind of universal behavior characteristic of disordered conductors. Systems with orthogonal and symplectic symmetrie…▽ MoreWe study the response of the transmission eigenvalue spectrum of disordered metallic conductors to an arbitrary external perturbation. For systems without time-reversal symmetry we find an exact non-perturbative solution for the two-point correlation function, which exhibits a new kind of universal behavior characteristic of disordered conductors. Systems with orthogonal and symplectic symmetries are studied in the hydrodynamic regime.△ Less"
Intensity correlations in electronic wave propagation in a disordered medium: the influence of spin-orbit scattering,Authors:A. M. S. Macedo,Abstract:We obtain explicit expressions for the correlation functions of transmission and reflection coefficients of coherent electronic waves propagating through a disordered quasi-one-dimensional medium with purely elastic diffusive scattering in the presence of spin-orbit interactions. We find in the metallic regime both large local intensity fluctuations and long-range correlations which ultimately l…▽ MoreWe obtain explicit expressions for the correlation functions of transmission and reflection coefficients of coherent electronic waves propagating through a disordered quasi-one-dimensional medium with purely elastic diffusive scattering in the presence of spin-orbit interactions. We find in the metallic regime both large local intensity fluctuations and long-range correlations which ultimately lead to universal conductance fluctuations. We show that the main effect of spin-orbit scattering is to suppress both local and long-range intensity fluctuations by a universal symmetry factor 4. We use a scattering approach based on random transfer matrices.△ Less
Parametric S-matrix fluctuations in quantum theory of chaotic scattering,Authors:A. M. S. Macedo,"Abstract:We study the effects of an arbitrary external perturbation in the statistical properties of the S-matrix of quantum chaotic scattering systems in the limit of isolated resonances. We derive, using supersymmetry, an exact non-perturbative expression for the parameter dependent autocorrelator of two S-matrix elements. Universality is obtained by appropriate rescaling of the physical parameters. We…▽ MoreWe study the effects of an arbitrary external perturbation in the statistical properties of the S-matrix of quantum chaotic scattering systems in the limit of isolated resonances. We derive, using supersymmetry, an exact non-perturbative expression for the parameter dependent autocorrelator of two S-matrix elements. Universality is obtained by appropriate rescaling of the physical parameters. We propose this universal function as a new signature of quantum chaos in open systems.△ Less"
Universal Parametric correlations at the soft edge of the spectrum of random matrix ensembles,Authors:A. M. S. Macedo,"Abstract:We extend a recent theory of parametric correlations in the spectrum of random matrices to study the response to an external perturbation of eigenvalues near the soft edge of the support. We demonstrate by explicit non-perturbative calculation that the two-point function for level density fluctuations becomes, after appropriate rescaling, a universal expression.We extend a recent theory of parametric correlations in the spectrum of random matrices to study the response to an external perturbation of eigenvalues near the soft edge of the support. We demonstrate by explicit non-perturbative calculation that the two-point function for level density fluctuations becomes, after appropriate rescaling, a universal expression.△ Less"
Effect of Wavefunction Renormalisation in N-Flavour Qed3 at Finite Temperature,"Authors:I. J. R. Aitchison,M. Klein-Kreisler","Abstract:A recent study of dynamical chiral symmetry breaking in N-flavour QED$_3$ at finite temperature is extended to include the effect of fermion wavefunction renormalisation in the Schwinger-Dyson equations. The simple ``zero-frequency'' truncation previously used is found to lead to unphysical results, especially as $T \to 0$. A modified set of equations is proposed, whose solutions behave in a way…▽ MoreA recent study of dynamical chiral symmetry breaking in N-flavour QED$_3$ at finite temperature is extended to include the effect of fermion wavefunction renormalisation in the Schwinger-Dyson equations. The simple ``zero-frequency'' truncation previously used is found to lead to unphysical results, especially as $T \to 0$. A modified set of equations is proposed, whose solutions behave in a way which is qualitatively similar to the $T=0$ solutions of Pennington et al. [5-8] who have made extensive studies of the effect of wavefunction renormalisation in this context, and who concluded that there was no critical $N_c$ (at T=0) above which chiral symmetry was restored. In contrast, we find that our modified equations predict a critical $N_c$ at $T \not= 0$, and an $N-T$ phase diagram very similar to the earlier study neglecting wavefunction renormalisation. The reason for the difference is traced to the different infrared behaviour of the vacuum polarisation at $T=0$ and at $T \not= 0$.△ Less"
Phenomenological implications from moduli fields in strings,Authors:A. de la Macorra,Abstract:We study some phenomenological consequences of having moduli fields with large vacuum expectation values (v.e.v.). The v.e.vs of the moduli are dynamically determined once supersymmetry is broken in 4D string theories. The study constraints the possible Yukawa interactions and modular weights for matter fields.We study some phenomenological consequences of having moduli fields with large vacuum expectation values (v.e.v.). The v.e.vs of the moduli are dynamically determined once supersymmetry is broken in 4D string theories. The study constraints the possible Yukawa interactions and modular weights for matter fields.△ Less
Unification Scale in String Theory,Authors:A. de la Macorra,"Abstract:We study the unification scale and gauge coupling constant in 4D string theory. We show that the fine structure constant is determined by the dimension of the hidden gauge group and only $SU(6)$ and $SO(9)$ are consistent with minimal string unification while the unification scale can be of order of $10^{16}\,GeV$.We study the unification scale and gauge coupling constant in 4D string theory. We show that the fine structure constant is determined by the dimension of the hidden gauge group and only $SU(6)$ and $SO(9)$ are consistent with minimal string unification while the unification scale can be of order of $10^{16}\,GeV$.△ Less"
Unification of couplings and soft supersymmetry breaking terms in 4D superstring models,"Authors:A. de la Macorra,G. G. Ross","Abstract:We consider the predictions for the hierarchy of mass scales, the fine structure constant, the radii of compactification and the soft SUSY breaking terms which follow if SUSY breaking is triggered by a gaugino condensate.We consider the predictions for the hierarchy of mass scales, the fine structure constant, the radii of compactification and the soft SUSY breaking terms which follow if SUSY breaking is triggered by a gaugino condensate.△ Less"
Space-Time Symmetries: P and CP Violation,"Authors:D. V. Ahluwalia,M. B. Jonnson,T. Goldman","Abstract:We begin with a few remarks on an explicit construction of a Bargmann-Wightman-Wigner-type quantum field theory [Phys. Lett. B {\bf 316}, 102 (1993)] in which bosons and associated antibosons have opposite relative intrinsic parities. We then construct $(1,0)\oplus(0,1)$ Majorana ($CP$ self conjugate) and Majorana-like ($CΓ^5$ self conjugate, $Γ^5=$ chirality operator) fields. We point out that…▽ MoreWe begin with a few remarks on an explicit construction of a Bargmann-Wightman-Wigner-type quantum field theory [Phys. Lett. B {\bf 316}, 102 (1993)] in which bosons and associated antibosons have opposite relative intrinsic parities. We then construct $(1,0)\oplus(0,1)$ Majorana ($CP$ self conjugate) and Majorana-like ($CΓ^5$ self conjugate, $Γ^5=$ chirality operator) fields. We point out that this new structure in the space time symmetries may be relevant to $P$ and $CP$ violation.△ Less"
A Magnetic Monopole in Pure SU(2) Gauge Theory,"Authors:J. Smit,A. J. Van Der Sijs","Abstract:The magnetic monopole in euclidean pure SU(2) gauge theory is investigated using a background field method on the lattice.
  With Monte Carlo methods we study the mass of the monopole in the full quantum theory.
  The monopole background under the quantum fluctuations is induced by imposing fixed monopole boundary conditions on the walls of a finite lattice volume.
  By varying the gauge couplin…▽ MoreThe magnetic monopole in euclidean pure SU(2) gauge theory is investigated using a background field method on the lattice.
  With Monte Carlo methods we study the mass of the monopole in the full quantum theory.
  The monopole background under the quantum fluctuations is induced by imposing fixed monopole boundary conditions on the walls of a finite lattice volume.
  By varying the gauge coupling it is possible to study monopoles with scales from the hadronic scale up to high energies.
  The results for the monopole mass are consistent with a conjecture we made previously in a realization of the dual superconductor hypothesis of confinement.△ Less"
Symmetries and String Field Theory in D=2,Authors:Michio Kaku,"Abstract:(This talk was presented at the Third International Wigner Symposium on Group Theory,Oxford, September, 1993.) Matrix models provides us with the most powerful framework in which to analyze D=2 string theory, yet some of its miraculous features, such as discrete states and $w(\infty)$, remain rather obscure, because the string degrees of freedom have been r…▽ More(This talk was presented at the Third International Wigner Symposium on Group Theory,Oxford, September, 1993.) Matrix models provides us with the most powerful framework in which to analyze D=2 string theory, yet some of its miraculous features, such as discrete states and $w(\infty)$, remain rather obscure, because the string degrees of freedom have been removed. Liouville theory, on the other hand, has all its string degrees of freedom intact, yet is notoriously difficult to solve. In this paper, we present the second quantized formulation of Liouville theory in D=2, where discrete states and $w(\infty)$ have a natural, field theoretic interpretation. We generalize the non-polynomial closed string field theory, first developed by the author and the Kyoto and MIT groups, to the D=2 case. We find that, in second quantized field theory language, the rather mysterious features of matrix models have an intuitively transparent interpretation, similar to standard gauge theory. Latex file.△ Less"
Wigner Functions in Curved Space-Time and Quantum Corrections to Thermal Equilibrium,Authors:Oleg A. Fonarev,"Abstract:A brief review of the Wigner functions method in curved space-time. Contribution to the 3rd International Wigner Symposium, 5th-11th September 1993,Oxford, UK.A brief review of the Wigner functions method in curved space-time. Contribution to the 3rd International Wigner Symposium, 5th-11th September 1993,Oxford, UK.△ Less"
Representation Theory of Analytic Holonomy C* Algebras,"Authors:Abhay Ashtekar,Jerzy Lewandowski","Abstract:Integral calculus on the space  of gauge equivalent connections is developed. Loops, knots, links and graphs feature prominently in this description. The framework is well--suited for quantization of diffeomorphism invariant theories of connections.
  The general setting is provided by the abelian C* algebra of functions on the quotient space of connections  generated by Wilson loops (i.e., by t…▽ MoreIntegral calculus on the space  of gauge equivalent connections is developed. Loops, knots, links and graphs feature prominently in this description. The framework is well--suited for quantization of diffeomorphism invariant theories of connections.
  The general setting is provided by the abelian C* algebra of functions on the quotient space of connections  generated by Wilson loops (i.e., by the traces of holonomies of connections around closed loops). The representation theory of this algebra leads to an interesting and powerful ``duality'' between gauge--equivalence classes of connections and certain equivalence classes of closed loops. In particular, regular measures on (a suitable completion of) connections/gauges  are in 1--1 correspondence with certain functions of loops and diffeomorphism invariant measures correspond to (generalized) knot and link invariants. By carrying out a non--linear extension of the theory of cylindrical measures on topological vector spaces, a faithful, diffeomorphism invariant measure is introduced. This measure can be used to define the Hilbert space of quantum states in theories of connections. The Wilson--loop functionals then serve as the configuration operators in the quantum theory.△ Less"
SCHRÖdinger Invariance and Strongly Anisotropic Critical Systems,Authors:Malte Henkel,"Abstract:The extension of strongly anisotropic or dynamical scaling to local scale invariance is investigated. For the special case of an anisotropy or dynamical exponent $θ=z=2$, the group of local scale transformation considered is the Schrödinger group, which can be obtained as the non-relativistic limit of the conformal group. The requirement of Schrödinger invariance determines the two-point functio…▽ MoreThe extension of strongly anisotropic or dynamical scaling to local scale invariance is investigated. For the special case of an anisotropy or dynamical exponent $θ=z=2$, the group of local scale transformation considered is the Schrödinger group, which can be obtained as the non-relativistic limit of the conformal group. The requirement of Schrödinger invariance determines the two-point function in the bulk and reduces the three-point function to a scaling form of a single variable. Scaling forms are also derived for the two-point function close to a free surface which can be either space-like or time-like. These results are reproduced in several exactly solvable statistical systems, namely the kinetic Ising model with Glauber dynamics, lattice diffusion, Lifshitz points in the spherical model and critical dynamics of the spherical model with a non-conserved order parameter. For generic values of $θ$, evidence from higher order Lifshitz points in the spherical model and from directed percolation suggests a simple scaling form of the two-point function.△ Less"
Vassiliev Invariants and the Loop States in Quantum Gravity,Authors:Louis H. Kauffman,"Abstract:An exposition of Vassiliev invariants is given in terms of the simplest approach to the functional integral construction of link invariants from Chern-Simons theory. This approach gives the top row evaluations of Vassiliev invariants for the classical Lie algebras, and a neat point of view on the results of Bar-Natan. It also clarifies the relation between Vassiliev invariants and the extension…▽ MoreAn exposition of Vassiliev invariants is given in terms of the simplest approach to the functional integral construction of link invariants from Chern-Simons theory. This approach gives the top row evaluations of Vassiliev invariants for the classical Lie algebras, and a neat point of view on the results of Bar-Natan. It also clarifies the relation between Vassiliev invariants and the extension of the bracket invariant to links with transverse double points that appears in the work of Bruegmann, Gambini and Pullin on the loop representation of quantum gravity. We see that the Vassiliev vertex is not just a transversal intersection of Wilson loops, but rather has the structure of Casimir insertion (up to first order of approximation) coming from the difference formula in the functional integral. (Figures are available from the author.)△ Less"
The Gauss Linking Number in Quantum Gravity,"Authors:R. Gambini,J. Pullin","Abstract:We show that the exponential of the Gauss (self) linking number of a knot is a solution of the Wheeler-DeWitt equation in loop space with a cosmological constant. Using this fact, it is straightforward to prove that the second coefficient of the Jones Polynomial is a solution of the Wheeler-DeWitt equation in loop space with no cosmological constant. We perform calculations from scratch, startin…▽ MoreWe show that the exponential of the Gauss (self) linking number of a knot is a solution of the Wheeler-DeWitt equation in loop space with a cosmological constant. Using this fact, it is straightforward to prove that the second coefficient of the Jones Polynomial is a solution of the Wheeler-DeWitt equation in loop space with no cosmological constant. We perform calculations from scratch, starting from the connection representation and give details of the proof. Implications for the possibility of generation of other solutions are also discussed.△ Less"
High-order galaxy correlation functions in the APM Galaxy Survey,Authors:Enrique Gaztanaga,"Abstract:We estimate J-point galaxy averaged correlation functions $\wbar_J(θ)$ for $J=2,...,9$, in a sample of the APM Galaxy Survey with more than $1.3 \times 10^6$ galaxies and a depth $ \calD \sim 400 \Mpc$. The hierarchical amplitudes $s_J=\wbar_J/\wbar_2^{J-1}$ are roughly constant, up to $J=9$, between $0.5 \Mpc$ and $2 \Mpc$ and decrease slowly for larger scales. At scales larger than $7 \Mpc$ we…▽ MoreWe estimate J-point galaxy averaged correlation functions $\wbar_J(θ)$ for $J=2,...,9$, in a sample of the APM Galaxy Survey with more than $1.3 \times 10^6$ galaxies and a depth $ \calD \sim 400 \Mpc$. The hierarchical amplitudes $s_J=\wbar_J/\wbar_2^{J-1}$ are roughly constant, up to $J=9$, between $0.5 \Mpc$ and $2 \Mpc$ and decrease slowly for larger scales. At scales larger than $7 \Mpc$ we find strong similarities between the statistical properties of the galaxy fluctuations and the theoretical properties of matter fluctuations evolving under the influence of gravity in an expanding universe on assumption that the initial fluctuations are small and Gaussian. This is most easily explained if at large scales there is no significant biasing between matter and galaxy fluctuations. The comparison of the skewness in the CfA and SSRS catalogues with comparable sub-samples of the APM indicates that the volume of a ``fair sample'' has to be much larger that the one in the combined CfA/SSRS catalogues.△ Less"
Topological Field Theory As The Key To Quantum Gravity,Authors:Louis Crane,"Abstract:Motivated by the similarity between CSW theory and the Chern Simons state for General Relativity in the Ashtekar variables, we explore what the universe would look like if it were in a state corresponding to a 3D TQFT. We end up with a construction of propagating ststes for parts of the universe and a Hilbert space corresponding to a certain approximation. The construction avoids path integrals,…▽ MoreMotivated by the similarity between CSW theory and the Chern Simons state for General Relativity in the Ashtekar variables, we explore what the universe would look like if it were in a state corresponding to a 3D TQFT. We end up with a construction of propagating ststes for parts of the universe and a Hilbert space corresponding to a certain approximation. The construction avoids path integrals, using instead recombination diagrams in a certain tensor category.△ Less"
"The k_t--functional for the interpolation couple L^\infty(dμ;L^1(dν)), L^\infty(dν;L^1(dμ))","Authors:Albrecht Hess,Gilles Pisier","Abstract:Let $(M,μ)$ and $(N,ν)$ be measure spaces. In this paper, we study the $K_t$--\,functional for the couple $$A_0=L^\infty(dμ\,; L^1(dν))\,,~~A_1=L^\infty(dν\,; L^1(dμ))\,. $$
  Here, and in what follows the vector valued $L^p$--\,spaces $L^p(dμ\,; L^q(dν))$ are meant in Bochner's sense.
  One of our main results is the following, which can be viewed as a refinement of a lemma due to Varopoulos [V…▽ MoreLet $(M,μ)$ and $(N,ν)$ be measure spaces. In this paper, we study the $K_t$--\,functional for the couple $$A_0=L^\infty(dμ\,; L^1(dν))\,,~~A_1=L^\infty(dν\,; L^1(dμ))\,. $$
  Here, and in what follows the vector valued $L^p$--\,spaces $L^p(dμ\,; L^q(dν))$ are meant in Bochner's sense.
  One of our main results is the following, which can be viewed as a refinement of a lemma due to Varopoulos [V].
  \proclaim Theorem 0.1. Let $(A_0,A_1)$ be as above. Then for all $f$ in $A_0+A_1$ we have $${1\over 2}\,K_t(f;\,A_0\,,A_1)\leq \sup\,\bigg\{ \Big(μ(E)\vee t^{-1}ν(F)\Big)^{-1} \int_{E\times F} \vert f\vert\,dμ\,dν\,\bigg\} \leq K_t(f;\,A_0\,,A_1)\,,$$ where the supremum runs over all measurable subsets $E\subset M\,,~ F\subset N$ with positive and finite measure and $u\!\vee\!v$ denotes the maximum of the reals $u$ and $v$.△ Less"
Heisenberg models and a particular isotropic model,Authors:A. J. van der Sijs,"Abstract:The Heisenberg model, a quantum mechanical analogue of the Ising model, has a large ground state degeneracy, due to the symmetry generated by the total spin. This symmetry is also responsible for degeneracies in the rest of the spectrum. We discuss the global structure of the spectrum of Heisenberg models with arbitrary couplings, using group theoretical methods. The Hilbert space breaks up in b…▽ MoreThe Heisenberg model, a quantum mechanical analogue of the Ising model, has a large ground state degeneracy, due to the symmetry generated by the total spin. This symmetry is also responsible for degeneracies in the rest of the spectrum. We discuss the global structure of the spectrum of Heisenberg models with arbitrary couplings, using group theoretical methods. The Hilbert space breaks up in blocks characterized by the quantum numbers of the total spin, $S$ and $M$, and each block is shown to constitute the representation space of an explicitly given irreducible representation of the symmetric group $S_N$, consisting of permutations of the $N$ spins in the system.
  In the second part of the paper we consider, as a concrete application, the model where each spin is coupled to all the other spins with equal strength. Its partition function is written as a single integral, elucidating its $N$-dependence. This provides a useful framework for studying finite size effects. We give explicit results for the heat capacity, revealing interesting behavior just around the phase transition.△ Less"
Designing a Theorem Prover,Authors:Lawrence C. Paulson,"Abstract:A step-by-step presentation of the code for a small theorem prover introduces theorem-proving techniques.  The programming language used is Standard ML. The prover operates on a sequent calculus formulation of first-order logic, which is briefly explained.  The implementation of unification and logical inference is shown.  The prover is demonstrated on several small examples, including one that…▽ MoreA step-by-step presentation of the code for a small theorem prover introduces theorem-proving techniques.  The programming language used is Standard ML. The prover operates on a sequent calculus formulation of first-order logic, which is briefly explained.  The implementation of unification and logical inference is shown.  The prover is demonstrated on several small examples, including one that shows its limitations.  The final part of the paper is a survey of contemporary research on interactive theorem proving.△ Less"
"Ansatz for Quark, Charged Lepton, and Neutrino Masses in SUSY GUTS","Authors:H. Dreiner,G. K. Leontaris,N. D. Tracas","Abstract:We extend a fermion mass matrix Ansatz by Giuduce to include neutrino masses. The previous predictions are maintained. With two additional parameters, a large Majorana neutrino mass and a hierarchy factor, we have seven {\it further} low energy predictions: the masses of the neutrinos, the mixing angles and the phase in the leptonic sector. We choose a reasonable hierarchy of Majorana masses and…▽ MoreWe extend a fermion mass matrix Ansatz by Giuduce to include neutrino masses. The previous predictions are maintained. With two additional parameters, a large Majorana neutrino mass and a hierarchy factor, we have seven {\it further} low energy predictions: the masses of the neutrinos, the mixing angles and the phase in the leptonic sector. We choose a reasonable hierarchy of Majorana masses and fit the overall mass scale according to a solution of the solar neutrino problem via the MSW mechanism, which is in agreement with the $^{37}Cl$, Kamiokande, and GALLEX data. We then also obtain a cosmologically interesting tau-neutrino mass.△ Less"
R-Parity Violation at HERA,"Authors:Jon Butterworth,Herbert Dreiner","Abstract:We summarize the signals at HERA in supersymmetric models with explicitly broken R-parity. As the most promising case, we consider in detail the resonant production of single squarks through an operator $L_1Q_i{ \bar D}_j$, a production process analogous to that for leptoquarks. However, the dominant decay of the squark to a quark and a photino leads to a very different experimental signature. W…▽ MoreWe summarize the signals at HERA in supersymmetric models with explicitly broken R-parity. As the most promising case, we consider in detail the resonant production of single squarks through an operator $L_1Q_i{ \bar D}_j$, a production process analogous to that for leptoquarks. However, the dominant decay of the squark to a quark and a photino leads to a very different experimental signature. We examine in particular the case where the photino decays to a positron and two quarks. Using a detailed Monte-Carlo procedure we obtain a discovery limit in the squark mass---Yukawa coupling plane. HERA can discover a squark for a mass as large as $270 \gev$ and for an R-parity violating Yukawa coupling as small as $5.8 \times 10^{-3}$.△ Less"
Bell's Inequality and $τ$-Physics at LEP,Authors:Herbert Dreiner,"Abstract:In this talk given at the TAU92 Workshop, Columbus, OH, Sept. 92, we summarize results presented in more detail in a recent paper by S. Abel, M. Dittmar and the author where we gave a general proof that Bell's inequality can not be tested at a collider experiment. In particular, a measurement of correlated tau-spins at LEP does not constitute a test of local realistic theories via Bell's inequal…▽ MoreIn this talk given at the TAU92 Workshop, Columbus, OH, Sept. 92, we summarize results presented in more detail in a recent paper by S. Abel, M. Dittmar and the author where we gave a general proof that Bell's inequality can not be tested at a collider experiment. In particular, a measurement of correlated tau-spins at LEP does not constitute a test of local realistic theories via Bell's inequality. The central point of the argument is that such tests, where the spins of two particles are inferred from a scattering distribution, can always be described by a local hidden variable theory. In response to questions at the workshop we go beyond the paper and show that an old experiment involving the measurement of the correlated spins of the two photons emitted in positronium decay via Compton-scattering is also not a viable test of Bell's inequality.△ Less"
Gauge invariant extremization,Authors:A. J. van der Sijs,"Abstract:Recently, Duncan and Mawhinney introduced a method to find saddle points of the action in simulations of non-abelian lattice gauge theory. The idea, called `extremization', is to minimize $\int(δS/δA_μ)^2$ instead of the action $S$ itself as in conventional `cooling'. The method was implemented in an explicitly gauge variant way, however, and gauge dependence showed up in the results.  Here we p…▽ MoreRecently, Duncan and Mawhinney introduced a method to find saddle points of the action in simulations of non-abelian lattice gauge theory. The idea, called `extremization', is to minimize $\int(δS/δA_μ)^2$ instead of the action $S$ itself as in conventional `cooling'. The method was implemented in an explicitly gauge variant way, however, and gauge dependence showed up in the results.  Here we present a gauge invariant formulaton of extremization on the lattice, applicable to any gauge group and any lattice action.  The procedure is worked out in detail for U(1) and SU(N) lattice gauge theory with the plaquette action.△ Less"
Gauge invariant extremization on the lattice,Authors:A. J. van der Sijs,"Abstract:Recently, a method was proposed and tested to find saddle points of the action in simulations of non-abelian lattice gauge theory. The idea, called `extremization', is to minimize $\int(\dl S/\dl A_μ)^2$. The method was implemented in an explicitly gauge variant way, however, and gauge dependence showed up in the results.
  Here we show how extremization can be formulated in a way that preserves…▽ MoreRecently, a method was proposed and tested to find saddle points of the action in simulations of non-abelian lattice gauge theory. The idea, called `extremization', is to minimize $\int(\dl S/\dl A_μ)^2$. The method was implemented in an explicitly gauge variant way, however, and gauge dependence showed up in the results.
  Here we show how extremization can be formulated in a way that preserves gauge invariance on the lattice. The method applies to any gauge group and any lattice action. The procedure is worked out in detail for the standard plaquette action with gauge groups U(1) and SU(N).△ Less"
The Riemannian manifold of all Riemannian metrics,"Authors:Olga Gil-Medrano,Peter W. Michor","Abstract:The space of all Riemannian metrics on a smooth second countable finite dimensional manifold is itself a smooth manifold modeled on the space of symmetric (0,2)-tensor fields with compact support. It carries a canonical Riemannian metric which is invariant under the action of the diffeomorphism group. We determine its geodesics, exponential mapping, curvature, and Jacobi fields in a very explici…▽ MoreThe space of all Riemannian metrics on a smooth second countable finite dimensional manifold is itself a smooth manifold modeled on the space of symmetric (0,2)-tensor fields with compact support. It carries a canonical Riemannian metric which is invariant under the action of the diffeomorphism group. We determine its geodesics, exponential mapping, curvature, and Jacobi fields in a very explicit manner.△ Less"
Pseudoriemannian metrics on spaces of bilinear structures,"Authors:Olga Gil-Medrano,Peter W. Michor,Martin Neuwirther","Abstract:The space of all non degenerate bilinear structures on a manifold $M$ carries a one parameter family of pseudo Riemannian metrics. We determine the geodesic equation, covariant derivative, curvature, and we solve the geodesic equation explicitly. Each space of pseudo Riemannian metrics with fixed signature is a geodesically closed submanifold. The space of non degenerate 2-forms is also a geodes…▽ MoreThe space of all non degenerate bilinear structures on a manifold $M$ carries a one parameter family of pseudo Riemannian metrics. We determine the geodesic equation, covariant derivative, curvature, and we solve the geodesic equation explicitly. Each space of pseudo Riemannian metrics with fixed signature is a geodesically closed submanifold. The space of non degenerate 2-forms is also a geodesically closed submanifold. Then we show that, if we fix a distribution on $M$, the space of all Riemannia metrics splits as the product of three spaces which are everywhere mutually orthogonal, for the usual metric. We investigate this situation in detail.△ Less"
